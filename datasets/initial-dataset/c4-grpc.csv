user_id,user_login,pull_request_url,comment_id,created_at,path,diff_hunk,content,code_smell
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20353,329824346,2019-09-30T23:11:02Z,src/core/ext/transport/chttp2/transport/hpack_encoder.cc,"@@ -54,27 +60,182 @@ /* if the probability of this item being seen again is < 1/x then don't add    it to the table */ #define ONE_ON_ADD_PROBABILITY (GRPC_CHTTP2_HPACKC_NUM_VALUES >> 1)-/* don't consider adding anything bigger than this to the hpack table */-#define MAX_DECODER_SPACE_USAGE 512--#define DATA_FRAME_HEADER_SIZE 9+/* Meaningful to encoder and parser on remote end. */+typedef uint32_t HpackEncoderIndex;+/* Internal-table bookkeeping (*not* the hpack index). */+typedef uint32_t HpackEncoderSlotHash;++struct SliceRefComparator {+  typedef grpc_slice_refcount* Type;+  static grpc_slice_refcount* Null() { return nullptr; }+  static bool Equals(grpc_slice_refcount* s1, grpc_slice_refcount* s2) {+    return s1 == s2;+  }+  static void Ref(grpc_slice_refcount* sref) { sref->Ref(); }+  static void Unref(grpc_slice_refcount* sref) { sref->Unref(); }+}; -static grpc_slice_refcount terminal_slice_refcount(-    grpc_slice_refcount::Type::STATIC);-static const grpc_slice terminal_slice = {-    &terminal_slice_refcount, /* refcount */-    {{0, nullptr}}            /* data.refcounted */+struct MetadataComparator {+  typedef grpc_mdelem Type;+  static const grpc_mdelem Null() { return {0}; }+  static bool Equals(const grpc_mdelem md1, const grpc_mdelem md2) {+    return md1.payload == md2.payload;+  }+  static void Ref(grpc_mdelem md) { GRPC_MDELEM_REF(md); }+  static void Unref(grpc_mdelem md) { GRPC_MDELEM_UNREF(md); } }; +/* Index table management */+template <typename H>+static HpackEncoderIndex HpackIndex(const H* hashtable,+                                    HpackEncoderSlotHash hash_index) {+  return hashtable[hash_index].index;+}++template <typename R, typename H>+static const R& GetEntry(const H* hashtable, HpackEncoderSlotHash hash_index) {","One of the major benefits of your refactoring is that we can now unit test the table management part of the hpack implementation (yay!). While this is not a blocker for this PR, I'd suggest considering adding unit tests or filing a bug to do so. We've recently had a bug with a hash table that might've been caught had we had proper refactoring / unit tests. It would be great to proactively add some unit tests to avoid similar bugs in the future.",X
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20353,329826341,2019-09-30T23:20:38Z,src/core/ext/transport/chttp2/transport/hpack_encoder.cc,"@@ -54,27 +60,182 @@ /* if the probability of this item being seen again is < 1/x then don't add    it to the table */ #define ONE_ON_ADD_PROBABILITY (GRPC_CHTTP2_HPACKC_NUM_VALUES >> 1)-/* don't consider adding anything bigger than this to the hpack table */-#define MAX_DECODER_SPACE_USAGE 512--#define DATA_FRAME_HEADER_SIZE 9+/* Meaningful to encoder and parser on remote end. */+typedef uint32_t HpackEncoderIndex;+/* Internal-table bookkeeping (*not* the hpack index). */+typedef uint32_t HpackEncoderSlotHash;++struct SliceRefComparator {+  typedef grpc_slice_refcount* Type;+  static grpc_slice_refcount* Null() { return nullptr; }+  static bool Equals(grpc_slice_refcount* s1, grpc_slice_refcount* s2) {+    return s1 == s2;+  }+  static void Ref(grpc_slice_refcount* sref) { sref->Ref(); }+  static void Unref(grpc_slice_refcount* sref) { sref->Unref(); }+}; -static grpc_slice_refcount terminal_slice_refcount(-    grpc_slice_refcount::Type::STATIC);-static const grpc_slice terminal_slice = {-    &terminal_slice_refcount, /* refcount */-    {{0, nullptr}}            /* data.refcounted */+struct MetadataComparator {+  typedef grpc_mdelem Type;+  static const grpc_mdelem Null() { return {0}; }+  static bool Equals(const grpc_mdelem md1, const grpc_mdelem md2) {+    return md1.payload == md2.payload;+  }+  static void Ref(grpc_mdelem md) { GRPC_MDELEM_REF(md); }+  static void Unref(grpc_mdelem md) { GRPC_MDELEM_UNREF(md); } }; +/* Index table management */+template <typename H>+static HpackEncoderIndex HpackIndex(const H* hashtable,+                                    HpackEncoderSlotHash hash_index) {+  return hashtable[hash_index].index;+}++template <typename R, typename H>+static const R& GetEntry(const H* hashtable, HpackEncoderSlotHash hash_index) {+  return hashtable[hash_index].value;+}++template <typename Cmp, typename H>+static bool TableEmptyAt(const H* hashtable, HpackEncoderSlotHash hash_index) {+  return Cmp::Equals(hashtable[hash_index].value, Cmp::Null());+}++template <typename Cmp, typename H, typename T>",What was the thought behind having the table management be a set of templated functions that take in a type H for the hashtable? Couldn't we just have an HpackEncoderTable class instead of having the hashtable type be a template parameter?,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20265,329892077,2019-10-01T06:14:12Z,test/core/end2end/generate_tests.bzl,"@@ -401,12 +410,25 @@ def grpc_end2end_tests():                 ""//:grpc"",                 ""//:gpr"",             ],-            tags = [""no_windows""],+            tags = _platform_support_tags(fopt),         )+         for t, topt in END2END_TESTS.items():             #print(_compatible(fopt, topt), f, t, fopt, topt)             if not _compatible(fopt, topt):                 continue++            native.sh_test(","It just runs the test with the default poller.If poller is specified, it's set into the `GRPC_POLL_STRATEGY` env variable, otherwise default poller will be used (default grpc behavior).https://github.com/grpc/grpc/blob/66c372b8ec1ec3c53e11ccdc47c394a4bb810cc2/test/core/end2end/end2end_test.sh#L20",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/19059,330059114,2019-10-01T13:33:20Z,src/csharp/Grpc.Core.Api/IAsyncStreamReader.cs,"@@ -50,7 +47,20 @@ namespace Grpc.Core     /// </para>     /// </summary>     /// <typeparam name=""T"">The message type.</typeparam>-    public interface IAsyncStreamReader<T> : IAsyncEnumerator<T>+    public interface IAsyncStreamReader<T>     {+        /// <summary>+        /// Gets the current element in the iteration.+        /// </summary>+        T Current { get; }++        /// <summary>+        /// Advances the reader to the next element in the sequence, returning the result asynchronously.+        /// </summary>+        /// <param name=""cancellationToken"">Cancellation token that can be used to cancel the operation.</param>+        /// <returns>+        /// Task containing the result of the operation: true if the reader was successfully advanced+        /// to the next element; false if the reader has passed the end of the sequence.</returns>+        Task<bool> MoveNext(CancellationToken cancellationToken);","No, there's an extension method that provides the parameterless MoveNext():https://github.com/grpc/grpc/pull/19059/files#diff-fa87174ba85fa18aefdd2f6f0c173750R39you need to make sure that extension is visible from your code and you should be fine.   (`using Grpc.Core;` should be enough?)",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20265,330164538,2019-10-01T16:53:48Z,test/core/end2end/generate_tests.bzl,"@@ -401,12 +410,25 @@ def grpc_end2end_tests():                 ""//:grpc"",                 ""//:gpr"",             ],-            tags = [""no_windows""],+            tags = _platform_support_tags(fopt),         )+         for t, topt in END2END_TESTS.items():             #print(_compatible(fopt, topt), f, t, fopt, topt)             if not _compatible(fopt, topt):                 continue++            native.sh_test(","If that's the case, shouldn't that mean there's a test duplicated between this `sh_test` and the following loop which generates a test for each poller?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20265,330173209,2019-10-01T17:13:45Z,test/core/end2end/generate_tests.bzl,"@@ -401,12 +410,25 @@ def grpc_end2end_tests():                 ""//:grpc"",                 ""//:gpr"",             ],-            tags = [""no_windows""],+            tags = _platform_support_tags(fopt),         )+         for t, topt in END2END_TESTS.items():             #print(_compatible(fopt, topt), f, t, fopt, topt)             if not _compatible(fopt, topt):                 continue++            native.sh_test(","yes, on linux it's basically ""duplicated"" in the sense there's one test with default poller and then a set of poller-specific tests.This is mostly harmless because even the ""default"" poller test is a legitimate on linux, so if it gets run ""by accident"" it's fine (we're just wasting some time). The ""default poller"" test actually gets marked with ""no_linux"" tag and we're filtering it out in the bazel RBE .rc files (see diff) to make sure we don't waste time running the same test twice on our CI.Ideally there would be a good way how to mark platform support for tests without needing to use --filter_build_tags and --filter_test_tags, but that's an open issue for now bazelbuild/bazel#3780 so we're doing what we can.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20426,330219421,2019-10-01T18:54:30Z,src/python/grpcio_tests/tests/testing/_server_application.py,"@@ -25,9 +27,25 @@ class FirstServiceServicer(services_pb2_grpc.FirstServiceServicer):     """"""Services RPCs."""""" +    def __init__(self):+        self._abort_lock = threading.RLock()+        self._abort_response = _application_common.ABORT_NO_STATUS_RESPONSE+     def UnUn(self, request, context):         if _application_common.UNARY_UNARY_REQUEST == request:             return _application_common.UNARY_UNARY_RESPONSE+        elif request == _application_common.ABORT_REQUEST:","optional: can we order the components of comparison to be ""expected"" == ""actual"" for all three of them, or the other way around?",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20426,330219756,2019-10-01T18:55:13Z,src/python/grpcio_tests/tests/testing/_server_test.py,"@@ -164,6 +164,19 @@ def test_infinite_request_stream_fake_time(self):          self.assertIs(code, grpc.StatusCode.DEADLINE_EXCEEDED) +    def test_servicer_context_abort(self):+        rpc = self._real_time_server.invoke_unary_unary(+            _application_testing_common.FIRST_SERVICE_UNUN, (),+            _application_common.ABORT_REQUEST, None)+        response, trailing_metadata, code, details = rpc.termination()","`response`, `trailing_metadata`, `details` are unused",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20426,330219892,2019-10-01T18:55:29Z,src/python/grpcio_tests/tests/testing/_server_test.py,"@@ -164,6 +164,19 @@ def test_infinite_request_stream_fake_time(self):          self.assertIs(code, grpc.StatusCode.DEADLINE_EXCEEDED) +    def test_servicer_context_abort(self):+        rpc = self._real_time_server.invoke_unary_unary(+            _application_testing_common.FIRST_SERVICE_UNUN, (),+            _application_common.ABORT_REQUEST, None)+        response, trailing_metadata, code, details = rpc.termination()+        self.assertIs(code, grpc.StatusCode.PERMISSION_DENIED)+        rpc = self._real_time_server.invoke_unary_unary(+            _application_testing_common.FIRST_SERVICE_UNUN, (),+            _application_common.ABORT_SUCCESS_QUERY, None)+        response, trailing_metadata, code, details = rpc.termination()","`trailing_metadata`, `details` are unused",X
702191,arjunroy,https://api.github.com/repos/grpc/grpc/pulls/20353,330224383,2019-10-01T19:05:28Z,src/core/ext/transport/chttp2/transport/hpack_encoder.cc,"@@ -54,27 +60,182 @@ /* if the probability of this item being seen again is < 1/x then don't add    it to the table */ #define ONE_ON_ADD_PROBABILITY (GRPC_CHTTP2_HPACKC_NUM_VALUES >> 1)-/* don't consider adding anything bigger than this to the hpack table */-#define MAX_DECODER_SPACE_USAGE 512--#define DATA_FRAME_HEADER_SIZE 9+/* Meaningful to encoder and parser on remote end. */+typedef uint32_t HpackEncoderIndex;+/* Internal-table bookkeeping (*not* the hpack index). */+typedef uint32_t HpackEncoderSlotHash;++struct SliceRefComparator {+  typedef grpc_slice_refcount* Type;+  static grpc_slice_refcount* Null() { return nullptr; }+  static bool Equals(grpc_slice_refcount* s1, grpc_slice_refcount* s2) {+    return s1 == s2;+  }+  static void Ref(grpc_slice_refcount* sref) { sref->Ref(); }+  static void Unref(grpc_slice_refcount* sref) { sref->Unref(); }+}; -static grpc_slice_refcount terminal_slice_refcount(-    grpc_slice_refcount::Type::STATIC);-static const grpc_slice terminal_slice = {-    &terminal_slice_refcount, /* refcount */-    {{0, nullptr}}            /* data.refcounted */+struct MetadataComparator {+  typedef grpc_mdelem Type;+  static const grpc_mdelem Null() { return {0}; }+  static bool Equals(const grpc_mdelem md1, const grpc_mdelem md2) {+    return md1.payload == md2.payload;+  }+  static void Ref(grpc_mdelem md) { GRPC_MDELEM_REF(md); }+  static void Unref(grpc_mdelem md) { GRPC_MDELEM_UNREF(md); } }; +/* Index table management */+template <typename H>+static HpackEncoderIndex HpackIndex(const H* hashtable,+                                    HpackEncoderSlotHash hash_index) {+  return hashtable[hash_index].index;+}++template <typename R, typename H>+static const R& GetEntry(const H* hashtable, HpackEncoderSlotHash hash_index) {+  return hashtable[hash_index].value;+}++template <typename Cmp, typename H>+static bool TableEmptyAt(const H* hashtable, HpackEncoderSlotHash hash_index) {+  return Cmp::Equals(hashtable[hash_index].value, Cmp::Null());+}++template <typename Cmp, typename H, typename T>","Hypothetically you could make a template class, that implements a hash table, that takes as template parameter the value type - which is what I did first. However, it does complicate the of hpack_encoder.h (specifically the definition of grpc_chttp2_hpack_compressor).If we end up converting that class to C++, then it may make sense to have this be an HpackEncoderTable, but I didn't want to have a mix of C-style and C++ style definitions in the meantime. ",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20434,330517255,2019-10-02T12:21:47Z,tools/remote_build/windows.bazelrc,"@@ -47,7 +47,7 @@ build --bes_timeout=60s build --bes_results_url=""https://source.cloud.google.com/results/invocations/"" build --project_id=grpc-testing -build --jobs=30+build --jobs=100","I increased the number of RBE windows workers in our pool (50->200), so increasing the number of parallel jobs here as well.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/19987,330746093,2019-10-02T20:07:05Z,src/core/lib/slice/slice_utils.h,"@@ -101,15 +107,16 @@ struct ManagedMemorySlice : public grpc_slice {   explicit ManagedMemorySlice(const char* string);   ManagedMemorySlice(const char* buf, size_t len);   explicit ManagedMemorySlice(const grpc_slice* slice);-  bool Equals(const grpc_slice& other) const {+  bool operator==(const grpc_slice& other) const {     if (refcount == other.refcount) {       return true;     }     return !grpc_slice_differs_refcounted(other, *this);   }-  bool Equals(const char* buf, const size_t len) const {-    return data.refcounted.length == len && buf != nullptr &&-           memcmp(buf, data.refcounted.bytes, len) == 0;+  bool operator!=(const grpc_slice& other) const { return !(*this == other); }+  bool operator==(std::pair<const char*, size_t> buflen) const {","Ironicly, Esun just yesterday finished the changes that now allow us to use the C++ standard library from within core, and just this morning he merged #20440, which replaces `grpc_core::Pair<>` with direct uses of `std::pair<>`.  So, sorry, but you'll have to back out that particular change. :(",
2873805,rmstar,https://api.github.com/repos/grpc/grpc/pulls/20464,331351117,2019-10-04T06:02:54Z,src/objective-c/tests/PerfTests/PerfTests.m,"@@ -181,7 +181,7 @@ - (void)testPingPongRPCWithV2API {   [self pingPongV2APIWithRequest:request numMessages:1000 options:options];    [self measureBlock:^{-    [self pingPongV2APIWithRequest:request numMessages:10000 options:options];+    [self pingPongV2APIWithRequest:request numMessages:1000 options:options];","It's to reduce test run time. Takes too long with 10k messages, and we don't really get any additional benefit with more messages. I've updated the PR description.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/20414,331730913,2019-10-05T03:37:00Z,setup.py,"@@ -136,6 +139,18 @@ ENABLE_DOCUMENTATION_BUILD = os.environ.get(     'GRPC_PYTHON_ENABLE_DOCUMENTATION_BUILD', False) +def check_linker_need_libatomic():+  """"""Test if linker on system needs libatomic.+  """"""+  code_test = (b'#include <atomic>\n' +",This isn't related with C++ standard but with how gcc and libatomic are implemented. GCC makes atomic operation library as a platform specific extension. ([ref](https://gcc.gnu.org/wiki/Atomic/GCCMM)) This code tries to use `__atomic_load_8` which is one of functions from `libatomic` and 64 bit value is chosen because GCC can generate code for values with a size <= 32 bits without `libatomic`.,
702191,arjunroy,https://api.github.com/repos/grpc/grpc/pulls/20453,332268252,2019-10-07T22:38:55Z,include/grpcpp/server_impl.h,"@@ -368,12 +370,15 @@ class Server : public grpc::ServerInterface, private grpc::GrpcLibraryCodegen {   // Handler for callback generic service, if any   std::unique_ptr<grpc::internal::MethodHandler> generic_handler_; -  // callback_cq_ references the callbackable completion queue associated+  // callback_cq_shards_references the callbackable completion queues associated   // with this server (if any). It is set on the first call to CallbackCQ().   // It is _not owned_ by the server; ownership belongs with its internal   // shutdown callback tag (invoked when the CQ is fully shutdown).   // It is protected by mu_-  CompletionQueue* callback_cq_ = nullptr;+  CompletionQueue* callback_cq_shards_ = nullptr;",Actually - cannot directly use std::vec or InlinedVector for the CQs themselves since we cannot tie the memory lifecycle of the completion queue to the server due to the current ownership semantics. Instead: will use a std::vector for the array of pointers to sharded completion queues.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20536,333127770,2019-10-09T17:04:15Z,tools/dockerfile/grpc_artifact_python_manylinux1_x64/Dockerfile,"@@ -28,11 +28,4 @@ RUN /opt/python/cp34-cp34m/bin/pip install cython RUN /opt/python/cp35-cp35m/bin/pip install cython RUN /opt/python/cp36-cp36m/bin/pip install cython RUN /opt/python/cp37-cp37m/bin/pip install cython--####################################################-# Install auditwheel with fix for namespace packages-RUN git clone https://github.com/pypa/auditwheel /usr/local/src/auditwheel-RUN cd /usr/local/src/auditwheel && git checkout 2.1-RUN /opt/python/cp36-cp36m/bin/pip install /usr/local/src/auditwheel-RUN rm /usr/local/bin/auditwheel-RUN cd /usr/local/bin && ln -s /opt/python/cp36-cp36m/bin/auditwheel+RUN /opt/python/cp38-cp38/bin/pip install cython","Good to know the ""m"" suffix is no longer needed: https://bugs.python.org/issue36707",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20536,333129736,2019-10-09T17:08:54Z,tools/run_tests/artifacts/build_artifact_python.sh,"@@ -79,12 +79,12 @@ ${SETARCH_CMD} ""${PYTHON}"" tools/distrib/python/grpcio_tools/setup.py bdist_whee if [ ""$GRPC_BUILD_MANYLINUX_WHEEL"" != """" ] then   for wheel in dist/*.whl; do-    ""${AUDITWHEEL}"" show ""$wheel"" | tee /dev/stderr |  grep -E -w 'manylinux(1|2010)_(x86_64|i686)'+    ""${AUDITWHEEL}"" show ""$wheel"" | tee /dev/stderr |  grep -E -w ""$AUDITWHEEL_PLAT""",Can you explain about how is this env is set? I can only find its usage.https://github.com/pypa/auditwheel/blob/c6ff475c3325ae393ffb5872b73cb4583d914a06/auditwheel/main_repair.py,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/20536,333171155,2019-10-09T18:39:46Z,tools/run_tests/artifacts/build_artifact_python.sh,"@@ -79,12 +79,12 @@ ${SETARCH_CMD} ""${PYTHON}"" tools/distrib/python/grpcio_tools/setup.py bdist_whee if [ ""$GRPC_BUILD_MANYLINUX_WHEEL"" != """" ] then   for wheel in dist/*.whl; do-    ""${AUDITWHEEL}"" show ""$wheel"" | tee /dev/stderr |  grep -E -w 'manylinux(1|2010)_(x86_64|i686)'+    ""${AUDITWHEEL}"" show ""$wheel"" | tee /dev/stderr |  grep -E -w ""$AUDITWHEEL_PLAT""",Here is a definition of this env: https://github.com/pypa/manylinux/blob/99c19386e670f2aecbe4a5d0397a10a04e2d1eeb/docker/Dockerfile-x86_64#L6,
10605667,chwarr,https://api.github.com/repos/grpc/grpc/pulls/20462,333199096,2019-10-09T19:44:56Z,include/grpc/support/alloc.h,"@@ -52,15 +45,6 @@ GPRAPI void* gpr_malloc_aligned(size_t size, size_t alignment); /** free memory allocated by gpr_malloc_aligned */ GPRAPI void gpr_free_aligned(void* ptr);","Since the core library allocates buffers and transfers ownership to the app, it needs to continue to provide deallocation functions, doesn't it? The app needs to be able to free the gRPC-allocated buffer, but the app's `free` and the core library's `free` may not be the same when the C runtime is statically linked (as is often the case when building gRPC for Windows). `gpr_free` means that the calling code can get back to the right heap.",
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20517,333587758,2019-10-10T15:27:46Z,src/core/ext/transport/chttp2/transport/hpack_encoder.cc,"@@ -452,10 +452,15 @@ static void add_key(grpc_chttp2_hpack_compressor* c, grpc_mdelem elem,   } } +template <bool static_entry> static void emit_indexed(grpc_chttp2_hpack_compressor* c, uint32_t elem_index,                          framer_state* st) {   GRPC_STATS_INC_HPACK_SEND_INDEXED();-  uint32_t len = GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);+  const uint32_t len =+      static_entry ? 1 : GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);","Could you explain the optimization here to me? It seems in the case of a static_entry, we could just call GRPC_CHTTP2_VARINT_LENGTH and it would return the desired length to us. Why is not entering GRPC_CHTTP2_VARINT_LENGTH and branching on static_entry an optimization?My concern here is that even if it's a minor optimization, it may not justify the extra code complexity.",X
702191,arjunroy,https://api.github.com/repos/grpc/grpc/pulls/20517,333616843,2019-10-10T16:25:20Z,src/core/ext/transport/chttp2/transport/hpack_encoder.cc,"@@ -452,10 +452,15 @@ static void add_key(grpc_chttp2_hpack_compressor* c, grpc_mdelem elem,   } } +template <bool static_entry> static void emit_indexed(grpc_chttp2_hpack_compressor* c, uint32_t elem_index,                          framer_state* st) {   GRPC_STATS_INC_HPACK_SEND_INDEXED();-  uint32_t len = GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);+  const uint32_t len =+      static_entry ? 1 : GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);","This looks like a branch but isn't. Specifically, notice that we're branching on a template parameter, which means that it's resolved at compile time - so it's straight line code, which makes static element encoding about 2.5% faster (from the microbenchmarks in #20267.)",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/20462,333720251,2019-10-10T20:28:37Z,include/grpc/support/alloc.h,"@@ -52,15 +45,6 @@ GPRAPI void* gpr_malloc_aligned(size_t size, size_t alignment); /** free memory allocated by gpr_malloc_aligned */ GPRAPI void gpr_free_aligned(void* ptr);","As chwarr said, it's still required to keep them in order to match `malloc` and `free` in case where the ownership of buffer allocated by application is passed to the gRPC library and vice versa. But, most of `grpc_malloc` and `gpr_free` can be replaced with the regular ones if its ownership doesn't transfer.",X
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20517,333787838,2019-10-11T00:30:13Z,src/core/ext/transport/chttp2/transport/hpack_encoder.cc,"@@ -452,10 +452,15 @@ static void add_key(grpc_chttp2_hpack_compressor* c, grpc_mdelem elem,   } } +template <bool static_entry> static void emit_indexed(grpc_chttp2_hpack_compressor* c, uint32_t elem_index,                          framer_state* st) {   GRPC_STATS_INC_HPACK_SEND_INDEXED();-  uint32_t len = GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);+  const uint32_t len =+      static_entry ? 1 : GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);","Ah, I see. A followup question: why do the template parameter thing instead of having two methods? This seems to impair readability at the call site (e.g., a reader wouldn't know what the ""false"" or ""true"" template parameter means). ",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20572,334082537,2019-10-11T16:50:38Z,src/core/ext/transport/chttp2/client/insecure/channel_create.cc,"@@ -109,3 +109,27 @@ grpc_channel* grpc_insecure_channel_create(const char* target,                                   target, GRPC_STATUS_INTERNAL,                                   ""Failed to create client channel""); }++/* The same as grpc_insecure_channel_create expect for:+ *   - doesn't create a new ExecCtx+ *   - doesn't call grpc_init or grpc_shutdown internally */+grpc_channel* grpc_insecure_channel_create_internal(","Instead of duplicating this code, how about just having the ALTS handshaker set the channel arg before it calls `grpc_insecure_channel_create()`?I don't actually care about the `ExecCtx` stacking.  The code can currently handle that, and we're trying to make `ExecCtx` go away completely in the long run anyway.",
702191,arjunroy,https://api.github.com/repos/grpc/grpc/pulls/20517,334163852,2019-10-11T20:33:17Z,src/core/ext/transport/chttp2/transport/hpack_encoder.cc,"@@ -452,10 +452,15 @@ static void add_key(grpc_chttp2_hpack_compressor* c, grpc_mdelem elem,   } } +template <bool static_entry> static void emit_indexed(grpc_chttp2_hpack_compressor* c, uint32_t elem_index,                          framer_state* st) {   GRPC_STATS_INC_HPACK_SEND_INDEXED();-  uint32_t len = GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);+  const uint32_t len =+      static_entry ? 1 : GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);","From a functionality point of view it will be the same - either way the compiler will output two methods.From a maintainability point of view: I have a preference for the template approach:1. It reduces the chance that the two methods drift (in case someone updates one but not the other).2. It makes it clear that they're fundamentally here to do the same thing.While readability at the call site now has one extra step (jump to the method declaration to see what the template is) the alternative would be having to track more methods, which I think is harder in the long run.",
702191,arjunroy,https://api.github.com/repos/grpc/grpc/pulls/20517,334670339,2019-10-14T21:28:43Z,src/core/lib/transport/metadata.h,"@@ -277,8 +277,8 @@ class RefcountedMdBase {   /* must be byte compatible with grpc_mdelem_data */   grpc_slice key_;   grpc_slice value_;-  grpc_core::Atomic<intptr_t> refcnt_;","No, this is to make the layout of this and the static md object match so that there's one less branch inside hpack_encoder.cc (where we lookup the hash for the metadata, which is either static or interned (which would use this struct)).",
702191,arjunroy,https://api.github.com/repos/grpc/grpc/pulls/20517,334670944,2019-10-14T21:30:43Z,src/core/ext/transport/chttp2/transport/hpack_encoder.cc,"@@ -859,7 +873,7 @@ void grpc_chttp2_encode_header(grpc_chttp2_hpack_compressor* c,         (static_index =              reinterpret_cast<grpc_core::StaticMetadata*>(GRPC_MDELEM_DATA(md))                  ->StaticIndex()) < GRPC_CHTTP2_LAST_STATIC_ENTRY) {-      emit_indexed(c, static_cast<uint32_t>(static_index + 1), &st);+      emit_indexed<true>(c, static_cast<uint32_t>(static_index + 1), &st);","The assumption here is safe; true means that the metadata is static, which we established as a necessary condition to enter this if branch.To make it safer, we could add a debug assert in emit_indexed() that ensures the metadata is indeed static when <true> is specified.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20555,334812249,2019-10-15T08:21:52Z,test/core/slice/slice_test.cc,"@@ -243,7 +243,13 @@ static void test_slice_interning(void) {   grpc_init();   grpc_slice src1 = grpc_slice_from_copied_string(""hello123456789123456789"");   grpc_slice src2 = grpc_slice_from_copied_string(""hello123456789123456789"");++  // Do not remove the log line. It actually supresses a compiler bug in windows+  // bazel opt build. See https://github.com/grpc/grpc/issues/20519","We could just skip the test and file an issue to followup. I'm still trying to find the actual MSVC bug listing to make 100% sure this is actually a compiler bug, but it's very time-consuming as I don't have much to go off of.So far:- I've been able to reproduce under bazel with VS2015, VS2017 and VS2019- when I build with cmake (Release build), I'm unable to reproduce -  which is pretty strange, but I haven't yet compared the exact compiler flags being used (but I checked they both run with /O2)",
1388179,buchgr,https://api.github.com/repos/grpc/grpc/pulls/20558,334946159,2019-10-15T13:20:37Z,bazel/grpc_build_system.bzl,"@@ -31,6 +31,10 @@ load(""@build_bazel_rules_apple//apple:ios.bzl"", ""ios_unit_test"") # The set of pollers to test against if a test exercises polling POLLERS = [""epollex"", ""epoll1"", ""poll""] +# set exec_properties = LARGE_MACHINE, to run the test on a large machine+# see //third_party/toolchains/machine_size for details+LARGE_MACHINE = { ""gceMachineType"" : ""n1-standard-8""}",It might be nicer to specify this in a .bzl file in the `//third_party/toolchains` directory in order to have all the platforms/toolchain stuff in one place and then also reuse `LARGE_MACHINE` for the platform definitions.,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20555,334974657,2019-10-15T14:09:30Z,test/core/slice/slice_test.cc,"@@ -241,9 +241,15 @@ static void test_slice_interning(void) {   LOG_TEST_NAME(""test_slice_interning"");    grpc_init();-  grpc_slice src1 = grpc_slice_from_copied_string(""hello123456789123456789"");-  grpc_slice src2 = grpc_slice_from_copied_string(""hello123456789123456789"");+  grpc_slice src1 = grpc_slice_from_copied_string(""hello1234567891234567891"");+  grpc_slice src2 = grpc_slice_from_copied_string(""hello1234567891234567891"");++  // make sure slices are refcounted to guarantee slices' start ptrs are+  // distinct (even on windows opt 64bit build).+  // See https://github.com/grpc/grpc/issues/20519+  GPR_ASSERT(src1.refcount);   GPR_ASSERT(GRPC_SLICE_START_PTR(src1) != GRPC_SLICE_START_PTR(src2));",alternatively we could add (&src1 != &src2) condition to the assert which also unbreaks the test.,
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20517,335124020,2019-10-15T18:59:49Z,src/core/ext/transport/chttp2/transport/hpack_encoder.cc,"@@ -452,10 +452,15 @@ static void add_key(grpc_chttp2_hpack_compressor* c, grpc_mdelem elem,   } } +template <bool static_entry> static void emit_indexed(grpc_chttp2_hpack_compressor* c, uint32_t elem_index,                          framer_state* st) {   GRPC_STATS_INC_HPACK_SEND_INDEXED();-  uint32_t len = GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);+  const uint32_t len =+      static_entry ? 1 : GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);","1. If we have two methods that have the common logic factored out into a helper function that they both call, doesn't that accomplish the same purpose of ensuring that the two methods don't drift?2. I don't quite understand the point about tracking more methods. If the methods are clearly named, then the reader can understand the high level idea of what's going on at the call site. They won't have to jump to the method declaration just to figure that out, and I think this is valuable for readability.Methods templated on a boolean seem to be as much of a code smell as boolean parameters.  From a readability perspective, I'd prefer the other approach.",X
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20517,335127440,2019-10-15T19:08:11Z,src/core/ext/transport/chttp2/transport/hpack_encoder.cc,"@@ -452,10 +452,15 @@ static void add_key(grpc_chttp2_hpack_compressor* c, grpc_mdelem elem,   } } +template <bool static_entry> static void emit_indexed(grpc_chttp2_hpack_compressor* c, uint32_t elem_index,                          framer_state* st) {   GRPC_STATS_INC_HPACK_SEND_INDEXED();-  uint32_t len = GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);+  const uint32_t len =+      static_entry ? 1 : GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);","Since we have differing opinions here, I'll defer to the TL on whether to use boolean template parameter methods or make two separate methods: @markdroth ",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20517,335161484,2019-10-15T20:30:27Z,src/core/ext/transport/chttp2/transport/hpack_encoder.cc,"@@ -452,10 +452,15 @@ static void add_key(grpc_chttp2_hpack_compressor* c, grpc_mdelem elem,   } } +template <bool static_entry> static void emit_indexed(grpc_chttp2_hpack_compressor* c, uint32_t elem_index,                          framer_state* st) {   GRPC_STATS_INC_HPACK_SEND_INDEXED();-  uint32_t len = GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);+  const uint32_t len =+      static_entry ? 1 : GRPC_CHTTP2_VARINT_LENGTH(elem_index, 1);","I don't have a hard-and-fast rule about this, so it's as much a judgement call for me as for anyone else.  But as a practical matter, I don't tend to find a lot of cases where a bool template parameter really makes sense.  If the function is very small, then duplicating it isn't a big risk for their implementations diverging, because the two versions are not doing a whole lot.  And if the function is very large, then we probably don't want to templatize it, since that will bloat the generated code size; in that case, some refactoring is probably called for.In this particular case, the function is so small that I think duplicating it is fine.  And please do add comments clarifying that the two versions do the same thing but are optimized for different cases.",
1388179,buchgr,https://api.github.com/repos/grpc/grpc/pulls/20558,335446819,2019-10-16T12:35:50Z,bazel/grpc_build_system.bzl,"@@ -31,6 +31,10 @@ load(""@build_bazel_rules_apple//apple:ios.bzl"", ""ios_unit_test"") # The set of pollers to test against if a test exercises polling POLLERS = [""epollex"", ""epoll1"", ""poll""] +# set exec_properties = LARGE_MACHINE, to run the test on a large machine+# see //third_party/toolchains/machine_size for details+LARGE_MACHINE = { ""gceMachineType"" : ""n1-standard-8""}","IIUC //third_party/toolchains is not imported into google3 either so putting it there should work? Before `exec_properties=LARGE_MACHINE` you used `exec_compatible_with=[""//third_party/toolchains/machine_size:large""]` and presumably just had copybara remove it on import? ",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20621,335566881,2019-10-16T16:01:07Z,CMakeLists.txt,"@@ -19470,7 +19470,7 @@ generate_pkgconfig(   ""gRPC""   ""high performance general RPC framework""   ""${gRPC_CORE_VERSION}""-  ""gpr""+  ""gpr openssl""   ""-lgrpc -laddress_sorting -lcares -lz""",@zackgalbreath  it would be nice to determine the list of dependencies based on our actual dependencies. Like this we have to maintain several versions of the same list.https://github.com/grpc/grpc/blob/b0c87fd26b446afc98b49bbb7756d5d8e45a961f/test/distrib/cpp/run_distrib_test_cmake_pkgconfig.sh#L58,
25311427,vam-google,https://api.github.com/repos/grpc/grpc/pulls/20150,335692846,2019-10-16T20:28:55Z,bazel/test/python_test_repo/BUILD,"@@ -60,3 +60,54 @@ py_test(     ],     python_version = ""PY3"", )++# Test compatibility of py_proto_library and py_grpc_library rules with+# proto_library targets as deps when the latter use import_prefix and/or+# strip_import_prefix arguments+proto_library(+    name = ""helloworld_moved_proto"",+    srcs = [""helloworld.proto""],+    deps = [+        ""@com_google_protobuf//:duration_proto"",+        ""@com_google_protobuf//:timestamp_proto"",+    ],+    import_prefix = ""google/cloud"",+    strip_import_prefix = """"+)++py_proto_library(+    name = ""helloworld_moved_py_pb2"",+    deps = ["":helloworld_moved_proto""],+)++py_grpc_library(+    name = ""helloworld_moved_py_pb2_grpc"",+    srcs = ["":helloworld_moved_proto""],+    deps = ["":helloworld_moved_py_pb2""],+)++py_test(+    name = ""import_moved_test"",+    main = ""helloworld.py"",+    srcs = [""helloworld.py""],+    deps = [+        "":helloworld_moved_py_pb2"",+        "":helloworld_moved_py_pb2_grpc"",+        "":duration_py_pb2"",+        "":timestamp_py_pb2"",+    ],+    imports = [+        ""_virtual_imports/helloworld_moved_proto"",","I have update the generation rule to return `PyInfo` provider as well with imports property set to point to `_virtual_imports`, and then pass that as deps to the subsequent py_library rule. This effectively allowed to hide `_virtual_imports` from the surface. ",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20598,335900157,2019-10-17T09:25:10Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/iomgr.pyx.pxi,"@@ -81,39 +82,83 @@ cdef grpc_error* asyncio_socket_getpeername(         grpc_custom_socket* grpc_socket,         const grpc_sockaddr* addr,         int* length) with gil:-    raise NotImplemented()+    peer = (<_AsyncioSocket>grpc_socket.impl).peername()++    cdef grpc_resolved_address c_addr+    hostname = str_to_bytes(peer[0])+    grpc_string_to_sockaddr(&c_addr, hostname, peer[1])+    string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)+    length[0] = c_addr.len+    return grpc_error_none()   cdef grpc_error* asyncio_socket_getsockname(         grpc_custom_socket* grpc_socket,         const grpc_sockaddr* addr,         int* length) with gil:-    raise NotImplemented()+    """"""Supplies sock_addr in add_socket_to_server.""""""+    cdef grpc_resolved_address c_addr+    socket = (<_AsyncioSocket>grpc_socket.impl)+    if socket is None:+        peer = ('0.0.0.0', 0)+    else:+        peer = socket.sockname()+    hostname = str_to_bytes(peer[0])+    grpc_string_to_sockaddr(&c_addr, hostname, peer[1])+    string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)+    length[0] = c_addr.len+    return grpc_error_none()   cdef grpc_error* asyncio_socket_listen(grpc_custom_socket* grpc_socket) with gil:-    raise NotImplemented()+    (<_AsyncioSocket>grpc_socket.impl).listen()+    return grpc_error_none()+++# TODO(lidiz) connects the so_reuse_port option to channel arguments+def _asyncio_apply_socket_options(object s, so_reuse_port=False):+  s.setsockopt(native_socket.SOL_SOCKET, native_socket.SO_REUSEADDR, 1)+  if so_reuse_port:","Not really fan of having dead code, why not just add the TODO here but doing nothing? If we could also add the github issue related to this better.",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20598,335907480,2019-10-17T09:40:06Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pxd.pxi,"@@ -0,0 +1,35 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++cdef class _HandlerCallDetails:+    cdef readonly str method+    cdef readonly tuple invocation_metadata+++cdef class RPCState:+    cdef grpc_call* call,+    cdef grpc_call_details details+    cdef grpc_metadata_array request_metadata++    cdef bytes method(self)+++cdef class _AioServerState:+    cdef Server server","That's great we can re-use all of the cython code behind the `Server` class, the only thing that could concern me is the `backup_shutdown_queue` [1] which uses the `poll` - so blocking? - for synchronization.I'm wondering if this might become an issue considering at what situations this callback queue is used, basically for starting and stopping the server,.WDTY?[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/server.pyx.pxi#L67",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20598,335910214,2019-10-17T09:45:26Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -0,0 +1,248 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++cdef class _HandlerCallDetails:+    def __cinit__(self, str method, tuple invocation_metadata):+        self.method = method+        self.invocation_metadata = invocation_metadata+++class _ServicerContextPlaceHolder(object): pass+++cdef class CallbackWrapper:+    cdef CallbackContext context+    cdef object future++    def __cinit__(self, object future):+        self.context.functor.functor_run = self.functor_run+        self.context.waiter = <cpython.PyObject*>(future)+        self.future = future++    @staticmethod+    cdef void functor_run(+            grpc_experimental_completion_queue_functor* functor,+            int succeed):+        cdef CallbackContext *context = <CallbackContext *>functor+        (<object>context.waiter).set_result(None)++    cdef grpc_experimental_completion_queue_functor *c_functor(self):+        return &self.context.functor+++cdef class RPCState:++    def __cinit__(self):+        grpc_metadata_array_init(&self.request_metadata)+        grpc_call_details_init(&self.details)++    cdef bytes method(self):+      return _slice_bytes(self.details.method)++    def __dealloc__(self):+        """"""Cleans the Core objects.""""""+        grpc_call_details_destroy(&self.details)+        grpc_metadata_array_destroy(&self.request_metadata)+        if self.call:+            grpc_call_unref(self.call)+++cdef _find_method_handler(RPCState rpc_state, list generic_handlers):+    # TODO(lidiz) connects Metadata to call details+    cdef _HandlerCallDetails handler_call_details = _HandlerCallDetails(+        rpc_state.method().decode(),+        tuple()+    )++    for generic_handler in generic_handlers:+        method_handler = generic_handler.service(handler_call_details)+        if method_handler is not None:+            return method_handler+    return None+++async def callback_start_batch(RPCState rpc_state, tuple operations, object+loop):+    """"""The callback version of start batch operations.""""""+    cdef _BatchOperationTag batch_operation_tag = _BatchOperationTag(None, operations, None)+    batch_operation_tag.prepare()++    cdef object future = loop.create_future()+    cdef CallbackWrapper wrapper = CallbackWrapper(future)+    # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+    # when calling ""await"". This is an over-optimization by Cython.+    cpython.Py_INCREF(wrapper)+    cdef grpc_call_error error = grpc_call_start_batch(+        rpc_state.call,+        batch_operation_tag.c_ops,+        batch_operation_tag.c_nops,+        wrapper.c_functor(), NULL)++    if error != GRPC_CALL_OK:+        raise RuntimeError(""Error with callback_start_batch {}"".format(error))++    await future+    cpython.Py_DECREF(wrapper)+    cdef grpc_event c_event+    batch_operation_tag.event(c_event)+++async def _handle_unary_unary_rpc(object method_handler, RPCState rpc_state, object loop):+    # Receives request message+    cdef tuple receive_ops = (+        ReceiveMessageOperation(_EMPTY_FLAGS),+    )+    await callback_start_batch(rpc_state, receive_ops, loop)++    # Deserializes the request message+    cdef bytes request_raw = receive_ops[0].message()+    cdef object request_message+    if method_handler.request_deserializer:+        request_message = method_handler.request_deserializer(request_raw)+    else:+        request_message = request_raw++    # Executes application logic+    cdef object response_message = await method_handler.unary_unary(request_message, _ServicerContextPlaceHolder())++    # Serializes the response message+    cdef bytes response_raw+    if method_handler.response_serializer:+        response_raw = method_handler.response_serializer(response_message)+    else:+        response_raw = response_message++    # Sends response message+    cdef tuple send_ops = (+        SendStatusFromServerOperation(+        tuple(), StatusCode.ok, b'', _EMPTY_FLAGS),+        SendInitialMetadataOperation(tuple(), _EMPTY_FLAGS),+        SendMessageOperation(response_raw, _EMPTY_FLAGS),+    )+    await callback_start_batch(rpc_state, send_ops, loop)+++async def _handle_rpc(_AioServerState server_state, RPCState rpc_state, object loop):+    # Finds the method handler (application logic)+    cdef object method_handler = _find_method_handler(+        rpc_state,+        server_state.generic_handlers+    )+    if method_handler is None:+        # TODO(lidiz) return unimplemented error to client side+        raise NotImplementedError()+    # TODO(lidiz) extend to all 4 types of RPC+    if method_handler.request_streaming or method_handler.response_streaming:+        raise NotImplementedError()+    else:+        await _handle_unary_unary_rpc(+            method_handler,+            rpc_state,+            loop+        )+++async def _server_call_request_call(_AioServerState server_state, object loop):+    cdef grpc_call_error error+    cdef RPCState rpc_state = RPCState()+    cdef object future = loop.create_future()+    cdef CallbackWrapper wrapper = CallbackWrapper(future)+    # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+    # when calling ""await"". This is an over-optimization by Cython.+    cpython.Py_INCREF(wrapper)+    error = grpc_server_request_call(+        server_state.server.c_server, &rpc_state.call, &rpc_state.details,+        &rpc_state.request_metadata,+        server_state.cq, server_state.cq,+        wrapper.c_functor()+    )+    if error != GRPC_CALL_OK:+        raise RuntimeError(""Error in _server_call_request_call: %s"" % error)++    await future+    cpython.Py_DECREF(wrapper)+    return rpc_state+++async def _server_main_loop(_AioServerState server_state):+    cdef object loop = asyncio.get_event_loop()+    cdef RPCState rpc_state+    cdef object waiter++    while True:+        rpc_state = await _server_call_request_call(+            server_state,+            loop)+        # await waiter++        loop.create_task(_handle_rpc(server_state, rpc_state, loop))+        await asyncio.sleep(0)",What's the rationale for wanting to have the request call being executed right now instead of waiting for the next loop iteration which will implicitly start the `_handle_rpc` coro?,X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20598,335917677,2019-10-17T10:01:02Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -0,0 +1,248 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++cdef class _HandlerCallDetails:+    def __cinit__(self, str method, tuple invocation_metadata):+        self.method = method+        self.invocation_metadata = invocation_metadata+++class _ServicerContextPlaceHolder(object): pass+++cdef class CallbackWrapper:","Any chance of promoting this class as the default and expected way for handling completion queue events in the `Aio` module? if so, could we remove the `CallbackContext` in favor of the`CallbackWrapper` implementation?https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/callbackcontext.pxd.pxi",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20598,335918870,2019-10-17T10:03:47Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -0,0 +1,248 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++cdef class _HandlerCallDetails:+    def __cinit__(self, str method, tuple invocation_metadata):+        self.method = method+        self.invocation_metadata = invocation_metadata+++class _ServicerContextPlaceHolder(object): pass+++cdef class CallbackWrapper:+    cdef CallbackContext context+    cdef object future++    def __cinit__(self, object future):+        self.context.functor.functor_run = self.functor_run+        self.context.waiter = <cpython.PyObject*>(future)+        self.future = future++    @staticmethod+    cdef void functor_run(+            grpc_experimental_completion_queue_functor* functor,+            int succeed):+        cdef CallbackContext *context = <CallbackContext *>functor+        (<object>context.waiter).set_result(None)","if the future was done - for example, canceled - this will raise an Exception [1], maybe we could do something like```pythonif not (<object>context.waiter).done():    (<object>context.waiter).set_result(None)else:    logging.debug(""Future was cancelled ....""```[1] https://github.com/python/cpython/blob/master/Lib/asyncio/futures.py#L231",X
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20659,336102321,2019-10-17T16:21:16Z,test/core/bad_client/bad_client.cc,"@@ -57,7 +57,7 @@ static void thd_func(void* arg) { }  /* Sets the done_write event */-static void set_done_write(void* arg, grpc_error* error) {+static void set_done_write(void* arg, grpc_error* /*error*/) {",Why do we take this parameter in at all?,X
28123841,guantaol,https://api.github.com/repos/grpc/grpc/pulls/20660,336189474,2019-10-17T19:36:37Z,src/core/lib/iomgr/executor/threadpool.h,"@@ -62,7 +62,7 @@ class ThreadPoolInterface { // NULL closure. class ThreadPoolWorker {  public:-  ThreadPoolWorker(const char* thd_name, ThreadPoolInterface* pool,+  ThreadPoolWorker(const char* thd_name, ThreadPoolInterface* /*pool*/,",I think we forgot to remove this parameter. It is unnecessary in ThreadPoolWorker. I will remove it in a later PR.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20598,336244333,2019-10-17T21:54:10Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -0,0 +1,248 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++cdef class _HandlerCallDetails:+    def __cinit__(self, str method, tuple invocation_metadata):+        self.method = method+        self.invocation_metadata = invocation_metadata+++class _ServicerContextPlaceHolder(object): pass+++cdef class CallbackWrapper:+    cdef CallbackContext context+    cdef object future++    def __cinit__(self, object future):+        self.context.functor.functor_run = self.functor_run+        self.context.waiter = <cpython.PyObject*>(future)+        self.future = future++    @staticmethod+    cdef void functor_run(+            grpc_experimental_completion_queue_functor* functor,+            int succeed):+        cdef CallbackContext *context = <CallbackContext *>functor+        (<object>context.waiter).set_result(None)++    cdef grpc_experimental_completion_queue_functor *c_functor(self):+        return &self.context.functor+++cdef class RPCState:++    def __cinit__(self):+        grpc_metadata_array_init(&self.request_metadata)+        grpc_call_details_init(&self.details)++    cdef bytes method(self):+      return _slice_bytes(self.details.method)++    def __dealloc__(self):+        """"""Cleans the Core objects.""""""+        grpc_call_details_destroy(&self.details)+        grpc_metadata_array_destroy(&self.request_metadata)+        if self.call:+            grpc_call_unref(self.call)+++cdef _find_method_handler(RPCState rpc_state, list generic_handlers):+    # TODO(lidiz) connects Metadata to call details+    cdef _HandlerCallDetails handler_call_details = _HandlerCallDetails(+        rpc_state.method().decode(),+        tuple()+    )++    for generic_handler in generic_handlers:+        method_handler = generic_handler.service(handler_call_details)+        if method_handler is not None:+            return method_handler+    return None+++async def callback_start_batch(RPCState rpc_state, tuple operations, object+loop):+    """"""The callback version of start batch operations.""""""+    cdef _BatchOperationTag batch_operation_tag = _BatchOperationTag(None, operations, None)+    batch_operation_tag.prepare()++    cdef object future = loop.create_future()+    cdef CallbackWrapper wrapper = CallbackWrapper(future)+    # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+    # when calling ""await"". This is an over-optimization by Cython.+    cpython.Py_INCREF(wrapper)+    cdef grpc_call_error error = grpc_call_start_batch(+        rpc_state.call,+        batch_operation_tag.c_ops,+        batch_operation_tag.c_nops,+        wrapper.c_functor(), NULL)++    if error != GRPC_CALL_OK:+        raise RuntimeError(""Error with callback_start_batch {}"".format(error))++    await future+    cpython.Py_DECREF(wrapper)+    cdef grpc_event c_event+    batch_operation_tag.event(c_event)+++async def _handle_unary_unary_rpc(object method_handler, RPCState rpc_state, object loop):+    # Receives request message+    cdef tuple receive_ops = (+        ReceiveMessageOperation(_EMPTY_FLAGS),+    )+    await callback_start_batch(rpc_state, receive_ops, loop)++    # Deserializes the request message+    cdef bytes request_raw = receive_ops[0].message()+    cdef object request_message+    if method_handler.request_deserializer:+        request_message = method_handler.request_deserializer(request_raw)+    else:+        request_message = request_raw++    # Executes application logic+    cdef object response_message = await method_handler.unary_unary(request_message, _ServicerContextPlaceHolder())++    # Serializes the response message+    cdef bytes response_raw+    if method_handler.response_serializer:+        response_raw = method_handler.response_serializer(response_message)+    else:+        response_raw = response_message++    # Sends response message+    cdef tuple send_ops = (+        SendStatusFromServerOperation(+        tuple(), StatusCode.ok, b'', _EMPTY_FLAGS),+        SendInitialMetadataOperation(tuple(), _EMPTY_FLAGS),+        SendMessageOperation(response_raw, _EMPTY_FLAGS),+    )+    await callback_start_batch(rpc_state, send_ops, loop)+++async def _handle_rpc(_AioServerState server_state, RPCState rpc_state, object loop):+    # Finds the method handler (application logic)+    cdef object method_handler = _find_method_handler(+        rpc_state,+        server_state.generic_handlers+    )+    if method_handler is None:+        # TODO(lidiz) return unimplemented error to client side+        raise NotImplementedError()+    # TODO(lidiz) extend to all 4 types of RPC+    if method_handler.request_streaming or method_handler.response_streaming:+        raise NotImplementedError()+    else:+        await _handle_unary_unary_rpc(+            method_handler,+            rpc_state,+            loop+        )+++async def _server_call_request_call(_AioServerState server_state, object loop):+    cdef grpc_call_error error+    cdef RPCState rpc_state = RPCState()+    cdef object future = loop.create_future()+    cdef CallbackWrapper wrapper = CallbackWrapper(future)+    # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+    # when calling ""await"". This is an over-optimization by Cython.+    cpython.Py_INCREF(wrapper)+    error = grpc_server_request_call(+        server_state.server.c_server, &rpc_state.call, &rpc_state.details,+        &rpc_state.request_metadata,+        server_state.cq, server_state.cq,+        wrapper.c_functor()+    )+    if error != GRPC_CALL_OK:+        raise RuntimeError(""Error in _server_call_request_call: %s"" % error)++    await future+    cpython.Py_DECREF(wrapper)+    return rpc_state+++async def _server_main_loop(_AioServerState server_state):+    cdef object loop = asyncio.get_event_loop()+    cdef RPCState rpc_state+    cdef object waiter++    while True:+        rpc_state = await _server_call_request_call(+            server_state,+            loop)+        # await waiter++        loop.create_task(_handle_rpc(server_state, rpc_state, loop))+        await asyncio.sleep(0)+++async def _server_start(_AioServerState server_state):+    server_state.server.start()+    await _server_main_loop(server_state)+++cdef class _AioServerState:+    def __cinit__(self):+        self.server = None+        self.cq = NULL+        self.generic_handlers = []+++cdef class AioServer:++    def __init__(self, thread_pool, generic_handlers, interceptors, options,+                 maximum_concurrent_rpcs, compression):+        self._state = _AioServerState()+        self._state.server = Server(options)+        self._state.cq = grpc_completion_queue_create_for_callback(+            NULL,+            NULL+        )+        grpc_server_register_completion_queue(+            self._state.server.c_server,+            self._state.cq,+            NULL+        )+        self.add_generic_rpc_handlers(generic_handlers)++        if interceptors:+            raise NotImplementedError()+        if maximum_concurrent_rpcs:+            raise NotImplementedError()+        if compression:+            raise NotImplementedError()+        if thread_pool:+            raise NotImplementedError()++    def add_generic_rpc_handlers(self, generic_rpc_handlers):+        for h in generic_rpc_handlers:+            self._state.generic_handlers.append(h)++    def add_insecure_port(self, address):+        return self._state.server.add_http2_port(address)++    def add_secure_port(self, address, server_credentials):+        return self._state.server.add_http2_port(address,+                                          server_credentials._credentials)++    async def start(self):+        loop = asyncio.get_event_loop()+        loop.create_task(_server_start(self._state))","I slightly twisted the semantic, so it allows multiple call to `start`, but will raise `RuntimeError` if called in other state.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20598,336245728,2019-10-17T21:58:26Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -0,0 +1,248 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++cdef class _HandlerCallDetails:+    def __cinit__(self, str method, tuple invocation_metadata):+        self.method = method+        self.invocation_metadata = invocation_metadata+++class _ServicerContextPlaceHolder(object): pass+++cdef class CallbackWrapper:+    cdef CallbackContext context+    cdef object future++    def __cinit__(self, object future):+        self.context.functor.functor_run = self.functor_run+        self.context.waiter = <cpython.PyObject*>(future)+        self.future = future++    @staticmethod+    cdef void functor_run(+            grpc_experimental_completion_queue_functor* functor,+            int succeed):+        cdef CallbackContext *context = <CallbackContext *>functor+        (<object>context.waiter).set_result(None)++    cdef grpc_experimental_completion_queue_functor *c_functor(self):+        return &self.context.functor+++cdef class RPCState:++    def __cinit__(self):+        grpc_metadata_array_init(&self.request_metadata)+        grpc_call_details_init(&self.details)++    cdef bytes method(self):+      return _slice_bytes(self.details.method)++    def __dealloc__(self):+        """"""Cleans the Core objects.""""""+        grpc_call_details_destroy(&self.details)+        grpc_metadata_array_destroy(&self.request_metadata)+        if self.call:+            grpc_call_unref(self.call)+++cdef _find_method_handler(RPCState rpc_state, list generic_handlers):+    # TODO(lidiz) connects Metadata to call details+    cdef _HandlerCallDetails handler_call_details = _HandlerCallDetails(+        rpc_state.method().decode(),+        tuple()+    )++    for generic_handler in generic_handlers:+        method_handler = generic_handler.service(handler_call_details)+        if method_handler is not None:+            return method_handler+    return None+++async def callback_start_batch(RPCState rpc_state, tuple operations, object+loop):+    """"""The callback version of start batch operations.""""""+    cdef _BatchOperationTag batch_operation_tag = _BatchOperationTag(None, operations, None)+    batch_operation_tag.prepare()++    cdef object future = loop.create_future()+    cdef CallbackWrapper wrapper = CallbackWrapper(future)+    # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+    # when calling ""await"". This is an over-optimization by Cython.+    cpython.Py_INCREF(wrapper)+    cdef grpc_call_error error = grpc_call_start_batch(+        rpc_state.call,+        batch_operation_tag.c_ops,+        batch_operation_tag.c_nops,+        wrapper.c_functor(), NULL)++    if error != GRPC_CALL_OK:+        raise RuntimeError(""Error with callback_start_batch {}"".format(error))++    await future+    cpython.Py_DECREF(wrapper)+    cdef grpc_event c_event+    batch_operation_tag.event(c_event)+++async def _handle_unary_unary_rpc(object method_handler, RPCState rpc_state, object loop):+    # Receives request message+    cdef tuple receive_ops = (+        ReceiveMessageOperation(_EMPTY_FLAGS),+    )+    await callback_start_batch(rpc_state, receive_ops, loop)++    # Deserializes the request message+    cdef bytes request_raw = receive_ops[0].message()+    cdef object request_message+    if method_handler.request_deserializer:+        request_message = method_handler.request_deserializer(request_raw)+    else:+        request_message = request_raw++    # Executes application logic+    cdef object response_message = await method_handler.unary_unary(request_message, _ServicerContextPlaceHolder())++    # Serializes the response message+    cdef bytes response_raw+    if method_handler.response_serializer:+        response_raw = method_handler.response_serializer(response_message)+    else:+        response_raw = response_message++    # Sends response message+    cdef tuple send_ops = (+        SendStatusFromServerOperation(+        tuple(), StatusCode.ok, b'', _EMPTY_FLAGS),+        SendInitialMetadataOperation(tuple(), _EMPTY_FLAGS),+        SendMessageOperation(response_raw, _EMPTY_FLAGS),+    )+    await callback_start_batch(rpc_state, send_ops, loop)+++async def _handle_rpc(_AioServerState server_state, RPCState rpc_state, object loop):+    # Finds the method handler (application logic)+    cdef object method_handler = _find_method_handler(+        rpc_state,+        server_state.generic_handlers+    )+    if method_handler is None:+        # TODO(lidiz) return unimplemented error to client side+        raise NotImplementedError()+    # TODO(lidiz) extend to all 4 types of RPC+    if method_handler.request_streaming or method_handler.response_streaming:+        raise NotImplementedError()+    else:+        await _handle_unary_unary_rpc(+            method_handler,+            rpc_state,+            loop+        )+++async def _server_call_request_call(_AioServerState server_state, object loop):+    cdef grpc_call_error error+    cdef RPCState rpc_state = RPCState()+    cdef object future = loop.create_future()+    cdef CallbackWrapper wrapper = CallbackWrapper(future)+    # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+    # when calling ""await"". This is an over-optimization by Cython.+    cpython.Py_INCREF(wrapper)+    error = grpc_server_request_call(+        server_state.server.c_server, &rpc_state.call, &rpc_state.details,+        &rpc_state.request_metadata,+        server_state.cq, server_state.cq,+        wrapper.c_functor()+    )+    if error != GRPC_CALL_OK:+        raise RuntimeError(""Error in _server_call_request_call: %s"" % error)++    await future+    cpython.Py_DECREF(wrapper)+    return rpc_state+++async def _server_main_loop(_AioServerState server_state):+    cdef object loop = asyncio.get_event_loop()+    cdef RPCState rpc_state+    cdef object waiter++    while True:+        rpc_state = await _server_call_request_call(+            server_state,+            loop)+        # await waiter++        loop.create_task(_handle_rpc(server_state, rpc_state, loop))+        await asyncio.sleep(0)+++async def _server_start(_AioServerState server_state):+    server_state.server.start()+    await _server_main_loop(server_state)+++cdef class _AioServerState:+    def __cinit__(self):+        self.server = None+        self.cq = NULL+        self.generic_handlers = []+++cdef class AioServer:++    def __init__(self, thread_pool, generic_handlers, interceptors, options,+                 maximum_concurrent_rpcs, compression):+        self._state = _AioServerState()+        self._state.server = Server(options)+        self._state.cq = grpc_completion_queue_create_for_callback(+            NULL,+            NULL+        )+        grpc_server_register_completion_queue(+            self._state.server.c_server,+            self._state.cq,+            NULL+        )+        self.add_generic_rpc_handlers(generic_handlers)++        if interceptors:+            raise NotImplementedError()+        if maximum_concurrent_rpcs:+            raise NotImplementedError()+        if compression:+            raise NotImplementedError()+        if thread_pool:+            raise NotImplementedError()++    def add_generic_rpc_handlers(self, generic_rpc_handlers):+        for h in generic_rpc_handlers:+            self._state.generic_handlers.append(h)++    def add_insecure_port(self, address):+        return self._state.server.add_http2_port(address)++    def add_secure_port(self, address, server_credentials):+        return self._state.server.add_http2_port(address,+                                          server_credentials._credentials)++    async def start(self):+        loop = asyncio.get_event_loop()+        loop.create_task(_server_start(self._state))+        await asyncio.sleep(0)++    def stop(self, unused_grace):+        pass",Yes. TODO added. Shutdown path is complex.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20598,336246900,2019-10-17T22:01:44Z,src/python/grpcio/grpc/experimental/aio/_server.py,"@@ -0,0 +1,175 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Server-side implementation of gRPC Asyncio Python.""""""++from typing import Text, Optional+import asyncio+import grpc+from grpc._cython import cygrpc+++class Server:+    """"""Serves RPCs.""""""++    def __init__(self, thread_pool, generic_handlers, interceptors, options,+                 maximum_concurrent_rpcs, compression):+        self._server = cygrpc.AioServer(thread_pool, generic_handlers,+                                        interceptors, options,+                                        maximum_concurrent_rpcs, compression)++    def add_generic_rpc_handlers(+            self,+            generic_rpc_handlers,+            # generic_rpc_handlers: Iterable[grpc.GenericRpcHandlers]+    ) -> None:+        """"""Registers GenericRpcHandlers with this Server.++        This method is only safe to call before the server is started.++        Args:+          generic_rpc_handlers: An iterable of GenericRpcHandlers that will be+          used to service RPCs.+        """"""+        self._server.add_generic_rpc_handlers(generic_rpc_handlers)++    def add_insecure_port(self, address: Text) -> int:+        """"""Opens an insecure port for accepting RPCs.++        This method may only be called before starting the server.++        Args:+          address: The address for which to open a port. If the port is 0,+            or not specified in the address, then gRPC runtime will choose a port.++        Returns:+          An integer port on which server will accept RPC requests.+        """"""+        return self._server.add_insecure_port(address)++    def add_secure_port(self, address: Text,+                        server_credentials: grpc.ServerCredentials) -> int:+        """"""Opens a secure port for accepting RPCs.++        This method may only be called before starting the server.++        Args:+          address: The address for which to open a port.+            if the port is 0, or not specified in the address, then gRPC+            runtime will choose a port.+          server_credentials: A ServerCredentials object.++        Returns:+          An integer port on which server will accept RPC requests.+        """"""+        return self._server.add_secure_port(address, server_credentials)++    async def start(self) -> None:+        """"""Starts this Server.++        This method may only be called once. (i.e. it is not idempotent).+        """"""+        await self._server.start()++    def stop(self, grace: Optional[float]) -> asyncio.Event:+        """"""Stops this Server.++        This method immediately stop service of new RPCs in all cases.++        If a grace period is specified, this method returns immediately+        and all RPCs active at the end of the grace period are aborted.+        If a grace period is not specified (by passing None for `grace`),+        all existing RPCs are aborted immediately and this method+        blocks until the last RPC handler terminates.++        This method is idempotent and may be called at any time.+        Passing a smaller grace value in a subsequent call will have+        the effect of stopping the Server sooner (passing None will+        have the effect of stopping the server immediately). Passing+        a larger grace value in a subsequent call *will not* have the+        effect of stopping the server later (i.e. the most restrictive+        grace value is used).++        Args:+          grace: A duration of time in seconds or None.++        Returns:+          A threading.Event that will be set when this Server has completely+          stopped, i.e. when running RPCs either complete or are aborted and+          all handlers have terminated.+        """"""+        raise NotImplementedError()++    async def wait_for_termination(self,+                                   timeout: Optional[float] = None) -> bool:+        """"""Block current thread until the server stops.++        This is an EXPERIMENTAL API.++        The wait will not consume computational resources during blocking, and+        it will block until one of the two following conditions are met:++        1) The server is stopped or terminated;+        2) A timeout occurs if timeout is not `None`.++        The timeout argument works in the same way as `threading.Event.wait()`.+        https://docs.python.org/3/library/threading.html#threading.Event.wait++        Args:+          timeout: A floating point number specifying a timeout for the+            operation in seconds.++        Returns:+          A bool indicates if the operation times out.+        """"""+        if timeout:+            raise NotImplementedError()+        # TODO(lidiz) replace this wait forever logic+        future = asyncio.get_event_loop().create_future()+        await future","It blocks indefinitely right now. So user can have a way to get notified if server stops, and a coroutine that has the same life span as the server.The completion of this semantic would be part of shutdown path.",
35502996,KyleFromKitware,https://api.github.com/repos/grpc/grpc/pulls/20537,336564897,2019-10-18T16:04:38Z,templates/CMakeLists.txt.template,"@@ -68,6 +68,17 @@       return ''     return 'endif()\n'   %>+  <%",Not a bad idea. I'll add it once the tests finish.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20630,336578531,2019-10-18T16:40:17Z,cmake/cares.cmake,"@@ -22,7 +22,7 @@ if(""${gRPC_CARES_PROVIDER}"" STREQUAL ""module"")     # See https://github.com/grpc/grpc/issues/17255     set(HAVE_LIBNSL OFF CACHE BOOL ""avoid cares dependency on libnsl"")   endif()-  add_subdirectory(third_party/cares/cares)+  add_subdirectory(""${CARES_ROOT_DIR}"" third_party/cares/cares)","That said, it seems that we do support this for other dependencies (so we could add for consistency?):https://github.com/grpc/grpc/blob/933acd31a8c06e09af495af1edc80645041f0d27/cmake/ssl.cmake#L16https://github.com/grpc/grpc/blob/933acd31a8c06e09af495af1edc80645041f0d27/cmake/protobuf.cmake#L27https://github.com/grpc/grpc/blob/933acd31a8c06e09af495af1edc80645041f0d27/cmake/zlib.cmake#L16If we do this, let's stay consistent with how it's done in the other .cmake files?",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/20462,336585198,2019-10-18T16:57:39Z,include/grpc/support/alloc.h,"@@ -52,15 +45,6 @@ GPRAPI void* gpr_malloc_aligned(size_t size, size_t alignment); /** free memory allocated by gpr_malloc_aligned */ GPRAPI void gpr_free_aligned(void* ptr);",I haven't looked through all cases involved and definitely it might be valuable to see that too. We might come up with more narrow API such as `grpc_alloc_metadata` and `gprc_free_metadata` if only metadata will go in and out across the API border instead of having general `gpr_malloc` and `gpr_free`.How about merging this first and working on `malloc` and `free`later? It seems that there are a few of usages even in internal repository.,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20598,336639341,2019-10-18T19:15:07Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/iomgr.pyx.pxi,"@@ -81,39 +82,82 @@ cdef grpc_error* asyncio_socket_getpeername(         grpc_custom_socket* grpc_socket,         const grpc_sockaddr* addr,         int* length) with gil:-    raise NotImplemented()+    peer = (<_AsyncioSocket>grpc_socket.impl).peername()++    cdef grpc_resolved_address c_addr+    hostname = str_to_bytes(peer[0])+    grpc_string_to_sockaddr(&c_addr, hostname, peer[1])+    string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)+    length[0] = c_addr.len+    return grpc_error_none()   cdef grpc_error* asyncio_socket_getsockname(         grpc_custom_socket* grpc_socket,         const grpc_sockaddr* addr,         int* length) with gil:-    raise NotImplemented()+    """"""Supplies sock_addr in add_socket_to_server.""""""+    cdef grpc_resolved_address c_addr+    socket = (<_AsyncioSocket>grpc_socket.impl)+    if socket is None:+        peer = ('0.0.0.0', 0)+    else:+        peer = socket.sockname()+    hostname = str_to_bytes(peer[0])+    grpc_string_to_sockaddr(&c_addr, hostname, peer[1])+    string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)+    length[0] = c_addr.len+    return grpc_error_none()   cdef grpc_error* asyncio_socket_listen(grpc_custom_socket* grpc_socket) with gil:-    raise NotImplemented()+    (<_AsyncioSocket>grpc_socket.impl).listen()+    return grpc_error_none()+++def _asyncio_apply_socket_options(object s, so_reuse_port=False):+    # TODO(https://github.com/grpc/grpc/issues/20667)+    # Connects the so_reuse_port option to channel arguments+    s.setsockopt(native_socket.SOL_SOCKET, native_socket.SO_REUSEADDR, 1)+    s.setsockopt(native_socket.IPPROTO_TCP, native_socket.TCP_NODELAY, True)   cdef grpc_error* asyncio_socket_bind(         grpc_custom_socket* grpc_socket,         const grpc_sockaddr* addr,         size_t len, int flags) with gil:-    raise NotImplemented()+    host, port = sockaddr_to_tuple(addr, len)+    try:+        try:+            socket = native_socket.socket(family=native_socket.AF_INET6)",This and the `except` block have a few similar lines that could probably be merged with a slight change in logic.,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20598,336647365,2019-10-18T19:38:39Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/socket.pyx.pxi,"@@ -92,7 +108,13 @@ cdef class _AsyncioSocket:                 grpc_socket_error(""read {}"".format(error_msg).encode())             ) -    cdef void connect(self, object host, object port, grpc_custom_connect_callback grpc_connect_cb):+    cdef void connect(self,+                      object host,+                      object port,+                      grpc_custom_connect_callback grpc_connect_cb):+        if self._reader:",This condition seems like it would only ever be met when `connect` is called twice in a row. Do we want this operation to be idempotent? Or should we instead raise an exception?,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20598,336650955,2019-10-18T19:49:34Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/socket.pyx.pxi,"@@ -132,3 +154,39 @@ cdef class _AsyncioSocket:     cdef void close(self):         if self.is_connected():             self._writer.close()++    def _new_connection_callback(self, object reader, object writer):+        client_socket = _AsyncioSocket.create(+            self._grpc_client_socket,+            reader,+            writer,+        )++        self._grpc_client_socket.impl = <void*>client_socket+        cpython.Py_INCREF(client_socket)+        # Accept callback expects to be called with:",It might be helpful to use a cython-style function signature here so the necessary types are unambiguous.,
10605667,chwarr,https://api.github.com/repos/grpc/grpc/pulls/20462,336657099,2019-10-18T20:07:05Z,include/grpc/support/alloc.h,"@@ -52,15 +45,6 @@ GPRAPI void* gpr_malloc_aligned(size_t size, size_t alignment); /** free memory allocated by gpr_malloc_aligned */ GPRAPI void gpr_free_aligned(void* ptr);","The following non-exhaustive list of APIs in the ""basic"" core API allocate and require the caller/app to free via `gpr_free`:* [`grpc_call_get_peer`][grpc_call_get_peer]* [`grpc_ssl_roots_override_result`][grpc_ssl_roots_override_result]* [`grpc_slice_to_c_string`][grpc_slice_to_c_string]There are a few more in more obscure parts of the library, like `gpr_strdup` and `gpr_asprintf`.[grpc_call_get_peer]: https://github.com/grpc/grpc/blob/834f0c780900d3ce6e917c23f6dacb4f6d8e4b56/include/grpc/grpc.h#L269-L277[grpc_ssl_roots_override_result]: https://github.com/grpc/grpc/blob/0362df725f1ed9db3cb3e50456e2ad59cb4a1c1b/include/grpc/grpc_security.h#L138-L145[grpc_slice_to_c_string]: https://github.com/grpc/grpc/blob/f8b1e6cb4401c062eba1f6edbffcbf438260fadd/include/grpc/slice.h#L164-L166",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20598,336662420,2019-10-18T20:23:35Z,src/python/grpcio/grpc/experimental/aio/_server.py,"@@ -0,0 +1,175 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Server-side implementation of gRPC Asyncio Python.""""""++from typing import Text, Optional+import asyncio+import grpc+from grpc._cython import cygrpc+++class Server:+    """"""Serves RPCs.""""""++    def __init__(self, thread_pool, generic_handlers, interceptors, options,+                 maximum_concurrent_rpcs, compression):+        self._server = cygrpc.AioServer(thread_pool, generic_handlers,+                                        interceptors, options,+                                        maximum_concurrent_rpcs, compression)++    def add_generic_rpc_handlers(+            self,+            generic_rpc_handlers,+            # generic_rpc_handlers: Iterable[grpc.GenericRpcHandlers]+    ) -> None:+        """"""Registers GenericRpcHandlers with this Server.++        This method is only safe to call before the server is started.++        Args:+          generic_rpc_handlers: An iterable of GenericRpcHandlers that will be+          used to service RPCs.+        """"""+        self._server.add_generic_rpc_handlers(generic_rpc_handlers)++    def add_insecure_port(self, address: Text) -> int:+        """"""Opens an insecure port for accepting RPCs.++        This method may only be called before starting the server.++        Args:+          address: The address for which to open a port. If the port is 0,+            or not specified in the address, then gRPC runtime will choose a port.++        Returns:+          An integer port on which server will accept RPC requests.+        """"""+        return self._server.add_insecure_port(address)++    def add_secure_port(self, address: Text,+                        server_credentials: grpc.ServerCredentials) -> int:+        """"""Opens a secure port for accepting RPCs.++        This method may only be called before starting the server.++        Args:+          address: The address for which to open a port.+            if the port is 0, or not specified in the address, then gRPC+            runtime will choose a port.+          server_credentials: A ServerCredentials object.++        Returns:+          An integer port on which server will accept RPC requests.+        """"""+        return self._server.add_secure_port(address, server_credentials)++    async def start(self) -> None:+        """"""Starts this Server.++        This method may only be called once. (i.e. it is not idempotent).+        """"""+        await self._server.start()++    def stop(self, grace: Optional[float]) -> asyncio.Event:+        """"""Stops this Server.++        This method immediately stop service of new RPCs in all cases.++        If a grace period is specified, this method returns immediately+        and all RPCs active at the end of the grace period are aborted.+        If a grace period is not specified (by passing None for `grace`),+        all existing RPCs are aborted immediately and this method+        blocks until the last RPC handler terminates.++        This method is idempotent and may be called at any time.+        Passing a smaller grace value in a subsequent call will have+        the effect of stopping the Server sooner (passing None will+        have the effect of stopping the server immediately). Passing+        a larger grace value in a subsequent call *will not* have the+        effect of stopping the server later (i.e. the most restrictive+        grace value is used).++        Args:+          grace: A duration of time in seconds or None.++        Returns:+          A threading.Event that will be set when this Server has completely+          stopped, i.e. when running RPCs either complete or are aborted and+          all handlers have terminated.+        """"""+        raise NotImplementedError()++    async def wait_for_termination(self,+                                   timeout: Optional[float] = None) -> bool:+        """"""Block current thread until the server stops.++        This is an EXPERIMENTAL API.++        The wait will not consume computational resources during blocking, and+        it will block until one of the two following conditions are met:++        1) The server is stopped or terminated;+        2) A timeout occurs if timeout is not `None`.++        The timeout argument works in the same way as `threading.Event.wait()`.+        https://docs.python.org/3/library/threading.html#threading.Event.wait++        Args:+          timeout: A floating point number specifying a timeout for the+            operation in seconds.++        Returns:+          A bool indicates if the operation times out.+        """"""+        if timeout:+            raise NotImplementedError()+        # TODO(lidiz) replace this wait forever logic+        future = asyncio.get_event_loop().create_future()+        await future+++def server(thread_pool=None,+           handlers=None,+           interceptors=None,+           options=None,+           maximum_concurrent_rpcs=None,+           compression=None):+    """"""Creates a Server with which RPCs can be serviced.++    Args:+      thread_pool: A futures.ThreadPoolExecutor to be used by the Server+        to execute RPC handlers.",My two cents: We should move `thread_pool` to the end of the arguments list and name it something like `migration_thread_pool`. The documentation for it should also clarify the use case.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20598,336669699,2019-10-18T20:46:02Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -0,0 +1,260 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++cdef class _HandlerCallDetails:+    def __cinit__(self, str method, tuple invocation_metadata):+        self.method = method+        self.invocation_metadata = invocation_metadata+++class _ServicerContextPlaceHolder(object): pass+++# TODO(https://github.com/grpc/grpc/issues/20669)+# Apply this to the client-side+cdef class CallbackWrapper:+    cdef CallbackContext context+    cdef object _keep_reference++    def __cinit__(self, object future):+        self.context.functor.functor_run = self.functor_run+        self.context.waiter = <cpython.PyObject*>(future)+        self._keep_reference = future++    @staticmethod+    cdef void functor_run(+            grpc_experimental_completion_queue_functor* functor,+            int succeed):+        cdef CallbackContext *context = <CallbackContext *>functor+        if succeed == 0:+            (<object>context.waiter).set_exception(RuntimeError())+        else:+            (<object>context.waiter).set_result(None)++    cdef grpc_experimental_completion_queue_functor *c_functor(self):+        return &self.context.functor+++cdef class RPCState:++    def __cinit__(self):+        grpc_metadata_array_init(&self.request_metadata)+        grpc_call_details_init(&self.details)++    cdef bytes method(self):+      return _slice_bytes(self.details.method)++    def __dealloc__(self):+        """"""Cleans the Core objects.""""""+        grpc_call_details_destroy(&self.details)+        grpc_metadata_array_destroy(&self.request_metadata)+        if self.call:+            grpc_call_unref(self.call)+++cdef _find_method_handler(RPCState rpc_state, list generic_handlers):+    # TODO(lidiz) connects Metadata to call details+    cdef _HandlerCallDetails handler_call_details = _HandlerCallDetails(+        rpc_state.method().decode(),+        tuple()+    )++    for generic_handler in generic_handlers:+        method_handler = generic_handler.service(handler_call_details)+        if method_handler is not None:+            return method_handler+    return None+++async def callback_start_batch(RPCState rpc_state, tuple operations, object+loop):+    """"""The callback version of start batch operations.""""""+    cdef _BatchOperationTag batch_operation_tag = _BatchOperationTag(None, operations, None)+    batch_operation_tag.prepare()++    cdef object future = loop.create_future()+    cdef CallbackWrapper wrapper = CallbackWrapper(future)+    # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+    # when calling ""await"". This is an over-optimization by Cython.+    cpython.Py_INCREF(wrapper)+    cdef grpc_call_error error = grpc_call_start_batch(+        rpc_state.call,+        batch_operation_tag.c_ops,+        batch_operation_tag.c_nops,+        wrapper.c_functor(), NULL)++    if error != GRPC_CALL_OK:+        raise RuntimeError(""Error with callback_start_batch {}"".format(error))++    await future+    cpython.Py_DECREF(wrapper)+    cdef grpc_event c_event+    batch_operation_tag.event(c_event)+++async def _handle_unary_unary_rpc(object method_handler, RPCState rpc_state, object loop):+    # Receives request message+    cdef tuple receive_ops = (+        ReceiveMessageOperation(_EMPTY_FLAGS),+    )+    await callback_start_batch(rpc_state, receive_ops, loop)++    # Deserializes the request message+    cdef bytes request_raw = receive_ops[0].message()+    cdef object request_message+    if method_handler.request_deserializer:+        request_message = method_handler.request_deserializer(request_raw)+    else:+        request_message = request_raw++    # Executes application logic+    cdef object response_message = await method_handler.unary_unary(request_message, _ServicerContextPlaceHolder())++    # Serializes the response message+    cdef bytes response_raw+    if method_handler.response_serializer:+        response_raw = method_handler.response_serializer(response_message)+    else:+        response_raw = response_message++    # Sends response message+    cdef tuple send_ops = (+        SendStatusFromServerOperation(+        tuple(), StatusCode.ok, b'', _EMPTY_FLAGS),+        SendInitialMetadataOperation(tuple(), _EMPTY_FLAGS),+        SendMessageOperation(response_raw, _EMPTY_FLAGS),+    )+    await callback_start_batch(rpc_state, send_ops, loop)+++async def _handle_rpc(_AioServerState server_state, RPCState rpc_state, object loop):",This could just take a list of generic_handlers instead of the whole `_AioServerState object`.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20598,336715319,2019-10-19T00:59:45Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pxd.pxi,"@@ -0,0 +1,43 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++cdef class _HandlerCallDetails:+    cdef readonly str method+    cdef readonly tuple invocation_metadata+++cdef class RPCState:+    cdef grpc_call* call,+    cdef grpc_call_details details+    cdef grpc_metadata_array request_metadata++    cdef bytes method(self)+++cdef enum AioServerStatus:+    AIO_SERVER_STATUS_UNKNOWN+    AIO_SERVER_STATUS_READY+    AIO_SERVER_STATUS_RUNNING+    AIO_SERVER_STATUS_STOPPED+++cdef class _AioServerState:+    cdef Server server+    cdef grpc_completion_queue *cq+    cdef list generic_handlers+    cdef AioServerStatus status+++cdef class AioServer:+    cdef _AioServerState _state","`_AioServerState` removed. Since Cython cannot use C pointers in Python function signature, I added a `_CallbackCompletionQueue` class to wrap the C pointer to completion queue.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20629,337116608,2019-10-21T16:30:08Z,templates/CMakeLists.txt.template,"@@ -264,13 +264,18 @@     endforeach()   endfunction() -  add_custom_target(plugins-    DEPENDS+  set(_gRPC_PLUGIN_LIST)   % for tgt in targets:   % if tgt.build == 'protoc':-    ${tgt.name}+  option(gRPC_BUILD_${tgt.name.upper()} ""Build ${tgt.name}"" ON)",I think the logic is complex enough that it deserves a comment here in the template (that this generates  options to disable/enable installation of protoc plugin and perhaps an example of how will such option look like). WDYT?,X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/18856,337130403,2019-10-21T16:59:21Z,include/grpcpp/impl/codegen/server_context_impl.h,"@@ -272,6 +279,22 @@ class ServerContext {   /// Applications never need to call this method.   grpc_call* c_call() { return call_; } +  /// NOTE: This is an API for advanced users who need custom allocators.+  /// Get and maybe mutate the allocator state associated with the current RPC.+  /// Currently only applicable for callback unary RPC methods.+  /// WARNING: This is experimental API and could be changed or removed.+  ::grpc::experimental::RpcAllocatorState* GetRpcAllocatorState() {+    return message_allocator_state_;+  }++  /// Get the default unary reactor for use in minimal reaction cases.+  /// WARNING: This is experimental API and could be changed or removed.+  experimental::ServerUnaryReactor* DefaultReactor() {","I'd like to ban it so that we can do something in this case later (like lazy initialization as Alan suggested). That said, I don't know if banning is the right decision so currently it's ok to call it multiple times. We can decide if that's the right choice.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20598,337141750,2019-10-21T17:24:26Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -0,0 +1,248 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++cdef class _HandlerCallDetails:+    def __cinit__(self, str method, tuple invocation_metadata):+        self.method = method+        self.invocation_metadata = invocation_metadata+++class _ServicerContextPlaceHolder(object): pass+++cdef class CallbackWrapper:+    cdef CallbackContext context+    cdef object future++    def __cinit__(self, object future):+        self.context.functor.functor_run = self.functor_run+        self.context.waiter = <cpython.PyObject*>(future)+        self.future = future++    @staticmethod+    cdef void functor_run(+            grpc_experimental_completion_queue_functor* functor,+            int succeed):+        cdef CallbackContext *context = <CallbackContext *>functor+        (<object>context.waiter).set_result(None)++    cdef grpc_experimental_completion_queue_functor *c_functor(self):+        return &self.context.functor+++cdef class RPCState:++    def __cinit__(self):+        grpc_metadata_array_init(&self.request_metadata)+        grpc_call_details_init(&self.details)++    cdef bytes method(self):+      return _slice_bytes(self.details.method)++    def __dealloc__(self):+        """"""Cleans the Core objects.""""""+        grpc_call_details_destroy(&self.details)+        grpc_metadata_array_destroy(&self.request_metadata)+        if self.call:+            grpc_call_unref(self.call)+++cdef _find_method_handler(RPCState rpc_state, list generic_handlers):+    # TODO(lidiz) connects Metadata to call details+    cdef _HandlerCallDetails handler_call_details = _HandlerCallDetails(+        rpc_state.method().decode(),+        tuple()+    )++    for generic_handler in generic_handlers:+        method_handler = generic_handler.service(handler_call_details)+        if method_handler is not None:+            return method_handler+    return None+++async def callback_start_batch(RPCState rpc_state, tuple operations, object+loop):+    """"""The callback version of start batch operations.""""""+    cdef _BatchOperationTag batch_operation_tag = _BatchOperationTag(None, operations, None)+    batch_operation_tag.prepare()++    cdef object future = loop.create_future()+    cdef CallbackWrapper wrapper = CallbackWrapper(future)+    # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+    # when calling ""await"". This is an over-optimization by Cython.+    cpython.Py_INCREF(wrapper)+    cdef grpc_call_error error = grpc_call_start_batch(+        rpc_state.call,+        batch_operation_tag.c_ops,+        batch_operation_tag.c_nops,+        wrapper.c_functor(), NULL)++    if error != GRPC_CALL_OK:+        raise RuntimeError(""Error with callback_start_batch {}"".format(error))++    await future+    cpython.Py_DECREF(wrapper)+    cdef grpc_event c_event+    batch_operation_tag.event(c_event)+++async def _handle_unary_unary_rpc(object method_handler, RPCState rpc_state, object loop):+    # Receives request message+    cdef tuple receive_ops = (+        ReceiveMessageOperation(_EMPTY_FLAGS),+    )+    await callback_start_batch(rpc_state, receive_ops, loop)++    # Deserializes the request message+    cdef bytes request_raw = receive_ops[0].message()+    cdef object request_message+    if method_handler.request_deserializer:+        request_message = method_handler.request_deserializer(request_raw)+    else:+        request_message = request_raw++    # Executes application logic+    cdef object response_message = await method_handler.unary_unary(request_message, _ServicerContextPlaceHolder())++    # Serializes the response message+    cdef bytes response_raw+    if method_handler.response_serializer:+        response_raw = method_handler.response_serializer(response_message)+    else:+        response_raw = response_message++    # Sends response message+    cdef tuple send_ops = (+        SendStatusFromServerOperation(+        tuple(), StatusCode.ok, b'', _EMPTY_FLAGS),+        SendInitialMetadataOperation(tuple(), _EMPTY_FLAGS),+        SendMessageOperation(response_raw, _EMPTY_FLAGS),+    )+    await callback_start_batch(rpc_state, send_ops, loop)+++async def _handle_rpc(_AioServerState server_state, RPCState rpc_state, object loop):+    # Finds the method handler (application logic)+    cdef object method_handler = _find_method_handler(+        rpc_state,+        server_state.generic_handlers+    )+    if method_handler is None:+        # TODO(lidiz) return unimplemented error to client side+        raise NotImplementedError()+    # TODO(lidiz) extend to all 4 types of RPC+    if method_handler.request_streaming or method_handler.response_streaming:+        raise NotImplementedError()+    else:+        await _handle_unary_unary_rpc(+            method_handler,+            rpc_state,+            loop+        )+++async def _server_call_request_call(_AioServerState server_state, object loop):+    cdef grpc_call_error error+    cdef RPCState rpc_state = RPCState()+    cdef object future = loop.create_future()+    cdef CallbackWrapper wrapper = CallbackWrapper(future)+    # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+    # when calling ""await"". This is an over-optimization by Cython.+    cpython.Py_INCREF(wrapper)+    error = grpc_server_request_call(+        server_state.server.c_server, &rpc_state.call, &rpc_state.details,+        &rpc_state.request_metadata,+        server_state.cq, server_state.cq,+        wrapper.c_functor()+    )+    if error != GRPC_CALL_OK:+        raise RuntimeError(""Error in _server_call_request_call: %s"" % error)++    await future+    cpython.Py_DECREF(wrapper)+    return rpc_state+++async def _server_main_loop(_AioServerState server_state):+    cdef object loop = asyncio.get_event_loop()+    cdef RPCState rpc_state+    cdef object waiter++    while True:+        rpc_state = await _server_call_request_call(+            server_state,+            loop)+        # await waiter++        loop.create_task(_handle_rpc(server_state, rpc_state, loop))+        await asyncio.sleep(0)","I have to say, I was wrong about the hanging. I did benchmark with and without `asyncio.sleep(0)`. Turn out, the without version performs better in short bursts. In longer scenario, the QPS is equivalent to the original version. The per RPC latency is not impacted by this change.```lidiz@ddev:grpc-go-client$ go run benchmarker.goFinished 40000 RPCs in 2.197975732s, QPS 18198.56Through put is 17.8 KiB per secondlidiz@ddev:grpc-go-client$ go run benchmarker.go -t 1000000Finished 1000000 RPCs in 1m20.167130808s, QPS 12473.94Through put is 12.2 KiB per second```The `asyncio.sleep(0)` is removed.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20708,337199180,2019-10-21T19:32:00Z,src/core/lib/slice/b64.cc,"@@ -65,7 +65,7 @@ char* grpc_base64_encode(const void* vdata, size_t data_size, int url_safe,   return result; } -size_t grpc_base64_estimate_encoded_size(size_t data_size, int url_safe,+size_t grpc_base64_estimate_encoded_size(size_t data_size, int /*url_safe*/,","This isn't implementing an interface, so if the parameter is not used, it should probably just be removed.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20708,337199800,2019-10-21T19:33:32Z,src/core/lib/surface/call_log_batch.cc,"@@ -108,8 +108,8 @@ char* grpc_op_string(const grpc_op* op) { }  void grpc_call_log_batch(const char* file, int line, gpr_log_severity severity,-                         grpc_call* call, const grpc_op* ops, size_t nops,-                         void* tag) {+                         grpc_call* /*call*/, const grpc_op* ops, size_t nops,","Similar comment here: If these params are unused, we should probably just remove them.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20708,337200867,2019-10-21T19:36:12Z,src/core/lib/surface/channel.cc,"@@ -513,8 +513,8 @@ void grpc_channel_destroy(grpc_channel* channel) {   GRPC_CHANNEL_INTERNAL_UNREF(channel, ""channel""); } -grpc_mdelem grpc_channel_get_reffed_status_elem_slowpath(grpc_channel* channel,-                                                         int i) {+grpc_mdelem grpc_channel_get_reffed_status_elem_slowpath(+    grpc_channel* /*channel*/, int i) {","This seems like another case where we should consider removing the parameter, although there may actually be a deeper problem here, because it seems strange for a `grpc_channel_*()` method to not actually use the channel for anything... Maybe this method should actually be renamed and possibly moved elsewhere.",X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/20704,337227707,2019-10-21T20:38:39Z,test/core/debug/stats_test.cc,"@@ -151,5 +151,9 @@ int main(int argc, char** argv) {   int ret = RUN_ALL_TESTS();   grpc_shutdown();   return ret;+#else+  // Avoid unused parameter warning for conditional parameters.+  (void)argc;","Yeah, this basically evaluates something (anything) and ignores the result. https://en.cppreference.com/w/c/language/cast : ""If type-name is void, then expression is evaluated for its side-effects and its returned value is discarded, same as when expression is used on its own, as an expression statement."" Since the result would then be a dead-value, a simple expression like this would get optimized out.",
35502996,KyleFromKitware,https://api.github.com/repos/grpc/grpc/pulls/20629,337240836,2019-10-21T21:09:13Z,templates/CMakeLists.txt.template,"@@ -356,18 +361,18 @@   % for tgt in targets:   % if tgt.build in [""all"", ""protoc"", ""tool"", ""test"", ""private""] and not tgt.boringssl:   % if tgt.build in [""test"", ""private""]:-  if (gRPC_BUILD_TESTS)+  if(gRPC_BUILD_TESTS)","It is indeed not the recommended style, though obviously there are many projects that use it. I thought it was a mistake and did not realize it was intentional. However, duplicating the condition in the `endif()` block is even *more* not-recommended... is it OK if I at least get rid of that for this particular `if()` block?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20721,337251446,2019-10-21T21:36:12Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -2908,7 +2908,7 @@ void CallData::RecvMessageReady(void* arg, grpc_error* error) { // recv_trailing_metadata handling // -void CallData::GetCallStatus(grpc_call_element* elem,+void CallData::GetCallStatus(grpc_call_element* /*elem*/,","If this isn't used, how about just removing the parameter?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20718,337253228,2019-10-21T21:41:00Z,src/core/ext/filters/http/client/http_client_filter.cc,"@@ -99,7 +99,7 @@ struct channel_data { }; }  // namespace -static grpc_error* client_filter_incoming_metadata(grpc_call_element* elem,+static grpc_error* client_filter_incoming_metadata(grpc_call_element* /*elem*/,",I think we can just remove this parameter.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20718,337253382,2019-10-21T21:41:31Z,src/core/ext/filters/http/server/http_server_filter.cc,"@@ -99,7 +99,7 @@ struct channel_data {  }  // namespace -static grpc_error* hs_filter_outgoing_metadata(grpc_call_element* elem,+static grpc_error* hs_filter_outgoing_metadata(grpc_call_element* /*elem*/,",I think we can just remove this parameter.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20720,337255115,2019-10-21T21:46:36Z,src/core/ext/filters/client_channel/lb_policy/subchannel_list.h,"@@ -281,7 +282,7 @@ SubchannelData<SubchannelListType, SubchannelDataType>::~SubchannelData() {  template <typename SubchannelListType, typename SubchannelDataType> void SubchannelData<SubchannelListType, SubchannelDataType>::-    UnrefSubchannelLocked(const char* reason) {+    UnrefSubchannelLocked(const char* /*reason*/) {",This parameter shouldn't be unused.  It should be logged as part of the message on line 288 below.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20691,337384280,2019-10-22T08:26:36Z,src/csharp/Grpc.Core.Api/SerializationContext.cs,"@@ -39,11 +39,22 @@ public virtual void Complete(byte[] payload)          /// <summary>         /// Gets buffer writer that can be used to write the serialized data. Once serialization is finished,-        /// <c>Complete()</c> needs to be called.+        /// <c>Complete()</c> needs to be called. A <c>null</c> value will be returned if serialization+        /// with a buffer writer is not supported.         /// </summary>         public virtual IBufferWriter<byte> GetBufferWriter()         {-            throw new NotImplementedException();+            return null;+        }++        /// <summary>+        /// Sets the payload length when writing serialized data a buffer writer. This method should be called before <c>GetBufferWriter</c>.","nit: Suggest improving the comment:""Sets the payload length when writing serialized data into a buffer writer. If the serializer knows the full payload length in advance, providing that information before obtaining the buffer writer using <c>GetBufferWriter</c> can improve serialization efficiency by avoiding copies. Calling this method is optional.... ""we should also add a note about what happens if the payloadLength is provided, but it doesn't match the actual amount of data written by the time Complete() is run.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20629,337405672,2019-10-22T09:12:43Z,templates/CMakeLists.txt.template,"@@ -356,18 +361,18 @@   % for tgt in targets:   % if tgt.build in [""all"", ""protoc"", ""tool"", ""test"", ""private""] and not tgt.boringssl:   % if tgt.build in [""test"", ""private""]:-  if (gRPC_BUILD_TESTS)+  if(gRPC_BUILD_TESTS)","sg, but please file an issue with the list of cmake style problems that we currently have in our CMakeLists.txt?also, is there some good linter for cmake files?",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/20708,337910746,2019-10-23T08:25:51Z,src/core/lib/surface/channel.cc,"@@ -513,8 +513,8 @@ void grpc_channel_destroy(grpc_channel* channel) {   GRPC_CHANNEL_INTERNAL_UNREF(channel, ""channel""); } -grpc_mdelem grpc_channel_get_reffed_status_elem_slowpath(grpc_channel* channel,-                                                         int i) {+grpc_mdelem grpc_channel_get_reffed_status_elem_slowpath(+    grpc_channel* /*channel*/, int i) {","This is 4-year old code originally added by @ctiller in #4188. Before that point, this function actually used the channel parameter. After #4188, the function had no more explicit reference to the channel but still kept its name and place. It was later split into an inline header and slowpath source component by @arjunroy in #19101 .",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/20708,337926487,2019-10-23T08:58:01Z,src/core/lib/surface/channel.cc,"@@ -513,8 +513,8 @@ void grpc_channel_destroy(grpc_channel* channel) {   GRPC_CHANNEL_INTERNAL_UNREF(channel, ""channel""); } -grpc_mdelem grpc_channel_get_reffed_status_elem_slowpath(grpc_channel* channel,-                                                         int i) {+grpc_mdelem grpc_channel_get_reffed_status_elem_slowpath(+    grpc_channel* /*channel*/, int i) {","Found the perfect place to put it: `src/core/lib/transport/status_metadata.{h,cc}` which already exist and turn a metadatum into a status_code . These functions just reverse those.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20753,338186153,2019-10-23T17:41:07Z,src/python/grpcio/grpc/__init__.py,"@@ -1953,6 +1953,17 @@ class Compression(enum.IntEnum):     Gzip = _compression.Gzip  +class ChannelOptions(object):+    """"""Indicates a channel option unique to gRPC Python.++     This enumeration is part of an EXPERIMENTAL API.++     Attributes:+       SingleThreadedUnaryStream: Perform unary-stream RPCs on a single thread.+  """"""+    SingleThreadedUnaryStream = ""SingleThreadedUnaryStream""","Also, I always support introducing `ChannelOptions` to our surface API. But I think we shouldn't create a dedicated class just yet. Alternatively, we can document it.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20753,338189624,2019-10-23T17:47:48Z,src/python/grpcio/grpc/_channel.py,"@@ -636,6 +719,55 @@ def future(self,                                deadline)  +class _SingleThreadedUnaryStreamMultiCallable(grpc.UnaryStreamMultiCallable):++    # pylint: disable=too-many-arguments+    def __init__(self, channel, method, request_serializer,+                 response_deserializer):+        self._channel = channel+        self._method = method+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer+        self._context = cygrpc.build_census_context()++    def __call__(  # pylint: disable=too-many-locals+            self,+            request,+            timeout=None,+            metadata=None,+            credentials=None,+            wait_for_ready=None,+            compression=None):+        deadline = _deadline(timeout)+        serialized_request = _common.serialize(request,+                                               self._request_serializer)+        if serialized_request is None:+            state = _RPCState((), (), (), grpc.StatusCode.INTERNAL,+                              'Exception serializing request!')+            raise _Rendezvous(state, None, None, deadline)","Should we use the same pattern as `_abort(state, code, details)`?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20753,338190457,2019-10-23T17:49:24Z,src/python/grpcio/grpc/_channel.py,"@@ -636,6 +719,55 @@ def future(self,                                deadline)  +class _SingleThreadedUnaryStreamMultiCallable(grpc.UnaryStreamMultiCallable):++    # pylint: disable=too-many-arguments+    def __init__(self, channel, method, request_serializer,+                 response_deserializer):+        self._channel = channel+        self._method = method+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer+        self._context = cygrpc.build_census_context()++    def __call__(  # pylint: disable=too-many-locals+            self,+            request,+            timeout=None,+            metadata=None,+            credentials=None,+            wait_for_ready=None,+            compression=None):+        deadline = _deadline(timeout)+        serialized_request = _common.serialize(request,+                                               self._request_serializer)+        if serialized_request is None:+            state = _RPCState((), (), (), grpc.StatusCode.INTERNAL,+                              'Exception serializing request!')+            raise _Rendezvous(state, None, None, deadline)++        state = _RPCState(_UNARY_STREAM_INITIAL_DUE, None, None, None, None)+        call_credentials = None if credentials is None else credentials._credentials+        initial_metadata_flags = _InitialMetadataFlags().with_wait_for_ready(+            wait_for_ready)+        augmented_metadata = _compression.augment_metadata(+            metadata, compression)+        operations_and_tags = (+            ((cygrpc.SendInitialMetadataOperation(augmented_metadata,+                                                  initial_metadata_flags),+              cygrpc.SendMessageOperation(serialized_request, _EMPTY_FLAGS),+              cygrpc.SendCloseFromClientOperation(_EMPTY_FLAGS),+              cygrpc.ReceiveStatusOnClientOperation(_EMPTY_FLAGS)), None),+            ((cygrpc.ReceiveInitialMetadataOperation(_EMPTY_FLAGS),), None),+        )","Can we break this list into multiple variables, and combine them back together? The parentheses is moved by YAPF. ",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20753,338190942,2019-10-23T17:50:19Z,src/python/grpcio/grpc/_channel.py,"@@ -1042,6 +1174,18 @@ def _augment_options(base_options, compression):     ),)  +def _separate_channel_options(options):+    """"""Separates core channel options from Python channel options.""""""+    core_options = []+    python_options = []+    for pair in options:+        if pair[0] == grpc.ChannelOptions.SingleThreadedUnaryStream:+            python_options.append(pair)+        else:+            core_options.append(pair)+    return python_options, core_options",`ChannelOptions` can be a private enum class.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20753,338194354,2019-10-23T17:56:48Z,src/python/grpcio_tests/tests/unit/_metadata_test.py,"@@ -202,6 +202,9 @@ def testUnaryUnary(self):     def testUnaryStream(self):         multi_callable = self._channel.unary_stream(_UNARY_STREAM)         call = multi_callable(_REQUEST, metadata=_INVOCATION_METADATA)+        # NOTE(gnossen): In the single-threaded case, we must consume at least+        # one message before the initial metadata will show up.+        next(call)",This seems like another regression. Is it possible to split the initial invocation of `start_batch` into two? And try to receive the `initial_metadata` before receiving messages.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20753,338195653,2019-10-23T17:59:16Z,src/python/grpcio/grpc/_channel.py,"@@ -267,7 +298,152 @@ def cancel(self):                 self._state.cancelled = True                 _abort(self._state, code, details)                 self._state.condition.notify_all()-            return False+                return True+            else:+                return False++    def add_callback(self, callback):+        """"""See grpc.RpcContext.add_callback""""""+        with self._state.condition:+            if self._state.callbacks is None:+                return False+            else:+                self._state.callbacks.append(callback)+                return True++    def initial_metadata(self):+        """"""See grpc.Call.initial_metadata""""""+        with self._state.condition:++            def _done():+                return self._state.initial_metadata is not None++            _common.wait(self._state.condition.wait, _done)",We might want to change the `condition.wait` to do other things that either move the RPC forward or notify application that the info is not ready.Applies to all `condition.wait` in `_SingleThreadedRendezvous`.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20753,338212480,2019-10-23T18:34:29Z,src/python/grpcio/grpc/_channel.py,"@@ -636,6 +719,55 @@ def future(self,                                deadline)  +class _SingleThreadedUnaryStreamMultiCallable(grpc.UnaryStreamMultiCallable):++    # pylint: disable=too-many-arguments+    def __init__(self, channel, method, request_serializer,+                 response_deserializer):+        self._channel = channel+        self._method = method+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer+        self._context = cygrpc.build_census_context()++    def __call__(  # pylint: disable=too-many-locals+            self,+            request,+            timeout=None,+            metadata=None,+            credentials=None,+            wait_for_ready=None,+            compression=None):+        deadline = _deadline(timeout)+        serialized_request = _common.serialize(request,+                                               self._request_serializer)+        if serialized_request is None:+            state = _RPCState((), (), (), grpc.StatusCode.INTERNAL,+                              'Exception serializing request!')+            raise _Rendezvous(state, None, None, deadline)","This raises the same exception as the unary-unary code path does. If anything, we should refactor https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_channel.py#L486 to have a more sensible interface.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20753,338215933,2019-10-23T18:41:26Z,src/python/grpcio_tests/tests/unit/_metadata_test.py,"@@ -202,6 +202,9 @@ def testUnaryUnary(self):     def testUnaryStream(self):         multi_callable = self._channel.unary_stream(_UNARY_STREAM)         call = multi_callable(_REQUEST, metadata=_INVOCATION_METADATA)+        # NOTE(gnossen): In the single-threaded case, we must consume at least+        # one message before the initial metadata will show up.+        next(call)","As far as I know, [`completion_queue_next`](https://github.com/grpc/grpc/blob/master/include/grpc/grpc.h#L135) gives us no guarantees about the order in which events will appear on the completion queue. So in order to make this happen, we'd have to do some level of buffering within the Rendezvous. Given that this change is fundamentally about performance and our target users are not blocked by this, I don't see it as being worth it within this PR.Adding as a TODO.",
2873805,rmstar,https://api.github.com/repos/grpc/grpc/pulls/20767,338255484,2019-10-23T20:14:08Z,src/objective-c/GRPCClient/GRPCCall.h,"@@ -55,43 +57,48 @@ NS_ASSUME_NONNULL_BEGIN - (void)didReceiveInitialMetadata:(nullable NSDictionary *)initialMetadata;  /**- * This method is deprecated and does not work with interceptors. To use GRPCCall2 interface with- * interceptor, implement didReceiveData: instead. To implement an interceptor, please leave this- * method unimplemented and implement didReceiveData: method instead. If this method and- * didReceiveRawMessage are implemented at the same time, implementation of this method will be- * ignored.- *  * Issued when a message is received from the server. The message is the raw data received from the  * server, with decompression and without proto deserialization.+ *+ * <b> This method is deprecated and does not work with interceptors.</b> To use GRPCCall2+ * interface with interceptor, implement didReceiveData: instead. To implement an interceptor,+ * please leave this method unimplemented and implement GRPCResponseHander::didReceiveData method+ * instead. If this method and didReceiveRawMessage are implemented at the same time, implementation+ * of this method will be ignored.  */ - (void)didReceiveRawMessage:(nullable NSData *)message;  /**- * Issued when a decompressed message is received from the server. The message is decompressed, and- * deserialized if a marshaller is provided to the call (marshaller is work in progress).+ * Issued when gRPC message is received from the server. The data is always decompressed with gRPC+ * or HTTP compression algorithm specified in the response header. \p data could be any type as+ * transport layer and interceptors may modify the type of the data (e.g. a Protobuf interceptor may+ * consume NSData typed data and issue GPBMessage typed data to user). Users should interpret \p+ * data according to the configuration of their calls. Interceptor authors should not assume the+ * type of \p data unless an agreement is made on how it should be used in a particular call+ * setting.  */ - (void)didReceiveData:(id)data;  /**- * Issued when a call finished. If the call finished successfully, \a error is nil and \a- * trainingMetadata consists any trailing metadata received from the server. Otherwise, \a error+ * Issued when a call finished. If the call finished successfully, \p error is nil and \p+ * trainingMetadata consists any trailing metadata received from the server. Otherwise, \p error",trailingMetadata,
6241635,coryan,https://api.github.com/repos/grpc/grpc/pulls/20565,338275809,2019-10-23T20:59:33Z,src/core/lib/security/credentials/alts/check_gcp_environment_windows.cc,"@@ -31,80 +31,70 @@ #include <grpc/support/log.h> #include <grpc/support/sync.h> -#define GRPC_ALTS_EXPECT_NAME_GOOGLE ""Google""-#define GRPC_ALTS_WINDOWS_CHECK_COMMAND ""powershell.exe""-#define GRPC_ALTS_WINDOWS_CHECK_COMMAND_ARGS \-  ""(Get-WmiObject -Class Win32_BIOS).Manufacturer""-#define GRPC_ALTS_WINDOWS_CHECK_BIOS_FILE ""windows_bios.data""--const size_t kBiosDataBufferSize = 256;--static bool g_compute_engine_detection_done = false;-static bool g_is_on_compute_engine = false;-static gpr_mu g_mu;-static gpr_once g_once = GPR_ONCE_INIT;- namespace grpc_core { namespace internal { -bool check_bios_data(const char* bios_data_file) {-  char* bios_data = read_bios_file(bios_data_file);-  bool result = !strcmp(bios_data, GRPC_ALTS_EXPECT_NAME_GOOGLE);-  remove(GRPC_ALTS_WINDOWS_CHECK_BIOS_FILE);-  gpr_free(bios_data);-  return result;+bool check_bios_data(const char*) {","Maybe not, I can remove it.  But it is declared here:https://github.com/grpc/grpc/blob/2b06d76cc06e2fa4016713c1cabec510a4f89e87/src/core/lib/security/credentials/alts/check_gcp_environment.h#L36-L45Which would leave a dangling declaration, and that cannot be removed because it is used in the Linux test:https://github.com/grpc/grpc/blob/2b06d76cc06e2fa4016713c1cabec510a4f89e87/test/core/security/check_gcp_environment_linux_test.cc#L41Thoughts?  I think it might be better to have a future PR remove the declaration and *only* declare it on the Linux test and not the header file.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20765,338490632,2019-10-24T10:09:48Z,templates/CMakeLists.txt.template,"@@ -295,57 +293,57 @@   add_custom_target(tools     DEPENDS tools_c tools_cxx) -  if (gRPC_BUILD_TESTS)-  add_custom_target(buildtests_c)-  % for tgt in targets:-  % if tgt.build == 'test' and not tgt.language == 'c++' and not tgt.get('external_deps', None) and not tgt.boringssl:-  ${get_platforms_condition_begin(tgt.platforms)}\-  add_dependencies(buildtests_c ${tgt.name})-  ${get_platforms_condition_end(tgt.platforms)}\-  % endif-  % endfor--  add_custom_target(buildtests_cxx)-  % for tgt in targets:-  % if tgt.build == 'test' and tgt.language == 'c++' and not tgt.get('external_deps', None) and not tgt.boringssl:-  ${get_platforms_condition_begin(tgt.platforms)}\-  add_dependencies(buildtests_cxx ${tgt.name})-  ${get_platforms_condition_end(tgt.platforms)}\-  % endif-  % endfor+  if(gRPC_BUILD_TESTS)+    add_custom_target(buildtests_c)",looks like the indentation of add_dependencies() in CMakeLists.txt is wrong.```if(gRPC_BUILD_TESTS)  add_custom_target(buildtests_c)    add_dependencies(buildtests_c algorithm_test)      add_dependencies(buildtests_c alloc_test)      add_dependencies(buildtests_c alpn_test)      add_dependencies(buildtests_c arena_test)      add_dependencies(buildtests_c avl_test)      add_dependencies(buildtests_c bad_server_response_test)      add_dependencies(buildtests_c bin_decoder_test)      add_dependencies(buildtests_c bin_encoder_test)    if(_gRPC_PLATFORM_LINUX)  add_dependencies(buildtests_c buffer_list_test)  endif()    add_dependencies(buildtests_c channel_create_test)      add_dependencies(buildtests_c chttp2_hpack_encoder_test)      add_dependencies(buildtests_c chttp2_stream_map_test)      add_dependencies(buildtests_c chttp2_varint_test)    if(_gRPC_PLATFORM_LINUX OR _gRPC_PLATFORM_MAC OR _gRPC_PLATFORM_POSIX)  add_dependencies(buildtests_c close_fd_test)  endif()```,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20770,338502762,2019-10-24T10:39:51Z,templates/CMakeLists.txt.template,"@@ -71,14 +71,20 @@    cmake_minimum_required(VERSION 3.5.1) -  set(PACKAGE_NAME      ""grpc"")-  set(PACKAGE_VERSION   ""${settings.cpp_version}"")-  set(gRPC_CORE_VERSION ""${settings.core_version}"")-  set(PACKAGE_STRING    ""<%text>${PACKAGE_NAME} ${PACKAGE_VERSION}</%text>"")-  set(PACKAGE_TARNAME   ""<%text>${PACKAGE_NAME}-${PACKAGE_VERSION}</%text>"")-  set(PACKAGE_BUGREPORT ""https://github.com/grpc/grpc/issues/"")+  set(PACKAGE_NAME        ""grpc"")+  set(PACKAGE_VERSION     ""${settings.cpp_version}"")+  set(gRPC_CORE_VERSION   ""${settings.core_version}"")+  set(gRPC_CPP_VERSION    ""${settings.cpp_version}"")+  set(gRPC_CSHARP_VERSION ""${settings.csharp_version}"")+  set(PACKAGE_STRING      ""<%text>${PACKAGE_NAME} ${PACKAGE_VERSION}</%text>"")+  set(PACKAGE_TARNAME     ""<%text>${PACKAGE_NAME}-${PACKAGE_VERSION}</%text>"")+  set(PACKAGE_BUGREPORT   ""https://github.com/grpc/grpc/issues/"")   project(<%text>${PACKAGE_NAME}</%text> LANGUAGES C CXX) +  string(REGEX REPLACE ""^([0-9]+)\\..*$"" ""\\1"" gRPC_CORE_SOVERSION ""<%text>${gRPC_CORE_VERSION}</%text>"")","it's not immediately obvious what it does. If you're looking to extract the major version only, I think settings.cpp_version.major should just work.See https://github.com/grpc/grpc/blob/master/tools/buildgen/plugins/expand_version.py",X
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/20565,338655985,2019-10-24T15:53:30Z,src/core/lib/security/credentials/alts/check_gcp_environment_windows.cc,"@@ -31,80 +31,70 @@ #include <grpc/support/log.h> #include <grpc/support/sync.h> -#define GRPC_ALTS_EXPECT_NAME_GOOGLE ""Google""-#define GRPC_ALTS_WINDOWS_CHECK_COMMAND ""powershell.exe""-#define GRPC_ALTS_WINDOWS_CHECK_COMMAND_ARGS \-  ""(Get-WmiObject -Class Win32_BIOS).Manufacturer""-#define GRPC_ALTS_WINDOWS_CHECK_BIOS_FILE ""windows_bios.data""--const size_t kBiosDataBufferSize = 256;--static bool g_compute_engine_detection_done = false;-static bool g_is_on_compute_engine = false;-static gpr_mu g_mu;-static gpr_once g_once = GPR_ONCE_INIT;- namespace grpc_core { namespace internal { -bool check_bios_data(const char* bios_data_file) {-  char* bios_data = read_bios_file(bios_data_file);-  bool result = !strcmp(bios_data, GRPC_ALTS_EXPECT_NAME_GOOGLE);-  remove(GRPC_ALTS_WINDOWS_CHECK_BIOS_FILE);-  gpr_free(bios_data);-  return result;+bool check_bios_data(const char*) {","You are right, we cannot remove it yet. Let me or my teammates have a separate PR to move check_bios_data() to Linux only implementation.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20783,338697746,2019-10-24T17:25:11Z,WORKSPACE,"@@ -2,12 +2,15 @@ workspace(name = ""com_github_grpc_grpc"")  load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"") load(""//bazel:grpc_deps.bzl"", ""grpc_deps"", ""grpc_test_only_deps"")+load(""//bazel:grpc_python_deps.bzl"", ""grpc_python_deps"") load(""@bazel_tools//tools/build_defs/repo:git.bzl"", ""git_repository"")  grpc_deps()  grpc_test_only_deps() +grpc_python_deps()",This should be redundant. [It's already called by `grpc_deps()`.](https://github.com/grpc/grpc/blob/690e313c3886eb1e766a6dce2ecb9fe6db4591ac/bazel/grpc_deps.bzl#L233),
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20771,338800324,2019-10-24T21:27:36Z,src/python/grpcio_tests/tests_aio/unit/_test_server.py,"@@ -18,38 +18,27 @@ from time import sleep  import grpc+from grpc.experimental import aio from src.proto.grpc.testing import messages_pb2 from src.proto.grpc.testing import test_pb2_grpc from tests.unit.framework.common import test_constants  -# TODO (https://github.com/grpc/grpc/issues/19762)-# Change for an asynchronous server version once it's implemented. class TestServiceServicer(test_pb2_grpc.TestServiceServicer):","Now that this is used as a library and not a binary, this class should probably become private, i.e. `_TestServiceServicer`.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20765,338911606,2019-10-25T07:05:26Z,templates/CMakeLists.txt.template,"@@ -295,57 +298,53 @@   add_custom_target(tools     DEPENDS tools_c tools_cxx) -  if (gRPC_BUILD_TESTS)-  add_custom_target(buildtests_c)-  % for tgt in targets:-  % if tgt.build == 'test' and not tgt.language == 'c++' and not tgt.get('external_deps', None) and not tgt.boringssl:-  ${get_platforms_condition_begin(tgt.platforms)}\-  add_dependencies(buildtests_c ${tgt.name})-  ${get_platforms_condition_end(tgt.platforms)}\-  % endif-  % endfor+  if(gRPC_BUILD_TESTS)+    add_custom_target(buildtests_c)+    % for tgt in targets:+    % if tgt.build == 'test' and not tgt.language == 'c++' and not tgt.get('external_deps', None) and not tgt.boringssl:+    ${platforms_condition_add_dependencies(tgt, 'buildtests_c')}","nit: this does `add_dependencies(buildtests_c  tgt.name)` , but  the parameter order of platforms_condition_add_dependencies is the reverse`tgt, 'buildtests_c'`, that seems confusing",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20765,338913878,2019-10-25T07:13:47Z,templates/CMakeLists.txt.template,"@@ -295,57 +298,53 @@   add_custom_target(tools     DEPENDS tools_c tools_cxx) -  if (gRPC_BUILD_TESTS)-  add_custom_target(buildtests_c)-  % for tgt in targets:-  % if tgt.build == 'test' and not tgt.language == 'c++' and not tgt.get('external_deps', None) and not tgt.boringssl:-  ${get_platforms_condition_begin(tgt.platforms)}\-  add_dependencies(buildtests_c ${tgt.name})-  ${get_platforms_condition_end(tgt.platforms)}\-  % endif-  % endfor+  if(gRPC_BUILD_TESTS)+    add_custom_target(buildtests_c)+    % for tgt in targets:+    % if tgt.build == 'test' and not tgt.language == 'c++' and not tgt.get('external_deps', None) and not tgt.boringssl:+    ${platforms_condition_add_dependencies(tgt, 'buildtests_c')}","One more issue: I'm not sure if introduction of `platforms_condition_add_dependencies` actually makes things more readable, especially because we had to add whitespaces some of the functions to end up with the correct indentation. Would just```${get_platforms_condition_begin(tgt.platforms)}  add_dependencies(buildtests_c ${tgt.name})${get_platforms_condition_end(tgt.platforms)}```(and removing the \n and spaces from the python functions) work?",
7313747,Yannic,https://api.github.com/repos/grpc/grpc/pulls/20783,339017741,2019-10-25T12:03:41Z,third_party/six.BUILD,"@@ -1,6 +1,13 @@+genrule(+  name = ""copy_six"",+  srcs = [""six-1.12.0/six.py""],+  outs = [""__init__.py""],+  cmd = ""cp $< $(@)"",+)+ py_library(-    name = ""six"",-    srcs = [""six.py""],-    srcs_version = ""PY2AND3"",-    visibility = [""//visibility:public""],+  name = ""six"",+  srcs = [""__init__.py""],","Actually, there is an upcoming incompatible change that breaks with the `strip_prefix` version (Thanks to @aaliddell for pointing it out in https://github.com/protocolbuffers/protobuf/pull/6795#issuecomment-546060749).> ... However, we can't know at the @six//:six py_library creation time what the legacy_create_init flag might be set to when anyone creates a py_binary that depends on six. Since a number of other repos depend on @six, we can't just fix all the py_binary definitions in protobuf. Therefore, by moving six.py to __init__.py, we force it to work regardless of the legacy_create_init flag, as it won't overwrite an existing __init__.py file.>> At some point legacy_create_init is getting flipped and removed, at which point this bodge can be fixed.",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20771,339048434,2019-10-25T13:20:39Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -83,9 +91,10 @@ def test_unary_call_times_out(self):                 self.assertIsNotNone(                     exception_context.exception.trailing_metadata()) -        self.loop.run_until_complete(coro())+        self._loop.run_until_complete(coro())   if __name__ == '__main__':+    aio.init_grpc_aio()","this shouldn't be necessary since the test uses the `AioTestBase` which at each `setUp` tries to initialize the `Aio` package.In any case having the feeling that `init_grpc_aio` is fundamentally broken since we are allowing to call it when there is no loop initialized, this might end up in a situation where the underlying gRPC library might make usage of a timer and not having any loop ready.Having the feeling that we would need to change the `init_grpc_aio` function, by either starting a loop or asking for a loop as a parameter and checking that the loop is running.",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20771,339049754,2019-10-25T13:23:31Z,src/python/grpcio_tests/commands.py,"@@ -125,7 +124,10 @@ def run(self):         import tests         loader = tests.Loader()         loader.loadTestsFromNames(['tests_aio'])-        runner = tests.Runner()+        # Even without dedicated threads, the framework will somehow spawn a+        # new thread for tests to run upon. New thread doesn't have event loop+        # attached by default, so initialization is needed.+        runner = tests.Runner(dedicated_threads=False)","I don't want to block this PR because it gives value yes or yes no matter what by reducing complexity. But, we should consider and investigate why current `Aio` tests do not work with having dedicated threads, which IMO should work.",X
35502996,KyleFromKitware,https://api.github.com/repos/grpc/grpc/pulls/20765,339123546,2019-10-25T15:54:55Z,templates/CMakeLists.txt.template,"@@ -295,57 +298,53 @@   add_custom_target(tools     DEPENDS tools_c tools_cxx) -  if (gRPC_BUILD_TESTS)-  add_custom_target(buildtests_c)-  % for tgt in targets:-  % if tgt.build == 'test' and not tgt.language == 'c++' and not tgt.get('external_deps', None) and not tgt.boringssl:-  ${get_platforms_condition_begin(tgt.platforms)}\-  add_dependencies(buildtests_c ${tgt.name})-  ${get_platforms_condition_end(tgt.platforms)}\-  % endif-  % endfor+  if(gRPC_BUILD_TESTS)+    add_custom_target(buildtests_c)+    % for tgt in targets:+    % if tgt.build == 'test' and not tgt.language == 'c++' and not tgt.get('external_deps', None) and not tgt.boringssl:+    ${platforms_condition_add_dependencies(tgt, 'buildtests_c')}","No longer relevant, as this function has been removed in the latest version.",
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20800,339219018,2019-10-25T20:06:31Z,test/core/compression/BUILD,"@@ -13,6 +13,7 @@ # limitations under the License.  load(""//bazel:grpc_build_system.bzl"", ""grpc_cc_library"", ""grpc_cc_test"", ""grpc_cc_binary"", ""grpc_package"")+load(""//third_party/grpc/test/core/util:grpc_fuzzer.bzl"", ""grpc_fuzzer"")","The bazel load for grpc_fuzzer should look like: load(""//test/core/util:grpc_fuzzer.bzl"", ""grpc_fuzzer"")See https://github.com/grpc/grpc/blob/7c306debb06294a048e488234a3aa469f962ee92/test/core/client_channel/BUILD#L21",
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20800,339220498,2019-10-25T20:10:53Z,test/core/compression/BUILD,"@@ -22,46 +23,86 @@ grpc_cc_test(     name = ""algorithm_test"",     srcs = [""algorithm_test.cc""],     language = ""C++"",+    uses_polling = False,     deps = [         ""//:gpr"",         ""//:grpc"",         ""//test/core/util:grpc_test_util"",     ],-    uses_polling = False, )  grpc_cc_test(     name = ""compression_test"",     srcs = [""compression_test.cc""],     language = ""C++"",+    uses_polling = False,     deps = [         ""//:gpr"",         ""//:grpc"",         ""//test/core/util:grpc_test_util"",     ],-    uses_polling = False,+)++grpc_fuzzer(+    name = ""message_compress_fuzzer"",+    srcs = [""message_compress_fuzzer.cc""],+    corpus = ""message_compress_corpus"",+    deps = [+        ""//third_party/grpc"",","Please remove ""third_party/grpc"" from all file paths (applies to the entire pull request). For an example of how paths should look, please see https://github.com/grpc/grpc/blob/7c306debb06294a048e488234a3aa469f962ee92/test/core/client_channel/BUILD#L23",
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20800,339223000,2019-10-25T20:17:41Z,test/core/compression/message_compress_fuzzer.cc,"@@ -0,0 +1,36 @@+#include <fuzzer/FuzzedDataProvider.h>","Every file needs a copyright message at the top. For an example, see https://github.com/grpc/grpc/blob/7c306debb06294a048e488234a3aa469f962ee92/test/core/client_channel/uri_fuzzer_test.cc#L3I realize this is not in our contributing guidelines, so I made a PR to update the guidelines: https://github.com/grpc/grpc/pull/20810",
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20800,339247730,2019-10-25T21:35:36Z,test/core/compression/stream_compression_fuzzer.cc,"@@ -0,0 +1,33 @@+#include <grpc/grpc.h>+#include <stdbool.h>+#include <stdint.h>+#include <string.h>++#include ""src/core/lib/compression/stream_compression.h""+#include ""src/core/lib/security/credentials/credentials.h""+#include ""test/core/util/memory_counters.h""++extern ""C"" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {+  grpc_core::testing::LeakDetector leak_detector(true);+  grpc_init();+  grpc_test_only_control_plane_credentials_force_init();+  auto* context = grpc_stream_compression_context_create(+      GRPC_STREAM_COMPRESSION_GZIP_COMPRESS);+  grpc_slice_buffer input_buffer;+  grpc_slice_buffer_init(&input_buffer);+  grpc_slice_buffer_add(+      &input_buffer,+      grpc_slice_from_copied_buffer(reinterpret_cast<const char*>(data), size));+  grpc_slice_buffer output_buffer;+  grpc_slice_buffer_init(&output_buffer);++  grpc_stream_compress(context, &input_buffer, &output_buffer, nullptr,+                       (~(size_t)0), GRPC_STREAM_COMPRESSION_FLUSH_SYNC);",Setting the max_output_size to 0 seems like it would skip most of the functionality we're trying to test here.https://github.com/grpc/grpc/blob/0c8bedca3334656a02dc39cf248adf5749a11d60/src/core/lib/compression/stream_compression_gzip.cc#L49,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20812,339250968,2019-10-25T21:48:31Z,src/python/grpcio/grpc/_channel.py,"@@ -249,17 +249,66 @@ def _done():     consumption_thread.start()  -class _SingleThreadedRendezvous(grpc.RpcError, grpc.Call):  # pylint: disable=too-many-ancestors-    """"""An RPC iterator operating entirely on a single thread.+def _rpc_state_string(class_name, rpc_state):+    """"""Calculates error string for RPC.""""""+    with rpc_state.condition:+        if rpc_state.code is None:+            return '<{} object>'.format(class_name)+        elif rpc_state.code is grpc.StatusCode.OK:+            return _OK_RENDEZVOUS_REPR_FORMAT.format(class_name, rpc_state.code,+                                                     rpc_state.details)+        else:+            return _NON_OK_RENDEZVOUS_REPR_FORMAT.format(+                class_name, rpc_state.code, rpc_state.details,+                rpc_state.debug_error_string)+++class _RpcError(grpc.RpcError, grpc.Call):+    """"""An RPC error not tied to the execution of a particular RPC.++    Attributes:+      _state: An instance of _RPCState.+    """"""++    def __init__(self, state):+        self._state = state++    def initial_metadata(self):+        with self._state.condition:+            return self._state.initial_metadata++    def trailing_metadata(self):+        with self._state.condition:+            return self._state.trailing_metadata++    def code(self):+        with self._state.condition:+            return self._state.code++    def details(self):+        with self._state.condition:+            return _common.decode(self._state.details)++    def debug_error_string(self):+        with self._state.condition:+            return _common.decode(self._state.debug_error_string)++    def _repr(self):+        return _rpc_state_string(self.__class__.__name__, self._state)++    def __repr__(self):+        return self._repr()++    def __str__(self):+        return self._repr() -    The __next__ method of _SingleThreadedRendezvous does not depend on the-    existence of any other thread, including the ""channel spin thread"".-    However, this means that its interface is entirely synchronous. So this-    class cannot fulfill the grpc.Future interface.++class _Rendezvous(grpc.RpcError, grpc.Call):+    """"""An RPC iterator.","Potentially, this can be a base class or mixin, so it doesn't semantically necessary to implement the `grpc.Call` interface. Then you can get rid of those empty functions.The `grpc.Call` interface can be implemented only by the two sub `_Rendezvous` classes.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20812,339251640,2019-10-25T21:51:25Z,src/python/grpcio/grpc/_channel.py,"@@ -314,70 +363,111 @@ def add_callback(self, callback):      def initial_metadata(self):         """"""See grpc.Call.initial_metadata""""""+        raise NotImplementedError()++    def trailing_metadata(self):+        """"""See grpc.Call.trailing_metadata""""""+        raise NotImplementedError()++    def code(self):+        """"""See grpc.Call.code""""""+        raise NotImplementedError()++    def details(self):+        """"""See grpc.Call.details""""""+        raise NotImplementedError()++    def __iter__(self):+        return self++    def next(self):+        return self._next()++    def __next__(self):+        return self._next()++    def _next(self):+        raise NotImplementedError()++    def debug_error_string(self):+        raise NotImplementedError()++    def _repr(self):+        return _rpc_state_string(self.__class__.__name__, self._state)++    def __repr__(self):+        return self._repr()++    def __str__(self):+        return self._repr()++    def __del__(self):         with self._state.condition:+            if self._state.code is None:+                self._state.code = grpc.StatusCode.CANCELLED+                self._state.details = 'Cancelled upon garbage collection!'+                self._state.cancelled = True+                self._call.cancel(+                    _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[self._state.code],+                    self._state.details)+                self._state.condition.notify_all() -            def _done():-                return self._state.initial_metadata is not None -            _common.wait(self._state.condition.wait, _done)+class _SingleThreadedRendezvous(_Rendezvous):  # pylint: disable=too-many-ancestors+    """"""An RPC iterator operating entirely on a single thread.++    The __next__ method of _SingleThreadedRendezvous does not depend on the+    existence of any other thread, including the ""channel spin thread"".+    However, this means that its interface is entirely synchronous. So this+    class cannot fulfill the grpc.Future interface.+    """"""++    def initial_metadata(self):+        """"""See grpc.Call.initial_metadata""""""+        with self._state.condition:+            # NOTE(gnossen): Based on our initial call batch, we are guaranteed+            # to receive initial metadata before any messages.+            while self._state.initial_metadata is None:+                self._get_next_event()             return self._state.initial_metadata      def trailing_metadata(self):         """"""See grpc.Call.trailing_metadata""""""         with self._state.condition:--            def _done():-                return self._state.trailing_metadata is not None--            _common.wait(self._state.condition.wait, _done)+            if self._state.trailing_metadata is None:+                raise RuntimeError(+                    ""Cannot get trailing metadata until RPC is completed."")","Shall we make those `RuntimeError` to `grpc.RpcError`? The details string can be the description here.My intention is to make sure the exceptions raised by gRPC framework are labeled, if possible. E.g. for `asyncio.Future` if the result is not ready, it would raise [`asyncio.InvalidStateError`](https://docs.python.org/3/library/asyncio-exceptions.html#asyncio.InvalidStateError). ",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20812,339254916,2019-10-25T22:05:20Z,src/python/grpcio/grpc/_channel.py,"@@ -747,21 +850,21 @@ def __call__(  # pylint: disable=too-many-locals         if serialized_request is None:             state = _RPCState((), (), (), grpc.StatusCode.INTERNAL,                               'Exception serializing request!')-            raise _Rendezvous(state, None, None, deadline)+            raise _RpcError(state)          state = _RPCState(_UNARY_STREAM_INITIAL_DUE, None, None, None, None)         call_credentials = None if credentials is None else credentials._credentials         initial_metadata_flags = _InitialMetadataFlags().with_wait_for_ready(             wait_for_ready)         augmented_metadata = _compression.augment_metadata(             metadata, compression)-        operations_and_tags = ((-            (cygrpc.SendInitialMetadataOperation(augmented_metadata,-                                                 initial_metadata_flags),-             cygrpc.SendMessageOperation(serialized_request, _EMPTY_FLAGS),-             cygrpc.SendCloseFromClientOperation(_EMPTY_FLAGS),-             cygrpc.ReceiveStatusOnClientOperation(_EMPTY_FLAGS)), None),) + (((-                 cygrpc.ReceiveInitialMetadataOperation(_EMPTY_FLAGS),), None),)+        operations_and_tags = (+            (((cygrpc.SendInitialMetadataOperation(augmented_metadata,+                                                   initial_metadata_flags),+               cygrpc.SendMessageOperation(serialized_request, _EMPTY_FLAGS),+               cygrpc.SendCloseFromClientOperation(_EMPTY_FLAGS)), None),) ++            (((cygrpc.ReceiveStatusOnClientOperation(_EMPTY_FLAGS),), None),) ++            (((cygrpc.ReceiveInitialMetadataOperation(_EMPTY_FLAGS),), None),))","Last time I think I also commented on this. Personally, I think splitting `operations_and_tags` helps readability:```Pythonsend_operations_and_tags = ...receive_status_operations_and_tags = ...receive_initial_metadata_operations_and_tags = ...operations_and_tags = [    send_operations_and_tags,    receive_status_operations_and_tags,    receive_initial_metadata_operations_and_tags,]```",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20812,339258291,2019-10-25T22:21:39Z,src/python/grpcio/grpc/_channel.py,"@@ -314,70 +363,111 @@ def add_callback(self, callback):      def initial_metadata(self):         """"""See grpc.Call.initial_metadata""""""+        raise NotImplementedError()++    def trailing_metadata(self):+        """"""See grpc.Call.trailing_metadata""""""+        raise NotImplementedError()++    def code(self):+        """"""See grpc.Call.code""""""+        raise NotImplementedError()++    def details(self):+        """"""See grpc.Call.details""""""+        raise NotImplementedError()++    def __iter__(self):+        return self++    def next(self):+        return self._next()++    def __next__(self):+        return self._next()++    def _next(self):+        raise NotImplementedError()++    def debug_error_string(self):+        raise NotImplementedError()++    def _repr(self):+        return _rpc_state_string(self.__class__.__name__, self._state)++    def __repr__(self):+        return self._repr()++    def __str__(self):+        return self._repr()++    def __del__(self):         with self._state.condition:+            if self._state.code is None:+                self._state.code = grpc.StatusCode.CANCELLED+                self._state.details = 'Cancelled upon garbage collection!'+                self._state.cancelled = True+                self._call.cancel(+                    _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[self._state.code],+                    self._state.details)+                self._state.condition.notify_all() -            def _done():-                return self._state.initial_metadata is not None -            _common.wait(self._state.condition.wait, _done)+class _SingleThreadedRendezvous(_Rendezvous):  # pylint: disable=too-many-ancestors+    """"""An RPC iterator operating entirely on a single thread.++    The __next__ method of _SingleThreadedRendezvous does not depend on the+    existence of any other thread, including the ""channel spin thread"".+    However, this means that its interface is entirely synchronous. So this+    class cannot fulfill the grpc.Future interface.+    """"""++    def initial_metadata(self):+        """"""See grpc.Call.initial_metadata""""""+        with self._state.condition:+            # NOTE(gnossen): Based on our initial call batch, we are guaranteed+            # to receive initial metadata before any messages.+            while self._state.initial_metadata is None:+                self._get_next_event()             return self._state.initial_metadata      def trailing_metadata(self):         """"""See grpc.Call.trailing_metadata""""""         with self._state.condition:--            def _done():-                return self._state.trailing_metadata is not None--            _common.wait(self._state.condition.wait, _done)+            if self._state.trailing_metadata is None:+                raise RuntimeError(+                    ""Cannot get trailing metadata until RPC is completed."")","I don't think `grpc.RpcError` is semantically appropriate. I was thinking about adding a `grpc.UsageError` or something like that. The difference is that `grpc.RpcError` implies ""something went wrong with the RPC"". Whereas, these new exceptions mean ""You're using the API wrong."" But I was apprehensive because that means a new class in `__init__.py`. If you think it's worth it, I'll add it.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20812,339258562,2019-10-25T22:22:53Z,src/python/grpcio/grpc/_channel.py,"@@ -388,65 +478,86 @@ def _next(self):                     elif self._state.code is not None:                         raise self -    def __next__(self):-        return self._next()+    def _next(self):+        with self._state.condition:+            if self._state.code is None:+                operating = self._call.operate(+                    (cygrpc.ReceiveMessageOperation(_EMPTY_FLAGS),), None)+                if operating:+                    self._state.due.add(cygrpc.OperationType.receive_message)+            elif self._state.code is grpc.StatusCode.OK:+                raise StopIteration()+            else:+                raise self+        return self._next_response() -    def next(self):-        return self._next()+    def debug_error_string(self):+        with self._state.condition:+            if self._state.debug_error_string is None:+                raise RuntimeError(+                    ""Cannot get debug error string until RPC is completed."")+            return _common.decode(self._state.debug_error_string) -    def __iter__(self):-        return self -    def debug_error_string(self):+class _MultiThreadedRendezvous(_Rendezvous, grpc.Future):  # pylint: disable=too-many-ancestors+    """"""An RPC iterator that depends on a channel spin thread.++    This iterator relies upon a per-channel thread running in the background,+    dequeueing events from the completion queue, and notifying threads waiting+    on the threading.Condition object in the _RPCState object.++    This extra thread allows _MultiThreadedRendezvous to fulfill the grpc.Future interface+    and to mediate a bidirection streaming RPC.+    """"""++    def initial_metadata(self):+        """"""See grpc.Call.initial_metadata""""""         with self._state.condition:              def _done():-                return self._state.debug_error_string is not None+                return self._state.initial_metadata is not None              _common.wait(self._state.condition.wait, _done)-            return _common.decode(self._state.debug_error_string)+            return self._state.initial_metadata -    def _repr(self):+    def trailing_metadata(self):+        """"""See grpc.Call.trailing_metadata""""""         with self._state.condition:-            if self._state.code is None:-                return '<{} object of in-flight RPC>'.format(-                    self.__class__.__name__)-            elif self._state.code is grpc.StatusCode.OK:-                return _OK_RENDEZVOUS_REPR_FORMAT.format(-                    self._state.code, self._state.details)-            else:-                return _NON_OK_RENDEZVOUS_REPR_FORMAT.format(-                    self._state.code, self._state.details,-                    self._state.debug_error_string) -    def __repr__(self):-        return self._repr()+            def _done():+                return self._state.trailing_metadata is not None","Personally, I think Python lambdas make code *less* readable. Their syntax is too similar to Python typing.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20812,339261219,2019-10-25T22:35:53Z,src/python/grpcio/grpc/_channel.py,"@@ -314,70 +363,111 @@ def add_callback(self, callback):      def initial_metadata(self):         """"""See grpc.Call.initial_metadata""""""+        raise NotImplementedError()++    def trailing_metadata(self):+        """"""See grpc.Call.trailing_metadata""""""+        raise NotImplementedError()++    def code(self):+        """"""See grpc.Call.code""""""+        raise NotImplementedError()++    def details(self):+        """"""See grpc.Call.details""""""+        raise NotImplementedError()++    def __iter__(self):+        return self++    def next(self):+        return self._next()++    def __next__(self):+        return self._next()++    def _next(self):+        raise NotImplementedError()++    def debug_error_string(self):+        raise NotImplementedError()++    def _repr(self):+        return _rpc_state_string(self.__class__.__name__, self._state)++    def __repr__(self):+        return self._repr()++    def __str__(self):+        return self._repr()++    def __del__(self):         with self._state.condition:+            if self._state.code is None:+                self._state.code = grpc.StatusCode.CANCELLED+                self._state.details = 'Cancelled upon garbage collection!'+                self._state.cancelled = True+                self._call.cancel(+                    _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[self._state.code],+                    self._state.details)+                self._state.condition.notify_all() -            def _done():-                return self._state.initial_metadata is not None -            _common.wait(self._state.condition.wait, _done)+class _SingleThreadedRendezvous(_Rendezvous):  # pylint: disable=too-many-ancestors+    """"""An RPC iterator operating entirely on a single thread.++    The __next__ method of _SingleThreadedRendezvous does not depend on the+    existence of any other thread, including the ""channel spin thread"".+    However, this means that its interface is entirely synchronous. So this+    class cannot fulfill the grpc.Future interface.+    """"""++    def initial_metadata(self):+        """"""See grpc.Call.initial_metadata""""""+        with self._state.condition:+            # NOTE(gnossen): Based on our initial call batch, we are guaranteed+            # to receive initial metadata before any messages.+            while self._state.initial_metadata is None:+                self._get_next_event()             return self._state.initial_metadata      def trailing_metadata(self):         """"""See grpc.Call.trailing_metadata""""""         with self._state.condition:--            def _done():-                return self._state.trailing_metadata is not None--            _common.wait(self._state.condition.wait, _done)+            if self._state.trailing_metadata is None:+                raise RuntimeError(+                    ""Cannot get trailing metadata until RPC is completed."")","I don't have strong opinion here, `RuntimeError` is also acceptable since we can't find a better type for it.Ideally we can use [`concurrent.futures.InvalidStateError`](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.InvalidStateError), but I just found out it is 3.8+.About adding `grpc.UsageError`, I think it is optional, since this is for experimental path, it would make sense to add the new exception type to experimental folder first.",
512410,sxlijin,https://api.github.com/repos/grpc/grpc/pulls/20800,339284987,2019-10-26T03:59:39Z,test/core/compression/message_compress_fuzzer.cc,"@@ -0,0 +1,36 @@+#include <fuzzer/FuzzedDataProvider.h>+#include <grpc/grpc.h>+#include <stdbool.h>+#include <stdint.h>+#include <string.h>++#include ""src/core/lib/compression/message_compress.h""+#include ""src/core/lib/security/credentials/credentials.h""+#include ""test/core/util/memory_counters.h""++extern ""C"" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {+  FuzzedDataProvider data_provider(data, size);+  const auto compression_algorithm =+      static_cast<grpc_message_compression_algorithm>(+          data_provider.ConsumeIntegralInRange<int>(","> what's the value in using the fuzzed input to choose the compression algorithm as opposed to testing each compression algorithmMostly using less code (no need for extra build targets/files) and it doesn't come at a very big readability compromise. Also, if a new compression algorithm is ever added, it gets fuzzing for free.> just using a random number generator bounded between 0 and 2?If a fuzzer finds a bug, it should be able to deterministically reproduce it with a given input (using an RNG will make it a flaky failure); in a similar vein, because the fuzzing that this will set up is coverage-guided, the fuzzing infrastructure will be able to explore all the compression options with the knowledge that it's doing so.",
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/20824,339691932,2019-10-28T17:21:34Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -25,6 +24,167 @@ from tests_aio.unit._test_base import AioTestBase  +class TestAioRpcError(unittest.TestCase):+    _TEST_INITIAL_METADATA = (""initial metadata"",)+    _TEST_TRAILING_METADATA = (""trailing metadata"",)++    def test_attributes(self):+        aio_rpc_error = aio.AioRpcError(",Now ``aio.AioRpcError`` is no longer a ``grpc.RpcError``?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20824,339764715,2019-10-28T20:02:01Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -13,15 +13,207 @@ # limitations under the License. """"""Invocation-side implementation of gRPC Asyncio Python."""""" import asyncio-from typing import Callable, Optional+from typing import Callable, Dict, Optional +import grpc from grpc import _common from grpc._cython import cygrpc  SerializingFunction = Callable[[str], bytes] DeserializingFunction = Callable[[bytes], str]  +class AioRpcError(grpc.RpcError):+    """"""An RpcError to be used by the asynchronous API.""""""",Two optional stuff: 1) instance variable https://docs.python.org/3/library/typing.html#typing.ClassVar; 2) `__slots__` for the data path class.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20824,339767577,2019-10-28T20:08:56Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -13,15 +13,207 @@ # limitations under the License. """"""Invocation-side implementation of gRPC Asyncio Python."""""" import asyncio-from typing import Callable, Optional+from typing import Callable, Dict, Optional +import grpc from grpc import _common from grpc._cython import cygrpc  SerializingFunction = Callable[[str], bytes] DeserializingFunction = Callable[[bytes], str]  +class AioRpcError(grpc.RpcError):+    """"""An RpcError to be used by the asynchronous API.""""""++    def __init__(self,+                 code: grpc.StatusCode,+                 details: Optional[str] = None,+                 initial_metadata: Optional[Dict] = None,+                 trailing_metadata: Optional[Dict] = None):+        self._code = code+        self._details = details+        self._initial_metadata = initial_metadata+        self._trailing_metadata = trailing_metadata+        super().__init__(self)++    def code(self) -> grpc.StatusCode:+        """"""+        Returns:+          The `grpc.StatusCode` status code.+        """"""+        return self._code++    def details(self) -> Optional[str]:+        """"""+        Returns:+          The description of the error.+        """"""+        return self._details++    def initial_metadata(self) -> Optional[Dict]:+        """"""+        Returns:+          The inital metadata received.+        """"""+        return self._initial_metadata++    def trailing_metadata(self) -> Optional[Dict]:+        """"""+        Returns:+          The trailing metadata received.+        """"""+        return self._trailing_metadata+++class Call:+    """""" Object for managing RPC calls, returned when an instance of+    `UnaryUnaryMultiCallable` object is called.+    """"""++    def __init__(self, aio_call: asyncio.Task,+                 response_deserializer: DeserializingFunction,+                 aio_call_cancel_status: cygrpc.AioCallCancelStatus):+        self._exception = None+        self._response = None+        self._code = None+        self._details = None+        self._initial_metadata = None+        self._trailing_metadata = None+        self._cancelled = False+        self._aio_call = aio_call+        self._aio_call_cancel_status = aio_call_cancel_status+        self._response_deserializer = response_deserializer++    def __del__(self):+        self.cancel()++    def cancel(self) -> bool:+        """"""+        Cancels the ongoing RPC request.++        Returns:+          True if the RPC can be canceled, False if was already cancelled or terminated.+        """"""+        if self.cancelled() or self.done():+            return False++        details = 'Locally cancelled by application!'+        code = grpc.StatusCode.CANCELLED+        self._aio_call_cancel_status.cancel(+            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code], details=details)+        self._aio_call.cancel()+        self._details = details+        self._code = code+        self._cancelled = True+        return True++    def cancelled(self) -> bool:+        """"""+        Returns if the RPC was cancelled.++        Returns:+          True if the requests was cancelled, False if not.+        """"""+        return self._cancelled++    def running(self) -> bool:+        """"""+        Returns if the RPC is running.++        Returns:+          True if the requests is running, False if it already terminated.+        """"""+        return self._code is None++    def done(self) -> bool:+        """"""+        Returns if the RPC has finished.++        Returns:+          True if the requests has finished, False is if still ongoing.+        """"""+        return self._code is not None++    async def initial_metadata(self):+        raise NotImplementedError++    async def trailing_metadata(self):+        raise NotImplementedError++    async def code(self) -> grpc.StatusCode:+        """"""+        Returns the `grpc.StatusCode` if the RPC is finished, otherwise first waits till the+        RPC finishes.++        Returns:+          The `grpc.StatusCode` status code.+        """"""+        if not self.done():+            try:+                await self+            except (asyncio.CancelledError, AioRpcError):+                pass++        return self._code++    async def details(self) -> str:+        """"""+        Returns the details if the RPC is finished, otherwise first waits till the+        RPC finishes.++        Returns:+          The details.+        """"""+        if not self.done():+            try:+                await self+            except (asyncio.CancelledError, AioRpcError):","I would recommend add a comment here, saying that if RPC throws exception, it is expected that the `code`, `details` are filled by the thrown exception.Also, we should make default value to `UNKNOWN`, check if `code` and `details` are set before returning.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20824,339768933,2019-10-28T20:12:03Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -25,6 +24,167 @@ from tests_aio.unit._test_base import AioTestBase  +class TestAioRpcError(unittest.TestCase):+    _TEST_INITIAL_METADATA = (""initial metadata"",)+    _TEST_TRAILING_METADATA = (""trailing metadata"",)++    def test_attributes(self):+        aio_rpc_error = aio.AioRpcError(+            grpc.StatusCode.CANCELLED,+            ""details"",+            initial_metadata=self._TEST_INITIAL_METADATA,+            trailing_metadata=self._TEST_TRAILING_METADATA)+        self.assertEqual(aio_rpc_error.code(), grpc.StatusCode.CANCELLED)+        self.assertEqual(aio_rpc_error.details(), ""details"")+        self.assertEqual(aio_rpc_error.initial_metadata(),+                         self._TEST_INITIAL_METADATA)+        self.assertEqual(aio_rpc_error.trailing_metadata(),+                         self._TEST_TRAILING_METADATA)+++class TestCall(AioTestBase):","Should we split this into a separate file? Unit test files grows fast, let's keep one test case class per file from start.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20824,339769336,2019-10-28T20:13:02Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -25,6 +24,167 @@ from tests_aio.unit._test_base import AioTestBase  +class TestAioRpcError(unittest.TestCase):+    _TEST_INITIAL_METADATA = (""initial metadata"",)+    _TEST_TRAILING_METADATA = (""trailing metadata"",)++    def test_attributes(self):+        aio_rpc_error = aio.AioRpcError(+            grpc.StatusCode.CANCELLED,+            ""details"",+            initial_metadata=self._TEST_INITIAL_METADATA,+            trailing_metadata=self._TEST_TRAILING_METADATA)+        self.assertEqual(aio_rpc_error.code(), grpc.StatusCode.CANCELLED)+        self.assertEqual(aio_rpc_error.details(), ""details"")+        self.assertEqual(aio_rpc_error.initial_metadata(),+                         self._TEST_INITIAL_METADATA)+        self.assertEqual(aio_rpc_error.trailing_metadata(),+                         self._TEST_TRAILING_METADATA)+++class TestCall(AioTestBase):++    def test_call_ok(self):++        async def coro():+            server_target, unused_server = await start_test_server()++            async with aio.insecure_channel(server_target) as channel:+                hi = channel.unary_unary(+                    '/grpc.testing.TestService/UnaryCall',+                    request_serializer=messages_pb2.SimpleRequest.+                    SerializeToString,+                    response_deserializer=messages_pb2.SimpleResponse.FromString+                )+                call = hi(messages_pb2.SimpleRequest())++                self.assertFalse(call.done())++                response = await call++                self.assertTrue(call.done())+                self.assertEqual(type(response), messages_pb2.SimpleResponse)+                self.assertEqual(await call.code(), grpc.StatusCode.OK)++                # response is cached at call object level, reentrance+                # returns again the same response+                response_retry = await call+                self.assertEqual(response, response_retry)++        self.loop.run_until_complete(coro())++    def test_call_rpc_error(self):++        async def coro():+            server_target, unused_server = await start_test_server()++            async with aio.insecure_channel(server_target) as channel:+                empty_call_with_sleep = channel.unary_unary(+                    ""/grpc.testing.TestService/EmptyCall"",+                    request_serializer=messages_pb2.SimpleRequest.+                    SerializeToString,+                    response_deserializer=messages_pb2.SimpleResponse.+                    FromString,+                )+                timeout = test_constants.SHORT_TIMEOUT / 2+                # TODO: Update once the async server is ready, change the+                # synchronization mechanism by removing the sleep(<timeout>)+                # as both components (client & server) will be on the same+                # process.+                call = empty_call_with_sleep(+                    messages_pb2.SimpleRequest(), timeout=timeout)++                with self.assertRaises(grpc.RpcError) as exception_context:+                    await call++                self.assertTrue(call.done())+                self.assertEqual(await call.code(),+                                 grpc.StatusCode.DEADLINE_EXCEEDED)++                # exception is cached at call object level, reentrance+                # returns again the same exception+                with self.assertRaises(+                        grpc.RpcError) as exception_context_retry:+                    await call++                self.assertEqual(exception_context.exception,+                                 exception_context_retry.exception)++        self.loop.run_until_complete(coro())++    def test_call_code_awaitable(self):++        async def coro():+            server_target, unused_server = await start_test_server()++            async with aio.insecure_channel(server_target) as channel:+                hi = channel.unary_unary(+                    '/grpc.testing.TestService/UnaryCall',+                    request_serializer=messages_pb2.SimpleRequest.+                    SerializeToString,+                    response_deserializer=messages_pb2.SimpleResponse.FromString+                )+                call = hi(messages_pb2.SimpleRequest())+                self.assertEqual(await call.code(), grpc.StatusCode.OK)++        self.loop.run_until_complete(coro())++    def test_call_details_awaitable(self):++        async def coro():+            server_target, unused_server = await start_test_server()++            async with aio.insecure_channel(server_target) as channel:+                hi = channel.unary_unary(+                    '/grpc.testing.TestService/UnaryCall',+                    request_serializer=messages_pb2.SimpleRequest.+                    SerializeToString,+                    response_deserializer=messages_pb2.SimpleResponse.FromString+                )+                call = hi(messages_pb2.SimpleRequest())+                self.assertEqual(await call.details(), None)++        self.loop.run_until_complete(coro())++    def test_cancel(self):++        async def coro():+            server_target, unused_server = await start_test_server()++            async with aio.insecure_channel(server_target) as channel:+                hi = channel.unary_unary(+                    '/grpc.testing.TestService/UnaryCall',+                    request_serializer=messages_pb2.SimpleRequest.+                    SerializeToString,+                    response_deserializer=messages_pb2.SimpleResponse.FromString+                )+                call = hi(messages_pb2.SimpleRequest())++                self.assertFalse(call.cancelled())++                # Force the loop to execute the RPC task, cython+                # code is executed.+                await asyncio.sleep(0)++                self.assertTrue(call.cancel())+                self.assertTrue(call.cancelled())+                self.assertFalse(call.cancel())++                with self.assertRaises(asyncio.CancelledError):+                    await call++                self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+                self.assertEqual(await call.details(),+                                 'Locally cancelled by application!')++                # exception is cached at call object level, reentrance",s/exception/Exception,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20824,339770828,2019-10-28T20:16:35Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -19,6 +19,27 @@ _EMPTY_FLAGS = 0 _EMPTY_METADATA = None _OP_ARRAY_LENGTH = 6 +class AioCallCancelStatus:","We can make it a `cdef class`, and make those two values `readonly` from Python layer.Also, I would prefer its name to be more generic, so the server side can use it as well.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20824,339772223,2019-10-28T20:19:35Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -133,8 +155,19 @@ cdef class _AioCall:                 self._waiter_call = None                 raise Exception(""Error with grpc_call_start_batch {}"".format(call_status)) -            await self._waiter_call-+            try:+                await self._waiter_call+            except asyncio.CancelledError:+                if cancel_status:+                    details = str_to_bytes(cancel_status.details)+                    self._references.append(details)","I'm not sure here. Do we need to keep this `details` in reference? Does the `<char *>` conversion copy the object, or it just accessing the low level data structure in `PyStr`?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20824,339774281,2019-10-28T20:24:25Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -13,15 +13,207 @@ # limitations under the License. """"""Invocation-side implementation of gRPC Asyncio Python."""""" import asyncio-from typing import Callable, Optional+from typing import Callable, Dict, Optional +import grpc from grpc import _common from grpc._cython import cygrpc  SerializingFunction = Callable[[str], bytes] DeserializingFunction = Callable[[bytes], str]  +class AioRpcError(grpc.RpcError):","`debug_error_string` is something not listed in the API, but quite helpful for users to debug. I hope we can still propagate it to application.See https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_channel.py#L149",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20805,339794912,2019-10-28T21:11:47Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/callbackcontext.pxd.pxi,"@@ -16,5 +16,5 @@ cimport cpython  cdef struct CallbackContext:",Maybe we can use c[lass-style docstrings](http://google.github.io/styleguide/pyguide.html#384-classes)?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20824,339795500,2019-10-28T21:13:07Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pxd.pxi,"@@ -20,8 +20,9 @@ cdef class _AioCall:         grpc_completion_queue * _cq         grpc_experimental_completion_queue_functor _functor         object _waiter_call+        list _references      @staticmethod-    cdef void functor_run(grpc_experimental_completion_queue_functor* functor, int succeed)+    cdef void functor_run(grpc_experimental_completion_queue_functor* functor, int succeed) with gil     @staticmethod-    cdef void watcher_call_functor_run(grpc_experimental_completion_queue_functor* functor, int succeed)+    cdef void watcher_call_functor_run(grpc_experimental_completion_queue_functor* functor, int succeed) with gil","TBH it was a surprise for me, with the previous code I suffered some random segfaults which they got solved automatically adding the `with gil`.I look into the `uvloop` implementation code and all of the callbacks [1] were using explicitly the `with gil`, so I've added and the issues disappeared.My rationale in front of this issue was, if we are saying explicitly that the C function of the library has to be called without keeping the GIL, we have to tell Cython that the callback - which could be executed immediately - needs to be executed acquiring the GIL yes or yes.I guess that we would need to change the `CallbackWrapper`, I can take care of it if you want.[1] https://github.com/MagicStack/uvloop/blob/7a4f00a3c22e52b88deb4b261e093e8affa4ea0a/uvloop/handles/tcp.pyx#L205",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20824,339806255,2019-10-28T21:41:24Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pxd.pxi,"@@ -20,8 +20,9 @@ cdef class _AioCall:         grpc_completion_queue * _cq         grpc_experimental_completion_queue_functor _functor         object _waiter_call+        list _references      @staticmethod-    cdef void functor_run(grpc_experimental_completion_queue_functor* functor, int succeed)+    cdef void functor_run(grpc_experimental_completion_queue_functor* functor, int succeed) with gil     @staticmethod-    cdef void watcher_call_functor_run(grpc_experimental_completion_queue_functor* functor, int succeed)+    cdef void watcher_call_functor_run(grpc_experimental_completion_queue_functor* functor, int succeed) with gil","Ok, I will dig into it a bit more, in any case theoretically the initialization of the gRPC under Aio asks explicitly for not using any thread at all[1]  for running the callbacks.[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi#L35",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20838,339817921,2019-10-28T22:16:25Z,src/python/grpcio_tests/tests/unit/framework/common/__init__.py,"@@ -11,3 +11,66 @@ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.++import contextlib+import socket+++def get_socket(bind_address='localhost',+               listen=True,+               sock_options=(socket.SO_REUSEPORT,)):","I think `SO_REUSEPORT` is actually appropriate in the cases where we're using this to reserve a port. If there isn't a period where this socket and the server's socket are bound to the same port, then there exists a race condition between the test server and other processes on the machine. I suggest that we simply limit usage of SO_REUSEPORT to those tests that actually need it. If we manage our ports properly, then crosstalk will not be an issue.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20805,339840210,2019-10-28T23:40:16Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -21,40 +21,140 @@ from src.proto.grpc.testing import messages_pb2 from src.proto.grpc.testing import benchmark_service_pb2_grpc from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants -_TEST_METHOD_PATH = ''+_SIMPLE_UNARY_UNARY = '/test/SimpleUnaryUnary'+_BLOCK_FOREVER = '/test/BlockForever'+_BLOCK_SHORTLY = '/test/BlockShortly'  _REQUEST = b'\x00\x00\x00' _RESPONSE = b'\x01\x01\x01'  -async def unary_unary(unused_request, unused_context):-    return _RESPONSE+class _GenericHandler(grpc.GenericRpcHandler): +    def __init__(self):+        self._called = asyncio.get_event_loop().create_future() -class GenericHandler(grpc.GenericRpcHandler):+    @staticmethod+    async def _unary_unary(unused_request, unused_context):+        return _RESPONSE -    def service(self, unused_handler_details):-        return grpc.unary_unary_rpc_method_handler(unary_unary)+    async def _block_forever(self, unused_request, unused_context):+        await asyncio.get_event_loop().create_future()++    async def _block_shortly(self, unused_request, unused_context):+        await asyncio.sleep(test_constants.SHORT_TIMEOUT / 2)+        return _RESPONSE++    def service(self, handler_details):+        self._called.set_result(None)+        if handler_details.method == _SIMPLE_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(self._unary_unary)+        if handler_details.method == _BLOCK_FOREVER:+            return grpc.unary_unary_rpc_method_handler(self._block_forever)+        if handler_details.method == _BLOCK_SHORTLY:+            return grpc.unary_unary_rpc_method_handler(self._block_shortly)++    async def wait_for_call(self):+        await self._called+++async def _start_test_server():+    server = aio.server()+    port = server.add_insecure_port('[::]:0')+    generic_handler = _GenericHandler()+    server.add_generic_rpc_handlers((generic_handler,))+    await server.start()+    return 'localhost:%d' % port, server, generic_handler   class TestServer(AioTestBase):      def test_unary_unary(self):          async def test_unary_unary_body():-            server = aio.server()-            port = server.add_insecure_port('[::]:0')-            server.add_generic_rpc_handlers((GenericHandler(),))-            await server.start()+            server_target, _, _ = await _start_test_server() -            async with aio.insecure_channel('localhost:%d' % port) as channel:-                unary_call = channel.unary_unary(_TEST_METHOD_PATH)+            async with aio.insecure_channel(server_target) as channel:+                unary_call = channel.unary_unary(_SIMPLE_UNARY_UNARY)                 response = await unary_call(_REQUEST)                 self.assertEqual(response, _RESPONSE)          self.loop.run_until_complete(test_unary_unary_body()) +    def test_shutdown(self):++        async def test_shutdown_body():+            _, server, _ = await _start_test_server()",There doesn't appear to be an assertion in this test case. Is the idea just that there will be no exception thrown and will terminate within the test runner's timeout?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20805,339844423,2019-10-29T00:00:10Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -21,40 +21,140 @@ from src.proto.grpc.testing import messages_pb2 from src.proto.grpc.testing import benchmark_service_pb2_grpc from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants -_TEST_METHOD_PATH = ''+_SIMPLE_UNARY_UNARY = '/test/SimpleUnaryUnary'+_BLOCK_FOREVER = '/test/BlockForever'+_BLOCK_SHORTLY = '/test/BlockShortly'  _REQUEST = b'\x00\x00\x00' _RESPONSE = b'\x01\x01\x01'  -async def unary_unary(unused_request, unused_context):-    return _RESPONSE+class _GenericHandler(grpc.GenericRpcHandler): +    def __init__(self):+        self._called = asyncio.get_event_loop().create_future() -class GenericHandler(grpc.GenericRpcHandler):+    @staticmethod+    async def _unary_unary(unused_request, unused_context):+        return _RESPONSE -    def service(self, unused_handler_details):-        return grpc.unary_unary_rpc_method_handler(unary_unary)+    async def _block_forever(self, unused_request, unused_context):+        await asyncio.get_event_loop().create_future()++    async def _block_shortly(self, unused_request, unused_context):+        await asyncio.sleep(test_constants.SHORT_TIMEOUT / 2)+        return _RESPONSE++    def service(self, handler_details):+        self._called.set_result(None)+        if handler_details.method == _SIMPLE_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(self._unary_unary)+        if handler_details.method == _BLOCK_FOREVER:+            return grpc.unary_unary_rpc_method_handler(self._block_forever)+        if handler_details.method == _BLOCK_SHORTLY:+            return grpc.unary_unary_rpc_method_handler(self._block_shortly)++    async def wait_for_call(self):+        await self._called+++async def _start_test_server():+    server = aio.server()+    port = server.add_insecure_port('[::]:0')+    generic_handler = _GenericHandler()+    server.add_generic_rpc_handlers((generic_handler,))+    await server.start()+    return 'localhost:%d' % port, server, generic_handler   class TestServer(AioTestBase):      def test_unary_unary(self):          async def test_unary_unary_body():-            server = aio.server()-            port = server.add_insecure_port('[::]:0')-            server.add_generic_rpc_handlers((GenericHandler(),))-            await server.start()+            server_target, _, _ = await _start_test_server() -            async with aio.insecure_channel('localhost:%d' % port) as channel:-                unary_call = channel.unary_unary(_TEST_METHOD_PATH)+            async with aio.insecure_channel(server_target) as channel:+                unary_call = channel.unary_unary(_SIMPLE_UNARY_UNARY)                 response = await unary_call(_REQUEST)                 self.assertEqual(response, _RESPONSE)          self.loop.run_until_complete(test_unary_unary_body()) +    def test_shutdown(self):++        async def test_shutdown_body():+            _, server, _ = await _start_test_server()+            await server.stop(None)++        self.loop.run_until_complete(test_shutdown_body())++    def test_shutdown_after_call(self):++        async def test_shutdown_body():+            server_target, server, _ = await _start_test_server()++            async with aio.insecure_channel(server_target) as channel:+                await channel.unary_unary(_SIMPLE_UNARY_UNARY)(_REQUEST)++            await server.stop(None)++        self.loop.run_until_complete(test_shutdown_body())++    def test_graceful_shutdown_success(self):++        async def test_graceful_shutdown_success_body():+            server_target, server, generic_handler = await _start_test_server()++            channel = aio.insecure_channel(server_target)+            call_task = self.loop.create_task(+                channel.unary_unary(_BLOCK_SHORTLY)(_REQUEST))+            await generic_handler.wait_for_call()++            await server.stop(test_constants.SHORT_TIMEOUT)","I'm concerned that there may be a race condition here that would cause this test not to actually check what we're hoping to check. Namely, that calling `stop()` on a server with an ongoing RPC completes successfully.We decided to instrument the test slightly to detect this condition and turn it into a failure. Then, to take that modified test and to spin up at least a thousand instances to ensure that this does not actually happen.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20805,339848445,2019-10-29T00:21:44Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +302,97 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start(backup_queue=False)+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown process starts, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def shutdown(self, grace):+        """"""Gracefully shutdown the C-Core server.++        Application should only call shutdown once.++        Args:+          grace: An optional float indicates the length of grace period in+            seconds.+        """"""+        if self._status != AIO_SERVER_STATUS_RUNNING:+            # The server either is shutting down, or not started.+            return+        cdef object shutdown_completed = self._loop.create_future()+        cdef CallbackWrapper wrapper = CallbackWrapper(+            shutdown_completed,+            SERVER_SHUTDOWN_FAILURE_HANDLER)+        # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+        # when calling ""await"". This is an over-optimization by Cython.+        cpython.Py_INCREF(wrapper)++        # Starts the shutdown process.+        # The shutdown callback won't be called unless there is no live RPC.+        grpc_server_shutdown_and_notify(+            self._server.c_server,+            self._cq._cq,+            wrapper.c_functor())+        self._server.is_shutting_down = True+        self._status = AIO_SERVER_STATUS_STOPPING++        # Ensures the serving task (coroutine) exits.+        try:+            await self._serving_task+        except _RequestCallError:+            pass++        if grace is None:+            # Directly cancels all calls+            grpc_server_cancel_all_calls(self._server.c_server)+            await shutdown_completed+        else:+            try:+                await asyncio.wait_for(asyncio.shield(shutdown_completed), grace)+            except asyncio.TimeoutError:+                # Cancels all ongoing calls by the end of grace period.+                grpc_server_cancel_all_calls(self._server.c_server)",Do we have any guarantees from core about how long this call will take to complete?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20805,339848557,2019-10-29T00:22:18Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +302,97 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start(backup_queue=False)+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown process starts, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def shutdown(self, grace):+        """"""Gracefully shutdown the C-Core server.++        Application should only call shutdown once.++        Args:+          grace: An optional float indicates the length of grace period in+            seconds.+        """"""+        if self._status != AIO_SERVER_STATUS_RUNNING:+            # The server either is shutting down, or not started.+            return+        cdef object shutdown_completed = self._loop.create_future()+        cdef CallbackWrapper wrapper = CallbackWrapper(+            shutdown_completed,+            SERVER_SHUTDOWN_FAILURE_HANDLER)+        # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+        # when calling ""await"". This is an over-optimization by Cython.+        cpython.Py_INCREF(wrapper)++        # Starts the shutdown process.+        # The shutdown callback won't be called unless there is no live RPC.+        grpc_server_shutdown_and_notify(+            self._server.c_server,+            self._cq._cq,+            wrapper.c_functor())+        self._server.is_shutting_down = True+        self._status = AIO_SERVER_STATUS_STOPPING++        # Ensures the serving task (coroutine) exits.+        try:+            await self._serving_task+        except _RequestCallError:+            pass++        if grace is None:+            # Directly cancels all calls+            grpc_server_cancel_all_calls(self._server.c_server)+            await shutdown_completed+        else:+            try:+                await asyncio.wait_for(asyncio.shield(shutdown_completed), grace)+            except asyncio.TimeoutError:+                # Cancels all ongoing calls by the end of grace period.+                grpc_server_cancel_all_calls(self._server.c_server)+                await shutdown_completed++        # Keeps wrapper object alive until now.+        cpython.Py_DECREF(wrapper)+        grpc_server_destroy(self._server.c_server)+        self._server.c_server = NULL+        self._server.is_shutdown = True+        self._status = AIO_SERVER_STATUS_STOPPED++        # Shuts down the completion queue+        await self._cq.shutdown()",Same question. Do we have any guarantees from core about how long this method will take to complete?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20805,339849826,2019-10-29T00:28:40Z,src/python/grpcio/grpc/_cython/_cygrpc/server.pyx.pxi,"@@ -61,16 +61,25 @@ cdef class Server:           self.c_server, queue.c_completion_queue, NULL)     self.registered_completion_queues.append(queue) -  def start(self):+  def start(self, backup_queue=True):+    """"""Start the Cython gRPC Server.+    +    Args:+      backup_queue: a bool indicates whether to spawn a backup completion","We definitely don't want to expose this to the user. In fact, I think now is the time to pay down this debt and figure out whether or not a backup queue is actually needed. For my part, I can't actually think of a case where it *is* needed.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,339855199,2019-10-29T00:57:23Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +302,97 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start(backup_queue=False)+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown process starts, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def shutdown(self, grace):+        """"""Gracefully shutdown the C-Core server.++        Application should only call shutdown once.++        Args:+          grace: An optional float indicates the length of grace period in+            seconds.+        """"""+        if self._status != AIO_SERVER_STATUS_RUNNING:+            # The server either is shutting down, or not started.+            return+        cdef object shutdown_completed = self._loop.create_future()+        cdef CallbackWrapper wrapper = CallbackWrapper(+            shutdown_completed,+            SERVER_SHUTDOWN_FAILURE_HANDLER)+        # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+        # when calling ""await"". This is an over-optimization by Cython.+        cpython.Py_INCREF(wrapper)++        # Starts the shutdown process.+        # The shutdown callback won't be called unless there is no live RPC.+        grpc_server_shutdown_and_notify(+            self._server.c_server,+            self._cq._cq,+            wrapper.c_functor())+        self._server.is_shutting_down = True+        self._status = AIO_SERVER_STATUS_STOPPING++        # Ensures the serving task (coroutine) exits.+        try:+            await self._serving_task+        except _RequestCallError:+            pass++        if grace is None:+            # Directly cancels all calls+            grpc_server_cancel_all_calls(self._server.c_server)+            await shutdown_completed+        else:+            try:+                await asyncio.wait_for(asyncio.shield(shutdown_completed), grace)+            except asyncio.TimeoutError:+                # Cancels all ongoing calls by the end of grace period.+                grpc_server_cancel_all_calls(self._server.c_server)","`grpc_server_cancel_all_calls` itself is none blocking. Semantically, I don't think it is doing heavy work in C-Core. What's your rationale of asking this?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,339855670,2019-10-29T00:59:59Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +302,97 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start(backup_queue=False)+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown process starts, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def shutdown(self, grace):+        """"""Gracefully shutdown the C-Core server.++        Application should only call shutdown once.++        Args:+          grace: An optional float indicates the length of grace period in+            seconds.+        """"""+        if self._status != AIO_SERVER_STATUS_RUNNING:+            # The server either is shutting down, or not started.+            return+        cdef object shutdown_completed = self._loop.create_future()+        cdef CallbackWrapper wrapper = CallbackWrapper(+            shutdown_completed,+            SERVER_SHUTDOWN_FAILURE_HANDLER)+        # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+        # when calling ""await"". This is an over-optimization by Cython.+        cpython.Py_INCREF(wrapper)++        # Starts the shutdown process.+        # The shutdown callback won't be called unless there is no live RPC.+        grpc_server_shutdown_and_notify(+            self._server.c_server,+            self._cq._cq,+            wrapper.c_functor())+        self._server.is_shutting_down = True+        self._status = AIO_SERVER_STATUS_STOPPING++        # Ensures the serving task (coroutine) exits.+        try:+            await self._serving_task+        except _RequestCallError:+            pass++        if grace is None:+            # Directly cancels all calls+            grpc_server_cancel_all_calls(self._server.c_server)+            await shutdown_completed+        else:+            try:+                await asyncio.wait_for(asyncio.shield(shutdown_completed), grace)+            except asyncio.TimeoutError:+                # Cancels all ongoing calls by the end of grace period.+                grpc_server_cancel_all_calls(self._server.c_server)+                await shutdown_completed++        # Keeps wrapper object alive until now.+        cpython.Py_DECREF(wrapper)+        grpc_server_destroy(self._server.c_server)+        self._server.c_server = NULL+        self._server.is_shutdown = True+        self._status = AIO_SERVER_STATUS_STOPPED++        # Shuts down the completion queue+        await self._cq.shutdown()","The shutdown of completion queue is similar to server that depends on triggering the shutdown event. I'm using the most conservative approach that make sure the server is fully stopped, then attempt to shutdown the completion queue.So, by the time this call is invoked, completion queue should be already empty, and shutdown immediately (depends on next event loop).",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,339856034,2019-10-29T01:02:14Z,src/python/grpcio/grpc/_cython/_cygrpc/server.pyx.pxi,"@@ -61,16 +61,25 @@ cdef class Server:           self.c_server, queue.c_completion_queue, NULL)     self.registered_completion_queues.append(queue) -  def start(self):+  def start(self, backup_queue=True):+    """"""Start the Cython gRPC Server.+    +    Args:+      backup_queue: a bool indicates whether to spawn a backup completion","This won't be exposed to our user. This Server class is existing API's Server class. I'm reusing around 40% of its code, so I added the option here. Will investigate if we can remove it completely. Reopen (https://github.com/grpc/grpc/issues/17515) to track this issue.Removing the queue is easy, but tracking the impact is complex that I think it deserves its own PR (even cherry-pick).",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,339857293,2019-10-29T01:09:43Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -21,40 +21,140 @@ from src.proto.grpc.testing import messages_pb2 from src.proto.grpc.testing import benchmark_service_pb2_grpc from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants -_TEST_METHOD_PATH = ''+_SIMPLE_UNARY_UNARY = '/test/SimpleUnaryUnary'+_BLOCK_FOREVER = '/test/BlockForever'+_BLOCK_SHORTLY = '/test/BlockShortly'  _REQUEST = b'\x00\x00\x00' _RESPONSE = b'\x01\x01\x01'  -async def unary_unary(unused_request, unused_context):-    return _RESPONSE+class _GenericHandler(grpc.GenericRpcHandler): +    def __init__(self):+        self._called = asyncio.get_event_loop().create_future() -class GenericHandler(grpc.GenericRpcHandler):+    @staticmethod+    async def _unary_unary(unused_request, unused_context):+        return _RESPONSE -    def service(self, unused_handler_details):-        return grpc.unary_unary_rpc_method_handler(unary_unary)+    async def _block_forever(self, unused_request, unused_context):+        await asyncio.get_event_loop().create_future()++    async def _block_shortly(self, unused_request, unused_context):+        await asyncio.sleep(test_constants.SHORT_TIMEOUT / 2)+        return _RESPONSE++    def service(self, handler_details):+        self._called.set_result(None)+        if handler_details.method == _SIMPLE_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(self._unary_unary)+        if handler_details.method == _BLOCK_FOREVER:+            return grpc.unary_unary_rpc_method_handler(self._block_forever)+        if handler_details.method == _BLOCK_SHORTLY:+            return grpc.unary_unary_rpc_method_handler(self._block_shortly)++    async def wait_for_call(self):+        await self._called+++async def _start_test_server():+    server = aio.server()+    port = server.add_insecure_port('[::]:0')","Yes. It is working on Kokoro. Discussed offline about the pattern (try to bound IPv6, fallback to IPv4).",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/18856,339909293,2019-10-29T06:27:17Z,include/grpcpp/impl/codegen/server_context_impl.h,"@@ -272,6 +279,36 @@ class ServerContext {   /// Applications never need to call this method.   grpc_call* c_call() { return call_; } +  /// NOTE: This is an API for advanced users who need custom allocators.+  /// Get and maybe mutate the allocator state associated with the current RPC.+  /// Currently only applicable for callback unary RPC methods.+  /// WARNING: This is experimental API and could be changed or removed.+  ::grpc::experimental::RpcAllocatorState* GetRpcAllocatorState() {+    return message_allocator_state_;+  }++  /// Get the default unary reactor for use in minimal reaction cases.","I added the words ""library-owned"" as well as a more detailed description to this comment.",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/18856,339909617,2019-10-29T06:29:19Z,include/grpcpp/impl/codegen/server_context_impl.h,"@@ -272,6 +279,36 @@ class ServerContext {   /// Applications never need to call this method.   grpc_call* c_call() { return call_; } +  /// NOTE: This is an API for advanced users who need custom allocators.+  /// Get and maybe mutate the allocator state associated with the current RPC.+  /// Currently only applicable for callback unary RPC methods.+  /// WARNING: This is experimental API and could be changed or removed.+  ::grpc::experimental::RpcAllocatorState* GetRpcAllocatorState() {+    return message_allocator_state_;+  }++  /// Get the default unary reactor for use in minimal reaction cases.","In reply to your other comment (which I somehow can't reply to through the UI), we're not using a factory precisely because this is supposed to avoid an allocation. I added a comment saying that they shouldn't call this more than once; although there's no enforcement mechanism now or even any negative impact, we may take advantage of this restriction in the future (e.g., to ease lazy initialization of the DefaultReactor).",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/20572,339935511,2019-10-29T08:12:11Z,src/core/ext/transport/chttp2/client/insecure/channel_create.cc,"@@ -109,3 +109,27 @@ grpc_channel* grpc_insecure_channel_create(const char* target,                                   target, GRPC_STATUS_INTERNAL,                                   ""Failed to create client channel""); }++/* The same as grpc_insecure_channel_create expect for:+ *   - doesn't create a new ExecCtx+ *   - doesn't call grpc_init or grpc_shutdown internally */+grpc_channel* grpc_insecure_channel_create_internal(","> I don't actually care about the ExecCtx stacking. The code can currently handle that, and we're trying to make ExecCtx go away completely in the long run anyway.A little late response but not sure I understand here BTW. Speaking generally, my view is that `ExecCtx` stacking is error prone due to re-entrant locking risk. https://github.com/grpc/grpc/pull/20596 actually tries hard to avoid it.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20572,340127669,2019-10-29T14:55:45Z,src/core/ext/transport/chttp2/client/insecure/channel_create.cc,"@@ -109,3 +109,27 @@ grpc_channel* grpc_insecure_channel_create(const char* target,                                   target, GRPC_STATUS_INTERNAL,                                   ""Failed to create client channel""); }++/* The same as grpc_insecure_channel_create expect for:+ *   - doesn't create a new ExecCtx+ *   - doesn't call grpc_init or grpc_shutdown internally */+grpc_channel* grpc_insecure_channel_create_internal(","@vjpai or @yashykt can comment in more detail, but it's my understanding that the ExecCtx code automatically detects when this stacking is happening and basically shunts all of the queued closures to the ""root"" ExecCtx instance, so there should not be any risk of deadlock.  At this point, I'd rather be putting effort into making ExecCtx go away instead of jumping through hoops trying to avoid stacking.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,340223238,2019-10-29T17:31:28Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -21,40 +21,140 @@ from src.proto.grpc.testing import messages_pb2 from src.proto.grpc.testing import benchmark_service_pb2_grpc from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants -_TEST_METHOD_PATH = ''+_SIMPLE_UNARY_UNARY = '/test/SimpleUnaryUnary'+_BLOCK_FOREVER = '/test/BlockForever'+_BLOCK_SHORTLY = '/test/BlockShortly'  _REQUEST = b'\x00\x00\x00' _RESPONSE = b'\x01\x01\x01'  -async def unary_unary(unused_request, unused_context):-    return _RESPONSE+class _GenericHandler(grpc.GenericRpcHandler): +    def __init__(self):+        self._called = asyncio.get_event_loop().create_future() -class GenericHandler(grpc.GenericRpcHandler):+    @staticmethod+    async def _unary_unary(unused_request, unused_context):+        return _RESPONSE -    def service(self, unused_handler_details):-        return grpc.unary_unary_rpc_method_handler(unary_unary)+    async def _block_forever(self, unused_request, unused_context):+        await asyncio.get_event_loop().create_future()++    async def _block_shortly(self, unused_request, unused_context):+        await asyncio.sleep(test_constants.SHORT_TIMEOUT / 2)+        return _RESPONSE++    def service(self, handler_details):+        self._called.set_result(None)+        if handler_details.method == _SIMPLE_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(self._unary_unary)+        if handler_details.method == _BLOCK_FOREVER:+            return grpc.unary_unary_rpc_method_handler(self._block_forever)+        if handler_details.method == _BLOCK_SHORTLY:+            return grpc.unary_unary_rpc_method_handler(self._block_shortly)++    async def wait_for_call(self):+        await self._called+++async def _start_test_server():+    server = aio.server()+    port = server.add_insecure_port('[::]:0')+    generic_handler = _GenericHandler()+    server.add_generic_rpc_handlers((generic_handler,))+    await server.start()+    return 'localhost:%d' % port, server, generic_handler   class TestServer(AioTestBase):      def test_unary_unary(self):          async def test_unary_unary_body():-            server = aio.server()-            port = server.add_insecure_port('[::]:0')-            server.add_generic_rpc_handlers((GenericHandler(),))-            await server.start()+            server_target, _, _ = await _start_test_server() -            async with aio.insecure_channel('localhost:%d' % port) as channel:-                unary_call = channel.unary_unary(_TEST_METHOD_PATH)+            async with aio.insecure_channel(server_target) as channel:+                unary_call = channel.unary_unary(_SIMPLE_UNARY_UNARY)                 response = await unary_call(_REQUEST)                 self.assertEqual(response, _RESPONSE)          self.loop.run_until_complete(test_unary_unary_body()) +    def test_shutdown(self):++        async def test_shutdown_body():+            _, server, _ = await _start_test_server()+            await server.stop(None)++        self.loop.run_until_complete(test_shutdown_body())++    def test_shutdown_after_call(self):++        async def test_shutdown_body():+            server_target, server, _ = await _start_test_server()++            async with aio.insecure_channel(server_target) as channel:+                await channel.unary_unary(_SIMPLE_UNARY_UNARY)(_REQUEST)++            await server.stop(None)++        self.loop.run_until_complete(test_shutdown_body())++    def test_graceful_shutdown_success(self):++        async def test_graceful_shutdown_success_body():+            server_target, server, generic_handler = await _start_test_server()++            channel = aio.insecure_channel(server_target)+            call_task = self.loop.create_task(+                channel.unary_unary(_BLOCK_SHORTLY)(_REQUEST))+            await generic_handler.wait_for_call()++            await server.stop(test_constants.SHORT_TIMEOUT)","I ran current version 1k times, all passed. And the more strict version 1k times, all passed.In the more strict version, this test case will measure the length of grace period, and ensure it is long enough.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,340253015,2019-10-29T18:27:24Z,src/python/grpcio/grpc/experimental/aio/_server.py,"@@ -83,35 +86,32 @@ def add_secure_port(self, address: Text,         """"""         await self._server.start() -    def stop(self, grace: Optional[float]) -> asyncio.Event:+    async def stop(self, grace: Optional[float]) -> None:         """"""Stops this Server. -        ""This method immediately stops the server from servicing new RPCs in+        This method immediately stops the server from servicing new RPCs in         all cases. -        If a grace period is specified, this method returns immediately-        and all RPCs active at the end of the grace period are aborted.-        If a grace period is not specified (by passing None for `grace`),-        all existing RPCs are aborted immediately and this method-        blocks until the last RPC handler terminates.+        If a grace period is specified, all RPCs active at the end of the grace+        period are aborted. -        This method is idempotent and may be called at any time.-        Passing a smaller grace value in a subsequent call will have-        the effect of stopping the Server sooner (passing None will-        have the effect of stopping the server immediately). Passing-        a larger grace value in a subsequent call *will not* have the-        effect of stopping the server later (i.e. the most restrictive-        grace value is used).+        If a grace period is not specified (by passing None for `grace`), all","This is due to AsyncIO semantic. To interact with callbacks from C-Core, the Python function `Server.stop` needs to be an `async def` function. So the usage becomes blocking instead of returning immediately:```Pythonserver = grpc.aio.server(...)...SOME_USER_LOGIC# server.stop() has to be awaited, or scheduled as Task.await server.stop(None)# orawait server.stop(10)```My original expectation was that ""stop"" should only be called in one coroutine.---Second thought, maybe it is a valid use case to use multiple coroutine to invoke shutdown of server. E.g., one coroutine for signal handling, one for receiving instruction from control plane.So, I refactored the shutdown function, so it can accept calls from different coroutine and honor the shortest grace period.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20805,340262670,2019-10-29T18:47:12Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/callbackcontext.pxd.pxi,"@@ -16,5 +16,5 @@ cimport cpython  cdef struct CallbackContext:     grpc_experimental_completion_queue_functor functor-    cpython.PyObject *waiter-+    cpython.PyObject *waiter  # asyncio.Future","I'm trying to understand why we can't use strong typing here. Is the idea that `cdef struct` has less overhead than `cdef class`?I'm trying to compare the code generated by the two:```C++/* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/callbackcontext.pxd.pxi"":17  * cimport cpython *  * cdef struct CallbackContext:             # <<<<<<<<<<<<<< *     grpc_experimental_completion_queue_functor functor  *     cpython.PyObject *waiter  # asyncio.Future     */                                                struct __pyx_t_7_cython_6cygrpc_CallbackContext {      // Pretty much a one-to-one mapping with the Cython code.  grpc_experimental_completion_queue_functor functor;  PyObject *waiter;                                    PyObject *failure_handler;};```Vs.```C++/* ""src/python/grpcio/grpc/_cython/_cygrpc/arguments.pxd.pxi"":22  *  *  * cdef class _ChannelArg:             # <<<<<<<<<<<<<<    *  *   cdef grpc_arg c_argument  */struct __pyx_obj_7_cython_6cygrpc__ChannelArg {  PyObject_HEAD // PyObject struct.  struct __pyx_vtabstruct_7_cython_6cygrpc__ChannelArg *__pyx_vtab; // Pointer to vtable.  grpc_arg c_argument;};```Typing is a really nice feature. If we're going to give it up, we should probably at least quantify the performance gains we're getting instead of it.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20824,340320101,2019-10-29T20:54:54Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/rpc_error.pyx.pxi,"@@ -14,7 +14,7 @@ """"""Exceptions for the aio version of the RPC calls.""""""  -cdef class _AioRpcError(Exception):+cdef class AioRpcError(Exception):","`AioRpcError` it's an internal `Exception` only used between the gRPC cython and gRPC Python world, the developer knows nothing since the `cygrpc. AioRpcError` is cast to a `grpc.aio.experimental.AioRpcError` during the `Call.__awaIt__` which is a Python class that derivates from the `grpc.RpcError`.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20805,340320663,2019-10-29T20:56:06Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -21,24 +24,60 @@ cdef class _HandlerCallDetails: class _ServicerContextPlaceHolder(object): pass  +cdef class CallbackFailureHandler:+    cdef str _c_core_api+    cdef object _error_details+    cdef object _exception_type+    cdef object _callback  # Callable[[Future], None]++    def __cinit__(self,+                  str c_core_api="""",+                  object error_details=""UNKNOWN"",+                  object exception_type=RuntimeError,",What's the advantage of making the exception type configurable?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20805,340324811,2019-10-29T21:05:07Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -21,24 +24,60 @@ cdef class _HandlerCallDetails: class _ServicerContextPlaceHolder(object): pass  +cdef class CallbackFailureHandler:+    cdef str _c_core_api+    cdef object _error_details+    cdef object _exception_type+    cdef object _callback  # Callable[[Future], None]++    def __cinit__(self,+                  str c_core_api="""",+                  object error_details=""UNKNOWN"",+                  object exception_type=RuntimeError,+                  object callback=None):+        """"""Handles failure by raising exception or execute a callbcak.+        +        The callback accepts a future, returns nothing. The callback is+        expected to finish the future either ""set_result"" or ""set_exception"".+        """"""+        if callback is None:","Okay. I see that you're using keyword arguments to create an interface that's one of:```def __cinit__(str c_core_api, object error_details, object exception_type)``` or```def __cinit__(object callback)```My impression is that `__cinit__` is meant as a place to do any heap allocations necessary for your extension type. This interface seems a bit complicated for an internal API.I think this object can have a trivial (default) `__cinit__`. All of its data types are trivially constructible. Perhaps have `__cinit__` do nothing, then have two separate methods that set it up according to each of the code paths currently in this method?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20805,340325303,2019-10-29T21:06:14Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -21,24 +24,60 @@ cdef class _HandlerCallDetails: class _ServicerContextPlaceHolder(object): pass  +cdef class CallbackFailureHandler:+    cdef str _c_core_api+    cdef object _error_details+    cdef object _exception_type+    cdef object _callback  # Callable[[Future], None]++    def __cinit__(self,+                  str c_core_api="""",+                  object error_details=""UNKNOWN"",+                  object exception_type=RuntimeError,+                  object callback=None):+        """"""Handles failure by raising exception or execute a callbcak.+        +        The callback accepts a future, returns nothing. The callback is+        expected to finish the future either ""set_result"" or ""set_exception"".+        """"""+        if callback is None:+            self._c_core_api = c_core_api+            self._error_details = error_details+            self._exception_type = exception_type    +            self._callback = self._raise_exception+        else:+            self._callback = callback++    def _raise_exception(self, object future):+        future.set_exception(self._exception_type(+            'Failed ""%s"": %s' % (self._c_core_api, self._error_details)+        ))++    cdef handle(self, object future):+        self._callback(future)++ # TODO(https://github.com/grpc/grpc/issues/20669) # Apply this to the client-side cdef class CallbackWrapper:-    cdef CallbackContext context-    cdef object _reference -    def __cinit__(self, object future):+    def __cinit__(self, object future, CallbackFailureHandler failure_handler):         self.context.functor.functor_run = self.functor_run-        self.context.waiter = <cpython.PyObject*>(future)-        self._reference = future+        self.context.waiter = <cpython.PyObject*>future+        self.context.failure_handler = <cpython.PyObject*>failure_handler+        # NOTE(lidiz) Not using a list here, because this class is critical in",I'm curious. What *would* you use a list for here?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20824,340328714,2019-10-29T21:13:59Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -13,15 +13,207 @@ # limitations under the License. """"""Invocation-side implementation of gRPC Asyncio Python."""""" import asyncio-from typing import Callable, Optional+from typing import Callable, Dict, Optional +import grpc from grpc import _common from grpc._cython import cygrpc  SerializingFunction = Callable[[str], bytes] DeserializingFunction = Callable[[bytes], str]  +class AioRpcError(grpc.RpcError):+    """"""An RpcError to be used by the asynchronous API.""""""",Are you suggesting to start using a lighter object for enveloping these simple attributes as a dataclass or a namedtuple (which uses __slots__ behind the scenes)?I did not want to open this can of worms considering that legacy API uses regular Python objects. And also considering that the exception is out of the happy path and any optimization in the none happy path is a nice to have.,
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/20854,340341987,2019-10-29T21:47:11Z,src/core/ext/filters/client_idle/client_idle_filter.cc,"@@ -27,12 +27,12 @@ #include ""src/core/lib/surface/channel_init.h"" #include ""src/core/lib/transport/http2_errors.h"" -// The idle filter is enabled in client channel by default.-// Set GRPC_ARG_CLIENT_IDLE_TIMEOUT_MS to [1000, INT_MAX) in channel args to-// configure the idle timeout.-#define DEFAULT_IDLE_TIMEOUT_MS (30 /*minutes*/ * 60 * 1000)-// The user input idle timeout smaller than this would be capped to it.-#define MIN_IDLE_TIMEOUT_MS (1 /*second*/ * 1000)+// The idle filter is disabled in client channel by default.",@nanahpang Can you provide some quantifiable results on whether or not the mutex version is a significant performance hit? That was the other thing that surprised me about #19727 - I didn't see performance numbers posted in the PR (as folks like @soheilhy and @arjunroy typically do in atomic and other optimization PRs),
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20805,340351822,2019-10-29T22:15:50Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +302,97 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start(backup_queue=False)+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown process starts, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def shutdown(self, grace):+        """"""Gracefully shutdown the C-Core server.++        Application should only call shutdown once.++        Args:+          grace: An optional float indicates the length of grace period in+            seconds.+        """"""+        if self._status != AIO_SERVER_STATUS_RUNNING:+            # The server either is shutting down, or not started.+            return+        cdef object shutdown_completed = self._loop.create_future()+        cdef CallbackWrapper wrapper = CallbackWrapper(+            shutdown_completed,+            SERVER_SHUTDOWN_FAILURE_HANDLER)+        # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+        # when calling ""await"". This is an over-optimization by Cython.+        cpython.Py_INCREF(wrapper)++        # Starts the shutdown process.+        # The shutdown callback won't be called unless there is no live RPC.+        grpc_server_shutdown_and_notify(+            self._server.c_server,+            self._cq._cq,+            wrapper.c_functor())+        self._server.is_shutting_down = True+        self._status = AIO_SERVER_STATUS_STOPPING++        # Ensures the serving task (coroutine) exits.+        try:+            await self._serving_task+        except _RequestCallError:+            pass++        if grace is None:+            # Directly cancels all calls+            grpc_server_cancel_all_calls(self._server.c_server)",Nit: There's some duplication between this case and the case that we *do* have a grace period. Dedupe?,X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20824,340352489,2019-10-29T22:17:58Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pxd.pxi,"@@ -20,8 +20,9 @@ cdef class _AioCall:         grpc_completion_queue * _cq         grpc_experimental_completion_queue_functor _functor         object _waiter_call+        list _references      @staticmethod-    cdef void functor_run(grpc_experimental_completion_queue_functor* functor, int succeed)+    cdef void functor_run(grpc_experimental_completion_queue_functor* functor, int succeed) with gil     @staticmethod-    cdef void watcher_call_functor_run(grpc_experimental_completion_queue_functor* functor, int succeed)+    cdef void watcher_call_functor_run(grpc_experimental_completion_queue_functor* functor, int succeed) with gil","Just double checked and seems that there are two threads, weird I would need to understand where this second thread comes from considering that the BT is only polluted of CPython functions and the way that Ive debug this was NOT USING the runner. So theoretically we should have only one thread (maybe unitest is running the tests in isolated threads?)But in any case, without having the `with gil` in the callback, Cython does not know that you are requiring to acquire the GIL since you released when you called the gRPC syscall. Just doublechecked what Cython does when you specifically add the`with gil`, it adds the following call:```  #ifdef WITH_THREAD  PyGILState_STATE __pyx_gilstate_save = __Pyx_PyGILState_Ensure();  #endif``` Seems that `PyGILState_Ensure` [1] is gonna make sure that you can safely call the C Python API (How I hate these tens of functions with similar semantics). TBH, I would say that 99% for sure that we should add this for the callbacks that are being called from the gRPC library.If you need 100% confidence it will take a bit more time[1] https://docs.python.org/2/c-api/init.html#c.PyGILState_Ensure ",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20805,340352707,2019-10-29T22:18:36Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +309,105 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start(backup_queue=False)+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def _start_shutting_down(self):+        """"""Prepares the server to shutting down (NOT coroutine-safe).""""""",Seems like the comment about coroutine safety belongs on a separate line in the docstring.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20824,340353930,2019-10-29T22:22:11Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -13,15 +13,207 @@ # limitations under the License. """"""Invocation-side implementation of gRPC Asyncio Python."""""" import asyncio-from typing import Callable, Optional+from typing import Callable, Dict, Optional +import grpc from grpc import _common from grpc._cython import cygrpc  SerializingFunction = Callable[[str], bytes] DeserializingFunction = Callable[[bytes], str]  +class AioRpcError(grpc.RpcError):+    """"""An RpcError to be used by the asynchronous API.""""""","For the first suggestion, I didn't state it clearly, I mean annotate the types of the instance variables.```Pythonclass AioRpcError(grpc.RpcError):    _code: grpc.StatusCode    _details: Text    ...```For the second suggestion, I don't know much about the drawbacks of `__slots__`, can we discuss them here? I personally votes for the changes that prevents users from hacking our implementation.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20824,340362373,2019-10-29T22:49:33Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -45,21 +46,20 @@ cdef class _AioCall:         return f""<{class_name} {id_}>""      @staticmethod-    cdef void functor_run(grpc_experimental_completion_queue_functor* functor, int succeed):+    cdef void functor_run(grpc_experimental_completion_queue_functor* functor, int succeed) with gil:         pass      @staticmethod-    cdef void watcher_call_functor_run(grpc_experimental_completion_queue_functor* functor, int succeed):+    cdef void watcher_call_functor_run(grpc_experimental_completion_queue_functor* functor, int succeed) with gil:         call = <_AioCall>(<CallbackContext *>functor).waiter -        assert call._waiter_call+        if not call._waiter_call.done():+            if succeed == 0:+                call._waiter_call.set_exception(Exception(""Some error occurred""))","This error is a bit vague. We should at least raise an error type particular to gRPC. As for the message, it's unfortunate that the Core API doesn't provide any more information. :/From a usability perspective, the least we can do is to advise the user on how to proceed. Maybe we can add a message on how to turn up logging. Something along the lines of.```An unspecified error occurred. More information can be found with increased logging. Try setting the environment variables GRPC_VERBOSITY=DEBUG and GRPC_TRACE=all.```(I realize that this PR didn't actually add these lines, but I'm just now noticing this. Feel free to add this as an issue instead of blocking merge on it.)",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,340368609,2019-10-29T23:11:30Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -21,24 +24,60 @@ cdef class _HandlerCallDetails: class _ServicerContextPlaceHolder(object): pass  +cdef class CallbackFailureHandler:+    cdef str _c_core_api+    cdef object _error_details+    cdef object _exception_type+    cdef object _callback  # Callable[[Future], None]++    def __cinit__(self,+                  str c_core_api="""",+                  object error_details=""UNKNOWN"",+                  object exception_type=RuntimeError,","Syntactic sugar for maintainer to catch specific type of exceptions, with exception type:```Pythontry:   ...A_MIGHTY_CORE_API(..., wrapper)except _YOUR_FAILURE_TYPE_FOR_THAT_CALLBACK:   ...ERROR_HANDLING_LOGIC```Instead of:```Pythontry:   ...A_MIGHTY_CORE_API(..., wrapper)except Exception as e:    if e.core_function_name == A_MIGHTY_CORE_API:       ...ERROR_HANDLING_LOGIC```",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,340371202,2019-10-29T23:21:10Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +309,105 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start(backup_queue=False)+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def _start_shutting_down(self):+        """"""Prepares the server to shutting down (NOT coroutine-safe).""""""",I re-organized the class docstring.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20824,340372267,2019-10-29T23:25:35Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -13,15 +13,209 @@ # limitations under the License. """"""Invocation-side implementation of gRPC Asyncio Python."""""" import asyncio-from typing import Callable, Optional+from typing import Callable, Dict, Optional +import grpc from grpc import _common from grpc._cython import cygrpc  SerializingFunction = Callable[[str], bytes] DeserializingFunction = Callable[[bytes], str]  +class AioRpcError(grpc.RpcError):+    """"""An RpcError to be used by the asynchronous API.""""""++    def __init__(self,+                 code: grpc.StatusCode,+                 details: Optional[str] = None,+                 initial_metadata: Optional[Dict] = None,+                 trailing_metadata: Optional[Dict] = None):+        super().__init__(self)+        self._code = code+        self._details = details+        self._initial_metadata = initial_metadata+        self._trailing_metadata = trailing_metadata++    def code(self) -> grpc.StatusCode:+        """"""+        Returns:+          The `grpc.StatusCode` status code.+        """"""+        return self._code++    def details(self) -> Optional[str]:+        """"""+        Returns:+          The description of the error.+        """"""+        return self._details++    def initial_metadata(self) -> Optional[Dict]:","I'm very much a fan of the change from `Sequence[Tuple[Text, int]]` to `Optional[Dict]`. :+1:Any chance we can make the type more specific by parameterizing?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,340373191,2019-10-29T23:28:55Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +309,105 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start(backup_queue=False)+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def _start_shutting_down(self):+        """"""Prepares the server to shutting down (NOT coroutine-safe).""""""+        # Starts the shutdown process.+        # The shutdown callback won't be called until there is no live RPC.+        grpc_server_shutdown_and_notify(+            self._server.c_server,+            self._cq._cq,+            self._shutdown_callback_wrapper.c_functor()) -    # TODO(https://github.com/grpc/grpc/issues/20668)-    # Implement Destruction Methods for AsyncIO Server-    def stop(self, unused_grace):-        pass+        # Ensures the serving task (coroutine) exits.+        try:+            await self._serving_task+        except _RequestCallError:+            pass++    async def shutdown(self, grace):+        """"""Gracefully shutdown the C-Core server.++        Application should only call shutdown once.++        Args:+          grace: An optional float indicating the length of grace period in+            seconds.+        """"""+        if self._status == AIO_SERVER_STATUS_READY or self._status == AIO_SERVER_STATUS_STOPPED:+            return++        async with self._shutdown_lock:+            if self._status == AIO_SERVER_STATUS_RUNNING:+                await self._start_shutting_down()+                self._server.is_shutting_down = True+                self._status = AIO_SERVER_STATUS_STOPPING","I'm trying to use code in `cygrpc.Server` they are depending on several boolean values which is the pattern I also dislike. But to make the re-used code work properly, I need to update those state. The solution is:1) Making existing server state better (and use that one);2) Copying the code to AsyncIO server.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20824,340374793,2019-10-29T23:35:38Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -25,6 +24,167 @@ from tests_aio.unit._test_base import AioTestBase  +class TestAioRpcError(unittest.TestCase):+    _TEST_INITIAL_METADATA = (""initial metadata"",)+    _TEST_TRAILING_METADATA = (""trailing metadata"",)++    def test_attributes(self):+        aio_rpc_error = aio.AioRpcError(+            grpc.StatusCode.CANCELLED,+            ""details"",+            initial_metadata=self._TEST_INITIAL_METADATA,+            trailing_metadata=self._TEST_TRAILING_METADATA)+        self.assertEqual(aio_rpc_error.code(), grpc.StatusCode.CANCELLED)+        self.assertEqual(aio_rpc_error.details(), ""details"")+        self.assertEqual(aio_rpc_error.initial_metadata(),+                         self._TEST_INITIAL_METADATA)+        self.assertEqual(aio_rpc_error.trailing_metadata(),+                         self._TEST_TRAILING_METADATA)+++class TestCall(AioTestBase):++    def test_call_ok(self):++        async def coro():+            server_target, unused_server = await start_test_server()++            async with aio.insecure_channel(server_target) as channel:+                hi = channel.unary_unary(+                    '/grpc.testing.TestService/UnaryCall',+                    request_serializer=messages_pb2.SimpleRequest.+                    SerializeToString,+                    response_deserializer=messages_pb2.SimpleResponse.FromString+                )+                call = hi(messages_pb2.SimpleRequest())++                self.assertFalse(call.done())++                response = await call++                self.assertTrue(call.done())+                self.assertEqual(type(response), messages_pb2.SimpleResponse)+                self.assertEqual(await call.code(), grpc.StatusCode.OK)++                # response is cached at call object level, reentrance+                # returns again the same response+                response_retry = await call+                self.assertEqual(response, response_retry)++        self.loop.run_until_complete(coro())++    def test_call_rpc_error(self):++        async def coro():+            server_target, unused_server = await start_test_server()",s/unused_server/_/ ?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20824,340388811,2019-10-30T00:41:23Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -13,15 +13,209 @@ # limitations under the License. """"""Invocation-side implementation of gRPC Asyncio Python."""""" import asyncio-from typing import Callable, Optional+from typing import Callable, Dict, Optional +import grpc from grpc import _common from grpc._cython import cygrpc  SerializingFunction = Callable[[str], bytes] DeserializingFunction = Callable[[bytes], str]  +class AioRpcError(grpc.RpcError):+    """"""An RpcError to be used by the asynchronous API.""""""++    def __init__(self,+                 code: grpc.StatusCode,+                 details: Optional[str] = None,+                 initial_metadata: Optional[Dict] = None,+                 trailing_metadata: Optional[Dict] = None):+        super().__init__(self)+        self._code = code+        self._details = details+        self._initial_metadata = initial_metadata+        self._trailing_metadata = trailing_metadata++    def code(self) -> grpc.StatusCode:+        """"""+        Returns:+          The `grpc.StatusCode` status code.+        """"""+        return self._code++    def details(self) -> Optional[str]:+        """"""+        Returns:+          The description of the error.+        """"""+        return self._details++    def initial_metadata(self) -> Optional[Dict]:+        """"""+        Returns:+          The inital metadata received.+        """"""+        return self._initial_metadata++    def trailing_metadata(self) -> Optional[Dict]:+        """"""+        Returns:+          The trailing metadata received.+        """"""+        return self._trailing_metadata+++class Call:+    """""" Object for managing RPC calls, returned when an instance of+    `UnaryUnaryMultiCallable` object is called.+    """"""++    _cancellation_details = 'Locally cancelled by application!'++    def __init__(self, aio_call: asyncio.Task,+                 response_deserializer: DeserializingFunction,+                 aio_call_cancel_status: cygrpc.AioCancelStatus) -> None:+        self._exception = None+        self._response = None+        self._code = None+        self._details = None+        self._initial_metadata = None+        self._trailing_metadata = None+        self._cancelled = False+        self._aio_call = aio_call",Nit: Seems a bit redundant to have `aio` in the name. It's already in a file and class specifically for asyncio.,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20824,340614133,2019-10-30T13:33:14Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -13,15 +13,209 @@ # limitations under the License. """"""Invocation-side implementation of gRPC Asyncio Python."""""" import asyncio-from typing import Callable, Optional+from typing import Callable, Dict, Optional +import grpc from grpc import _common from grpc._cython import cygrpc  SerializingFunction = Callable[[str], bytes] DeserializingFunction = Callable[[bytes], str]  +class AioRpcError(grpc.RpcError):+    """"""An RpcError to be used by the asynchronous API.""""""++    def __init__(self,+                 code: grpc.StatusCode,+                 details: Optional[str] = None,+                 initial_metadata: Optional[Dict] = None,+                 trailing_metadata: Optional[Dict] = None):+        super().__init__(self)+        self._code = code+        self._details = details+        self._initial_metadata = initial_metadata+        self._trailing_metadata = trailing_metadata++    def code(self) -> grpc.StatusCode:+        """"""+        Returns:+          The `grpc.StatusCode` status code.+        """"""+        return self._code++    def details(self) -> Optional[str]:+        """"""+        Returns:+          The description of the error.+        """"""+        return self._details++    def initial_metadata(self) -> Optional[Dict]:","like `Dict[str, bytes]`? or maybe we can leave this decision for the metadata PR [1] where we will have more knowledge about the type returned.[1] https://github.com/grpc/grpc/issues/20144",
175566,kcwu,https://api.github.com/repos/grpc/grpc/pulls/20589,340679673,2019-10-30T15:15:54Z,src/core/lib/surface/channel.cc,"@@ -122,11 +122,15 @@ grpc_channel* grpc_channel_create_with_builder(           static_cast<uint32_t>(args->args[i].value.integer) |           0x1; /* always support no compression */     } else if (0 == strcmp(args->args[i].key, GRPC_ARG_CHANNELZ_CHANNEL_NODE)) {-      GPR_ASSERT(args->args[i].type == GRPC_ARG_POINTER);","Here are my gdb output```$ gdb -q --args ./out/libfuzzer-ar/grpc_api_fuzzer clusterfuzz-testcase-minimized-grpc_api_fuzzer-5664490786258944Reading symbols from ./out/libfuzzer-ar/grpc_api_fuzzer...done.(gdb) rStarting program: /btrfs/chromium/fuzzing/src/out/libfuzzer-ar/grpc_api_fuzzer clusterfuzz-testcase-minimized-grpc_api_fuzzer-5664490786258944[Thread debugging using libthread_db enabled]Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".INFO: Seed: 748101600INFO: Loaded 1 modules   (50669 inline 8-bit counters): 50669 [0x5555561223f0, 0x55555612e9dd),INFO: Loaded 1 PC tables (50669 PCs): 50669 [0x55555612e9e0,0x5555561f48b0),/btrfs/chromium/fuzzing/src/out/libfuzzer-ar/grpc_api_fuzzer: Running 1 inputs 1 time(s) each.Running: clusterfuzz-testcase-minimized-grpc_api_fuzzer-5664490786258944Thread 1 ""grpc_api_fuzzer"" received signal SIGABRT, Aborted.__GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:5050      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.(gdb) bt#0  __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:50#1  0x00007ffff708d8ad in __GI_abort () at abort.c:79#2  0x0000555555d9861c in grpc_channel_create_with_builder (builder=<optimized out>, channel_stack_type=<optimized out>) at ../../third_party/grpc/src/src/core/lib/surface/channel.cc:126#3  0x0000555555d98cee in grpc_channel_create (target=0x6020000040f0 ""dns:grpc.qqbaad"", input_args=<optimized out>, channel_stack_type=GRPC_CLIENT_CHANNEL, optional_transport=0x0, resource_user=<optimized out>) at ../../third_party/grpc/src/src/core/lib/surface/channel.cc:260#4  0x0000555555c9fa66 in grpc_core::Chttp2InsecureClientChannelFactory::CreateChannel (this=<optimized out>, target=<optimized out>, args=<optimized out>) at ../../third_party/grpc/src/src/core/ext/transport/chttp2/client/insecure/channel_create.cc:65#5  0x0000555555c9f573 in grpc_insecure_channel_create (target=<optimized out>, args=<optimized out>, reserved=<optimized out>) at ../../third_party/grpc/src/src/core/ext/transport/chttp2/client/insecure/channel_create.cc:101#6  0x0000555555ab583a in LLVMFuzzerTestOneInput (data=<optimized out>, size=<optimized out>) at ../../third_party/grpc/src/test/core/end2end/fuzzers/api_fuzzer.cc:801#7  0x0000555555b150c0 in fuzzer::Fuzzer::ExecuteCallback (this=0x617000000080, Data=<optimized out>, Size=<optimized out>) at ../../third_party/libFuzzer/src/FuzzerLoop.cpp:553#8  0x0000555555ae2aeb in fuzzer::RunOneTest (F=<optimized out>, InputFilePath=<optimized out>, MaxLen=<optimized out>) at ../../third_party/libFuzzer/src/FuzzerDriver.cpp:292#9  0x0000555555ae858b in fuzzer::FuzzerDriver (argc=<optimized out>, argv=<optimized out>, Callback=<optimized out>) at ../../third_party/libFuzzer/src/FuzzerDriver.cpp:775#10 0x0000555555b29ebd in main (argc=2, argv=0x7fffffffcd38) at ../../third_party/libFuzzer/src/FuzzerMain.cpp:19(gdb) frame 2#2  0x0000555555d9861c in grpc_channel_create_with_builder (builder=<optimized out>, channel_stack_type=<optimized out>) at ../../third_party/grpc/src/src/core/lib/surface/channel.cc:126126           GPR_ASSERT(args->args[i].value.pointer.p != nullptr);```",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20824,340694923,2019-10-30T15:39:25Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -25,6 +24,167 @@ from tests_aio.unit._test_base import AioTestBase  +class TestAioRpcError(unittest.TestCase):+    _TEST_INITIAL_METADATA = (""initial metadata"",)+    _TEST_TRAILING_METADATA = (""trailing metadata"",)++    def test_attributes(self):+        aio_rpc_error = aio.AioRpcError(+            grpc.StatusCode.CANCELLED,+            ""details"",+            initial_metadata=self._TEST_INITIAL_METADATA,+            trailing_metadata=self._TEST_TRAILING_METADATA)+        self.assertEqual(aio_rpc_error.code(), grpc.StatusCode.CANCELLED)+        self.assertEqual(aio_rpc_error.details(), ""details"")+        self.assertEqual(aio_rpc_error.initial_metadata(),+                         self._TEST_INITIAL_METADATA)+        self.assertEqual(aio_rpc_error.trailing_metadata(),+                         self._TEST_TRAILING_METADATA)+++class TestCall(AioTestBase):++    def test_call_ok(self):++        async def coro():+            server_target, unused_server = await start_test_server()++            async with aio.insecure_channel(server_target) as channel:+                hi = channel.unary_unary(+                    '/grpc.testing.TestService/UnaryCall',+                    request_serializer=messages_pb2.SimpleRequest.+                    SerializeToString,+                    response_deserializer=messages_pb2.SimpleResponse.FromString+                )+                call = hi(messages_pb2.SimpleRequest())++                self.assertFalse(call.done())++                response = await call++                self.assertTrue(call.done())+                self.assertEqual(type(response), messages_pb2.SimpleResponse)+                self.assertEqual(await call.code(), grpc.StatusCode.OK)++                # response is cached at call object level, reentrance+                # returns again the same response+                response_retry = await call","I've changed the `self.assertEqual(response, response_retry)` for a `self.assertIs(response, response_retry)` which should check that the object returned after is the same that was cached. The same for the exceptions.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20824,340702575,2019-10-30T15:51:13Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -133,8 +134,21 @@ cdef class _AioCall:                 self._waiter_call = None                 raise Exception(""Error with grpc_call_start_batch {}"".format(call_status)) -            await self._waiter_call-+            try:+                await self._waiter_call+            except asyncio.CancelledError:+                if cancel_status:+                    details = str_to_bytes(cancel_status.details())+                    self._references.append(details)+                    c_details = <char *>details+                    call_status = grpc_call_cancel_with_status(+                        call, cancel_status.code(), c_details, NULL)+                else:+                    call_status = grpc_call_cancel(+                        call, NULL)+                if call_status != GRPC_CALL_OK:+                    raise Exception(""RPC call couldn't be cancelled, error {}"".format(call_status))","Having the feeling that this should happen only in some situations where the failure needs to be a full error, which should end up by stopping the program.For example, it could happen when invalid params could be given to the  `grpc_call_cancel` function, because of a bug in the `Aio` library. Maybe also when there is no more memory for allocating new objects within the gRPC C++ core library.We should expect, correct me if I'm wrong, that this path should almost never occur, and if it does then raising an exception because something wrong is happening.Making it as a gRPC-specific error might hide the urgency of the issue. ",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20824,340722049,2019-10-30T16:23:49Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -13,15 +13,207 @@ # limitations under the License. """"""Invocation-side implementation of gRPC Asyncio Python."""""" import asyncio-from typing import Callable, Optional+from typing import Callable, Dict, Optional +import grpc from grpc import _common from grpc._cython import cygrpc  SerializingFunction = Callable[[str], bytes] DeserializingFunction = Callable[[bytes], str]  +class AioRpcError(grpc.RpcError):+    """"""An RpcError to be used by the asynchronous API.""""""++    def __init__(self,+                 code: grpc.StatusCode,+                 details: Optional[str] = None,+                 initial_metadata: Optional[Dict] = None,+                 trailing_metadata: Optional[Dict] = None):+        self._code = code+        self._details = details+        self._initial_metadata = initial_metadata+        self._trailing_metadata = trailing_metadata+        super().__init__(self)++    def code(self) -> grpc.StatusCode:+        """"""+        Returns:+          The `grpc.StatusCode` status code.+        """"""+        return self._code++    def details(self) -> Optional[str]:+        """"""+        Returns:+          The description of the error.+        """"""+        return self._details++    def initial_metadata(self) -> Optional[Dict]:+        """"""+        Returns:+          The inital metadata received.+        """"""+        return self._initial_metadata++    def trailing_metadata(self) -> Optional[Dict]:+        """"""+        Returns:+          The trailing metadata received.+        """"""+        return self._trailing_metadata+++class Call:+    """""" Object for managing RPC calls, returned when an instance of+    `UnaryUnaryMultiCallable` object is called.+    """"""++    def __init__(self, aio_call: asyncio.Task,+                 response_deserializer: DeserializingFunction,+                 aio_call_cancel_status: cygrpc.AioCallCancelStatus):+        self._exception = None+        self._response = None+        self._code = None+        self._details = None+        self._initial_metadata = None+        self._trailing_metadata = None+        self._cancelled = False+        self._aio_call = aio_call+        self._aio_call_cancel_status = aio_call_cancel_status+        self._response_deserializer = response_deserializer++    def __del__(self):+        self.cancel()++    def cancel(self) -> bool:+        """"""+        Cancels the ongoing RPC request.++        Returns:+          True if the RPC can be canceled, False if was already cancelled or terminated.+        """"""+        if self.cancelled() or self.done():+            return False","Implemented, no `READY` state since the call is instantiated when is already `READY`",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20824,340756037,2019-10-30T17:25:30Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -13,15 +13,209 @@ # limitations under the License. """"""Invocation-side implementation of gRPC Asyncio Python."""""" import asyncio-from typing import Callable, Optional+from typing import Callable, Dict, Optional +import grpc from grpc import _common from grpc._cython import cygrpc  SerializingFunction = Callable[[str], bytes] DeserializingFunction = Callable[[bytes], str]  +class AioRpcError(grpc.RpcError):+    """"""An RpcError to be used by the asynchronous API.""""""++    def __init__(self,+                 code: grpc.StatusCode,+                 details: Optional[str] = None,+                 initial_metadata: Optional[Dict] = None,+                 trailing_metadata: Optional[Dict] = None):+        super().__init__(self)+        self._code = code+        self._details = details+        self._initial_metadata = initial_metadata+        self._trailing_metadata = trailing_metadata++    def code(self) -> grpc.StatusCode:+        """"""+        Returns:+          The `grpc.StatusCode` status code.+        """"""+        return self._code++    def details(self) -> Optional[str]:+        """"""+        Returns:+          The description of the error.+        """"""+        return self._details++    def initial_metadata(self) -> Optional[Dict]:",Exactly what I meant. I'm fine deferring that until the metadata PR. But can we add a TODO on one of these types so we know to come back to it?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20866,340761749,2019-10-30T17:36:19Z,src/core/ext/filters/client_channel/http_proxy.cc,"@@ -42,16 +42,19 @@  * credentials if present in the 'http_proxy' env var, otherwise leaves it  * unchanged. It is caller's responsibility to gpr_free user_cred.  */-static char* get_http_proxy_server(char** user_cred) {+static char* get_http_proxy_server(const grpc_channel_args* args, char** user_cred) {   GPR_ASSERT(user_cred != nullptr);   char* proxy_name = nullptr;   char** authority_strs = nullptr;   size_t authority_nstrs;-  /* Prefer using 'grpc_proxy'. Fallback on 'http_proxy' if it is not set.+  /* Check channel arguments in case a per channel proxy is set. Otherwise,+   * prefer using 'grpc_proxy'. Fallback on 'http_proxy' if it is not set.    * Also prefer using 'https_proxy' with fallback on 'http_proxy'. The    * fallback behavior can be removed if there's a demand for it.    */-  char* uri_str = gpr_getenv(""grpc_proxy"");+  const grpc_arg* arg = grpc_channel_args_find(args, GRPC_ARG_HTTP_PROXY);",Please use `grpc_channel_args_find_string()` here:https://github.com/grpc/grpc/blob/e3fb73fec501f2b7b27999ea9b10e41d7ac4ea22/src/core/lib/channel/channel_args.h#L93,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20824,340767537,2019-10-30T17:47:31Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -0,0 +1,245 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Invocation-side implementation of gRPC Asyncio Python.""""""+import asyncio+import enum+from typing import Callable, Dict, Optional, ClassVar++import grpc+from grpc import _common+from grpc._cython import cygrpc++DeserializingFunction = Callable[[bytes], str]+++class AioRpcError(grpc.RpcError):+    """"""An RpcError to be used by the asynchronous API.""""""++    _code: grpc.StatusCode+    _details: Optional[str]+    _initial_metadata: Optional[Dict]+    _trailing_metadata: Optional[Dict]++    def __init__(self,+                 code: grpc.StatusCode,+                 details: Optional[str] = None,+                 initial_metadata: Optional[Dict] = None,+                 trailing_metadata: Optional[Dict] = None):+        super().__init__(self)+        self._code = code+        self._details = details+        self._initial_metadata = initial_metadata+        self._trailing_metadata = trailing_metadata++    def code(self) -> grpc.StatusCode:+        """"""",This is more about Google's convention that the docstring should follow certain pattern. Can you take a look at http://google.github.io/styleguide/pyguide.html#384-classes? This apply to other docstrings in this file.,
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/20829,340786288,2019-10-30T18:23:46Z,src/php/ext/grpc/completion_queue.h,"@@ -23,6 +23,10 @@  #include <grpc/grpc.h> +#if PHP_MAJOR_VERSION >= 8",Why is this needed only in this file but not other similar `.h` files?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,340789508,2019-10-30T18:29:41Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +302,97 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start(backup_queue=False)+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown process starts, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def shutdown(self, grace):+        """"""Gracefully shutdown the C-Core server.++        Application should only call shutdown once.++        Args:+          grace: An optional float indicates the length of grace period in+            seconds.+        """"""+        if self._status != AIO_SERVER_STATUS_RUNNING:+            # The server either is shutting down, or not started.+            return+        cdef object shutdown_completed = self._loop.create_future()+        cdef CallbackWrapper wrapper = CallbackWrapper(+            shutdown_completed,+            SERVER_SHUTDOWN_FAILURE_HANDLER)+        # NOTE(lidiz) Without Py_INCREF, the wrapper object will be destructed+        # when calling ""await"". This is an over-optimization by Cython.+        cpython.Py_INCREF(wrapper)++        # Starts the shutdown process.+        # The shutdown callback won't be called unless there is no live RPC.+        grpc_server_shutdown_and_notify(+            self._server.c_server,+            self._cq._cq,+            wrapper.c_functor())+        self._server.is_shutting_down = True+        self._status = AIO_SERVER_STATUS_STOPPING++        # Ensures the serving task (coroutine) exits.+        try:+            await self._serving_task+        except _RequestCallError:+            pass++        if grace is None:+            # Directly cancels all calls+            grpc_server_cancel_all_calls(self._server.c_server)+            await shutdown_completed+        else:+            try:+                await asyncio.wait_for(asyncio.shield(shutdown_completed), grace)+            except asyncio.TimeoutError:+                # Cancels all ongoing calls by the end of grace period.+                grpc_server_cancel_all_calls(self._server.c_server)+                await shutdown_completed++        # Keeps wrapper object alive until now.+        cpython.Py_DECREF(wrapper)+        grpc_server_destroy(self._server.c_server)+        self._server.c_server = NULL+        self._server.is_shutdown = True+        self._status = AIO_SERVER_STATUS_STOPPED++        # Shuts down the completion queue+        await self._cq.shutdown() -    # TODO(https://github.com/grpc/grpc/issues/20668)-    # Implement Destruction Methods for AsyncIO Server-    def stop(self, unused_grace):-        pass+    def __dealloc__(self):","I wrote a test case locally to spin up 100k servers and left them to `gc` in batches. If the server isn't collected properly, the port will run out. I didn't add it to our test case, because it took sooo long to finish and behavior might be different across platform.But I can confirm that with `grpc.aio.Server.__del__` the ports will got reused, and without `grpc.aio.Server.__del__` the test will fail.---Back to the discussion about calling order. I tried to print logs for deallocation, the `grpc.aio.Server.__del__` comes before `cygrpc.AioServer.__dealloc__`. Theoretically, the `Server` holds a reference to `AioServer`, so `AioServer` could only be deallocated after `Server` is fully gone which is after `__del__` execution.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20805,340865936,2019-10-30T21:25:32Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def _start_shutting_down(self):+        """"""Prepares the server to shutting down.++        This coroutine function is NOT coroutine-safe.+        """"""+        # The shutdown callback won't be called until there is no live RPC.+        grpc_server_shutdown_and_notify(+            self._server.c_server,+            self._cq._cq,+            self._shutdown_callback_wrapper.c_functor())++        # Ensures the serving task (coroutine) exits.+        try:+            await self._serving_task+        except _RequestCallError:","Have you experimented that exception? so having the `REQUEST_CALL_FAILURE_HANDLER` being executed.I'm asking because I'm not sure if we should propagate this exception until this piece of the code which doesn't bother if behind the scenes gRPC library couldn't manage a request call, so maybe this exception should have a narrowed scope in this line [1][1] https://github.com/grpc/grpc/pull/20805/files#diff-b53c077f7911f53dc8ce7656e16d35cdR317",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20805,340867729,2019-10-30T21:29:27Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(","If `_RequestCallError` notfies a transient error I would go for something like this```pythontry:    rpc_state = await _server_call_request_call(...)except _RequestCallError:    logging.warning(""...."")    continue```If the `_RequestCallError` is a terminal error so the server can not keep attrending requets then I will try to raise another exception or just break the loop```pythontry:    rpc_state = await _server_call_request_call(...)except _RequestCallError:    logging.warning(""...."")    break```",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20805,340879541,2019-10-30T22:01:06Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))","An open question, why we should not take care of stopping also the running tasks?Otherwise, correct me if I'm wrong, we have the risk of stopping the server and stoping finally the program having still Asyncio Tasks in the event loop, which might result on logging in the console information about some destroyed tasks that were never finished [1]So, having the feeling that besides asking the gRPC library for stopping all RPC tasks, we should take care after of stopping all of the ongoing Asyncio tasks.[1] https://github.com/python/cpython/blob/master/Lib/asyncio/tasks.py#L168",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,340889229,2019-10-30T22:32:23Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -186,54 +223,65 @@ async def _server_call_request_call(Server server,     return rpc_state  -async def _server_main_loop(Server server,-                            _CallbackCompletionQueue cq,-                            list generic_handlers):-    cdef object loop = asyncio.get_event_loop()-    cdef RPCState rpc_state--    while True:-        rpc_state = await _server_call_request_call(-            server,-            cq,-            loop)--        loop.create_task(_handle_rpc(generic_handlers, rpc_state, loop))---async def _server_start(Server server,-                        _CallbackCompletionQueue cq,-                        list generic_handlers):-    server.start()-    await _server_main_loop(server, cq, generic_handlers)+cdef _CallbackFailureHandler CQ_SHUTDOWN_FAILURE_HANDLER = _CallbackFailureHandler(+    'grpc_completion_queue_shutdown',+    'Unknown',+    RuntimeError)   cdef class _CallbackCompletionQueue: -    def __cinit__(self):+    def __cinit__(self, object loop):+        self._loop = loop+        self._shutdown_completed = loop.create_future()+        self._wrapper = CallbackWrapper(+            self._shutdown_completed,+            CQ_SHUTDOWN_FAILURE_HANDLER)         self._cq = grpc_completion_queue_create_for_callback(-            NULL,+            self._wrapper.c_functor(),             NULL         )      cdef grpc_completion_queue* c_ptr(self):         return self._cq+    +    async def shutdown(self):+        grpc_completion_queue_shutdown(self._cq)+        await self._shutdown_completed+        grpc_completion_queue_destroy(self._cq)+++cdef _CallbackFailureHandler SERVER_SHUTDOWN_FAILURE_HANDLER = _CallbackFailureHandler(+    'grpc_server_shutdown_and_notify',+    'Unknown',+    RuntimeError)   cdef class AioServer: -    def __init__(self, thread_pool, generic_handlers, interceptors, options,-                 maximum_concurrent_rpcs, compression):+    def __init__(self, loop, thread_pool, generic_handlers, interceptors,+                 options, maximum_concurrent_rpcs, compression):+        # NOTE(lidiz) Core objects won't be deallocated automatically.+        # If AioServer.shutdown is not called, those objects will leak.         self._server = Server(options)-        self._cq = _CallbackCompletionQueue()-        self._status = AIO_SERVER_STATUS_READY-        self._generic_handlers = []+        self._cq = _CallbackCompletionQueue(loop)         grpc_server_register_completion_queue(             self._server.c_server,             self._cq.c_ptr(),             NULL         )++        self._loop = loop+        self._status = AIO_SERVER_STATUS_READY+        self._generic_handlers = []         self.add_generic_rpc_handlers(generic_handlers)+        self._serving_task = None++        self._shutdown_lock = asyncio.Lock()","Almost all of the keyword argument `loop=` in the surface of `asyncio` is marked deprecated in 3.8, and will be removed in 3.10. E.g. https://docs.python.org/3/library/asyncio-task.html",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,340890212,2019-10-30T22:35:57Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(","I have a slightly different view here. I'm explicitly wanting the error to be ignored only if it is in shutdown phase. Otherwise, `_server_main_loop` is the serving coroutine, if it exits unexpectedly, I hope it raise to application layer to notify users.One improvement I see is that it could use an `grpc.RpcError`. But the semantic will be a bit weird here, since it is not an RPC specific error.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,340891567,2019-10-30T22:40:53Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))","In the shutdown process, it would cancel all gRPC calls (by the end of the grace period). Ongoing RPC will be cancelled, and pending RPC will fail once started. The tricky part here is whether to track pending RPC or not.For tasks created in gRPC framework on the server-side, if it is not `await`ed by any caller, then the exception won't be propagated to the main thread. Hence, it would just be another entry of log for users application, even if we trace all ongoing RPC, and cancel them explicitly.On the other hand, tracking all ongoing RPC might slightly drag the performance.If you feel strongly, I'm okay to change it.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,340891985,2019-10-30T22:42:32Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def _start_shutting_down(self):+        """"""Prepares the server to shutting down.++        This coroutine function is NOT coroutine-safe.+        """"""+        # The shutdown callback won't be called until there is no live RPC.+        grpc_server_shutdown_and_notify(+            self._server.c_server,+            self._cq._cq,+            self._shutdown_callback_wrapper.c_functor())++        # Ensures the serving task (coroutine) exits.+        try:+            await self._serving_task+        except _RequestCallError:","As in another comment, I hope that exception to be logged or propagate to application if the serving coroutine unexpectedly exits. The only case we are safe to ignore that is in the shutdown process, hence I only catch that exception here.",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/20316,340952118,2019-10-31T04:01:01Z,src/core/tsi/ssl_transport_security.cc,"@@ -567,21 +568,81 @@ static tsi_result ssl_ctx_use_private_key(SSL_CTX* context, const char* pem_key,   EVP_PKEY* private_key = nullptr;   BIO* pem;   GPR_ASSERT(pem_key_size <= INT_MAX);-  pem = BIO_new_mem_buf((void*)pem_key, static_cast<int>(pem_key_size));-  if (pem == nullptr) return TSI_OUT_OF_RESOURCES;-  do {-    private_key = PEM_read_bio_PrivateKey(pem, nullptr, nullptr, (void*)"""");-    if (private_key == nullptr) {-      result = TSI_INVALID_ARGUMENT;-      break;-    }-    if (!SSL_CTX_use_PrivateKey(context, private_key)) {-      result = TSI_INVALID_ARGUMENT;-      break;-    }-  } while (0);-  if (private_key != nullptr) EVP_PKEY_free(private_key);-  BIO_free(pem);++// BoringSSL does not have ENGINE_load_dynamic, ENGINE_by_id+// support.+#ifndef OPENSSL_IS_BORINGSSL+  if (strncmp(pem_key, ""engine:"", sizeof(""engine:"") - 1) == 0)+  {+    do {+      char *p, *last;+      ENGINE* engine;+      gpr_log(GPR_INFO, ""ENGINE key specified"");++      p = (char*)pem_key + sizeof(""engine:"") - 1;+      last = (char*)strchr(p, ':');+      if (last == nullptr) {+        gpr_log(GPR_ERROR, ""No engine defined %s"", p);+        result = TSI_INVALID_ARGUMENT;+        break;+      }++      ENGINE_load_dynamic();++      *last = '\0';+      engine = ENGINE_by_id(p);+      if (engine == nullptr) {+        gpr_log(GPR_ERROR, ""Cannot load engine %s"", p);+        result = TSI_INVALID_ARGUMENT;+        break;+      }++      *last++ = ':';++      if (!ENGINE_set_default(engine, ENGINE_METHOD_ALL)) {+        gpr_log(GPR_ERROR, ""Cannot set default methods for engine"");+        result = TSI_INVALID_ARGUMENT;+        break;        +      }++      if (!ENGINE_init(engine)) {+        gpr_log(GPR_ERROR, ""ENGINE Initialization failed"");+        result = TSI_INVALID_ARGUMENT;+        break;+      }++      private_key = ENGINE_load_private_key(engine, last, 0, 0);+      if (private_key == nullptr) {+        gpr_log(GPR_ERROR, ""ENGINE load private key() failed"");+        result = TSI_INVALID_ARGUMENT;+        break;+      }++      if (!SSL_CTX_use_PrivateKey(context, private_key)) {+          result = TSI_INVALID_ARGUMENT;+          break;+      }+    } while (0);","Do we need to EVP_PKEY_free() the private key here (similar to line 643 below)?If not, you can return TSI_INVALID_ARGUMENT directly rather than set result and break in lines 596, 604, 610, 617, 622.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20663,341143601,2019-10-31T13:46:06Z,BUILDING.md,"@@ -130,6 +130,28 @@ From the grpc repository root $ bazel build :all ``` +## cmake: Linux, Using Make+Run from grpc directory after cloning the repo with --recursive or updating submodules.+```+$ mkdir build+$ cd build+$ cmake ..+$ make+```++If you want to build shared libraries (`.so` files), run `cmake` with `-DBUILD_SHARED_LIBS=ON`.","because BUILDING.md is mostly for developer build (not for end-users), wouldn't it be better to have the `-DBUILD_SHARED_LIBS=ON` tip in https://github.com/grpc/grpc/blob/master/src/cpp/README.md (which is more focused on how I use gRPC as an end user)?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20663,341150559,2019-10-31T13:58:32Z,BUILDING.md,"@@ -155,3 +179,16 @@ installed to be able to compile the C/C++ sources. > cmake .. -GNinja -DCMAKE_BUILD_TYPE=Release > cmake --build . ```++If you want to build DLLs, run `cmake` with `-DBUILD_SHARED_LIBS=ON`.++## cmake: Install After Build","the main documentation in terms of how to use gRPC as a user is here: https://github.com/grpc/grpc/blob/master/src/cpp/README.md, so perhaps we should move there? (and only add a note here?).Btw, this is already mentioned here: https://github.com/grpc/grpc/blob/master/BUILDING.md#build-from-source (feel free to rephrase if the wording is not clear).",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/20800,341221510,2019-10-31T15:54:30Z,test/core/compression/stream_compression_fuzzer.cc,"@@ -0,0 +1,51 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <stdbool.h>+#include <stdint.h>+#include <string.h>++#include ""src/core/lib/compression/stream_compression.h""+#include ""src/core/lib/security/credentials/credentials.h""+#include ""test/core/util/memory_counters.h""++extern ""C"" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {+  grpc_core::testing::LeakDetector leak_detector(true);+  grpc_init();+  grpc_test_only_control_plane_credentials_force_init();+  auto* context = grpc_stream_compression_context_create(+      GRPC_STREAM_COMPRESSION_GZIP_COMPRESS);+  grpc_slice_buffer input_buffer;+  grpc_slice_buffer_init(&input_buffer);+  grpc_slice_buffer_add(+      &input_buffer,+      grpc_slice_from_copied_buffer(reinterpret_cast<const char*>(data), size));+  grpc_slice_buffer output_buffer;+  grpc_slice_buffer_init(&output_buffer);++  grpc_stream_compress(context, &input_buffer, &output_buffer, nullptr,+                       ~(size_t)-1, GRPC_STREAM_COMPRESSION_FLUSH_SYNC);",This is ok but I think this parameter can probably also be taken from the fuzzing data to test out different cases. Optional.,
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/20800,341221736,2019-10-31T15:54:51Z,test/core/compression/stream_decompression_fuzzer.cc,"@@ -0,0 +1,52 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <stdbool.h>+#include <stdint.h>+#include <string.h>++#include ""src/core/lib/compression/stream_compression.h""+#include ""src/core/lib/security/credentials/credentials.h""+#include ""test/core/util/memory_counters.h""++extern ""C"" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {+  grpc_core::testing::LeakDetector leak_detector(true);+  grpc_init();+  grpc_test_only_control_plane_credentials_force_init();+  auto* context = grpc_stream_compression_context_create(+      GRPC_STREAM_COMPRESSION_GZIP_DECOMPRESS);+  grpc_slice_buffer input_buffer;+  grpc_slice_buffer_init(&input_buffer);+  grpc_slice_buffer_add(+      &input_buffer,+      grpc_slice_from_copied_buffer(reinterpret_cast<const char*>(data), size));+  grpc_slice_buffer output_buffer;+  grpc_slice_buffer_init(&output_buffer);+  bool end_of_context;++  grpc_stream_decompress(context, &input_buffer, &output_buffer, nullptr,+                         (size_t)-1, &end_of_context);",Ditto. This is ok but I think this parameter can probably also be taken from the fuzzing data to test out different cases. Optional.,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20805,341226610,2019-10-31T16:02:56Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -186,54 +223,65 @@ async def _server_call_request_call(Server server,     return rpc_state  -async def _server_main_loop(Server server,-                            _CallbackCompletionQueue cq,-                            list generic_handlers):-    cdef object loop = asyncio.get_event_loop()-    cdef RPCState rpc_state--    while True:-        rpc_state = await _server_call_request_call(-            server,-            cq,-            loop)--        loop.create_task(_handle_rpc(generic_handlers, rpc_state, loop))---async def _server_start(Server server,-                        _CallbackCompletionQueue cq,-                        list generic_handlers):-    server.start()-    await _server_main_loop(server, cq, generic_handlers)+cdef _CallbackFailureHandler CQ_SHUTDOWN_FAILURE_HANDLER = _CallbackFailureHandler(+    'grpc_completion_queue_shutdown',+    'Unknown',+    RuntimeError)   cdef class _CallbackCompletionQueue: -    def __cinit__(self):+    def __cinit__(self, object loop):+        self._loop = loop+        self._shutdown_completed = loop.create_future()+        self._wrapper = CallbackWrapper(+            self._shutdown_completed,+            CQ_SHUTDOWN_FAILURE_HANDLER)         self._cq = grpc_completion_queue_create_for_callback(-            NULL,+            self._wrapper.c_functor(),             NULL         )      cdef grpc_completion_queue* c_ptr(self):         return self._cq+    +    async def shutdown(self):+        grpc_completion_queue_shutdown(self._cq)+        await self._shutdown_completed+        grpc_completion_queue_destroy(self._cq)+++cdef _CallbackFailureHandler SERVER_SHUTDOWN_FAILURE_HANDLER = _CallbackFailureHandler(+    'grpc_server_shutdown_and_notify',+    'Unknown',+    RuntimeError)   cdef class AioServer: -    def __init__(self, thread_pool, generic_handlers, interceptors, options,-                 maximum_concurrent_rpcs, compression):+    def __init__(self, loop, thread_pool, generic_handlers, interceptors,+                 options, maximum_concurrent_rpcs, compression):+        # NOTE(lidiz) Core objects won't be deallocated automatically.+        # If AioServer.shutdown is not called, those objects will leak.         self._server = Server(options)-        self._cq = _CallbackCompletionQueue()-        self._status = AIO_SERVER_STATUS_READY-        self._generic_handlers = []+        self._cq = _CallbackCompletionQueue(loop)         grpc_server_register_completion_queue(             self._server.c_server,             self._cq.c_ptr(),             NULL         )++        self._loop = loop+        self._status = AIO_SERVER_STATUS_READY+        self._generic_handlers = []         self.add_generic_rpc_handlers(generic_handlers)+        self._serving_task = None++        self._shutdown_lock = asyncio.Lock()","Then IMO we should remove `self._loop = loop` and use the `asyncio.get_event_loop()` when it is used for create a future. Otherwise, we are in a kind of a middle-ground which is not consistent.In any case, we might end up in the situation in the next future where we decide to add the `loop` kw into the `Channel` and `Server`. But this is a discussion for later on.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20805,341229820,2019-10-31T16:08:40Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(","But is `_RequestCallError` a temporary error? if so, we should keep running the loop. If not is more a kind of termination error that should stop the server and communicates the user in some way that the server is stoped.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20805,341240684,2019-10-31T16:28:32Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def _start_shutting_down(self):+        """"""Prepares the server to shutting down.++        This coroutine function is NOT coroutine-safe.+        """"""+        # The shutdown callback won't be called until there is no live RPC.+        grpc_server_shutdown_and_notify(+            self._server.c_server,+            self._cq._cq,+            self._shutdown_callback_wrapper.c_functor())++        # Ensures the serving task (coroutine) exits.+        try:+            await self._serving_task+        except _RequestCallError:+            pass++    async def shutdown(self, grace):+        """"""Gracefully shutdown the C-Core server.++        Application should only call shutdown once.++        Args:+          grace: An optional float indicating the length of grace period in+            seconds.+        """"""+        if self._status == AIO_SERVER_STATUS_READY or self._status == AIO_SERVER_STATUS_STOPPED:+            return++        async with self._shutdown_lock:+            if self._status == AIO_SERVER_STATUS_RUNNING:+                await self._start_shutting_down()+                self._server.is_shutting_down = True+                self._status = AIO_SERVER_STATUS_STOPPING++        if grace is None:+            # Directly cancels all calls+            grpc_server_cancel_all_calls(self._server.c_server)+            await self._shutdown_completed+        else:+            try:+                await asyncio.wait_for(asyncio.shield(self._shutdown_completed), grace)","Umm, but what `shield` does is protect the coroutine to not be canceled because of the parent one was, I don't get the point why it is useful in this situation. Consider a situation where `shield` could be useful:```pythonasync def handler(self, request):    if requets.host not in cache:        ip = await asyncio.wait_for( asyuncio.shield(dns_resolution_and_cache(request.hostname)))```In the previous scenario no matter what we want to finish the resolution and store the result in a cache.How is this pattern related to the `shutdown` coroutine called by the user and the shielding of the `_shutdown_completed`? I'm a bit lost, sorry.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,341342831,2019-10-31T20:08:03Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(","Yes, my intent is to make it a termination error.I added a `_crash_exception` field for the server, and will raise it at `wait_for_termination`. This is my solution to notify users. Do you have better suggestions?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,341345058,2019-10-31T20:13:52Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))","How to inject exception to a running coroutine is a language-level open question. From a framework point of view, IMO we should focus on offering users the ability to do so, e.g. provide `add_done_callback` so they can stop their long-running-batch-process.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,341352626,2019-10-31T20:32:59Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))+     async def start(self):         if self._status == AIO_SERVER_STATUS_RUNNING:             return         elif self._status != AIO_SERVER_STATUS_READY:             raise RuntimeError('Server not in ready state')          self._status = AIO_SERVER_STATUS_RUNNING-        loop = asyncio.get_event_loop()-        loop.create_task(_server_start(-            self._server,-            self._cq,-            self._generic_handlers,-        ))+        cdef object server_started = self._loop.create_future()+        self._serving_task = self._loop.create_task(self._server_main_loop(server_started))+        # Needs to explicitly wait for the server to start up.+        # Otherwise, the actual start time of the server is un-controllable.+        await server_started++    async def _start_shutting_down(self):+        """"""Prepares the server to shutting down.++        This coroutine function is NOT coroutine-safe.+        """"""+        # The shutdown callback won't be called until there is no live RPC.+        grpc_server_shutdown_and_notify(+            self._server.c_server,+            self._cq._cq,+            self._shutdown_callback_wrapper.c_functor())++        # Ensures the serving task (coroutine) exits.+        try:+            await self._serving_task+        except _RequestCallError:+            pass++    async def shutdown(self, grace):+        """"""Gracefully shutdown the C-Core server.++        Application should only call shutdown once.++        Args:+          grace: An optional float indicating the length of grace period in+            seconds.+        """"""+        if self._status == AIO_SERVER_STATUS_READY or self._status == AIO_SERVER_STATUS_STOPPED:+            return++        async with self._shutdown_lock:+            if self._status == AIO_SERVER_STATUS_RUNNING:+                await self._start_shutting_down()+                self._server.is_shutting_down = True+                self._status = AIO_SERVER_STATUS_STOPPING++        if grace is None:+            # Directly cancels all calls+            grpc_server_cancel_all_calls(self._server.c_server)+            await self._shutdown_completed+        else:+            try:+                await asyncio.wait_for(asyncio.shield(self._shutdown_completed), grace)","There are two questions need to be answered:### What does the `asyncio.shield` provides?In `asyncio` world, if parent coroutine is cancelled, child coroutine, awaited coroutines will also be cancelled. Just as you said. But there is a slightly different case for `asyncio.wait_for` here, it would cancel all supplied coroutines if timeout occurs. So, adding the `asyncio.shield` here protects supplied future being cancelled by `asyncio.wait_for`. A glance at the [doc](https://docs.python.org/3/library/asyncio-task.html#asyncio.wait_for):> Wait for the aw awaitable to complete with a timeout.> > If aw is a coroutine it is automatically scheduled as a Task.> > timeout can either be None or a float or int number of seconds to wait for. If timeout is None, block until the future completes.> > If a timeout occurs, it cancels the task and raises asyncio.TimeoutError.> > To avoid the task cancellation, wrap it in shield().### Why do we need it?`_shutdown_completed` future is bound to the execution of server shutdown callback. This callback indicates the readiness of Core to destroy C server object safely. I'm taking the most conservative route here, that we must make sure the Core is ready, before we proceed to the server destruction (otherwise, undefined behavior, segfault, abort, might happen). So, we cannot cancel the `_shutdown_completed` future, and want to ensure its preciseness.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20805,341375213,2019-10-31T21:33:03Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))",IMO sooner or later we will end up canceling explicitly all Asyncio tasks. But let's iterate. The current code is a big step forward.,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/20462,341389333,2019-10-31T22:19:48Z,include/grpc/support/alloc.h,"@@ -52,15 +45,6 @@ GPRAPI void* gpr_malloc_aligned(size_t size, size_t alignment); /** free memory allocated by gpr_malloc_aligned */ GPRAPI void gpr_free_aligned(void* ptr);","Thank you chwarr@. From your list, the idea to have separate free methods for each caller doesn't seem valid since there are a couple of functions. markrdroth@ I don't think that we can change this in this way.",
35056280,srini100,https://api.github.com/repos/grpc/grpc/pulls/20888,341402813,2019-10-31T23:17:35Z,src/core/lib/iomgr/ev_poll_posix.cc,"@@ -1379,7 +1379,9 @@ static void reset_event_manager_on_fork() {   gpr_mu_lock(&fork_fd_list_mu);   while (fork_fd_list_head != nullptr) {     if (fork_fd_list_head->fd != nullptr) {-      close(fork_fd_list_head->fd->fd);+      if (!fork_fd_list_head->fd->closed) {+        close(fork_fd_list_head->fd->fd);",Shouldn't fork_fd_list_head->fd->closed be set after closing the fd?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20888,341414616,2019-11-01T00:23:17Z,src/core/lib/iomgr/ev_poll_posix.cc,"@@ -1379,7 +1379,9 @@ static void reset_event_manager_on_fork() {   gpr_mu_lock(&fork_fd_list_mu);   while (fork_fd_list_head != nullptr) {     if (fork_fd_list_head->fd != nullptr) {-      close(fork_fd_list_head->fd->fd);+      if (!fork_fd_list_head->fd->closed) {+        close(fork_fd_list_head->fd->fd);","Good catch! I thought if the linked list is depleted then maybe no one cares about the state of the `grpc_fd`, but I'm not 100% sure if other portion of code won't use those state after forking.I created another patch at https://github.com/grpc/grpc/pull/20893. But I not sure about if we have sufficient test coverage for behaviors after forking.CC @ericgribkoff",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,341418551,2019-11-01T00:50:36Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(","For the first suggestion, I can understand the rational of logging it (and I do logged it in the callback). I think my later change is the superset of what you are suggesting? Or did I miss something here?Can you explain more about what do you think is the ideal way of structure this part? By the way, I hope the callback of the `_serving_task` covers not only `_RequestCallError` but all type of exceptions.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20462,341603198,2019-11-01T14:45:14Z,include/grpc/support/alloc.h,"@@ -52,15 +45,6 @@ GPRAPI void* gpr_malloc_aligned(size_t size, size_t alignment); /** free memory allocated by gpr_malloc_aligned */ GPRAPI void gpr_free_aligned(void* ptr);","I don't see any reason that this is impossible.  For example, for `grpc_call_get_peer()`, we could change its return type to be a custom peer object with its own dtor.  For `grpc_slice_to_c_string()`, we could investigate changing it to not duplicate the string but instead return a pointer to the existing string and make it the caller's responsibility to duplicate it.  We just have to look at each of these cases on a case-by-case basis.In any case, I agree that this is future work; it's not something we need to address in this PR.",X
7281574,nicolasnoble,https://api.github.com/repos/grpc/grpc/pulls/20801,341768770,2019-11-01T22:13:38Z,test/core/slice/b64_decode_fuzzer.cc,"@@ -0,0 +1,31 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <fuzzer/FuzzedDataProvider.h>","You don't actually need this header, we created our own system for that exact purpose. See the following files:https://github.com/grpc/grpc/blob/22d8f8f0850f34af1b89ef71138efb84bfddd87e/test/core/util/one_corpus_entry_fuzzer.cchttps://github.com/grpc/grpc/blob/618a3f561d4a93f263cca23abad086ed8f4d5e86/test/core/util/grpc_fuzzer.bzlhttps://github.com/grpc/grpc/blob/24b529e408bcf10f067663f1bb745b08a58f898c/test/core/json/BUILD#L23-L33",
7281574,nicolasnoble,https://api.github.com/repos/grpc/grpc/pulls/20801,341769885,2019-11-01T22:19:13Z,test/core/slice/b64_decode_fuzzer.cc,"@@ -0,0 +1,31 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <fuzzer/FuzzedDataProvider.h>",And a usage of it here: https://github.com/grpc/grpc/blob/24b529e408bcf10f067663f1bb745b08a58f898c/test/core/json/fuzzer.cc,
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20800,341782784,2019-11-01T23:30:42Z,test/core/compression/message_decompress_fuzzer.cc,"@@ -0,0 +1,54 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <fuzzer/FuzzedDataProvider.h>+#include <grpc/grpc.h>+#include <stdbool.h>+#include <stdint.h>+#include <string.h>++#include ""src/core/lib/compression/message_compress.h""+#include ""src/core/lib/security/credentials/credentials.h""+#include ""test/core/util/memory_counters.h""++extern ""C"" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {+  FuzzedDataProvider data_provider(data, size);","Because FuzzedDataProvider doesn't work in a standalone build of gRPC (one without oss-fuzz), I suggest using a proto instead. I don't think we have a proto fuzzer, but you could define a proto with the relevant structure and construct it from this raw data. In the event the proto is not valid, you can just return 0. If it's valid, you can proceed with the actual fuzzing. ",X
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/20801,341782938,2019-11-01T23:31:41Z,test/core/slice/b64_decode_fuzzer.cc,"@@ -0,0 +1,31 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <fuzzer/FuzzedDataProvider.h>","Copying my comment from your other PR here:Because FuzzedDataProvider doesn't work in a standalone build of gRPC (one without oss-fuzz), I suggest using a proto instead. I don't think we have a proto fuzzer, but you could define a proto with the relevant structure and construct it from this raw data. In the event the proto is not valid, you can just return 0. If it's valid, you can proceed with the actual fuzzing.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20905,341790458,2019-11-02T00:38:39Z,src/python/grpcio/grpc/_channel.py,"@@ -263,35 +263,45 @@ def _rpc_state_string(class_name, rpc_state):                 rpc_state.debug_error_string)  -class _RpcError(grpc.RpcError, grpc.Call):+class _RpcError(grpc.RpcError, grpc.Call, grpc.Future):     """"""An RPC error not tied to the execution of a particular RPC. +    The state passed to _RpcError must be guaranteed not to be accessed by any+    other threads.","Can we use the copy-on-creation mode here? So the `RpcState` object being copied in the initialization function? In my mind, `_RpcError` is a data structure to store error info, it should be fine to keep a snapshot of the `RpcState`.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20905,341790564,2019-11-02T00:40:02Z,src/python/grpcio/grpc/_channel.py,"@@ -263,35 +263,45 @@ def _rpc_state_string(class_name, rpc_state):                 rpc_state.debug_error_string)  -class _RpcError(grpc.RpcError, grpc.Call):+class _RpcError(grpc.RpcError, grpc.Call, grpc.Future):     """"""An RPC error not tied to the execution of a particular RPC. +    The state passed to _RpcError must be guaranteed not to be accessed by any+    other threads.++    The RPC represented by the state object must not be in-progress.+     Attributes:       _state: An instance of _RPCState.     """"""      def __init__(self, state):+        if state.cancelled:+            raise ValueError(+                ""Cannot instantiate an _RpcError for a cancelled RPC."")+        if state.code is grpc.StatusCode.OK:+            raise ValueError(+                ""Cannot instantiate an _RpcError for a successfully completed RPC.""+            )+        if state.code is None:+            raise ValueError(+                ""Cannot instantiate an _RpcError for an incomplete RPC."")","If we are copying state, maybe we can map above exception cases into corresponding `code` and `details`? E.g. if the `code` is `None` during initialization, maybe we can assign `UNKNOWN` to it.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20905,341790706,2019-11-02T00:42:06Z,src/python/grpcio/grpc/_channel.py,"@@ -302,6 +312,41 @@ def __repr__(self):     def __str__(self):         return self._repr() +    def cancel(self):+        """"""See grpc.Future.cancel.""""""+        return False++    def cancelled(self):+        """"""See grpc.Future.cancelled.""""""+        return False++    def running(self):+        """"""See grpc.Future.running.""""""+        return False++    def done(self):+        """"""See grpc.Future.done.""""""+        return True++    def result(self, timeout=None):  # pylint: disable=unused-argument+        """"""See grpc.Future.result.""""""+        raise self++    def exception(self, timeout=None):  # pylint: disable=unused-argument+        """"""See grpc.Future.exception.""""""+        return self++    def traceback(self, timeout=None):  # pylint: disable=unused-argument+        """"""See grpc.Future.traceback.""""""+        try:+            raise self+        except grpc.RpcError:+            return sys.exc_info()[2]++    def add_done_callback(self, fn, timeout=None):  # pylint: disable=unused-argument","nit: for `pylint: disable=unused-argument`, can we name it `unused_timeout` to suppress the warning?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20905,341791034,2019-11-02T00:46:48Z,src/python/grpcio_tests/tests/unit/_interceptor_test.py,"@@ -70,9 +72,13 @@ def handle_unary_unary(self, request, servicer_context):                 'testkey',                 'testvalue',             ),))+        if request == _EXCEPTION_REQUEST:+            raise RuntimeError()","It might be helpful to raise a special exception private to this test case, so the test case can deterministic catch it. In case, we somehow perform some change that causes `RuntimeError`.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20907,341999628,2019-11-04T11:13:27Z,templates/CMakeLists.txt.template,"@@ -664,7 +665,7 @@     ""high performance general RPC framework""     ""<%text>${gRPC_CORE_VERSION}</%text>""     ""gpr openssl""-    ""-lgrpc -laddress_sorting -lcares -lz""+    ""-lgrpc -laddress_sorting -lupb -lcares -lz""","I think this is wrong. upb is not a real dependency, it's going to be statically linked into libgrpc (all the other libraries are explicit dependencies that can be installed separately, which is not the case of upb, as we know).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20918,342006682,2019-11-04T11:33:49Z,bazel/grpc_build_system.bzl,"@@ -87,6 +89,11 @@ def grpc_cc_library(     linkopts = if_not_windows([""-pthread""])     if use_cfstream:         linkopts = linkopts + if_mac([""-framework CoreFoundation""])+    # Temporary hack for GRPC_USE_ABSL {","if you say ""temporary hack"" you also need to add some more details, like this I don't know anything :-)",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/20805,342120226,2019-11-04T15:47:15Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))","Sorry for not being so much elaborated in my answers, my mine concern was with the orphan Asyncio task which they could be doing useless work since the request was already canceled under the hood.True that in an environment where the shutdown was conducted using a grace period you should expect that during that time All Asyncio tasks should finish.Only in scenarios where there was no grace period or the request took too much time the Asyncio tasks would be left running while the request was already canceled.As I said, if we eventually need to make the shutdown process more consistent we will always have the chance of closing explicitly the Asyncio tasks by just sending a cancellation event.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/20462,342156233,2019-11-04T16:52:32Z,include/grpc/support/alloc.h,"@@ -52,15 +45,6 @@ GPRAPI void* gpr_malloc_aligned(size_t size, size_t alignment); /** free memory allocated by gpr_malloc_aligned */ GPRAPI void gpr_free_aligned(void* ptr);","The change I am proposing here does not have to be in C++.  When I said ""with its own dtor"", I didn't necessarily mean that it's a C++ class with a dtor; it could equally be a C type with a destruction method.  For example, we could do something like this:```typedef struct grpc_peer grpc_peer;// Gets the name of the peer from the peer object.// Caller does not take ownership of the returned string.const char* grpc_peer_get_name(grpc_peer* peer);// Destroys the peer object.void grpc_peer_destroy(grpc_peer* peer);// Returns the peer of the call.grpc_peer* grpc_call_get_peer(grpc_call* call);```In any case, I agree that we can discuss this later.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20918,342209877,2019-11-04T18:48:19Z,bazel/grpc_build_system.bzl,"@@ -87,6 +89,11 @@ def grpc_cc_library(     linkopts = if_not_windows([""-pthread""])     if use_cfstream:         linkopts = linkopts + if_mac([""-framework CoreFoundation""])+    # Temporary hack for GRPC_USE_ABSL {","If you have a temporary hack, please add a TODO with an associated Github issue.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20905,342222909,2019-11-04T19:16:25Z,src/python/grpcio/grpc/_channel.py,"@@ -263,35 +263,45 @@ def _rpc_state_string(class_name, rpc_state):                 rpc_state.debug_error_string)  -class _RpcError(grpc.RpcError, grpc.Call):+class _RpcError(grpc.RpcError, grpc.Call, grpc.Future):     """"""An RPC error not tied to the execution of a particular RPC. +    The state passed to _RpcError must be guaranteed not to be accessed by any+    other threads.++    The RPC represented by the state object must not be in-progress.+     Attributes:       _state: An instance of _RPCState.     """"""      def __init__(self, state):+        if state.cancelled:+            raise ValueError(+                ""Cannot instantiate an _RpcError for a cancelled RPC."")+        if state.code is grpc.StatusCode.OK:+            raise ValueError(+                ""Cannot instantiate an _RpcError for a successfully completed RPC.""+            )+        if state.code is None:+            raise ValueError(+                ""Cannot instantiate an _RpcError for an incomplete RPC."")","Since all tests are passing, I guess I'm talking about some corner cases here.For the semantic of the `_RpcError` class, IMO it is responsible to construct an informative exception to propagate to application.In cases that needs to raise an exception, we should prioritize the propagate of any debug details, and make sure the construction of the exception do not fail. If the initialization raises an exception, the original exception will be lost.So, for cancelled RPC and incomplete RPC, I think it is fine to allow them to be `grpc.RpcError`. For successfully completed RPC, I agree we should raise an `ValueError` since that might indicate logic error in our framework.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,342230930,2019-11-04T19:33:24Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -255,21 +303,112 @@ cdef class AioServer:         return self._server.add_http2_port(address,                                           server_credentials._credentials) +    async def _server_main_loop(self,+                                object server_started):+        self._server.start()+        cdef RPCState rpc_state+        server_started.set_result(True)++        while True:+            # When shutdown begins, no more new connections.+            if self._status != AIO_SERVER_STATUS_RUNNING:+                break++            rpc_state = await _server_call_request_call(+                self._server,+                self._cq,+                self._loop)++            self._loop.create_task(_handle_rpc(+                self._generic_handlers,+                rpc_state,+                self._loop))","Now I see what you are suggesting. I have a fix in `_handle_cancellation_from_core`, but I don't have a perfect solution here.The naive way of handling this is use a list to track every single task created, and explicitly cancel them during shutdown. Personally, I would like to defer using this approach as much as possible, because it requires extensive tracking for RPC life cycle in our wrapper layer and it might impair performance.Instead, I choose to handle the event propagated from Core. It should handle more variety of scenarios (other than shutdown, but also application cancellation). However, it requires a separate coroutine to await the cancel event. Admittedly, it slows down the server. The cancellation feature is essential for gRPC wrapper, we needs it for the feature parity.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/20805,342237469,2019-11-04T19:47:23Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -186,54 +223,65 @@ async def _server_call_request_call(Server server,     return rpc_state  -async def _server_main_loop(Server server,-                            _CallbackCompletionQueue cq,-                            list generic_handlers):-    cdef object loop = asyncio.get_event_loop()-    cdef RPCState rpc_state--    while True:-        rpc_state = await _server_call_request_call(-            server,-            cq,-            loop)--        loop.create_task(_handle_rpc(generic_handlers, rpc_state, loop))---async def _server_start(Server server,-                        _CallbackCompletionQueue cq,-                        list generic_handlers):-    server.start()-    await _server_main_loop(server, cq, generic_handlers)+cdef _CallbackFailureHandler CQ_SHUTDOWN_FAILURE_HANDLER = _CallbackFailureHandler(+    'grpc_completion_queue_shutdown',+    'Unknown',+    RuntimeError)   cdef class _CallbackCompletionQueue: -    def __cinit__(self):+    def __cinit__(self, object loop):+        self._loop = loop+        self._shutdown_completed = loop.create_future()+        self._wrapper = CallbackWrapper(+            self._shutdown_completed,+            CQ_SHUTDOWN_FAILURE_HANDLER)         self._cq = grpc_completion_queue_create_for_callback(-            NULL,+            self._wrapper.c_functor(),             NULL         )      cdef grpc_completion_queue* c_ptr(self):         return self._cq+    +    async def shutdown(self):+        grpc_completion_queue_shutdown(self._cq)+        await self._shutdown_completed+        grpc_completion_queue_destroy(self._cq)+++cdef _CallbackFailureHandler SERVER_SHUTDOWN_FAILURE_HANDLER = _CallbackFailureHandler(+    'grpc_server_shutdown_and_notify',+    'Unknown',+    RuntimeError)   cdef class AioServer: -    def __init__(self, thread_pool, generic_handlers, interceptors, options,-                 maximum_concurrent_rpcs, compression):+    def __init__(self, loop, thread_pool, generic_handlers, interceptors,+                 options, maximum_concurrent_rpcs, compression):+        # NOTE(lidiz) Core objects won't be deallocated automatically.+        # If AioServer.shutdown is not called, those objects will leak.         self._server = Server(options)-        self._cq = _CallbackCompletionQueue()-        self._status = AIO_SERVER_STATUS_READY-        self._generic_handlers = []+        self._cq = _CallbackCompletionQueue(loop)         grpc_server_register_completion_queue(             self._server.c_server,             self._cq.c_ptr(),             NULL         )++        self._loop = loop+        self._status = AIO_SERVER_STATUS_READY+        self._generic_handlers = []         self.add_generic_rpc_handlers(generic_handlers)+        self._serving_task = None++        self._shutdown_lock = asyncio.Lock()","I was biased about this topic due to calling `asyncio.get_event_loop` seems verbose and slow. In fact, it is not, it only take 28 ns to finish which is much less than average 70+ ns overhead for normal Python functions. (It is still verbose, but not too slow.)```Spent 0.28s for 10000000 runs of get_event_loop28.40 ns```However, if we use variable directly, we can avoid even paying the 28 ns. This might sound strange, but I switched back to your proposal which is using the deprecating `loop=` parameter for AsyncIO primitives.---**EDIT**: Add one more data point, Guido himself is in favor of skipping the `loop=` parameter, see [discussion](https://groups.google.com/forum/#!msg/python-tulip/yF9C-rFpiKk/tk5oA3GLHAAJ).",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20905,342277368,2019-11-04T21:25:07Z,src/python/grpcio/grpc/_channel.py,"@@ -302,6 +312,41 @@ def __repr__(self):     def __str__(self):         return self._repr() +    def cancel(self):+        """"""See grpc.Future.cancel.""""""+        return False++    def cancelled(self):+        """"""See grpc.Future.cancelled.""""""+        return False","My intent for this class was to only represent RPCs that were completed with error, not including cancellation. To do otherwise would entail a whole lot of copying from the error-related sections of `_MultiThreadedRendezvous`. Take a look at the actual usages of this class. The assumptions used in these methods are entirely reasonable for those usages.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/20918,342321406,2019-11-04T23:30:05Z,tools/internal_ci/linux/grpc_bazel_build_in_docker.sh,"@@ -25,3 +25,4 @@ git clone /var/local/jenkins/grpc /var/local/git/grpc ${name}') cd /var/local/git/grpc bazel build --spawn_strategy=standalone --genrule_strategy=standalone :all test/... examples/...+bazel build --spawn_strategy=standalone --genrule_strategy=standalone --define=GRPC_USE_ABSL=1 :grpc",I added the comment to the script saying this is temporary. This test will be here until absl is fully integrated and make sure that gRPC can work with abseil.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/20905,342331364,2019-11-05T00:10:26Z,src/python/grpcio/grpc/_channel.py,"@@ -302,6 +312,41 @@ def __repr__(self):     def __str__(self):         return self._repr() +    def cancel(self):+        """"""See grpc.Future.cancel.""""""+        return False++    def cancelled(self):+        """"""See grpc.Future.cancelled.""""""+        return False",Discussed offline. This class is supposed to have a very limited scope. The name has been changed to `_InactiveRpcError` to indicate this.,
28123841,guantaol,https://api.github.com/repos/grpc/grpc/pulls/19311,342866573,2019-11-06T00:31:00Z,test/core/iomgr/pollset_windows_starvation_test.cc,"@@ -0,0 +1,108 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include ""src/core/lib/iomgr/port.h""++#include ""src/core/lib/iomgr/tcp_server.h""","Do you need tcp_server.h, and all the other header files? I do not check them closely, but please remove all the unnecessary headers. Thanks!Also, I am not sure whether you create this test from scratch, or base it on some other grpc tests. ev_epollex_linux_test.cc may be a good reference, which tests a polling engine on Linux. To avoid running this test on platforms other than Windows, you can use the same trick as https://github.com/grpc/grpc/blob/master/test/core/iomgr/ev_epollex_linux_test.cc#L21. Basically you can guard the test under GRPC_WINSOCK_SOCKET.",X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/20950,342877488,2019-11-06T01:19:20Z,test/cpp/end2end/client_callback_end2end_test.cc,"@@ -176,6 +183,34 @@ class ClientCallbackEnd2endTest     }   } +  void CallResponseCallback() {+    gpr_log(GPR_ERROR, ""KRS: CallResponseCallback"");+    std::lock_guard<std::mutex> l1(mu1);+    ResponseCallback();","This is a deadlock caused by the function call, not by the callback. Because this calls ReponseCallback while locked and then ResponseCallback tries to take the lock.",X
12239891,karthikravis,https://api.github.com/repos/grpc/grpc/pulls/20950,342879933,2019-11-06T01:29:37Z,test/cpp/end2end/client_callback_end2end_test.cc,"@@ -176,6 +183,34 @@ class ClientCallbackEnd2endTest     }   } +  void CallResponseCallback() {+    gpr_log(GPR_ERROR, ""KRS: CallResponseCallback"");+    std::lock_guard<std::mutex> l1(mu1);+    ResponseCallback();","Yeah, I think the previous version did not capture my intent. I have modified it now. ReponseCallback is being called from the test now which makes the RPC call and we still see the deadlock.",
512410,sxlijin,https://api.github.com/repos/grpc/grpc/pulls/20800,342885000,2019-11-06T01:54:33Z,test/core/compression/message_compress_fuzzer.cc,"@@ -0,0 +1,58 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <stdbool.h>+#include <stdint.h>+#include <string.h>++#include ""src/core/lib/compression/message_compress.h""+#include ""src/core/lib/security/credentials/credentials.h""+#include ""test/core/util/memory_counters.h""++bool squelch = true;+bool leak_check = true;++extern ""C"" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {+  if (size < 1) return 0;++  // Instead of rolling something complicated to convert a uint8_t to the enum,+  // just bail out if it isn't trivially convertible.+  if (data[0] >= GRPC_MESSAGE_COMPRESS_ALGORITHMS_COUNT) return 0;+  const auto compression_algorithm =+      static_cast<grpc_message_compression_algorithm>(data[0]);++  grpc_core::testing::LeakDetector leak_detector(true);+  grpc_init();+  grpc_test_only_control_plane_credentials_force_init();+  grpc_slice_buffer input_buffer;+  grpc_slice_buffer_init(&input_buffer);+  grpc_slice_buffer_add(&input_buffer,+                        grpc_slice_from_copied_buffer(+                            reinterpret_cast<const char*>(data), size - 1));","Oops, this is supposed to be `data + 1`. The idea is that the first byte the fuzzer gives us is used to select the compression algo, and the rest are used for the input.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/20963,343408697,2019-11-07T01:13:12Z,src/core/lib/security/transport/security_handshaker.cc,"@@ -141,12 +141,12 @@ SecurityHandshaker::~SecurityHandshaker() {   gpr_mu_destroy(&mu_);   tsi_handshaker_destroy(handshaker_);   tsi_handshaker_result_destroy(handshaker_result_);-  if (endpoint_to_destroy_ != nullptr) {","IIC this is has some behavior change, since we're now unconditionally destroying `args_->endpoint` and `args_->read_buffer`, where before they would only be destroyed upon handshake failure?",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/20873,343417374,2019-11-07T01:54:34Z,test/core/tsi/alts/handshaker/alts_tsi_handshaker_test.cc,"@@ -261,6 +287,25 @@ static void on_client_next_success_cb(tsi_result status, void* user_data,   GPR_ASSERT(memcmp(ALTS_TSI_HANDSHAKER_TEST_PEER_IDENTITY,                     peer.properties[1].value.data,                     peer.properties[1].value.length) == 0);+  /* Validate alts context. */+  upb::Arena context_arena;+  grpc_gcp_AltsContext* ctx = grpc_gcp_AltsContext_parse(+      peer.properties[3].value.data, peer.properties[3].value.length,+      context_arena.ptr());+  GPR_ASSERT(ctx != nullptr);+  upb_strview application_protocol =+      grpc_gcp_AltsContext_application_protocol(ctx);+  upb_strview record_protocol = grpc_gcp_AltsContext_record_protocol(ctx);+  upb_strview peer_account = grpc_gcp_AltsContext_peer_service_account(ctx);+  upb_strview local_account = grpc_gcp_AltsContext_local_service_account(ctx);+  GPR_ASSERT(memcmp(ALTS_TSI_HANDSHAKER_TEST_APPLICATION_PROTOCOL,+                    application_protocol.data, application_protocol.size) == 0);+  GPR_ASSERT(memcmp(ALTS_TSI_HANDSHAKER_TEST_RECORD_PROTOCOL,+                    record_protocol.data, record_protocol.size) == 0);+  GPR_ASSERT(memcmp(ALTS_TSI_HANDSHAKER_TEST_PEER_IDENTITY, peer_account.data,+                    peer_account.size) == 0);+  GPR_ASSERT(memcmp(ALTS_TSI_HANDSHAKER_TEST_LOCAL_IDENTITY, local_account.data,+                    local_account.size) == 0);",Please add a similar test to `on_server_next_success_cb`.,
35502996,KyleFromKitware,https://api.github.com/repos/grpc/grpc/pulls/20663,343744376,2019-11-07T16:23:31Z,BUILDING.md,"@@ -155,3 +179,16 @@ installed to be able to compile the C/C++ sources. > cmake .. -GNinja -DCMAKE_BUILD_TYPE=Release > cmake --build . ```++If you want to build DLLs, run `cmake` with `-DBUILD_SHARED_LIBS=ON`.++## cmake: Install After Build",The README.md section about [make](https://github.com/grpc/grpc/blob/master/src/cpp/README.md#make) points back to BUILDING.md. It seems weird to me to have part of the build doc be in README.md and part be in BUILDING.md. We should make it consistent.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20975,344017154,2019-11-08T07:29:33Z,src/objective-c/BoringSSL-GRPC.podspec,"@@ -128,22 +128,23 @@ Pod::Spec.new do |s|   end   s.subspec 'Implementation' do |ss|     ss.header_mappings_dir = '.'-    ss.source_files = 'ssl/*.{h,cc}',-                      'ssl/**/*.{h,cc}',-                      '*.{h,c}',-                      'crypto/*.{h,c}',-                      'crypto/**/*.{h,c}',-                      'third_party/fiat/*.{h,c}'+    ss.source_files = 'ssl/*.{h,c,cc}',+                      'ssl/**/*.{h,c,cc}',+                      '*.{h,c,cc}', # for generated file such as err_data.c","looks like there's only `err_data.c` and `crypto_test_data.cc` and both have been there for long time, so perhaps we can list them explicitly?",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/20975,344320306,2019-11-08T18:58:33Z,src/objective-c/BoringSSL-GRPC.podspec,"@@ -128,22 +128,23 @@ Pod::Spec.new do |s|   end   s.subspec 'Implementation' do |ss|     ss.header_mappings_dir = '.'-    ss.source_files = 'ssl/*.{h,cc}',-                      'ssl/**/*.{h,cc}',-                      '*.{h,c}',-                      'crypto/*.{h,c}',-                      'crypto/**/*.{h,c}',-                      'third_party/fiat/*.{h,c}'+    ss.source_files = 'ssl/*.{h,c,cc}',+                      'ssl/**/*.{h,c,cc}',+                      '*.{h,c,cc}', # for generated file such as err_data.c",I'll list err_data.c as the other one is for testing.,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/20929,344391795,2019-11-08T22:29:01Z,src/core/tsi/alts/handshaker/alts_handshaker_client.cc,"@@ -200,6 +309,23 @@ static tsi_result make_grpc_call(alts_handshaker_client* c, bool is_start) {   memset(ops, 0, sizeof(ops));   grpc_op* op = ops;   if (is_start) {+    op->op = GRPC_OP_RECV_STATUS_ON_CLIENT;+    op->data.recv_status_on_client.trailing_metadata = nullptr;+    op->data.recv_status_on_client.status = &client->handshake_status_code;+    op->data.recv_status_on_client.status_details =+        &client->handshake_status_details;+    op->flags = 0;+    op->reserved = nullptr;+    op++;+    GPR_ASSERT(op - ops <= kHandshakerClientOpNum);+    gpr_ref(&client->refs);+    grpc_call_error call_error =","The client's first handshake message response, in successful case, should result in a received message having no `result` field, and an OK status. In this case, `maybe_complete_tsi_next` will not wait for the `RECV_STATUS` op to complete.Eventually, the last handshaker message will arrive, having at least either non-OK status or a filled in `result` field. In this case, we'll wait for the handshake RPC's status to be received, and then invoke the TSI next callback just once.Lastly, note that if the RECV_STATUS op completes while we don't have a TSI next callback outstanding (e.g. security handshaker is waiting for a read from remote peer), then we <i>won't</i> have a non-null `pending_recv_message_result`, and so we won't accidentally invoke the TSI next callback an extra time",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/20929,344422186,2019-11-09T01:42:57Z,src/core/tsi/alts/handshaker/alts_handshaker_client.cc,"@@ -200,6 +309,23 @@ static tsi_result make_grpc_call(alts_handshaker_client* c, bool is_start) {   memset(ops, 0, sizeof(ops));   grpc_op* op = ops;   if (is_start) {+    op->op = GRPC_OP_RECV_STATUS_ON_CLIENT;+    op->data.recv_status_on_client.trailing_metadata = nullptr;+    op->data.recv_status_on_client.status = &client->handshake_status_code;+    op->data.recv_status_on_client.status_details =+        &client->handshake_status_details;+    op->flags = 0;+    op->reserved = nullptr;+    op++;+    GPR_ASSERT(op - ops <= kHandshakerClientOpNum);+    gpr_ref(&client->refs);+    grpc_call_error call_error =","> In the last handshaker message, why do we wait for the handshake RPC's status to be received?My motivation for this is to mainly just to try to be sure that we continue to drive the handshake RPC forward until it fully completes. This is basically trying to leverage the fact that there is an outstanding TSI next call (which ultimately suggests that the associated subchannel hasn't yet connected, and that someone <i>should</i> be polling on the relevant file descriptors of this handshake RPC), to make sure that we finish the handshake RPC in a timely manner.In a degenerate case, suppose that the handshake server sends a reply message, and so the RECV_MESSAGE op completes, but the data on the wire hasn't yet arrived to complete the RECV_STATUS op. If we completed the TSI handshake at this point, then the subchannel could become ready, the application could quickly complete its RPC, and then the application could stop polling on the completion queue of its RPC (if it e.g. only wanted to do a single RPC). The data necessary to complete the RECV_STATUS op on the wire could then show up just after, but because we don't have anyone guaranteed to be polling on the relevant file descriptors of the handshake RPC (except for the optional backup poller, which runs on a fixed time interval), the handshake RPC may just hang there and never ""complete"".I.e., this provides a kind of guarantee that all handshake RPCs are either driven forward until full completion in a timely manner, or be cancelled (due to handshake failure or subchannel connection timeout).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20919,344637148,2019-11-11T10:05:36Z,build.yaml,"@@ -2110,6 +2110,8 @@ libs:   - grpc   filegroups:   - grpcpp_channelz_proto+  extra_cond:","This seems pretty adhoc, and also in the long run we'll be getting rid of build.yaml, so it doesn't make sense to add new properties like this.We could just specialcase directly in CMakeLists.txt.template if needed, but I'm not sure it's worth doing it considering that gRPC_USE_PROTO_LITE is probably used only by few.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20976,344791190,2019-11-11T16:19:50Z,CMakeLists.txt,"@@ -76,11 +76,16 @@ set_property(CACHE gRPC_PROTOBUF_PROVIDER PROPERTY STRINGS ""module"" ""package"") set(gRPC_PROTOBUF_PACKAGE_TYPE """" CACHE STRING ""Algorithm for searching protobuf package"") set_property(CACHE gRPC_PROTOBUF_PACKAGE_TYPE PROPERTY STRINGS ""CONFIG"" ""MODULE"") -set(gRPC_GFLAGS_PROVIDER ""module"" CACHE STRING ""Provider of gflags library"")-set_property(CACHE gRPC_GFLAGS_PROVIDER PROPERTY STRINGS ""module"" ""package"")+if(gRPC_BUILD_TESTS)+  set(gRPC_GFLAGS_PROVIDER ""module"" CACHE STRING ""Provider of gflags library"")+  set_property(CACHE gRPC_GFLAGS_PROVIDER PROPERTY STRINGS ""module"" ""package"") -set(gRPC_BENCHMARK_PROVIDER ""module"" CACHE STRING ""Provider of benchmark library"")-set_property(CACHE gRPC_BENCHMARK_PROVIDER PROPERTY STRINGS ""module"" ""package"")+  set(gRPC_BENCHMARK_PROVIDER ""module"" CACHE STRING ""Provider of benchmark library"")+  set_property(CACHE gRPC_BENCHMARK_PROVIDER PROPERTY STRINGS ""module"" ""package"")+else()+  set(gRPC_GFLAGS_PROVIDER ""none"")","If we do this, I think there should be an explicit branch for this value in https://github.com/grpc/grpc/blob/af1eaa0d058cdd319a975c2afeaf6383bdec2753/cmake/benchmark.cmake#L14, otherwise things could be too confusing.But as it turns out, this change might not be necessary at all (see the other comment).",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/20929,344809927,2019-11-11T16:58:54Z,src/core/tsi/alts/handshaker/alts_handshaker_client.cc,"@@ -200,6 +309,23 @@ static tsi_result make_grpc_call(alts_handshaker_client* c, bool is_start) {   memset(ops, 0, sizeof(ops));   grpc_op* op = ops;   if (is_start) {+    op->op = GRPC_OP_RECV_STATUS_ON_CLIENT;+    op->data.recv_status_on_client.trailing_metadata = nullptr;+    op->data.recv_status_on_client.status = &client->handshake_status_code;+    op->data.recv_status_on_client.status_details =+        &client->handshake_status_details;+    op->flags = 0;+    op->reserved = nullptr;+    op++;+    GPR_ASSERT(op - ops <= kHandshakerClientOpNum);+    gpr_ref(&client->refs);+    grpc_call_error call_error =","In xds_cleint.cc (https://github.com/grpc/grpc/blob/master/src/core/ext/filters/client_channel/xds/xds_client.cc), we also applied the similar model, i.e., scheduling a `grpc_closure` upon receiving message `RECV_MESSAGE` and status `RECV_STATUS`.  ",
7281574,nicolasnoble,https://api.github.com/repos/grpc/grpc/pulls/21089,345428047,2019-11-12T20:29:38Z,src/core/lib/channel/channelz_registry.cc,"@@ -281,12 +252,6 @@ char* grpc_channelz_get_socket(intptr_t socket_id) {           grpc_core::channelz::BaseNode::EntityType::kSocket) {     return nullptr;   }-  grpc_json* top_level_json = grpc_json_create(GRPC_JSON_OBJECT);-  grpc_json* json = top_level_json;-  grpc_json* socket_json = socket_node->RenderJson();-  socket_json->key = ""socket"";-  grpc_json_link_child(json, socket_json, nullptr);-  char* json_str = grpc_json_dump_to_string(top_level_json, 0);-  grpc_json_destroy(top_level_json);-  return json_str;+  json j = {{""socket"", socket_node->RenderJson()}};+  return gpr_strdup(j.dump().c_str());","It looks to me this `gpr_strdup(j.dump().c_str())` pattern is sprinkled all over the place. Not sure if it'd be relevant to have a helper? Note sure which is more cumbersome.Side-node: even if we fully removed custom malloc/free overrides, gpr_strdup still needs to exist to cross dll boundaries.",X
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/20722,345517600,2019-11-13T00:34:12Z,src/core/tsi/alts/handshaker/alts_handshaker_client.cc,"@@ -358,6 +335,164 @@ static tsi_result make_grpc_call(alts_handshaker_client* c, bool is_start) {   return TSI_OK; } +// TODO(apolcyn): remove this global queue when we can safely rely+// on a MAX_CONCURRENT_STREAMS setting in the ALTS handshake server to+// limit the number of concurrent handshakes.+namespace {++class HandshakeQueue {+ public:+  HandshakeQueue(size_t max_outstanding_handshakes)+      : max_outstanding_handshakes_(max_outstanding_handshakes) {}++  ~HandshakeQueue() { GRPC_COMBINER_UNREF(combiner_, ""~HandshakeQueue""); }++  void EnqueueHandshakeAndMaybeStartSome(alts_grpc_handshaker_client* client) {+    Arg* arg = grpc_core::New<Arg>();+    arg->client = client;+    arg->self = this;+    GRPC_CLOSURE_INIT(&arg->closure, EnqueueHandshakeAndMaybeStartSomeLocked,+                      arg, nullptr);+    combiner_->Run(&arg->closure, GRPC_ERROR_NONE);+  }++ private:+  struct Node {+    alts_grpc_handshaker_client* client = nullptr;+    Node* next = nullptr;+    Node* prev = nullptr;+  };++  struct Arg {+    alts_grpc_handshaker_client* client;+    HandshakeQueue* self;+    grpc_closure closure;+  };++  static void EnqueueHandshakeAndMaybeStartSomeLocked(+      void* arg, grpc_error* unused_error) {+    Arg* args = static_cast<Arg*>(arg);+    args->self->EnqueueHandshakeAndMaybeStartSomeInnerLocked(args->client);+    grpc_core::Delete(args);+  }++  void EnqueueHandshakeAndMaybeStartSomeInnerLocked(+      alts_grpc_handshaker_client* client) {+    if (client == nullptr) {+      outstanding_handshakes_--;+    } else {+      Node* node = grpc_core::New<Node>();+      node->client = client;+      gpr_log(GPR_DEBUG,+              ""HandshakeQueue:%p: outstanding_handshakes_:%ld ""+              ""old head_:%p new head_: %p tail_:%p"",+              this, outstanding_handshakes_, head_, node, tail_);+      if (head_ == nullptr) {+        GPR_ASSERT(tail_ == nullptr);+        head_ = node;+        tail_ = node;+      } else {+        GPR_ASSERT(tail_ != nullptr);+        node->next = head_;+        GPR_ASSERT(head_->prev == nullptr);+        head_->prev = node;+        head_ = node;+      }+    }+    while (outstanding_handshakes_ < max_outstanding_handshakes_ &&+           tail_ != nullptr) {+      gpr_log(GPR_DEBUG,+              ""start handshake ""+              ""outstanding_handshakes_:%ld ""+              ""head_:%p tail_:%p"",+              outstanding_handshakes_, head_, tail_);+      GPR_ASSERT(outstanding_handshakes_ < max_outstanding_handshakes_);+      continue_make_grpc_call(tail_->client, true /* is_start */);+      Node* prev = tail_;+      tail_ = tail_->prev;+      grpc_core::Delete(prev);+      outstanding_handshakes_++;+    }+    if (tail_ == nullptr) {+      head_ = nullptr;+    }+  }++  grpc_core::Combiner* combiner_ = grpc_combiner_create();+  Node* head_ = nullptr;+  Node* tail_ = nullptr;+  size_t outstanding_handshakes_ = 0;+  const size_t max_outstanding_handshakes_;+};++gpr_once g_queued_handshakes_init = GPR_ONCE_INIT;+/* Using separate queues for client and server handshakes is a+ * hack that's mainly intended to satisfy the alts_concurrent_connectivity_test,+ * which runs many concurrent handshakes where both endpoints+ * are in the same process; this situation is problematic with a+ * single queue because we have a high chance of using up all outstanding+ * slots in the queue, such that there aren't any+ * mutual client/server handshakes outstanding at the same time and+ * able to make progress. */+HandshakeQueue* g_client_handshake_queue;+HandshakeQueue* g_server_handshake_queue;","It seems weird to keep an unused data structure in non-test code. If we increase the queue size in the test, can the problem be solved? ",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/20722,345522128,2019-11-13T00:52:47Z,src/core/tsi/alts/handshaker/alts_handshaker_client.cc,"@@ -358,6 +335,164 @@ static tsi_result make_grpc_call(alts_handshaker_client* c, bool is_start) {   return TSI_OK; } +// TODO(apolcyn): remove this global queue when we can safely rely+// on a MAX_CONCURRENT_STREAMS setting in the ALTS handshake server to+// limit the number of concurrent handshakes.+namespace {++class HandshakeQueue {+ public:+  HandshakeQueue(size_t max_outstanding_handshakes)+      : max_outstanding_handshakes_(max_outstanding_handshakes) {}++  ~HandshakeQueue() { GRPC_COMBINER_UNREF(combiner_, ""~HandshakeQueue""); }++  void EnqueueHandshakeAndMaybeStartSome(alts_grpc_handshaker_client* client) {+    Arg* arg = grpc_core::New<Arg>();+    arg->client = client;+    arg->self = this;+    GRPC_CLOSURE_INIT(&arg->closure, EnqueueHandshakeAndMaybeStartSomeLocked,+                      arg, nullptr);+    combiner_->Run(&arg->closure, GRPC_ERROR_NONE);+  }++ private:+  struct Node {+    alts_grpc_handshaker_client* client = nullptr;+    Node* next = nullptr;+    Node* prev = nullptr;+  };++  struct Arg {+    alts_grpc_handshaker_client* client;+    HandshakeQueue* self;+    grpc_closure closure;+  };++  static void EnqueueHandshakeAndMaybeStartSomeLocked(+      void* arg, grpc_error* unused_error) {+    Arg* args = static_cast<Arg*>(arg);+    args->self->EnqueueHandshakeAndMaybeStartSomeInnerLocked(args->client);+    grpc_core::Delete(args);+  }++  void EnqueueHandshakeAndMaybeStartSomeInnerLocked(+      alts_grpc_handshaker_client* client) {+    if (client == nullptr) {+      outstanding_handshakes_--;+    } else {+      Node* node = grpc_core::New<Node>();+      node->client = client;+      gpr_log(GPR_DEBUG,+              ""HandshakeQueue:%p: outstanding_handshakes_:%ld ""+              ""old head_:%p new head_: %p tail_:%p"",+              this, outstanding_handshakes_, head_, node, tail_);+      if (head_ == nullptr) {+        GPR_ASSERT(tail_ == nullptr);+        head_ = node;+        tail_ = node;+      } else {+        GPR_ASSERT(tail_ != nullptr);+        node->next = head_;+        GPR_ASSERT(head_->prev == nullptr);+        head_->prev = node;+        head_ = node;+      }+    }+    while (outstanding_handshakes_ < max_outstanding_handshakes_ &&+           tail_ != nullptr) {+      gpr_log(GPR_DEBUG,+              ""start handshake ""+              ""outstanding_handshakes_:%ld ""+              ""head_:%p tail_:%p"",+              outstanding_handshakes_, head_, tail_);+      GPR_ASSERT(outstanding_handshakes_ < max_outstanding_handshakes_);+      continue_make_grpc_call(tail_->client, true /* is_start */);+      Node* prev = tail_;+      tail_ = tail_->prev;+      grpc_core::Delete(prev);+      outstanding_handshakes_++;+    }+    if (tail_ == nullptr) {+      head_ = nullptr;+    }+  }++  grpc_core::Combiner* combiner_ = grpc_combiner_create();+  Node* head_ = nullptr;+  Node* tail_ = nullptr;+  size_t outstanding_handshakes_ = 0;+  const size_t max_outstanding_handshakes_;+};++gpr_once g_queued_handshakes_init = GPR_ONCE_INIT;+/* Using separate queues for client and server handshakes is a+ * hack that's mainly intended to satisfy the alts_concurrent_connectivity_test,+ * which runs many concurrent handshakes where both endpoints+ * are in the same process; this situation is problematic with a+ * single queue because we have a high chance of using up all outstanding+ * slots in the queue, such that there aren't any+ * mutual client/server handshakes outstanding at the same time and+ * able to make progress. */+HandshakeQueue* g_client_handshake_queue;+HandshakeQueue* g_server_handshake_queue;","Note that this data structure <i>is</i> used in the real code (i.e. outside the test) - `g_client_handshake_queue` is used by client side handshakes and `g_server_handshake_queue` is used by server side handshakes.The problem being addressed here is basically the same problem that https://github.com/grpc/grpc/blob/970f703337ecc19017a737cc4eb831cd50fd0fec/test/core/tsi/alts/handshaker/alts_concurrent_connectivity_test.cc#L180 currently addresses in the test - so the workaround isn't fundamentally new.For the purpose of keeping `alts_concurrent_connectivity_test` passing, my reason for preferring the existing workaround over increasing the handshake queue size to a very large value, is the latter workaround would IMO make the test less realistic of production situations than the former workaround by using an artificial value for the queue size.",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/20722,345528043,2019-11-13T01:17:46Z,src/core/tsi/alts/handshaker/alts_handshaker_client.cc,"@@ -358,6 +335,164 @@ static tsi_result make_grpc_call(alts_handshaker_client* c, bool is_start) {   return TSI_OK; } +// TODO(apolcyn): remove this global queue when we can safely rely+// on a MAX_CONCURRENT_STREAMS setting in the ALTS handshake server to+// limit the number of concurrent handshakes.+namespace {++class HandshakeQueue {+ public:+  HandshakeQueue(size_t max_outstanding_handshakes)+      : max_outstanding_handshakes_(max_outstanding_handshakes) {}++  ~HandshakeQueue() { GRPC_COMBINER_UNREF(combiner_, ""~HandshakeQueue""); }++  void EnqueueHandshakeAndMaybeStartSome(alts_grpc_handshaker_client* client) {+    Arg* arg = grpc_core::New<Arg>();+    arg->client = client;+    arg->self = this;+    GRPC_CLOSURE_INIT(&arg->closure, EnqueueHandshakeAndMaybeStartSomeLocked,+                      arg, nullptr);+    combiner_->Run(&arg->closure, GRPC_ERROR_NONE);+  }++ private:+  struct Node {+    alts_grpc_handshaker_client* client = nullptr;+    Node* next = nullptr;+    Node* prev = nullptr;+  };++  struct Arg {+    alts_grpc_handshaker_client* client;+    HandshakeQueue* self;+    grpc_closure closure;+  };++  static void EnqueueHandshakeAndMaybeStartSomeLocked(+      void* arg, grpc_error* unused_error) {+    Arg* args = static_cast<Arg*>(arg);+    args->self->EnqueueHandshakeAndMaybeStartSomeInnerLocked(args->client);+    grpc_core::Delete(args);+  }++  void EnqueueHandshakeAndMaybeStartSomeInnerLocked(+      alts_grpc_handshaker_client* client) {+    if (client == nullptr) {+      outstanding_handshakes_--;+    } else {+      Node* node = grpc_core::New<Node>();+      node->client = client;+      gpr_log(GPR_DEBUG,+              ""HandshakeQueue:%p: outstanding_handshakes_:%ld ""+              ""old head_:%p new head_: %p tail_:%p"",+              this, outstanding_handshakes_, head_, node, tail_);+      if (head_ == nullptr) {+        GPR_ASSERT(tail_ == nullptr);+        head_ = node;+        tail_ = node;+      } else {+        GPR_ASSERT(tail_ != nullptr);+        node->next = head_;+        GPR_ASSERT(head_->prev == nullptr);+        head_->prev = node;+        head_ = node;+      }+    }+    while (outstanding_handshakes_ < max_outstanding_handshakes_ &&+           tail_ != nullptr) {+      gpr_log(GPR_DEBUG,+              ""start handshake ""+              ""outstanding_handshakes_:%ld ""+              ""head_:%p tail_:%p"",+              outstanding_handshakes_, head_, tail_);+      GPR_ASSERT(outstanding_handshakes_ < max_outstanding_handshakes_);+      continue_make_grpc_call(tail_->client, true /* is_start */);+      Node* prev = tail_;+      tail_ = tail_->prev;+      grpc_core::Delete(prev);+      outstanding_handshakes_++;+    }+    if (tail_ == nullptr) {+      head_ = nullptr;+    }+  }++  grpc_core::Combiner* combiner_ = grpc_combiner_create();+  Node* head_ = nullptr;+  Node* tail_ = nullptr;+  size_t outstanding_handshakes_ = 0;+  const size_t max_outstanding_handshakes_;+};++gpr_once g_queued_handshakes_init = GPR_ONCE_INIT;+/* Using separate queues for client and server handshakes is a+ * hack that's mainly intended to satisfy the alts_concurrent_connectivity_test,+ * which runs many concurrent handshakes where both endpoints+ * are in the same process; this situation is problematic with a+ * single queue because we have a high chance of using up all outstanding+ * slots in the queue, such that there aren't any+ * mutual client/server handshakes outstanding at the same time and+ * able to make progress. */+HandshakeQueue* g_client_handshake_queue;+HandshakeQueue* g_server_handshake_queue;","Understood. I meant that `g_client_handshake_queue` is not used when the code is running by the server side and vice versa. I agree that choosing a very large queue size is also hacky, and the current approach seems more reasonable. ",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/20722,345528621,2019-11-13T01:19:55Z,test/core/tsi/alts/handshaker/alts_handshaker_client_test.cc,"@@ -353,18 +354,33 @@ static void schedule_request_invalid_arg_test() {   alts_handshaker_client_set_grpc_caller_for_testing(config->client,                                                      check_must_not_be_called);   /* Check client_start. */-  GPR_ASSERT(alts_handshaker_client_start_client(nullptr) ==-             TSI_INVALID_ARGUMENT);+  {+    grpc_core::ExecCtx exec_ctx;","I unfortunately have trouble seeing a good way of doing that, the main thing being that these APIs are normally called from within core, and so can expect to have an `ExecCtx` already available.A couple of ideas:1) We could change them to perhaps check if an ExecCtx exists and only instantiate one if so. This would add a decent amount of complication to the code though only for the purpose of this unit test's use case.2) We could add some wrappers around these, could be exposed with e.g. the rest of the `_test_only` utilities, that could in turn call this APIs but with an ExecCtx",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/20722,345531983,2019-11-13T01:34:39Z,test/core/tsi/alts/handshaker/alts_handshaker_client_test.cc,"@@ -353,18 +354,33 @@ static void schedule_request_invalid_arg_test() {   alts_handshaker_client_set_grpc_caller_for_testing(config->client,                                                      check_must_not_be_called);   /* Check client_start. */-  GPR_ASSERT(alts_handshaker_client_start_client(nullptr) ==-             TSI_INVALID_ARGUMENT);+  {+    grpc_core::ExecCtx exec_ctx;","In general, the problem this pattern addresses is that we need to make sure that if we're ever scheduling a C-core closure, we need to make sure that we have an `ExecCtx` active for the current thread, otherwise we'll crash.Before this PR, it just happened that APIs such as `alts_handshaker_client_start_client` didn't schedule any closures internally. However, since this PR starts scheduling the `EnqueueHandshakeAndMaybeStartSomeLocked` call as a closure on a combiner, as a first step of creating a new handshake RPC (from within `alts_handshaker_client_start`), we just need to ensure that we have an ExecCtx active.",
394885,soheilhy,https://api.github.com/repos/grpc/grpc/pulls/18856,346364666,2019-11-14T15:04:06Z,include/grpcpp/impl/codegen/server_callback_impl.h,"@@ -222,34 +215,55 @@ class ServerCallbackReaderWriter { };  // The following classes are the reactor interfaces that are to be implemented-// by the user, returned as the result of the method handler for a callback-// method, and activated by the call to OnStarted. The library guarantees that-// OnStarted will be called for any reactor that has been created using a-// method handler registered on a service. No operation initiation method may be-// called until after the call to OnStarted.-// Note that none of the classes are pure; all reactions have a default empty-// reaction so that the user class only needs to override those classes that it-// cares about.+// by the user, returned as the output parameter of the method handler for a+// callback method. Note that none of the classes are pure; all reactions have a+// default empty reaction so that the user class only needs to override those+// classes that it cares about.  /// \a ServerBidiReactor is the interface for a bidirectional streaming RPC. template <class Request, class Response> class ServerBidiReactor : public internal::ServerReactor {  public:+  // NOTE: Initializing stream_ as a constructor initializer rather than a+  //       default initializer because gcc-4.x requires a copy constructor for+  //       default initializing a templated member, which isn't ok for atomic.+  // TODO(vjpai): Switch to default constructor and default initializer when+  //              gcc-4.x is no longer supported+  ServerBidiReactor() : stream_(nullptr) {}   ~ServerBidiReactor() = default; -  /// Do NOT call any operation initiation method (names that start with Start)-  /// until after the library has called OnStarted on this object.-   /// Send any initial metadata stored in the RPC context. If not invoked,   /// any initial metadata will be passed along with the first Write or the   /// Finish (if there are no writes).-  void StartSendInitialMetadata() { stream_->SendInitialMetadata(); }+  void StartSendInitialMetadata() {+    auto* stream = stream_.load(std::memory_order_acquire);+    if (stream == nullptr) {+      grpc::internal::MutexLock l(&stream_mu_);+      stream = stream_.load(std::memory_order_acquire);+      if (stream == nullptr) {+        send_initial_metadata_wanted_ = true;+        return;+      }+    }+    stream->SendInitialMetadata();+  }    /// Initiate a read operation.   ///   /// \param[out] req Where to eventually store the read message. Valid when   ///                 the library calls OnReadDone-  void StartRead(Request* req) { stream_->Read(req); }+  void StartRead(Request* req) {+    auto* stream = stream_.load(std::memory_order_acquire);+    if (stream == nullptr) {+      grpc::internal::MutexLock l(&stream_mu_);+      stream = stream_.load(std::memory_order_acquire);",this can similarly be `relaxed` here and below,
303201,JamesNK,https://api.github.com/repos/grpc/grpc/pulls/21120,346395283,2019-11-14T15:54:32Z,src/csharp/Grpc.HealthCheck/HealthServiceImpl.cs,"@@ -86,17 +111,124 @@ public void ClearAll()         /// <returns>The asynchronous response.</returns>         public override Task<HealthCheckResponse> Check(HealthCheckRequest request, ServerCallContext context)         {+            HealthCheckResponse response = GetHealthCheckResponse(request.Service, throwOnNotFound: true);++            return Task.FromResult(response);+        }++        /// <summary>+        /// Performs a watch for the serving status of the requested service.+        /// The server will immediately send back a message indicating the current+        /// serving status.  It will then subsequently send a new message whenever+        /// the service's serving status changes.+        ///+        /// If the requested service is unknown when the call is received, the+        /// server will send a message setting the serving status to+        /// SERVICE_UNKNOWN but will *not* terminate the call.  If at some+        /// future point, the serving status of the service becomes known, the+        /// server will send a new message with the service's serving status.+        ///+        /// If the call terminates with status UNIMPLEMENTED, then clients+        /// should assume this method is not supported and should not retry the+        /// call.  If the call terminates with any other status (including OK),+        /// clients should retry the call with appropriate exponential backoff.+        /// </summary>+        /// <param name=""request"">The request received from the client.</param>+        /// <param name=""responseStream"">Used for sending responses back to the client.</param>+        /// <param name=""context"">The context of the server-side call handler being invoked.</param>+        /// <returns>A task indicating completion of the handler.</returns>+        public override async Task Watch(HealthCheckRequest request, IServerStreamWriter<HealthCheckResponse> responseStream, ServerCallContext context)+        {+            string service = request.Service;+            TaskCompletionSource<object> watchTcs = new TaskCompletionSource<object>();++            HealthCheckResponse response = GetHealthCheckResponse(service, throwOnNotFound: false);+            await responseStream.WriteAsync(response);+             lock (myLock)             {-                var service = request.Service;+                if (!watchers.TryGetValue(service, out List<IServerStreamWriter<HealthCheckResponse>> serverStreamWriters))+                {+                    serverStreamWriters = new List<IServerStreamWriter<HealthCheckResponse>>();+                    watchers.Add(service, serverStreamWriters);+                }++                serverStreamWriters.Add(responseStream);+            }++            // Handle the Watch call being canceled+            context.CancellationToken.Register(() => {+                lock (myLock)+                {+                    if (watchers.TryGetValue(service, out List<IServerStreamWriter<HealthCheckResponse>> serverStreamWriters))+                    {+                        // Remove the response stream from the watchers+                        if (serverStreamWriters.Remove(responseStream))+                        {+                            // Remove empty collection if service has no more response streams+                            if (serverStreamWriters.Count == 0)+                            {+                                watchers.Remove(service);+                            }+                        }+                    }+                }++                // Allow watch method to exit.+                watchTcs.TrySetResult(null);+            }); +            // Wait for call to be cancelled before exiting.+            await watchTcs.Task;+        }++        private HealthCheckResponse GetHealthCheckResponse(string service, bool throwOnNotFound)+        {+            HealthCheckResponse response = null;+            lock (myLock)+            {                 HealthCheckResponse.Types.ServingStatus status;                 if (!statusMap.TryGetValue(service, out status))                 {-                    // TODO(jtattermusch): returning specific status from server handler is not supported yet.-                    throw new RpcException(new Status(StatusCode.NotFound, """"));+                    if (throwOnNotFound)+                    {+                        // TODO(jtattermusch): returning specific status from server handler is not supported yet.+                        throw new RpcException(new Status(StatusCode.NotFound, """"));+                    }+                    else+                    {+                        status = HealthCheckResponse.Types.ServingStatus.ServiceUnknown;+                    }+                }+                response = new HealthCheckResponse { Status = status };+            }++            return response;+        }++        private HealthCheckResponse.Types.ServingStatus GetServiceStatus(string service)+        {+            if (statusMap.TryGetValue(service, out HealthCheckResponse.Types.ServingStatus s))+            {+                return s;+            }+            else+            {+                return HealthCheckResponse.Types.ServingStatus.ServiceUnknown;+            }+        }++        private void NotifyStatus(string service, HealthCheckResponse.Types.ServingStatus status)+        {+            if (watchers.TryGetValue(service, out List<IServerStreamWriter<HealthCheckResponse>> serverStreamWriters))+            {+                HealthCheckResponse response = new HealthCheckResponse { Status = status };++                foreach (IServerStreamWriter<HealthCheckResponse> serverStreamWriter in serverStreamWriters)+                {+                    // TODO(JamesNK): This will fail if a pending write is already in progress.","I can add a separate lock that is used when updating and accessing watchers. That doesn't solve this TODO though.Two issues here:1. The existing SetStatus/RemoveStatus/ClearAll public methods aren't async.2. ServerStreamWriter.WriteAsync doesn't allow multiple parallel callers.Couple of options:1. Add async versions of SetStatus/RemoveStatus/ClearAll and surround notifying status to the write in an SemaphoreSlim lock. Obsolete the sync methods.2. Leave sync methods but wrap writer with a type that manages queuing up pending statuses to send. If multiple statuses are set at the same time the writer will empty the queue as quickly as it can.It seems like option 2, trying to write to a ServerStreamWriter from many threads, would be a common problem. Has this been solved before?",X
394885,soheilhy,https://api.github.com/repos/grpc/grpc/pulls/18856,346428794,2019-11-14T16:51:58Z,include/grpcpp/impl/codegen/server_callback_impl.h,"@@ -353,835 +393,308 @@ class ServerBidiReactor : public internal::ServerReactor {   // customization point.   virtual void InternalBindStream(       ServerCallbackReaderWriter<Request, Response>* stream) {-    stream_ = stream;+    grpc::internal::MutexLock l(&stream_mu_);+    stream_.store(stream, std::memory_order_release);+    if (send_initial_metadata_wanted_) {+      stream->SendInitialMetadata();+    }+    if (read_wanted_ != nullptr) {+      stream->Read(read_wanted_);+    }+    if (write_and_finish_wanted_) {+      stream->WriteAndFinish(write_wanted_, std::move(write_options_wanted_),+                             std::move(status_wanted_));+    } else {+      if (write_wanted_ != nullptr) {+        stream->Write(write_wanted_, std::move(write_options_wanted_));+      }+      if (finish_wanted_) {+        stream->Finish(std::move(status_wanted_));+      }+    }   } -  ServerCallbackReaderWriter<Request, Response>* stream_;+  grpc::internal::Mutex stream_mu_;+  std::atomic<ServerCallbackReaderWriter<Request, Response>*> stream_;+  bool send_initial_metadata_wanted_ = false;+  bool write_and_finish_wanted_ = false;+  bool finish_wanted_ = false;+  Request* read_wanted_ = nullptr;+  const Response* write_wanted_ = nullptr;+  ::grpc::WriteOptions write_options_wanted_;+  ::grpc::Status status_wanted_; };  /// \a ServerReadReactor is the interface for a client-streaming RPC.-template <class Request, class Response>+template <class Request> class ServerReadReactor : public internal::ServerReactor {  public:+  ServerReadReactor() : reader_(nullptr) {}   ~ServerReadReactor() = default;    /// The following operation initiations are exactly like ServerBidiReactor.-  void StartSendInitialMetadata() { reader_->SendInitialMetadata(); }-  void StartRead(Request* req) { reader_->Read(req); }-  void Finish(::grpc::Status s) { reader_->Finish(std::move(s)); }--  /// Similar to ServerBidiReactor::OnStarted, except that this also provides-  /// the response object that the stream fills in before calling Finish.-  /// (It must be filled in if status is OK, but it may be filled in otherwise.)-  ///-  /// \param[in] context The context object now associated with this RPC-  /// \param[in] resp The response object to be used by this RPC-  virtual void OnStarted(::grpc_impl::ServerContext* /*context*/,-                         Response* /*resp*/) {}+  void StartSendInitialMetadata() {+    auto* reader = reader_.load(std::memory_order_acquire);+    if (reader == nullptr) {+      grpc::internal::MutexLock l(&reader_mu_);+      reader = reader_.load(std::memory_order_acquire);+      if (reader == nullptr) {+        send_initial_metadata_wanted_ = true;+        return;+      }+    }+    reader->SendInitialMetadata();+  }+  void StartRead(Request* req) {+    auto* reader = reader_.load(std::memory_order_acquire);+    if (reader == nullptr) {+      grpc::internal::MutexLock l(&reader_mu_);+      reader = reader_.load(std::memory_order_acquire);+      if (reader == nullptr) {+        read_wanted_ = req;+        return;+      }+    }+    reader->Read(req);+  }+  void Finish(::grpc::Status s) {+    auto* reader = reader_.load(std::memory_order_acquire);+    if (reader == nullptr) {+      grpc::internal::MutexLock l(&reader_mu_);+      reader = reader_.load(std::memory_order_acquire);+      if (reader == nullptr) {+        finish_wanted_ = true;+        status_wanted_ = std::move(s);+        return;+      }+    }+    reader->Finish(std::move(s));+  }    /// The following notifications are exactly like ServerBidiReactor.   virtual void OnSendInitialMetadataDone(bool /*ok*/) {}   virtual void OnReadDone(bool /*ok*/) {}-  void OnDone() override {}+  void OnDone() override = 0;   void OnCancel() override {}   private:   friend class ServerCallbackReader<Request>;+   // May be overridden by internal implementation details. This is not a public   // customization point.   virtual void InternalBindReader(ServerCallbackReader<Request>* reader) {-    reader_ = reader;+    grpc::internal::MutexLock l(&reader_mu_);+    reader_.store(reader, std::memory_order_release);+    if (send_initial_metadata_wanted_) {+      reader->SendInitialMetadata();+    }+    if (read_wanted_ != nullptr) {+      reader->Read(read_wanted_);+    }+    if (finish_wanted_) {+      reader->Finish(std::move(status_wanted_));+    }","Thank you for add the resets.  It just occurred to me to ask whether it's ok to call `Finish()`, `Read()` and `SendInitialMetadata()` while holding the lock? My question is mainly about performance.We can potentially just get and reset the pointers and call the stream methods after Lock is released.",X
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/18856,346480053,2019-11-14T18:37:56Z,include/grpcpp/impl/codegen/server_context_impl.h,"@@ -88,36 +95,29 @@ namespace grpc { class GenericServerContext; class ServerInterface; +namespace experimental {+class GenericCallbackServerContext;+}  // namespace experimental+ namespace internal { class Call; }  // namespace internal  namespace testing { class InteropServerContextInspector; class ServerContextTestSpouse;+class DefaultReactorTestPeer;","Why is this class called a ""peer"", but the class above is a ""spouse""?",X
28123841,guantaol,https://api.github.com/repos/grpc/grpc/pulls/19311,346503774,2019-11-14T19:28:01Z,test/core/iomgr/pollset_windows_starvation_test.cc,"@@ -0,0 +1,119 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#if defined(GRPC_WINSOCK_SOCKET)++#include <vector>++#include <grpc/grpc.h>+#include <grpc/support/time.h>++#include ""src/core/lib/gprpp/thd.h""+#include ""src/core/lib/iomgr/exec_ctx.h""+#include ""src/core/lib/iomgr/iocp_windows.h""+#include ""src/core/lib/iomgr/iomgr_internal.h""+#include ""src/core/lib/iomgr/pollset.h""+#include ""src/core/lib/iomgr/pollset_windows.h""+#include ""src/core/lib/surface/init.h""+#include ""test/core/util/test_config.h""++struct ThreadParams {+  gpr_cv cv;+  gpr_mu mu;+  int complete;+};++int main(int argc, char** argv) {+  grpc_init();++  // Create three threads that all start queueing for work.+  //+  // The first one becomes the active poller for work and the two other+  // threads go into the poller queue.+  //+  // When work arrives, the first one notifies the next active poller,+  // this wakes the second thread - however all this does is return from+  // the grpc_pollset_work function. It's up to that thread to figure+  // out if it still wants to queue for more work or if it should kick+  // other pollers.+  //+  // Previously that kick only affected pollers in the same pollset, thus+  // leaving the third thread stuck in the poller queue. Now the pollset-+  // specific grpc_pollset_kick will also kick pollers from other pollsets+  // if there are no pollers in the current pollset. This frees up the+  // last thread and completes the test.+  ThreadParams params = {};+  gpr_cv_init(&params.cv);+  gpr_mu_init(&params.mu);+  std::vector<grpc_core::Thread> threads;+  for (int i = 0; i < 3; i++) {+    grpc_core::Thread thd(+        ""Poller"",+        [](void* params) {+          ThreadParams* tparams = static_cast<ThreadParams*>(params);+          grpc_core::ExecCtx exec_ctx;++          gpr_mu* mu;+          grpc_pollset pollset = {};+          grpc_pollset_init(&pollset, &mu);++          gpr_mu_lock(mu);++          // Queue for work and once we're done, make sure to kick the remaining+          // threads.+          grpc_millis deadline = grpc_timespec_to_millis_round_up(+              grpc_timeout_seconds_to_deadline(5));+          grpc_error* error;+          error = grpc_pollset_work(&pollset, NULL, deadline);+          error = grpc_pollset_kick(&pollset, NULL);++          gpr_mu_unlock(mu);++          {+            gpr_mu_lock(&tparams->mu);+            tparams->complete++;+            gpr_cv_signal(&tparams->cv);+            gpr_mu_unlock(&tparams->mu);+          }+        },+        &params);+    thd.Start();+    threads.push_back(std::move(thd));+  }++  // Wait for the threads to start working and then kick one of them.+  gpr_sleep_until(grpc_timeout_milliseconds_to_deadline(10));","To be honest, I do not think this is a reliable way to wait for all the three threads to have been blocked in grpc_pollset_work. If any of the thread has not started grpc_pollset_work at this point, it will be stuck there later for 5 seconds, which will make this test flaky.However, I do not have a good answer to completely eliminate this flakiness. A few suggestions:1. I think two threads should be enough for this test. This will reduce the contention on the mutex at Ln 74;2. Move the deadline calculation out of the critical section. This will ensure that each thread can finish its critical section sooner.3. Consider extending this sleep time, which will give us a higher chance that both threads have entered grpc_pollset_work.@jtattermusch Do you have any other suggestions?",X
385385,Rantanen,https://api.github.com/repos/grpc/grpc/pulls/19311,346527681,2019-11-14T20:22:57Z,test/core/iomgr/pollset_windows_starvation_test.cc,"@@ -0,0 +1,119 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#if defined(GRPC_WINSOCK_SOCKET)++#include <vector>++#include <grpc/grpc.h>+#include <grpc/support/time.h>++#include ""src/core/lib/gprpp/thd.h""+#include ""src/core/lib/iomgr/exec_ctx.h""+#include ""src/core/lib/iomgr/iocp_windows.h""+#include ""src/core/lib/iomgr/iomgr_internal.h""+#include ""src/core/lib/iomgr/pollset.h""+#include ""src/core/lib/iomgr/pollset_windows.h""+#include ""src/core/lib/surface/init.h""+#include ""test/core/util/test_config.h""++struct ThreadParams {+  gpr_cv cv;+  gpr_mu mu;+  int complete;+};++int main(int argc, char** argv) {+  grpc_init();++  // Create three threads that all start queueing for work.+  //+  // The first one becomes the active poller for work and the two other+  // threads go into the poller queue.+  //+  // When work arrives, the first one notifies the next active poller,+  // this wakes the second thread - however all this does is return from","This is the active poller wait segment:https://github.com/grpc/grpc/blob/a8ab03d7587bc86361b61ba1d91ba619af285d50/src/core/lib/iomgr/pollset_windows.cc#L124-L154> The first one becomes the active poller for workLine 130 is where the active poller waits for more work. The if-statement is guarded with `if (g_active_poller == NULL)` and few lines later sets the `g_active_poller = &worker` to ensure there's only one active poller running code inside that if-statement.> When work arrives, the first one notifies the next ~~active~~ queued poller, this wakes the second thread.After the first thread receives work and exits the `grpc_iocp_work` call on L130, it will proceed to L136..146https://github.com/grpc/grpc/blob/a8ab03d7587bc86361b61ba1d91ba619af285d50/src/core/lib/iomgr/pollset_windows.cc#L136-L146This bit of code tries to acquire another queued poller (first through the current pollset and then through the global queue). If such poller is available, it gets kicked.This ensures the _second_ thread will be waken up.> It's up to that thread to figure out if it still wants to queue for more work or if it should kick other pollers.The above code will only ever kick one additional poller. If the second thread queues for more work, it will then kick the third thread once it receives work. However if the second thread decides it's not needed and wants to terminate, it ends up notifying the pollsets through the `pollset_kick` function, which did not have the same ""local or global"" functionality as the `pollset_work`.------------Save for ""next active poller"" vs ""next queued poller"" issue, I don't think there's anything inaccurate in the comment.",
385385,Rantanen,https://api.github.com/repos/grpc/grpc/pulls/19311,346541711,2019-11-14T20:56:32Z,test/core/iomgr/pollset_windows_starvation_test.cc,"@@ -0,0 +1,119 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#if defined(GRPC_WINSOCK_SOCKET)++#include <vector>++#include <grpc/grpc.h>+#include <grpc/support/time.h>++#include ""src/core/lib/gprpp/thd.h""+#include ""src/core/lib/iomgr/exec_ctx.h""+#include ""src/core/lib/iomgr/iocp_windows.h""+#include ""src/core/lib/iomgr/iomgr_internal.h""+#include ""src/core/lib/iomgr/pollset.h""+#include ""src/core/lib/iomgr/pollset_windows.h""+#include ""src/core/lib/surface/init.h""+#include ""test/core/util/test_config.h""++struct ThreadParams {+  gpr_cv cv;+  gpr_mu mu;+  int complete;+};++int main(int argc, char** argv) {+  grpc_init();++  // Create three threads that all start queueing for work.+  //+  // The first one becomes the active poller for work and the two other+  // threads go into the poller queue.+  //+  // When work arrives, the first one notifies the next active poller,+  // this wakes the second thread - however all this does is return from+  // the grpc_pollset_work function. It's up to that thread to figure+  // out if it still wants to queue for more work or if it should kick+  // other pollers.+  //+  // Previously that kick only affected pollers in the same pollset, thus+  // leaving the third thread stuck in the poller queue. Now the pollset-+  // specific grpc_pollset_kick will also kick pollers from other pollsets+  // if there are no pollers in the current pollset. This frees up the+  // last thread and completes the test.+  ThreadParams params = {};+  gpr_cv_init(&params.cv);+  gpr_mu_init(&params.mu);+  std::vector<grpc_core::Thread> threads;+  for (int i = 0; i < 3; i++) {+    grpc_core::Thread thd(+        ""Poller"",+        [](void* params) {+          ThreadParams* tparams = static_cast<ThreadParams*>(params);+          grpc_core::ExecCtx exec_ctx;++          gpr_mu* mu;+          grpc_pollset pollset = {};+          grpc_pollset_init(&pollset, &mu);++          gpr_mu_lock(mu);++          // Queue for work and once we're done, make sure to kick the remaining+          // threads.+          grpc_millis deadline = grpc_timespec_to_millis_round_up(+              grpc_timeout_seconds_to_deadline(5));+          grpc_error* error;+          error = grpc_pollset_work(&pollset, NULL, deadline);+          error = grpc_pollset_kick(&pollset, NULL);++          gpr_mu_unlock(mu);++          {+            gpr_mu_lock(&tparams->mu);+            tparams->complete++;+            gpr_cv_signal(&tparams->cv);+            gpr_mu_unlock(&tparams->mu);+          }+        },+        &params);+    thd.Start();+    threads.push_back(std::move(thd));+  }++  // Wait for the threads to start working and then kick one of them.+  gpr_sleep_until(grpc_timeout_milliseconds_to_deadline(10));","Also replacing the deadlines with infinite timeouts. The 1-5 second deadlines were there mostly to ensure the failure case would be detected quicker, but given the risk of flakiness, I guess we can rely on the 5-minute ""test is hanging, let's terminate it"" logic provided by the test runner.The success case is still 0.1sec according to the test runner, it's on the failure case that gets delayed.",
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/18856,346582733,2019-11-14T22:37:10Z,include/grpcpp/test/default_reactor_test_peer.h,"@@ -0,0 +1,50 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPCPP_TEST_DEFAULT_REACTOR_TEST_PEER_H+#define GRPCPP_TEST_DEFAULT_REACTOR_TEST_PEER_H++#include <grpcpp/server_context.h>++namespace grpc {+namespace testing {++/// A test-only class to access private members and methods of ServerContext.",I would suggest providing a bit more of a description here. Maybe saying that this class provides a surface for unit-testing unary services that use the default reactor.It would also be helpful to provide some comments for how to use this.,X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/18856,346599904,2019-11-14T23:33:52Z,include/grpcpp/impl/codegen/server_context_impl.h,"@@ -88,36 +95,29 @@ namespace grpc { class GenericServerContext; class ServerInterface; +namespace experimental {+class GenericCallbackServerContext;+}  // namespace experimental+ namespace internal { class Call; }  // namespace internal  namespace testing { class InteropServerContextInspector; class ServerContextTestSpouse;+class DefaultReactorTestPeer;",https://abseil.io/tips/135 consistently calls it a Peer and says that it's sometimes called a spouse. (This was earlier in an internal ToTW 135).  We can't change the name of the Spouse now (to avoid breaking existing users) but we shouldn't add deprecated terms.,
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/18856,346602810,2019-11-14T23:44:26Z,include/grpcpp/impl/codegen/server_context_impl.h,"@@ -268,13 +273,40 @@ class ServerContext {     async_notify_when_done_tag_ = tag;   } -  /// Should be used for framework-level extensions only.-  /// Applications never need to call this method.-  grpc_call* c_call() { return call_; }+  /// NOTE: This is an API for advanced users who need custom allocators.+  /// Get and maybe mutate the allocator state associated with the current RPC.+  /// Currently only applicable for callback unary RPC methods.+  /// WARNING: This is experimental API and could be changed or removed.+  ::grpc::experimental::RpcAllocatorState* GetRpcAllocatorState() {+    return message_allocator_state_;+  }++  /// Get a library-owned default unary reactor for use in minimal reaction","Added, along with explaining the use of optimized minimal reactions.",
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/18856,346615399,2019-11-15T00:36:39Z,include/grpcpp/impl/codegen/server_context_impl.h,"@@ -88,36 +95,29 @@ namespace grpc { class GenericServerContext; class ServerInterface; +namespace experimental {+class GenericCallbackServerContext;+}  // namespace experimental+ namespace internal { class Call; }  // namespace internal  namespace testing { class InteropServerContextInspector; class ServerContextTestSpouse;+class DefaultReactorTestPeer;","I don't see anything in there that suggests Spouse is deprecated.It would be nice if we could be internally consistent, but I honestly prefer the term ""peer"" over ""spouse"", so I guess this is fine.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/21202,346660867,2019-11-15T04:49:08Z,src/csharp/Grpc.Core/Internal/AsyncCall.cs,"@@ -626,6 +626,14 @@ private void HandleFinished(bool success, ClientSideStatus receivedStatus)             if (status.StatusCode != StatusCode.OK)             {                 streamingResponseCallFinishedTcs.SetException(new RpcException(status, receivedStatus.Trailers));+                if (status.StatusCode == StatusCode.Cancelled)+                {+                    // Make sure the exception set to the Task is observed,+                    // otherwise this can trigger ""Unobserved exception"" when the response stream+                    // is not read until its end and the task created by the TCS is garbage collected.+                    // See https://github.com/grpc/grpc/issues/17458+                    var _ = streamingResponseCallFinishedTcs.Task.Exception;","Alternately, log this exception to the grpc logger?",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/21202,346735617,2019-11-15T09:40:57Z,src/csharp/Grpc.Core/Internal/AsyncCall.cs,"@@ -626,6 +626,14 @@ private void HandleFinished(bool success, ClientSideStatus receivedStatus)             if (status.StatusCode != StatusCode.OK)             {                 streamingResponseCallFinishedTcs.SetException(new RpcException(status, receivedStatus.Trailers));+                if (status.StatusCode == StatusCode.Cancelled)","Even if we escape the using statement and cancel the call (due to the `grpc_call_unref` IIC), I'm not certain the status is guaranteed to be cancelled (in case there was e.g. a race with a ""deadline exceeded"" status).Can we safely do the following Exception suppression, unconditionally?",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/21211,346919783,2019-11-15T16:58:12Z,include/grpcpp/impl/codegen/sync.h,"@@ -56,13 +54,7 @@ class Mutex {   const gpr_mu* get() const { return &mu_; }   private:-  union {-    gpr_mu mu_;-    std::mutex do_not_use_sth_;-#ifdef GPR_HAS_PTHREAD_H-    pthread_mutex_t do_not_use_pth_;-#endif-  };+  gpr_mu mu_; };","This one is a special case. We have to keep this one. We need a union that is as big as the bigger of gpr_mu, std::mutex, and pthread_mutex_t in order to be able to run multi-platform. This has caused problems for us in the past.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/21211,346953023,2019-11-15T18:26:18Z,tools/run_tests/sanity/cpp_banned_constructs.sh,"@@ -29,3 +29,12 @@ egrep -Irn \     egrep -v include/grpcpp/impl/codegen/sync.h | \     diff - /dev/null +#+# Prevent the include of disallowed C++ headers.+#++egrep -Irn \+    '^#include (<mutex>|<condition_variable>|<thread>|<ratio>|<filesystem>|<future>|<system_error>)' \+    include/grpc include/grpcpp src/core | \",Oops my bad. Fixed it.,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/21211,347039133,2019-11-15T22:37:54Z,include/grpcpp/impl/codegen/sync.h,"@@ -56,13 +54,7 @@ class Mutex {   const gpr_mu* get() const { return &mu_; }   private:-  union {-    gpr_mu mu_;-    std::mutex do_not_use_sth_;-#ifdef GPR_HAS_PTHREAD_H-    pthread_mutex_t do_not_use_pth_;-#endif-  };+  gpr_mu mu_; };",I did actually and it seems that there was ODR violation and it required this hack to work it around. But we're not sure it's still required. I'll try to remove this later.,X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/19296,347667929,2019-11-18T23:45:45Z,src/core/lib/iomgr/tcp_server_utils_posix_common.cc,"@@ -157,6 +157,13 @@ grpc_error* grpc_tcp_server_prepare_socket(grpc_tcp_server* s, int fd,     if (err != GRPC_ERROR_NONE) goto error;   } +#ifdef GRPC_LINUX_ERRQUEUE+  err = grpc_set_socket_zerocopy(fd);",Oh you're right. Why are we setting this on the listening socket too?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21246,347731555,2019-11-19T05:00:58Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -44,11 +62,11 @@ def _timeout_to_deadline(self, timeout: int) -> Optional[int]:     def __call__(self,                  request,","What should be the type for request? I would suggest using `typing.Generic` for the class, and use `TypeVar` here.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21246,347732860,2019-11-19T05:08:59Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -123,6 +191,14 @@ def __init__(self, target, options, credentials, compression):         if compression:             raise NotImplementedError(""TODO: compression not implemented yet"") +        if interceptors is None:+            self._unary_unary_interceptors = None+        else:+            self._unary_unary_interceptors = list(+                filter(+                    lambda interceptor: isinstance(interceptor, UnaryUnaryClientInterceptor),+                    interceptors)) or None","Instead of filter out silently, do you think we should raise an exception? Make least surprise?",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21246,348042930,2019-11-19T16:48:55Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -88,22 +101,77 @@ def __call__(self,          serialized_request = _common.serialize(request,                                                self._request_serializer)-        timeout = self._timeout_to_deadline(timeout)         aio_cancel_status = cygrpc.AioCancelStatus()-        aio_call = asyncio.ensure_future(-            self._channel.unary_unary(self._method, serialized_request, timeout,-                                      aio_cancel_status),-            loop=self._loop)++        if self._interceptors:+            client_call_details = _ClientCallDetails(+                self._method, timeout, metadata, credentials, wait_for_ready,+                compression)+            aio_call = asyncio.ensure_future(+                self._wrap_call_in_interceptors(+                    client_call_details, serialized_request, aio_cancel_status),+                loop=self._loop)","Yes, new tasks automatically inherit all context from the parent tasks [1].For futures would depend on the task context where a future is being accessed, but for the relevant part the following code:```python    await future```The task awaiting for this future would be bound to the task context, nothing else.[1] https://github.com/python/cpython/blob/edad4d89e357c92f70c0324b937845d652b20afd/Lib/asyncio/tasks.py#L164",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21246,348050888,2019-11-19T17:02:09Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -88,22 +101,77 @@ def __call__(self,          serialized_request = _common.serialize(request,                                                self._request_serializer)-        timeout = self._timeout_to_deadline(timeout)         aio_cancel_status = cygrpc.AioCancelStatus()-        aio_call = asyncio.ensure_future(-            self._channel.unary_unary(self._method, serialized_request, timeout,-                                      aio_cancel_status),-            loop=self._loop)++        if self._interceptors:+            client_call_details = _ClientCallDetails(+                self._method, timeout, metadata, credentials, wait_for_ready,+                compression)+            aio_call = asyncio.ensure_future(+                self._wrap_call_in_interceptors(+                    client_call_details, serialized_request, aio_cancel_status),+                loop=self._loop)+        else:+            aio_call = asyncio.ensure_future(+                self._call(self._method, serialized_request, timeout,+                           aio_cancel_status),+                loop=self._loop)+         return Call(aio_call, self._response_deserializer, aio_cancel_status) +    async def _wrap_call_in_interceptors(+            self, client_call_details: ClientCallDetails, request: bytes,+            aio_cancel_status: cygrpc.AioCancelStatus):+        """"""Run the RPC call wraped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterable[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails, request: bytes):+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                return await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)+            else:+                return await self._call(client_call_details.method, request,+                                        client_call_details.timeout,+                                        aio_cancel_status)","Despite this would solve one of the main problems that I mentioned, having the interceptor not forced to make two `awaits` for resolving the `Call` since the continuation function is not a coroutine,  the main down-side effect of this would be having to create as many Asyncio tasks as many interceptors we have, which defenitly might impact in the performance.I can measure what would be the impact in terms of QPS and see if there is any substantial degradation.But I do not dislike this idea, because it would provide consistency in the API and allowing cancellation.Thanks!",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21246,348063228,2019-11-19T17:26:14Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -0,0 +1,216 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import collections+import logging+import unittest++import grpc++from grpc.experimental import aio+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from src.proto.grpc.testing import messages_pb2+++class _ClientCallDetails(+        collections.namedtuple(+            '_ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):+    pass","Considering that this is tests I won't be worried about typing since we are not enforncing typing in the tests, we want to do it?Another discussion is how to improve this vague object [1] which might be not only an interface but also an implementation since the `ClientCallDetails` needs to be instantiated within the interceptors. For me there are two pending discussions here:Do we need to make the `ClientCallDetails` a first citizen class implementation instead of having this vague interface since interceptors need to instantiate them? So having something like```pythonclass ClientCallDetails(        collections.namedtuple(            '_ClientCallDetails',            ('method', 'timeout', 'metadata', 'credentials')),        grpc.ClientCallDetails):    method: int    timeout: float    metadata: dict    credentials: dict```We could even add a clone method for making life easier to the developers```pythonclass ClientCallDetails(        collections.namedtuple(            '_ClientCallDetails',            ('method', 'timeout', 'metadata', 'credentials')),        grpc.ClientCallDetails):    method: int    timeout: float    metadata: dict    credentials: dict    def clone(self, method=None, timeout=None, metadata=None, credentials=None):        """"""""Clone the ClientDetails to a new one and override any of the attributes with a new        value with giving a None value""""""```The second discussion would be, can we make usage of something more explicit than a `namedtuple`?- Yes a namedutple with some class typing?- Use data classes [2] that they play well with typing. IIRC they are available since Python 3.7 so it won't be an option if I'm not wrong since we are providing support for 3.5.X and beyond.- Use a regular Python class which again plays well with typing but might have some performance footprint.- Use an external library like `attrs` [3] IMO I would go for an improved version fo the `namedtuple` by having thee class typing and providing a first citizen class implementation rather than an interface.WDTY?[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/__init__.py#L406[2] https://docs.python.org/3/library/dataclasses.html[2] https://www.attrs.org/en/stable/",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21246,348073043,2019-11-19T17:45:40Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -88,22 +101,77 @@ def __call__(self,          serialized_request = _common.serialize(request,                                                self._request_serializer)-        timeout = self._timeout_to_deadline(timeout)         aio_cancel_status = cygrpc.AioCancelStatus()-        aio_call = asyncio.ensure_future(-            self._channel.unary_unary(self._method, serialized_request, timeout,-                                      aio_cancel_status),-            loop=self._loop)++        if self._interceptors:+            client_call_details = _ClientCallDetails(+                self._method, timeout, metadata, credentials, wait_for_ready,+                compression)+            aio_call = asyncio.ensure_future(+                self._wrap_call_in_interceptors(+                    client_call_details, serialized_request, aio_cancel_status),+                loop=self._loop)+        else:+            aio_call = asyncio.ensure_future(+                self._call(self._method, serialized_request, timeout,+                           aio_cancel_status),+                loop=self._loop)+         return Call(aio_call, self._response_deserializer, aio_cancel_status) +    async def _wrap_call_in_interceptors(+            self, client_call_details: ClientCallDetails, request: bytes,+            aio_cancel_status: cygrpc.AioCancelStatus):+        """"""Run the RPC call wraped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterable[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails, request: bytes):+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                return await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)+            else:+                return await self._call(client_call_details.method, request,+                                        client_call_details.timeout,+                                        aio_cancel_status)","I remember I performed a benchmark to measure the difference between directly `await` and create an `Task`. The cost should be similar since underlying they are all wrappers of the `call_soon` API. But as we know, the concrete benchmark result is always better than assumptions. We can make the decision based on the result.```Spent 16.08s for 1000000 runs of directly_await16082.14 nsSpent 15.65s for 1000000 runs of schedule_task15653.08 ns``````Pythonimport asyncioimport timeitimport timeasync def work():    return ""I am not working""async def directly_await():    await work()def schedule_task():    return asyncio.create_task(work())async def main():    number = 1000000    start_time = time.time()    await asyncio.gather(*(directly_await() for i in range(number)))    elapsed_time = time.time() - start_time    print(""Spent %.2fs for %d runs of %s"" % (elapsed_time, number, 'directly_await'))    print(""%.2f ns"" % (elapsed_time / number * 1e9))        start_time = time.time()    await asyncio.gather(*(schedule_task() for i in range(number)))    elapsed_time = time.time() - start_time    print(""Spent %.2fs for %d runs of %s"" % (elapsed_time, number, 'schedule_task'))    print(""%.2f ns"" % (elapsed_time / number * 1e9))if __name__ == ""__main__"":    asyncio.run(main())```",
14241278,ajamato,https://api.github.com/repos/grpc/grpc/pulls/21249,348704888,2019-11-20T19:34:18Z,bazel/grpc_deps.bzl,"@@ -81,11 +81,21 @@ def grpc_deps():         actual = ""@com_github_grpc_grpc//:grpc++_codegen_proto"",     ) +    native.bind(","I am not sure if this is possible, I made a change which won't build. From what I understand, bind is basically aliasing. I removed it and referred to the actual string in the BUILD file. But perhaps you do preprocessing on the BUILD/bzl files to remove the @XYZ strings? Is there some config for that, which I also need to update? Is it only pre processing the bzl, and not BUILD files? Just a guess. What do you recommend here?ajamato@ajamato-linux0:~/grpc$ bazel test :allINFO: Running bazel wrapper (see //tools/bazel for details), bazel version 1.0.0 will be used instead of system-wide bazel installation.INFO: Writing tracer profile to '/usr/local/google/home/ajamato/.cache/bazel/_bazel_ajamato/7d9cb851a3ed2d28aac44248d4b651f6/command.profile.gz'DEBUG: /usr/local/google/home/ajamato/.cache/bazel/_bazel_ajamato/7d9cb851a3ed2d28aac44248d4b651f6/external/bazel_toolchains/rules/rbe_repo/checked_in.bzl:226:13: rbe_msan not using checked in configs; Bazel version 1.0.0 was picked/selected with '[""9.0.0"", ""10.0.0""]' compatible configs but none match the 'env = {""ABI_LIBC_VERSION"": ""glibc_2.19"", ""ABI_VERSION"": ""clang"", ""BAZEL_COMPILER"": ""clang"", ""BAZEL_HOST_SYSTEM"": ""i686-unknown-linux-gnu"", ""BAZEL_TARGET_CPU"": ""k8"", ""BAZEL_TARGET_LIBC"": ""glibc_2.19"", ""BAZEL_TARGET_SYSTEM"": ""x86_64-unknown-linux-gnu"", ""CC"": ""clang"", ""CC_TOOLCHAIN_NAME"": ""linux_gnu_x86"", ""BAZEL_LINKOPTS"": ""-lc++:-lc++abi:-lm""}', 'config_repos = None',and/or 'create_cc_configs = True' passed as attrsERROR: /usr/local/google/home/ajamato/grpc/BUILD:2276:1: //:grpc_opencensus_plugin: invalid label '//external:@io_opencensus_cpp//opencensus/trace:context_util' in element 5 of attribute 'deps' in 'cc_library' rule: invalid target name '@io_opencensus_cpp//opencensus/trace:context_util': target names may not contain '//' path separatorsERROR: error loading package '': Package '' contains errorsINFO: Elapsed time: 0.138sINFO: 0 processes.FAILED: Build did NOT complete successfully (0 packages loaded)FAILED: Build did NOT complete successfully (0 packages loaded)",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21246,348746315,2019-11-20T21:10:44Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -88,22 +101,77 @@ def __call__(self,          serialized_request = _common.serialize(request,                                                self._request_serializer)-        timeout = self._timeout_to_deadline(timeout)         aio_cancel_status = cygrpc.AioCancelStatus()-        aio_call = asyncio.ensure_future(-            self._channel.unary_unary(self._method, serialized_request, timeout,-                                      aio_cancel_status),-            loop=self._loop)++        if self._interceptors:+            client_call_details = _ClientCallDetails(+                self._method, timeout, metadata, credentials, wait_for_ready,+                compression)+            aio_call = asyncio.ensure_future(+                self._wrap_call_in_interceptors(+                    client_call_details, serialized_request, aio_cancel_status),+                loop=self._loop)+        else:+            aio_call = asyncio.ensure_future(+                self._call(self._method, serialized_request, timeout,+                           aio_cancel_status),+                loop=self._loop)+         return Call(aio_call, self._response_deserializer, aio_cancel_status) +    async def _wrap_call_in_interceptors(+            self, client_call_details: ClientCallDetails, request: bytes,+            aio_cancel_status: cygrpc.AioCancelStatus):+        """"""Run the RPC call wraped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterable[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails, request: bytes):+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                return await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)+            else:+                return await self._call(client_call_details.method, request,+                                        client_call_details.timeout,+                                        aio_cancel_status)","Umm, be careful the `gather` internally wraps coros into tasks [1], so you are comparing the same pattern implicitly, no direct call at all.Ive changed a bit the script for comparing both patterns, see the following script and the results```pythonimport asyncioimport timeitimport timeasync def work():    return ""I am not working""async def directly_await():    await work()async def schedule_task():    return await asyncio.create_task(work())async def main():    number = 1000000    start_time = time.time()    for i in range(number):        await directly_await()    elapsed_time = time.time() - start_time    print(""Spent %.2fs for %d runs of %s"" % (elapsed_time, number, 'directly_await'))    print(""%.2f ns"" % (elapsed_time / number * 1e9))    start_time = time.time()    for i in range(number):        await schedule_task()    elapsed_time = time.time() - start_time    print(""Spent %.2fs for %d runs of %s"" % (elapsed_time, number, 'schedule_task'))    print(""%.2f ns"" % (elapsed_time / number * 1e9))if __name__ == ""__main__"":    asyncio.run(main())```The results are:``` bash$ python /tmp/coros_vs_tasks.pySpent 0.33s for 1000000 runs of directly_await332.50 nsSpent 16.96s for 1000000 runs of schedule_task16958.59 ns```Wrapping the coros in tasks means having a slow down of the code ~ x50 times. I guess that the `await`/`yield from` has a really small footprint since it can be processed by the CPython in one just one bytecode operation and some x86 instructions while creating a task and scheduling means a lot of Python code.[1] https://github.com/python/cpython/blob/master/Lib/asyncio/tasks.py#L806",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21249,348762640,2019-11-20T21:49:01Z,bazel/grpc_deps.bzl,"@@ -81,11 +81,21 @@ def grpc_deps():         actual = ""@com_github_grpc_grpc//:grpc++_codegen_proto"",     ) +    native.bind(",I see. This appears to be a failure of `grpc_cc_library`'s implementation. It will take a bit more effort to fix than is reasonable to expect from this PR. The binds will be fine for now.,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21246,348775546,2019-11-20T22:20:07Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,46 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+from typing import Callable++import grpc","I'm gonna use for using a new fresh implementation provided by the `aio` module with a clone method, so removing it from the test",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21246,348776716,2019-11-20T22:22:59Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -0,0 +1,216 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import collections+import logging+import unittest++import grpc++from grpc.experimental import aio+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from src.proto.grpc.testing import messages_pb2+++class _ClientCallDetails(+        collections.namedtuple(+            '_ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):+    pass",I will go for making the `ClientCallDetails` as a first citizen class provided by the `Aio` module.,
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/21196,348848071,2019-11-21T00:46:13Z,src/cpp/common/alts_context.cc,"@@ -0,0 +1,45 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include ""alts_context.h""++#include <grpc/grpc_security.h>++#include ""src/core/tsi/alts/handshaker/alts_tsi_handshaker.h""+#include ""src/cpp/common/secure_auth_context.h""++namespace grpc {+std::unique_ptr<gcp::AltsContext> GetAltsContextFromAuthContext(+    const AuthContext& auth_context) {+  std::vector<string_ref> ctx_vector =+      auth_context.FindPropertyValues(TSI_ALTS_CONTEXT);+  if (ctx_vector.size() != 1) {+    gpr_log(GPR_ERROR, ""contains zero or more than one ALTS context."");+    return nullptr;+  }+  std::unique_ptr<gcp::AltsContext> uniq_ctx(new gcp::AltsContext());+  std::string serialized_ctx(ctx_vector.front().data(),+                             ctx_vector.front().size());+  bool success = uniq_ctx.get()->ParseFromString(serialized_ctx);",Consider using ParseFromArray or something similar so that you do not need to construct a string just for parsing.,
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/21196,348848442,2019-11-21T00:47:49Z,test/cpp/common/alts_context_test.cc,"@@ -0,0 +1,98 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include ""src/cpp/common/alts_context.h""++#include <grpcpp/security/auth_context.h>+#include <gtest/gtest.h>++#include ""src/core/tsi/alts/handshaker/alts_tsi_handshaker.h""+#include ""src/cpp/common/secure_auth_context.h""+#include ""src/proto/grpc/gcp/altscontext.pb.h""+#include ""test/cpp/util/string_ref_helper.h""++using grpc::testing::ToString;++namespace grpc {+namespace {++class AltsContextTest : public ::testing::Test {};","If you do not need a test class, you can remove it and just use TEST() instead of TEST_F().",X
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/21196,348849078,2019-11-21T00:50:19Z,src/cpp/common/alts_context.h,"@@ -0,0 +1,36 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_INTERNAL_CPP_COMMON_ALTS_CONTEXT_H+#define GRPC_INTERNAL_CPP_COMMON_ALTS_CONTEXT_H++#include <grpcpp/security/auth_context.h>++#include ""src/core/lib/security/context/security_context.h""+#include ""src/proto/grpc/gcp/altscontext.pb.h""++namespace grpc {+// GetAltsContextFromAuthContext helps to get the AltsContext from AuthContext.+// Please make sure the underlying protocol is ALTS before calling this+// function, otherwise a nullptr will be returned.+std::unique_ptr<gcp::AltsContext> GetAltsContextFromAuthContext(+    const AuthContext& auth_context);","Note if this is considered an API for all users, it should go to somewhere under `include/grpcpp` ... If it is just for some internal library etc, then this is fine.",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/20530,348909129,2019-11-21T06:07:20Z,src/core/tsi/ssl_transport_security.cc,"@@ -1732,7 +1757,11 @@ tsi_result tsi_create_ssl_client_handshaker_factory_with_options(     tsi_ssl_handshaker_factory_unref(&impl->base);     return result;   }-  SSL_CTX_set_verify(ssl_context, SSL_VERIFY_PEER, nullptr);+  if (options->server_verification_option == GRPC_SSL_SKIP_SERVER_VERIFICATION) {","Unfortunately, the design of transport security is that it is independent module and namespace from gRPC. Thus ssl_transport_security cannot depend on grpc_security.h here and we should not use GRPC_SSL_SKIP_SERVER_VERIFICATION directly.You can refer how GRPC_SSL_DONT_REQUEST_CLIENT_CERTIFICATE converts to TSI_DONT_REQUEST_CLIENT_CERTIFICATE, as an example. This is a little bit inconvenient though. ",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21194,349083103,2019-11-21T13:28:33Z,templates/src/objective-c/BoringSSL-GRPC.podspec.template,"@@ -225,25 +223,14 @@       % endfor       EOF -      # The symbol prefixing mechanism is performed by redefining BoringSSL symbols with ""#define-      # SOME_BORINGSSL_SYMBOL GRPC_SHADOW_SOME_BORINGSSL_SYMBOL"". Unfortunately, some symbols are-      # already redefined as macros in BoringSSL headers in the form ""#define SOME_BORINGSSL_SYMBOL-      # SOME_BORINGSSL_SYMBOL"" Such type of redefinition will cause ""SOME_BORINGSSL_SYMBOL redefined""-      # error when using together with our prefix header. So the workaround in the below lines removes-      # all such type of #define directives.-      sed -i'.back' '/^#define \\([A-Za-z0-9_]*\\) \\1/d' include/openssl/*.h-      # Remove lines of the format below for the same reason above-      #     #define SOME_BORINGSSL_SYMBOL ${""\\""}-      #         SOME_BORINGSSL_SYMBOL-      sed -i'.back' '/^#define.*\\\\$/{N;/^#define \\([A-Za-z0-9_]*\\) *\\\\\\n *\\1/d;}' include/openssl/*.h+      # Grab prefix header from Github repo+      curl -o include/openssl/boringssl_prefix_symbols.h -L https://raw.githubusercontent.com/grpc/grpc/master/src/objective-c/boringssl_prefix_headers/boringssl_prefix_symbols-${boringssl_commit}.h","I understand the challenge here, but this downloads the right version of boringssl_prefix_symbols (depending on boringssl's SHA) from master of our repository. That means we'd have to keep an ever-growing list of boringssl shadow headers in master of our repository forever (one version for each SHA we use), and that's not a good solution. Also I don't like that the download command can fail and that can lead to random errors when building.Would inlining a base64  gzip compressed version of boringssl_prefix_symbols.h fit in the podspec? I know it's dirty but it's better than the challenges of this approach.Checking in a generated version of boringssl_prefix_symbols.h into our repo is fine, but then regenerating project could inline the base64 compressed version of it into the podspec (which make the podspec standalone).",
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/21196,349194611,2019-11-21T16:41:16Z,src/cpp/common/alts_context.cc,"@@ -0,0 +1,45 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include ""alts_context.h""++#include <grpc/grpc_security.h>++#include ""src/core/tsi/alts/handshaker/alts_tsi_handshaker.h""+#include ""src/cpp/common/secure_auth_context.h""++namespace grpc {+std::unique_ptr<gcp::AltsContext> GetAltsContextFromAuthContext(+    const AuthContext& auth_context) {+  std::vector<string_ref> ctx_vector =+      auth_context.FindPropertyValues(TSI_ALTS_CONTEXT);+  if (ctx_vector.size() != 1) {+    gpr_log(GPR_ERROR, ""contains zero or more than one ALTS context."");+    return nullptr;+  }+  std::unique_ptr<gcp::AltsContext> uniq_ctx(new gcp::AltsContext());+  std::string serialized_ctx(ctx_vector.front().data(),+                             ctx_vector.front().size());+  bool success = uniq_ctx.get()->ParseFromString(serialized_ctx);","I meant to do this```bool success = uniq_ctx.get()->ParseFromArray(ctx_vector[0].data(), ctx_vector[0].size());```",
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/21196,349198198,2019-11-21T16:47:25Z,BUILD,"@@ -427,6 +427,22 @@ grpc_cc_library(     ], ) +grpc_cc_library(+    name = ""grpc++_alts"",+    srcs = [+        ""src/cpp/common/alts_context.cc"",+    ],+    hdrs = [+        ""include/grpcpp/alts_context.h"",","Note by only changing this file, you are only adding this library for the users building with bazel. If they build with make, they will not be able to use this library. Not sure whether this is intended.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21194,349202145,2019-11-21T16:53:59Z,gRPC-Core.podspec,"@@ -185,8 +185,8 @@ Pod::Spec.new do |s|     ss.header_mappings_dir = '.'     ss.libraries = 'z'     ss.dependency ""#{s.name}/Interface"", version-    ss.dependency 'BoringSSL-GRPC', '0.0.5'-    ss.compiler_flags = '-DGRPC_SHADOW_BORINGSSL_SYMBOLS'+    ss.dependency 'BoringSSL-GRPC', '0.0.6'+    ss.compiler_flags = '-DBORINGSSL_PREFIX=GRPC'","And by using BoringSSL's approach, we no longer need `GRPC_SHADOW_BORINGSSL_SYMBOLS` as that is gRPC's macro to enable symbol prefixing.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21194,349208525,2019-11-21T17:04:23Z,templates/src/objective-c/BoringSSL-GRPC.podspec.template,"@@ -225,25 +223,14 @@       % endfor       EOF -      # The symbol prefixing mechanism is performed by redefining BoringSSL symbols with ""#define-      # SOME_BORINGSSL_SYMBOL GRPC_SHADOW_SOME_BORINGSSL_SYMBOL"". Unfortunately, some symbols are-      # already redefined as macros in BoringSSL headers in the form ""#define SOME_BORINGSSL_SYMBOL-      # SOME_BORINGSSL_SYMBOL"" Such type of redefinition will cause ""SOME_BORINGSSL_SYMBOL redefined""-      # error when using together with our prefix header. So the workaround in the below lines removes-      # all such type of #define directives.-      sed -i'.back' '/^#define \\([A-Za-z0-9_]*\\) \\1/d' include/openssl/*.h-      # Remove lines of the format below for the same reason above-      #     #define SOME_BORINGSSL_SYMBOL ${""\\""}-      #         SOME_BORINGSSL_SYMBOL-      sed -i'.back' '/^#define.*\\\\$/{N;/^#define \\([A-Za-z0-9_]*\\) *\\\\\\n *\\1/d;}' include/openssl/*.h+      # Grab prefix header from Github repo+      curl -o include/openssl/boringssl_prefix_symbols.h -L https://raw.githubusercontent.com/grpc/grpc/master/src/objective-c/boringssl_prefix_headers/boringssl_prefix_symbols-${boringssl_commit}.h        # We are renaming openssl to openssl_grpc so that there is no conflict with openssl if it exists       find . -type f \\( -path '*.h' -or -path '*.cc' -or -path '*.c' \\) -print0 | xargs -0 -L1 sed -E -i'.grpc_back' 's;#include <openssl/;#include <openssl_grpc/;g'-    END_OF_COMMAND+      # BoringSSL include boringssl_prefix_symbols.h without any prefix, which does not match the -    # Redefine symbols to avoid conflict when the same app also depends on OpenSSL. The list of-    # symbols are src/objective-c/grpc_shadow_boringssl_symbol_list.-    # This is the last part of this file.-    s.prefix_header_contents = -      ${expand_symbol_list(settings.grpc_shadow_boringssl_symbols)}+      # Xcode import style. We add it here so that Xcode knows where to find it.+      find . -type f \\( -path '*.h' -or -path '*.cc' -or -path '*.c' \\) -print0 | xargs -0 -L1 sed -E -i'.grpc_back' 's;#include <boringssl_prefix_symbols.h>;#include <openssl_grpc/boringssl_prefix_symbols.h>;g'","I don't think so because if users build their projects as Framework, Apple does require Framework name to be the first segment in the include statement (see [Apple's doc](https://developer.apple.com/library/archive/documentation/MacOSX/Conceptual/BPFrameworks/Tasks/IncludingFrameworks.html)). The only possible way to avoid it is to hack the ""Header search path"" of the generated `gRPC-Core` and `BoringSSL-GRPC` targets pointing to the downloaded BoringSSL repo. Not sure if it'll work for Framework (because in that case the two targets are including a header outside of the Framework's Header directory). And if it works, IMHO it's an even more dirty hack.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21194,349217377,2019-11-21T17:21:10Z,tools/distrib/upgrade_boringssl_objc.sh,"@@ -0,0 +1,45 @@+#!/bin/bash+# Copyright 2018 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# Generate the list of boringssl symbols that need to be shadowed based on the+# current boringssl submodule. Requires local toolchain to build boringssl.++set -e++cd ""$(dirname $0)""+cd ../../third_party/boringssl++BORINGSSL_COMMIT=$(git rev-parse HEAD)+BORINGSSL_PREFIX_HEADERS_DIR=src/objective-c/boringssl_prefix_headers++# Do the following in grpc root directory+cd ../..++docker build tools/dockerfile/grpc_objc/generate_boringssl_prefix_header -t grpc/boringssl_prefix_header","I am using Docker for a few reasons:* People also use Mac for the same purpose too (this process is for ObjC). Installing dependencies on Mac is not as trivial as on Linux.* I tend to not mess with user's workspace. As you see the process involves building BoringSSL and creating a symbol list file. The BoringSSL in the user's workspace may have their own changes. And I do need to `rm -rf` the artifacts which makes me a bit uncomfortable. Docker just get rid of all those matters.* We are already using Docker in `tools/distrib/clang_format_code.sh`. As to your question, even if we do not use Docker, it would be pretty much combining the 2 shell scripts into one; I don't think there's much to remove there other than the Docker related lines. So I don't think Docker complicate things too.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21194,349224786,2019-11-21T17:36:36Z,tools/dockerfile/grpc_objc/generate_boringssl_prefix_header/generate_boringssl_prefix_header.sh,"@@ -0,0 +1,28 @@+#!/bin/bash++[ $# == 1 ] || { echo ""Usage: generate_boringssl_prefix_header.sh <boringssl_commit>"" ; exit 1 ; }++git clone -n https://github.com/google/boringssl.git","Part of the reason was I did not find a way to cleanly copy both `/tools/dockerfile/grpc_objc/generate_boringssl_prefix_header/generate_boringssl_prefix_header.sh` and `/third_party/boringssl` into the container. To make it possible I probably need to set the Docker build context to be gRPC's root dir, but that means transferring the entire grpc directory to Docker when running `docker build`, which takes much more time than `git clone` when I tried it.And also for the same sanity reason as above, it does not cost too much to get a clean version of BoringSSL.",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/21215,349239816,2019-11-21T18:09:43Z,src/core/lib/security/credentials/composite/composite_credentials.h,"@@ -86,9 +86,16 @@ class grpc_composite_call_credentials : public grpc_call_credentials {   void cancel_get_request_metadata(grpc_credentials_mdelem_array* md_array,                                    grpc_error* error) override; +  grpc_security_level security_level() override { return security_level_; }++  void set_security_level(grpc_security_level security_level) override {+    security_level_ = security_level;+  }+   const CallCredentialsList& inner() const { return inner_; }   private:+  grpc_security_level security_level_;",nit: move this line after push_to_inner()We should group data member together.,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21202,349286916,2019-11-21T19:52:43Z,src/csharp/Grpc.Core/Internal/AsyncCall.cs,"@@ -626,6 +626,14 @@ private void HandleFinished(bool success, ClientSideStatus receivedStatus)             if (status.StatusCode != StatusCode.OK)             {                 streamingResponseCallFinishedTcs.SetException(new RpcException(status, receivedStatus.Trailers));+                if (status.StatusCode == StatusCode.Cancelled)","I didn't want to assume too much. In general users should be exhausting the streams, not doing so can lead to other problems (so it's fair that they are warned about an unobserved exception).The case when the call is cancelled is one situation when not exhausting the response stream is safe, so I wanted to suppress in this special case.I think one more thing I can check for is the ""cancellationRequested"" flag (whether Cancel() was called), which should make this supression even more reliable.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21246,349378264,2019-11-22T00:01:27Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -27,13 +32,26 @@ class UnaryUnaryMultiCallable:     """"""Afford invoking a unary-unary RPC from client-side in an asynchronous way."""""" -    def __init__(self, channel: cygrpc.AioChannel, method: bytes,-                 request_serializer: SerializingFunction,-                 response_deserializer: DeserializingFunction) -> None:+    _channel: cygrpc.AioChannel+    _method: bytes+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction+    _interceptors: Optional[List[UnaryUnaryClientInterceptor]]+    _loop: asyncio.AbstractEventLoop++    def __init__(+            self,+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction,+            interceptors: Optional[List[UnaryUnaryClientInterceptor]] = None",Still a list here.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21246,349381990,2019-11-22T00:17:04Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -0,0 +1,216 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import collections+import logging+import unittest++import grpc++from grpc.experimental import aio+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from src.proto.grpc.testing import messages_pb2+++class _ClientCallDetails(+        collections.namedtuple(+            '_ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):+    pass","First, if this is just an implementation of `grpc.ClientCallDetails`, maybe it shouldn't be a first class citizen.Second, the `clone` method is reasonable. If we want to add it, we need to add it to both stack. Technically, adding new API should go through the [gRFC process](https://github.com/grpc/proposal). You can write a short document to give people a chance to comment, most gRFC are quite short.Third, based on the [Google Style Guide](http://google.github.io/styleguide/pyguide.html#384-classes), the recommended way of writing class docstring is that: 1) short summary in one line; 2) long summery can spread multiple lines, even include examples; 3) `Args`, `Returns`, `Exceptions` come after.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21246,349382572,2019-11-22T00:19:38Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -88,22 +101,77 @@ def __call__(self,          serialized_request = _common.serialize(request,                                                self._request_serializer)-        timeout = self._timeout_to_deadline(timeout)         aio_cancel_status = cygrpc.AioCancelStatus()-        aio_call = asyncio.ensure_future(-            self._channel.unary_unary(self._method, serialized_request, timeout,-                                      aio_cancel_status),-            loop=self._loop)++        if self._interceptors:+            client_call_details = _ClientCallDetails(+                self._method, timeout, metadata, credentials, wait_for_ready,+                compression)+            aio_call = asyncio.ensure_future(+                self._wrap_call_in_interceptors(+                    client_call_details, serialized_request, aio_cancel_status),+                loop=self._loop)+        else:+            aio_call = asyncio.ensure_future(+                self._call(self._method, serialized_request, timeout,+                           aio_cancel_status),+                loop=self._loop)+         return Call(aio_call, self._response_deserializer, aio_cancel_status) +    async def _wrap_call_in_interceptors(+            self, client_call_details: ClientCallDetails, request: bytes,+            aio_cancel_status: cygrpc.AioCancelStatus):+        """"""Run the RPC call wraped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterable[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails, request: bytes):+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                return await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)+            else:+                return await self._call(client_call_details.method, request,+                                        client_call_details.timeout,+                                        aio_cancel_status)","This problem is going to bite us quickly, without access to the `Call` object, we will find the functionality of interceptors worse than normal call, e.g. no access to `cancel`, `initial_metadata`, `is_active`.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21246,349387540,2019-11-22T00:40:25Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -0,0 +1,221 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import collections+import logging+import unittest++import grpc++from grpc.experimental import aio+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from src.proto.grpc.testing import messages_pb2+++class TestUnaryUnaryClientInterceptor(AioTestBase):++    def test_invalid(self):++        class InvalidInterceptor:+            """"""Just an invalid Interceptor""""""++        with self.assertRaises(ValueError):+            aio.insecure_channel("""", interceptors=[InvalidInterceptor()])++    def test_executed_right_order(self):++        interceptors_executed = []++        class Interceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for testing if the interceptor is being called""""""++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                interceptors_executed.append(self)+                return await continuation(client_call_details, request)++        async def coro():+            interceptors = [Interceptor() for i in range(2)]++            server_target, _ = await start_test_server()  # pylint: disable=unused-variable++            async with aio.insecure_channel(+                    server_target, interceptors=interceptors) as channel:+                multicallable = channel.unary_unary(+                    '/grpc.testing.TestService/UnaryCall',+                    request_serializer=messages_pb2.SimpleRequest.+                    SerializeToString,+                    response_deserializer=messages_pb2.SimpleResponse.FromString+                )+                response = await multicallable(messages_pb2.SimpleRequest())++                # Check that all interceptors were executed, and were executed+                # in the right order.+                self.assertEqual(interceptors_executed, interceptors)++                self.assertEqual(type(response), messages_pb2.SimpleResponse)",[`assertIsInstance`](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertIsInstance),
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21246,349388438,2019-11-22T00:44:37Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -0,0 +1,221 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import collections+import logging+import unittest++import grpc++from grpc.experimental import aio+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from src.proto.grpc.testing import messages_pb2+++class TestUnaryUnaryClientInterceptor(AioTestBase):++    def test_invalid(self):++        class InvalidInterceptor:+            """"""Just an invalid Interceptor""""""++        with self.assertRaises(ValueError):+            aio.insecure_channel("""", interceptors=[InvalidInterceptor()])++    def test_executed_right_order(self):++        interceptors_executed = []++        class Interceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for testing if the interceptor is being called""""""++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                interceptors_executed.append(self)+                return await continuation(client_call_details, request)++        async def coro():+            interceptors = [Interceptor() for i in range(2)]++            server_target, _ = await start_test_server()  # pylint: disable=unused-variable++            async with aio.insecure_channel(+                    server_target, interceptors=interceptors) as channel:+                multicallable = channel.unary_unary(+                    '/grpc.testing.TestService/UnaryCall',+                    request_serializer=messages_pb2.SimpleRequest.+                    SerializeToString,+                    response_deserializer=messages_pb2.SimpleResponse.FromString+                )+                response = await multicallable(messages_pb2.SimpleRequest())++                # Check that all interceptors were executed, and were executed+                # in the right order.+                self.assertEqual(interceptors_executed, interceptors)++                self.assertEqual(type(response), messages_pb2.SimpleResponse)++        self.loop.run_until_complete(coro())++    @unittest.expectedFailure+    # TODO(https://github.com/grpc/grpc/issues/20144) Once metadata support is+    # implemented in the client-side, this test must be implemented.+    def test_modify_metadata(self):+        raise NotImplementedError()++    @unittest.expectedFailure+    # TODO(https://github.com/grpc/grpc/issues/20532) Once credentials support is+    # implemented in the client-side, this test must be implemented.+    def test_modify_credentials(self):+        raise NotImplementedError()++    def test_status_code_observability(self):++        class StatusCodeObservabilityInterceptor(+                aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for observe the status code returned by the RPC""""""++            def __init__(self):+                self.status_codes_observed = {+                    grpc.StatusCode.OK: 0,+                    grpc.StatusCode.CANCELLED: 0,+                    grpc.StatusCode.DEADLINE_EXCEEDED: 0+                }++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                status_code = grpc.StatusCode.OK+                try:+                    return await continuation(client_call_details, request)+                except aio.AioRpcError as aio_rpc_error:+                    status_code = aio_rpc_error.code()+                    raise+                except asyncio.CancelledError:+                    status_code = grpc.StatusCode.CANCELLED+                    raise+                finally:+                    self.status_codes_observed[status_code] += 1++        async def coro():+            interceptor = StatusCodeObservabilityInterceptor()+            server_target, server = await start_test_server()++            async with aio.insecure_channel(+                    server_target, interceptors=[interceptor]) as channel:++                # when no error StatusCode.OK is observed+                multicallable = channel.unary_unary(+                    '/grpc.testing.TestService/UnaryCall',+                    request_serializer=messages_pb2.SimpleRequest.+                    SerializeToString,+                    response_deserializer=messages_pb2.SimpleResponse.FromString+                )++                await multicallable(messages_pb2.SimpleRequest())++                self.assertEqual(+                    interceptor.status_codes_observed[grpc.StatusCode.OK], 1)+                self.assertEqual(interceptor.status_codes_observed[+                    grpc.StatusCode.CANCELLED], 0)+                self.assertEqual(interceptor.status_codes_observed[+                    grpc.StatusCode.DEADLINE_EXCEEDED], 0)++                # when cancellation StatusCode.CANCELLED is observed+                call = multicallable(messages_pb2.SimpleRequest())",Can this be broken up into multiple test cases? I see at least three independent cases: - happy path - cancelled call - deadline exceededIndependent test cases take less time to run and provide better signal when there's a problem.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21246,349389578,2019-11-22T00:49:34Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,57 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import collections+from typing import Any, Dict, Callable, Optional++import grpc+++class ClientCallDetails(","Lidi and I had a pretty long discussion about this. The use of `namedtuple` means that this object will be immutable, which will help ensure that interceptors do not overwrite values that other interceptors are depending on staying the same. Good stuff. :+1:",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21246,349392144,2019-11-22T01:00:52Z,src/python/grpcio/grpc/experimental/aio/__init__.py,"@@ -37,15 +39,21 @@ def insecure_channel(target, options=None, compression=None):         in gRPC Core runtime) to configure the channel.       compression: An optional value indicating the compression method to be         used over the lifetime of the channel. This is an EXPERIMENTAL option.+      interceptors: An optional list of interceptors that will be executed for",s/list/sequence/ ?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21246,349658675,2019-11-22T15:40:00Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -88,22 +101,77 @@ def __call__(self,          serialized_request = _common.serialize(request,                                                self._request_serializer)-        timeout = self._timeout_to_deadline(timeout)         aio_cancel_status = cygrpc.AioCancelStatus()-        aio_call = asyncio.ensure_future(-            self._channel.unary_unary(self._method, serialized_request, timeout,-                                      aio_cancel_status),-            loop=self._loop)++        if self._interceptors:+            client_call_details = _ClientCallDetails(+                self._method, timeout, metadata, credentials, wait_for_ready,+                compression)+            aio_call = asyncio.ensure_future(+                self._wrap_call_in_interceptors(+                    client_call_details, serialized_request, aio_cancel_status),+                loop=self._loop)+        else:+            aio_call = asyncio.ensure_future(+                self._call(self._method, serialized_request, timeout,+                           aio_cancel_status),+                loop=self._loop)+         return Call(aio_call, self._response_deserializer, aio_cancel_status) +    async def _wrap_call_in_interceptors(+            self, client_call_details: ClientCallDetails, request: bytes,+            aio_cancel_status: cygrpc.AioCancelStatus):+        """"""Run the RPC call wraped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterable[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails, request: bytes):+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                return await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)+            else:+                return await self._call(client_call_details.method, request,+                                        client_call_details.timeout,+                                        aio_cancel_status)","There is something that I'm missing out for sure, sorry for jeopardizing this thread with another question, but it's quite related.**does the gRPC library provide a way for returning the initial metadata, payload and trailing metadata when they are available?** So, not having to wait until the whole RPC has finished [1].This question IMO is fundamental, while in frameworks like `Aiohttp` the headers can be, for example, _awaited_ without having to _await_ for the full response I did not see this in the gRPC library, if it does exist my bad.So considering a **No** for my question - which I might be totally wrong - my idea was that the current payload that is currently returned by the Cython call [1] would be eventually changed for returning more attributes, having something like this:```pythonif receive_status_on_client_operation.code() == StatusCode.ok:    return (receive_message_operation.message(), receive_initial_metadata_operation.initial_metadata() ,....)```So the coroutine published by the Cython code would return not only the response but also the other metadata related to that call, this would mean that the interceptors would receive these other attributes too, most likely by converting this tuple to a kind of `ResponseDetails` object` here [2], so the interceptors would be able to do something like this.```pythonasync def intercept_unary_unary(self, continuation,                                        client_call_details, request):    interceptors_executed.append(self)    response_detail = await continuation(client_call_details, request)    print(response_detail.initial_metadata())    print(response_detail.trailing_metadata())    return response_detail```So interceptors would have always the chance for accessing other attributes.Am I wrong and exists an interface for gathering the metadata and response when they are available independently?PD: In any case, I'm starting to think that the plumbing that is being done between the Python code and the Cython code for canceling an ongoing RPC is not the right one, having the feeling that the way for making it more robust and for the future has the chance for reading partial response data - like initial metadata - we would need to change a bit the plumbing, it would mean removing the current `AioCancelStatus` [3] class and returning the whole `AioCall` object and use it within the `Call` instance for canceling an RPC..[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi#L142[2] https://github.com/grpc/grpc/pull/21246/files#diff-b44bc9a6efb8a6ba5b7e5037b6863b2cR148-R161[3] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/cancel_status.pyx.pxi#L17",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21246,349661774,2019-11-22T15:46:06Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -13,27 +13,46 @@ # limitations under the License. """"""Invocation-side implementation of gRPC Asyncio Python."""""" import asyncio-from typing import Callable, Optional+import functools+from typing import Callable, Optional, List, Sequence, Iterator, Dict, TypeVar  from grpc import _common from grpc._cython import cygrpc+from grpc._interceptor import _ClientCallDetails  from ._call import Call+from ._call import AioRpcError+from ._interceptor import ClientCallDetails+from ._interceptor import UnaryUnaryClientInterceptor  SerializingFunction = Callable[[str], bytes] DeserializingFunction = Callable[[bytes], str]+Request = TypeVar('Request')   class UnaryUnaryMultiCallable:     """"""Afford invoking a unary-unary RPC from client-side in an asynchronous way."""""" -    def __init__(self, channel: cygrpc.AioChannel, method: bytes,-                 request_serializer: SerializingFunction,-                 response_deserializer: DeserializingFunction) -> None:+    _channel: cygrpc.AioChannel",for the `UnaryUnaryMultiCallable` class? does it make sense since it could be only a few instances?,X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21246,349662437,2019-11-22T15:47:28Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,57 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import collections+from typing import Any, Dict, Callable, Optional++import grpc+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: int",My bad :(,
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/20568,349739563,2019-11-22T18:43:04Z,test/cpp/util/spiffe_test_credentials.cc,"@@ -0,0 +1,138 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include ""test/cpp/util/spiffe_test_credentials.h""+#include ""test/core/end2end/data/ssl_test_data.h""++namespace grpc {+namespace testing {++class TestSyncTlsCredentialReload+    : public ::grpc_impl::experimental::TlsCredentialReloadInterface {+  // Sync implementation.+  int Schedule(+      ::grpc_impl::experimental::TlsCredentialReloadArg* arg) override {+    struct ::grpc_impl::experimental::TlsKeyMaterialsConfig::PemKeyCertPair+        pem_key_cert_pair = {test_server1_key, test_server1_cert};+    std::vector<+        struct ::grpc_impl::experimental::TlsKeyMaterialsConfig::PemKeyCertPair>+        pem_key_cert_pair_list = {pem_key_cert_pair};+    arg->set_key_materials(test_root_cert, pem_key_cert_pair_list);+    arg->set_status(GRPC_SSL_CERTIFICATE_CONFIG_RELOAD_NEW);+    return 0;+  }+};++class TestSyncTlsServerAuthorizationCheck+    : public ::grpc_impl::experimental::TlsServerAuthorizationCheckInterface {+  // Sync implementation.+  int Schedule(+      ::grpc_impl::experimental::TlsServerAuthorizationCheckArg* arg) override {+    GPR_ASSERT(arg != nullptr);+    arg->set_success(1);+    arg->set_status(GRPC_STATUS_OK);+    return 0;+  }+};++static void TestAsyncTlsServerAuthorizationCheckCallback(+    ::grpc_impl::experimental::TlsServerAuthorizationCheckArg* arg) {+  GPR_ASSERT(arg != nullptr);+  arg->set_success(1);+  arg->set_status(GRPC_STATUS_OK);+  arg->OnServerAuthorizationCheckDoneCallback();+}++class TestAsyncTlsServerAuthorizationCheck+    : public ::grpc_impl::experimental::TlsServerAuthorizationCheckInterface {+ public:+  TestAsyncTlsServerAuthorizationCheck(SpiffeThreadList* thread_list)+      : thread_list_(thread_list) {}++  // Async implementation.+  int Schedule(+      ::grpc_impl::experimental::TlsServerAuthorizationCheckArg* arg) override {+    GPR_ASSERT(arg != nullptr);+    server_authz_check_thread_ =+        std::thread(TestAsyncTlsServerAuthorizationCheckCallback, arg);+    thread_list_->add_thread(std::move(server_authz_check_thread_));+    return 1;+  }++ private:+  std::thread server_authz_check_thread_;",Can we define it as a function local variable instead of class member?,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21246,349739996,2019-11-22T18:44:10Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -88,22 +101,77 @@ def __call__(self,          serialized_request = _common.serialize(request,                                                self._request_serializer)-        timeout = self._timeout_to_deadline(timeout)         aio_cancel_status = cygrpc.AioCancelStatus()-        aio_call = asyncio.ensure_future(-            self._channel.unary_unary(self._method, serialized_request, timeout,-                                      aio_cancel_status),-            loop=self._loop)++        if self._interceptors:+            client_call_details = _ClientCallDetails(+                self._method, timeout, metadata, credentials, wait_for_ready,+                compression)+            aio_call = asyncio.ensure_future(+                self._wrap_call_in_interceptors(+                    client_call_details, serialized_request, aio_cancel_status),+                loop=self._loop)+        else:+            aio_call = asyncio.ensure_future(+                self._call(self._method, serialized_request, timeout,+                           aio_cancel_status),+                loop=self._loop)+         return Call(aio_call, self._response_deserializer, aio_cancel_status) +    async def _wrap_call_in_interceptors(+            self, client_call_details: ClientCallDetails, request: bytes,+            aio_cancel_status: cygrpc.AioCancelStatus):+        """"""Run the RPC call wraped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterable[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails, request: bytes):+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                return await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)+            else:+                return await self._call(client_call_details.method, request,+                                        client_call_details.timeout,+                                        aio_cancel_status)","> does the gRPC library provide a way for returning the initial metadata, payload and trailing metadata when they are available?Well, as you already familiar with the C-Core API, the operation you mentioning above are done by separated operations. E.g. [receiving initial metadata](https://github.com/grpc/grpc/blob/master/include/grpc/impl/codegen/grpc_types.h#L567), [receiving final status including trailing metadata](https://github.com/grpc/grpc/blob/master/include/grpc/impl/codegen/grpc_types.h#L577). If we are coding for non unary-unary cases, the operations usually are sent in multiple batches (see [code](https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_channel.py#L737)). This usage is the intended usage of the `grpc_call_start_batch` API, which allows the wrapper writer to have fine grained control. One more implementation details about how the initial_metadata is set, see [code](https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_channel.py#L132). Receiving individual payloads or trailing metadata have similar design.From API perspective, the [`_Rendezvous`](https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_channel.py#L351) class has implemented a blocking logic that calls to `initial_metadata`, `code`, etc. will block until the resource is available. In unary case, I agree, they are not particularly useful, but in streaming cases they are quite handy.> Proposal of `response_detail`.Personally, I'm a fan of simplicity. The root of the pain we have today (in this thread, and in gRFC) is the design of the related classes. I totally agree that if we introduce the new concept, the design will be much simpler, just like `requests` and `aiohttp`. For HTTP/1.1, the message and status arrived in the same time. But HTTP/2 is more or less a streaming protocol, so we need to give application a chance to perform business logic between the arrival of initial headers, payloads, and trailing headers.In code, the unary-unary case may looks a bit complex.```Pythonasync def intercept_unary_unary(self, continuation,                                        client_call_details, request):    interceptors_executed.append(self)    call = continuation(client_call_details, request)    print(await call.initial_metadata())    print(await call.trailing_metadata())    return await response_detail```But if we introduce streaming on either side, it will look more nature.```Pythonasync def intercept_unary_stream(self, continuation,                                        client_call_details, request):    interceptors_executed.append(self)    call = continuation(client_call_details, request)    try:        _CONFIGURATE_TRACING(await call.initial_metadata())    except:        call.cancel()        raise    else:        async def response in call:            yield response```> PD: In any case, I'm starting to think that the plumbing that is being done between the Python code and the Cython code for canceling an ongoing RPC is not the right one, having the feeling that the way for making it more robust and for the future has the chance for reading partial response data - like initial metadata - we would need to change a bit the plumbing, it would mean removing the current AioCancelStatus [3] class and returning the whole AioCall object and use it within the Call instance for canceling an RPC..I have similar feeling, and I'm experimenting alternatives in my streaming PR. In short, my current choice is using a future to allow the application communicate with the Cython layer, so the `cancel` can happen in any other coroutine (especially user's), and being handled quickly.",X
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/20568,349755790,2019-11-22T19:22:21Z,src/cpp/common/tls_credentials_options.cc,"@@ -274,6 +294,11 @@ TlsCredentialsOptions::TlsCredentialsOptions(   } } +/** Whenever a TlsCredentialsOptions instance is created, the caller takes+ *  ownership of the c_credentials_options_ pointer (see e.g. the implementation+ *  of the TlsCredentials API in secure_credentials.cc). For this reason, the+ *  TlsCredentialsOptions destructor is not responsible for freeing+ *  c_credentials_options_. **/","Outside of tests, the only callers are the `TlsCredentials` and `TlsServerCredentials` API's that are found in the files credentials.h, credentials_impl.h, server_credentials.h, and server_credentials_impl.h.And yes, I can add this discussion and a list of the callers in the tls_credentials_options.h file where the destructor is exposed.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21246,349878040,2019-11-23T15:57:23Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -88,22 +101,77 @@ def __call__(self,          serialized_request = _common.serialize(request,                                                self._request_serializer)-        timeout = self._timeout_to_deadline(timeout)         aio_cancel_status = cygrpc.AioCancelStatus()-        aio_call = asyncio.ensure_future(-            self._channel.unary_unary(self._method, serialized_request, timeout,-                                      aio_cancel_status),-            loop=self._loop)++        if self._interceptors:+            client_call_details = _ClientCallDetails(+                self._method, timeout, metadata, credentials, wait_for_ready,+                compression)+            aio_call = asyncio.ensure_future(+                self._wrap_call_in_interceptors(+                    client_call_details, serialized_request, aio_cancel_status),+                loop=self._loop)+        else:+            aio_call = asyncio.ensure_future(+                self._call(self._method, serialized_request, timeout,+                           aio_cancel_status),+                loop=self._loop)+         return Call(aio_call, self._response_deserializer, aio_cancel_status) +    async def _wrap_call_in_interceptors(+            self, client_call_details: ClientCallDetails, request: bytes,+            aio_cancel_status: cygrpc.AioCancelStatus):+        """"""Run the RPC call wraped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterable[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails, request: bytes):+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                return await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)+            else:+                return await self._call(client_call_details.method, request,+                                        client_call_details.timeout,+                                        aio_cancel_status)","Ok, now I got the point of this line [1] which I did not understand. So in the synchronous client are created as many tags - in our use case would be different callbacks - as many batch operations and it's up to the driver decide what operations are batched together - I guess that a batch is considered done when all operations that belong to a specific batch are finished.Oks, if this is the case I do think also that the use case of returning everything in one shot is fundamentally broken considering that data could be already there.My next question then would be, **what would need to be plumbed?** So considering that both - the caller and the interceptors - have to have access to different data - initial_metdata, , trailing_metadata, response, others? - that might be received in different moments how they and its respective methods would need to be chained? how is this being solved in other languages like Java, Go or C++? what happeen's with the current Python client?The easy part is the code executed before making the call, as all code interceptors are being executed before making the final call we can say that the following would be what would happen in the real world```caller -- > interceptor 1 (work before the call is done) --> Interceptor N (work before the call is done) --> final call```True that because how is it designed right now, the caller would receive any early exception raised in the code executed before making the call once the caller would be trying to access the response or any of the caller attributes. Take as an example the following code where the caller receives an exception from an interceptor that implements a circuit breaker.```pythoncall = Hi(....)try:   await callexcept CicruitOpen:    logging.warning(""Circuit open for the stub Hi!!!!"")    raise```The `CicruitOpen` exception would be indeed raised in the code executed before making the call, but by design would be raised later.The none easy part is what would happen with the code executed right after the interceptor. There are at least two options. Option 1: Every step needs to wait the finalization of the previous step. So for the following code:```pythonasync def interceptor_2(request, clientDetails, continuation)    call = await(interceptor)    response = await call    return callasync def interceptor_1(request, clientDetails, continuation)    call = await(interceptor)    await call.metadata()    return callasync def caller(..)    call = hi(...)    await call.metadata()```Both the `caller` and the `interceptor_1` though only were accessing the metadata - that could be already there, both would need to wait till the response is already there because the `interceptor_2` blocked everything since this last interceptor was waiting for the final response.Option2, only the `response` is being wired, so `metadata` can be accessed when it's available from any place. In that case, the previous code both - the `caller` and the `interceptor_1`- would have access to the `metadata`.  But one of the likely down-side effects by allowing this would be the chances of having data races in the client-side, for example:```pythonasync def interceptor_1(request, clientDetails, continuation)    call = await(interceptor)    metadata = await call.metadata()    try:        await do_something_async()    except:       call.cancel()       raise    return callasync def caller(..)    call = hi(...)    metadata = await call.metadata()  ```Nothing would guarantee you that the `caller` wouldn't see the metadata for a call that would be eventually canceled by one of the interceptors. Thoughts? Having the feeling that simpler the better, so going for option 1 would need to be the way to go.[1] https://github.com/grpc/grpc/blob/6950e15882f28e43685e948a7e5227bfcef398cd/src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi#L249",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21246,350453840,2019-11-25T22:21:33Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -88,22 +101,77 @@ def __call__(self,          serialized_request = _common.serialize(request,                                                self._request_serializer)-        timeout = self._timeout_to_deadline(timeout)         aio_cancel_status = cygrpc.AioCancelStatus()-        aio_call = asyncio.ensure_future(-            self._channel.unary_unary(self._method, serialized_request, timeout,-                                      aio_cancel_status),-            loop=self._loop)++        if self._interceptors:+            client_call_details = _ClientCallDetails(+                self._method, timeout, metadata, credentials, wait_for_ready,+                compression)+            aio_call = asyncio.ensure_future(+                self._wrap_call_in_interceptors(+                    client_call_details, serialized_request, aio_cancel_status),+                loop=self._loop)+        else:+            aio_call = asyncio.ensure_future(+                self._call(self._method, serialized_request, timeout,+                           aio_cancel_status),+                loop=self._loop)+         return Call(aio_call, self._response_deserializer, aio_cancel_status) +    async def _wrap_call_in_interceptors(+            self, client_call_details: ClientCallDetails, request: bytes,+            aio_cancel_status: cygrpc.AioCancelStatus):+        """"""Run the RPC call wraped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterable[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails, request: bytes):+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                return await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)+            else:+                return await self._call(client_call_details.method, request,+                                        client_call_details.timeout,+                                        aio_cancel_status)","@pfreixes I like the idea of option 2. It's much less rigid and allows certain interceptors to execute and terminate early (e.g. when initial metadata arrives). You bring up a valid point about data races. But it seems this could be fixed with asyncio locking primitives.In your example above, we would take an `asyncio.Lock` in `call.metadata().__await__`. Then, in `interceptor_1` we would hold the lock between when we read the metadata and when we write to it (or cancel).Order becomes more important in this model though, because if you actually want a particular interceptor to do something early, like when initial metadata arrives, it will need to be placed early in the interceptor pipeline. If an interceptor that awaits the whole RPC comes before it, then it will not execute early.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21246,350747000,2019-11-26T13:42:00Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -88,22 +101,77 @@ def __call__(self,          serialized_request = _common.serialize(request,                                                self._request_serializer)-        timeout = self._timeout_to_deadline(timeout)         aio_cancel_status = cygrpc.AioCancelStatus()-        aio_call = asyncio.ensure_future(-            self._channel.unary_unary(self._method, serialized_request, timeout,-                                      aio_cancel_status),-            loop=self._loop)++        if self._interceptors:+            client_call_details = _ClientCallDetails(+                self._method, timeout, metadata, credentials, wait_for_ready,+                compression)+            aio_call = asyncio.ensure_future(+                self._wrap_call_in_interceptors(+                    client_call_details, serialized_request, aio_cancel_status),+                loop=self._loop)+        else:+            aio_call = asyncio.ensure_future(+                self._call(self._method, serialized_request, timeout,+                           aio_cancel_status),+                loop=self._loop)+         return Call(aio_call, self._response_deserializer, aio_cancel_status) +    async def _wrap_call_in_interceptors(+            self, client_call_details: ClientCallDetails, request: bytes,+            aio_cancel_status: cygrpc.AioCancelStatus):+        """"""Run the RPC call wraped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterable[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails, request: bytes):+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                return await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)+            else:+                return await self._call(client_call_details.method, request,+                                        client_call_details.timeout,+                                        aio_cancel_status)","Interceptors are executed in order and are implicitly chained when they are given during the channel instantiation, so I guess that by design order is predictable and there is no way for having a data race between interceptors.So I guess that the only place where the Option 2 would be used will be between the caller and the interceptors, where the caller would have full freedom on reading the metadata once it is available instead of having to wait till all interceptors have finished.Regarding the example of the lock just let me put an argument against this pattern, while the usage of a lock might have sense within the caller code having it between an interceptor and the caller would be weird to me, where most likely both pieces of code would have nothing in common, being maintained by different owners.In any case, I would make the changes without adding any restriction and allowing the client/caller to read the metadata at any moment.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21202,350775836,2019-11-26T14:33:22Z,src/csharp/Grpc.Core/Internal/AsyncCall.cs,"@@ -626,6 +626,14 @@ private void HandleFinished(bool success, ClientSideStatus receivedStatus)             if (status.StatusCode != StatusCode.OK)             {                 streamingResponseCallFinishedTcs.SetException(new RpcException(status, receivedStatus.Trailers));+                if (status.StatusCode == StatusCode.Cancelled)+                {+                    // Make sure the exception set to the Task is observed,+                    // otherwise this can trigger ""Unobserved exception"" when the response stream+                    // is not read until its end and the task created by the TCS is garbage collected.+                    // See https://github.com/grpc/grpc/issues/17458+                    var _ = streamingResponseCallFinishedTcs.Task.Exception;",I think that wouldn't be useful as in most cases this exception actually will be observed in the user code (and we cannot know in advance if it will be).,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20663,350839127,2019-11-26T16:12:32Z,BUILDING.md,"@@ -155,3 +179,16 @@ installed to be able to compile the C/C++ sources. > cmake .. -GNinja -DCMAKE_BUILD_TYPE=Release > cmake --build . ```++If you want to build DLLs, run `cmake` with `-DBUILD_SHARED_LIBS=ON`.++## cmake: Install After Build","The idea was that the section about `make` in README.md points back to BUILDING.md for only for the build instructions, and includes the extra instructions on how to install and I expected we would follow the pattern for cmake as well.I agree with the point that perhaps having part of the build instructions in one document and the other part in another document, but I wanted to make a very clear difference between 1. contributor-only instructions for building and developing gRPC itself2. instructions for including gRPC C++ as a dependency in another project (as a user).Ideally the build part of the instructions for the users should be so simple, that duplicating it in README.md shouldn't be a problem and the link to BUILDING.md  would be there only to troubleshooting purposes or as a way to learn more about the build. WDYT?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20663,350842033,2019-11-26T16:16:29Z,BUILDING.md,"@@ -130,6 +130,28 @@ From the grpc repository root $ bazel build :all ``` +## cmake: Linux, Using Make+Run from grpc directory after cloning the repo with --recursive or updating submodules.+```+$ mkdir build+$ cd build+$ cmake ..+$ make+```++If you want to build shared libraries (`.so` files), run `cmake` with `-DBUILD_SHARED_LIBS=ON`.++## cmake: Linux, Using Ninja (faster build)","Can we move adding the linux ninja instructions to a separate pull request? I'm happy to consider it, but it feels it doesn't belong to this PR (the goal here is to try improve the quality of the existing instructions rather than adding even more ways to build)",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21270,350975247,2019-11-26T20:55:09Z,tools/internal_ci/helper_scripts/prepare_build_macos_rc,"@@ -54,9 +53,14 @@ time pip install --user virtualenv time pip install --user --upgrade Mako six tox setuptools twisted pyyaml pyjwt cryptography requests export PYTHONPATH=/Library/Python/3.4/site-packages -# Install Python 3.7+# Install Python 3.7 and Python 3.8 time curl -O https://www.python.org/ftp/python/3.7.0/python-3.7.0-macosx10.9.pkg+time curl -O https://www.python.org/ftp/python/3.8.0/python-3.8.0-macosx10.9.pkg+echo ""ae0717a02efea3b0eb34aadc680dc498 python-3.7.0-macosx10.9.pkg"" > /tmp/python_installer_checksum.md5+echo ""f5f9ae9f416170c6355cab7256bb75b5 python-3.8.0-macosx10.9.pkg"" >> /tmp/python_installer_checksum.md5","I suppose this would have already bitten us if this were a problem, but are we in control of the version of Mac OS that's being used here? If so, this is sort of fragile. If not, this version may be *quite* old. In fact, the operating system isn't even called ""OS X"" anymore.This version looks like it's from 2013. We should probably add a TODO and file an issue for updating this. At some point, this version will probably no longer be adequate.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/21202,350975816,2019-11-26T20:56:28Z,src/csharp/Grpc.Core/Internal/AsyncCall.cs,"@@ -626,6 +626,14 @@ private void HandleFinished(bool success, ClientSideStatus receivedStatus)             if (status.StatusCode != StatusCode.OK)             {                 streamingResponseCallFinishedTcs.SetException(new RpcException(status, receivedStatus.Trailers));+                if (status.StatusCode == StatusCode.Cancelled)","Can we make that check, for if cancellation has been requested (or another equivalent check that indicates e.g. if `Dispose` has been called)?Without that, my concern is that the issue described in https://github.com/grpc/grpc/issues/17458#issuecomment-552844393 can still happen, just more rarely (e.g. if the call has a deadline and it just happens to go off right around the time that we `Dispose` the call, causing the same type of problem but changing status to ""deadline exceeded"").",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21271,350977245,2019-11-26T20:59:55Z,tools/internal_ci/helper_scripts/install_python38.ps1,"@@ -0,0 +1,67 @@+#!/usr/bin/env powershell+# Install Python 3.8 for x64 and x86 in order to build wheels on Windows.++Set-StrictMode -Version 2++# Avoid ""Could not create SSL/TLS secure channel""+[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12++function Install-Python {+    Param(+        [string]$PythonVersion,+        [string]$PythonInstaller,+        [string]$PythonInstallPath,+        [string]$PythonInstallerHash+    )+    $PythonInstallerUrl = ""https://www.python.org/ftp/python/$PythonVersion/$PythonInstaller""+    $PythonInstallerPath = ""C:\tools\$PythonInstaller""++    # Downloads installer+    Write-Host ""Downloading the Python installer: $PythonInstallerUrl => $PythonInstallerPath""+    Invoke-WebRequest -Uri $PythonInstallerUrl -OutFile $PythonInstallerPath++    # Validates checksum+    $HashFromDownload = Get-FileHash -Path $PythonInstallerPath -Algorithm MD5+    if ($HashFromDownload.Hash -ne $PythonInstallerHash) {+        throw ""Invalid Python installer: failed checksum!""+    }+    Write-Host ""Python installer $PythonInstallerPath validated.""++    # Installs Python+    & $PythonInstallerPath /passive InstallAllUsers=1 PrependPath=1 Include_test=0 TargetDir=$PythonInstallPath+    if (-Not $?) {+        throw ""The Python installation exited with error!""+    }++    # Validates Python",We probably want a longer explanatory comment here considering that this is a hack.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21271,350997719,2019-11-26T21:50:21Z,tools/internal_ci/helper_scripts/install_python38.ps1,"@@ -0,0 +1,67 @@+#!/usr/bin/env powershell+# Install Python 3.8 for x64 and x86 in order to build wheels on Windows.++Set-StrictMode -Version 2++# Avoid ""Could not create SSL/TLS secure channel""+[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12++function Install-Python {+    Param(+        [string]$PythonVersion,+        [string]$PythonInstaller,+        [string]$PythonInstallPath,+        [string]$PythonInstallerHash+    )+    $PythonInstallerUrl = ""https://www.python.org/ftp/python/$PythonVersion/$PythonInstaller""+    $PythonInstallerPath = ""C:\tools\$PythonInstaller""++    # Downloads installer+    Write-Host ""Downloading the Python installer: $PythonInstallerUrl => $PythonInstallerPath""+    Invoke-WebRequest -Uri $PythonInstallerUrl -OutFile $PythonInstallerPath++    # Validates checksum+    $HashFromDownload = Get-FileHash -Path $PythonInstallerPath -Algorithm MD5+    if ($HashFromDownload.Hash -ne $PythonInstallerHash) {+        throw ""Invalid Python installer: failed checksum!""+    }+    Write-Host ""Python installer $PythonInstallerPath validated.""++    # Installs Python+    & $PythonInstallerPath /passive InstallAllUsers=1 PrependPath=1 Include_test=0 TargetDir=$PythonInstallPath+    if (-Not $?) {+        throw ""The Python installation exited with error!""+    }++    # Validates Python+    $PythonBinary = ""$PythonInstallPath\python.exe""+    while ($true) {","The 5 minutes deadline added.I was concerning that even if we poison this script, we can't stop the execution of the rest of the CI build. Admittedly, adding deadline is better.",
7281574,nicolasnoble,https://api.github.com/repos/grpc/grpc/pulls/21278,351445766,2019-11-27T18:42:45Z,src/core/ext/filters/client_channel/parse_address.cc,"@@ -56,7 +58,20 @@ bool grpc_parse_unix(const grpc_uri* uri,   if (path_len == maxlen) return false;   un->sun_family = AF_UNIX;   strcpy(un->sun_path, uri->path);+#ifdef GRPC_HAVE_ABSTRACT_UNIX_SOCKET","I would avoid that pattern of `#ifdef` completely. This codepath should be no adverse effect on platforms that do not support abstract unix domain sockets, aside from the fact it simply won't work if passing something with an '@' in the name, which wouldn't change anything from the other codepath.",X
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21316,351500266,2019-11-27T21:14:37Z,examples/objective-c/route_guide/ViewControllers.m,"@@ -244,46 +238,46 @@ - (void)execRequest {     return;   } -  GRPCStreamingProtoCall *call = [_service recordRouteWithResponseHandler:self-                                                              callOptions:nil];+  void (^handler)(RTGRouteSummary *response, NSError *error) =+      ^(RTGRouteSummary *response, NSError *error) {+        if (response) {+          NSString *str = [NSString+              stringWithFormat:@""%@\nFinished trip with %i points\nPassed %i features\n""+                                ""Travelled %i meters\nIt took %i seconds"",+                               self.outputLabel.text, response.pointCount, response.featureCount,+                               response.distance, response.elapsedTime];+          self.outputLabel.text = str;+          NSLog(@""Finished trip with %i points"", response.pointCount);+          NSLog(@""Passed %i features"", response.featureCount);+          NSLog(@""Travelled %i meters"", response.distance);+          NSLog(@""It took %i seconds"", response.elapsedTime);+        } else {+          NSString *str =+              [NSString stringWithFormat:@""%@\nRPC error: %@"", self.outputLabel.text, error];+          self.outputLabel.text = str;+          NSLog(@""RPC error: %@"", error);+        }+      };++  GRPCStreamingProtoCall *call =+      [_service recordRouteWithResponseHandler:[[GRPCUnaryResponseHandler alloc]",The name of the class is confusing. It's request streaming but response is unary.,X
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21316,351506311,2019-11-27T21:33:50Z,examples/objective-c/route_guide/ViewControllers.m,"@@ -244,46 +238,46 @@ - (void)execRequest {     return;   } -  GRPCStreamingProtoCall *call = [_service recordRouteWithResponseHandler:self-                                                              callOptions:nil];+  void (^handler)(RTGRouteSummary *response, NSError *error) =+      ^(RTGRouteSummary *response, NSError *error) {+        if (response) {+          NSString *str = [NSString+              stringWithFormat:@""%@\nFinished trip with %i points\nPassed %i features\n""+                                ""Travelled %i meters\nIt took %i seconds"",+                               self.outputLabel.text, response.pointCount, response.featureCount,+                               response.distance, response.elapsedTime];+          self.outputLabel.text = str;+          NSLog(@""Finished trip with %i points"", response.pointCount);+          NSLog(@""Passed %i features"", response.featureCount);+          NSLog(@""Travelled %i meters"", response.distance);+          NSLog(@""It took %i seconds"", response.elapsedTime);+        } else {+          NSString *str =+              [NSString stringWithFormat:@""%@\nRPC error: %@"", self.outputLabel.text, error];+          self.outputLabel.text = str;+          NSLog(@""RPC error: %@"", error);+        }+      };++  GRPCStreamingProtoCall *call =+      [_service recordRouteWithResponseHandler:[[GRPCUnaryResponseHandler alloc]",I don't think we can change the name of the class since it's already published. I'll make additional documentation to clarify it.,
5279114,ZHmao,https://api.github.com/repos/grpc/grpc/pulls/21336,352562114,2019-12-02T12:10:40Z,src/python/grpcio_tests/tests_aio/unit/_test_server.py,"@@ -19,10 +19,27 @@ from src.proto.grpc.testing import test_pb2_grpc from tests.unit.framework.common import test_constants +_INITIAL_METADATA_KEY = ""initial-md-key""+_TRAILING_METADATA_KEY = ""trailing-md-key-bin""+++def _maybe_echo_metadata(servicer_context):+    """"""Copies metadata from request to response if it is present.""""""+    invocation_metadata = dict(servicer_context.invocation_metadata())+    if _INITIAL_METADATA_KEY in invocation_metadata:+        initial_metadatum = (_INITIAL_METADATA_KEY,+                             invocation_metadata[_INITIAL_METADATA_KEY])+        servicer_context.send_initial_metadata((initial_metadatum,))+    if _TRAILING_METADATA_KEY in invocation_metadata:+        trailing_metadatum = (_TRAILING_METADATA_KEY,+                              invocation_metadata[_TRAILING_METADATA_KEY])+        servicer_context.set_trailing_metadata((trailing_metadatum,))+  class _TestServiceServicer(test_pb2_grpc.TestServiceServicer):      async def UnaryCall(self, request, context):+        # _maybe_echo_metadata(context)","I think we can not get metadata from here until server-side implement it, the `context` is a placeholder[1] and server-side just send fake metadata[2] now, so I comment this line.[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi#L78[2] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi#L88-L94",
224720,zackgalbreath,https://api.github.com/repos/grpc/grpc/pulls/21329,352609613,2019-12-02T14:00:57Z,BUILDING.md,"@@ -155,3 +155,30 @@ installed to be able to compile the C/C++ sources. > cmake .. -GNinja -DCMAKE_BUILD_TYPE=Release > cmake --build . ```++## Building with make (on UNIX systems)++NOTE: `make` used to be gRPC's default build system, but we're no longer recommending it. You should use `bazel` or `cmake` instead.","```suggestionNOTE: `make` used to be gRPC's default build system, but we're no longer recommending it. You should use `bazel` or `CMake` instead.```",
224720,zackgalbreath,https://api.github.com/repos/grpc/grpc/pulls/21329,352611222,2019-12-02T14:04:04Z,src/cpp/README.md,"@@ -30,11 +30,23 @@ To add gRPC as a dependency in bazel:   grpc_deps()   ``` -NOTE: currently bazel is only supported for building gRPC on Linux.+## cmake++`cmake` is your best option if you cannot use bazel. It supports building on Linux, MacOS and Windows (official support) but also has a good chance of working on other platforms (no promises!). `cmake` has good+support for crosscompiling and can be used for targeting Android platform.++If your project is using cmake, there are several ways to add gRPC dependency.","```suggestionIf your project is using CMake, there are several ways to add gRPC dependency.```",
224720,zackgalbreath,https://api.github.com/repos/grpc/grpc/pulls/21329,352612581,2019-12-02T14:06:50Z,src/cpp/README.md,"@@ -30,11 +30,23 @@ To add gRPC as a dependency in bazel:   grpc_deps()   ``` -NOTE: currently bazel is only supported for building gRPC on Linux.+## cmake++`cmake` is your best option if you cannot use bazel. It supports building on Linux, MacOS and Windows (official support) but also has a good chance of working on other platforms (no promises!). `cmake` has good+support for crosscompiling and can be used for targeting Android platform.++If your project is using cmake, there are several ways to add gRPC dependency.","I'd like to split this section up into ""depending on gRPC"" (find_package, always) and building gRPC as a dependency (superbuild, git submodule, etc). But IMO that change can wait for a subsequent PR.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21336,352685677,2019-12-02T16:12:39Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -114,6 +125,22 @@ def test_call_to_the_void(self):          self.loop.run_until_complete(coro()) +    def test_unary_unary_metadata(self):++        async def coro():+            server_target, _ = await start_test_server()  # pylint: disable=unused-variable++            async with aio.insecure_channel(server_target) as channel:+                hi = channel.unary_unary(+                    _UNARY_CALL_METHOD,+                    request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                    response_deserializer=messages_pb2.SimpleResponse.FromString)+                call = hi(messages_pb2.SimpleRequest(), metadata=_INVOCATION_METADATA)+                self.assertIsNotNone(await call.initial_metadata())+                self.assertIsNotNone(await call.trailing_metadata())","Correct me if I'm wrong, but we are basically testing the default value here? because the `_INVOCATION_METADATA` is not finally wired in the server-side since `_maybe_echo_metadata` does not work, amb I wrong?If so, maybe for having a reliable coverage we would need to implement something that allows the code to test that the metadata is received. For that - one solution that does not require the context- is create another gRPC endpoint, for example, `MetadataEcho` which would go through to all of hte metadata and will make an echo of the metadata found as metadata.Besides that, and in the same way as being done in the `cal_test.py` for the details, I would implement a couple of tests for testing that `initial_metadata` and `trailing_metadata` are _awaitable_, in these tests we could double-check that the value returned is the default value when there is no metada returned by the user.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21329,352739602,2019-12-02T17:51:41Z,BUILDING.md,"@@ -155,3 +155,30 @@ installed to be able to compile the C/C++ sources. > cmake .. -GNinja -DCMAKE_BUILD_TYPE=Release > cmake --build . ```++## Building with make (on UNIX systems)++NOTE: `make` used to be gRPC's default build system, but we're no longer recommending it. You should use `bazel` or `cmake` instead.+While still supported, make should only be used inside the gRPC repository (to satisfy some project-internal needs)","""inside the gRPC repository"" is maybe a little ambiguous. Perhaps ""The makefile is only intended for internal usage and is not meant for public consumption."".",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21336,352746181,2019-12-02T18:05:46Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -182,11 +183,23 @@ def done(self) -> bool:         """"""         return self._state is not _RpcState.ONGOING -    async def initial_metadata(self):-        raise NotImplementedError()+    async def initial_metadata(self) -> Sequence[Tuple[Text, AnyStr]]:","Returning a plain old `Dict` would lose the order of the metadata entries, which we currently preserve with `Sequence[Tuple[Text, AnyStr]]`. Perhaps [`OrderedDict`](https://docs.python.org/2/library/collections.html#ordereddict-objects) instead?",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21336,352794725,2019-12-02T19:50:08Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -139,7 +145,12 @@ cdef class _AioCall:             self._destroy_grpc_call()          if receive_status_on_client_operation.code() == StatusCode.ok:-            return receive_message_operation.message()+            return UnaryUnaryOpsResult(+                initial_metadata=receive_initial_metadata_operation.initial_metadata(),+                message=receive_message_operation.message(),+                code=receive_status_on_client_operation.code(),+                details=receive_status_on_client_operation.details(),+                trailing_metadata=receive_status_on_client_operation.trailing_metadata())","In the PR for implementing the interceptors Im gonna segregate theseattributed as attributes belonging to the aio call object. Just forunblocking the work here, if you agree, I would consider this namedtuple asa temporary thing.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21336,352799787,2019-12-02T20:00:43Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -139,7 +145,12 @@ cdef class _AioCall:             self._destroy_grpc_call()          if receive_status_on_client_operation.code() == StatusCode.ok:-            return receive_message_operation.message()+            return UnaryUnaryOpsResult(+                initial_metadata=receive_initial_metadata_operation.initial_metadata(),+                message=receive_message_operation.message(),+                code=receive_status_on_client_operation.code(),+                details=receive_status_on_client_operation.details(),+                trailing_metadata=receive_status_on_client_operation.trailing_metadata())","Personally, if this PR is blocking your work in interceptors, I would say they are probably better check in together? The amount of code is not huge, should be fine merging into what you have in your refactor version.This temporary design here is on critical data path. My intention is that if you are sensing that this needs to be refactored soon, why not perform the refactor all together?",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21336,352806181,2019-12-02T20:14:38Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -139,7 +145,12 @@ cdef class _AioCall:             self._destroy_grpc_call()          if receive_status_on_client_operation.code() == StatusCode.ok:-            return receive_message_operation.message()+            return UnaryUnaryOpsResult(+                initial_metadata=receive_initial_metadata_operation.initial_metadata(),+                message=receive_message_operation.message(),+                code=receive_status_on_client_operation.code(),+                details=receive_status_on_client_operation.details(),+                trailing_metadata=receive_status_on_client_operation.trailing_metadata())","It's not really blocking me but if it gets merged first than mine the `namedtuple` would be removed.But true that the number of changes is not so big, If everybody agrees I can ask @ZHmao for opening the PR against my branch and deliver later a PR which will address both things, hte interceptor and support for metadata.Do you kinda agree @ZHmao? If so, could you open the PR against this branch [1] and close this?[1] https://github.com/Skyscanner/grpc/tree/client_unaryunary_interceptors",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21340,353157319,2019-12-03T12:46:16Z,tools/internal_ci/helper_scripts/prepare_build_windows.bat,"@@ -39,6 +39,9 @@ set PATH=%LOCALAPPDATA%\Microsoft\dotnet;%PATH% @rem TODO(jtattermusch): try to eliminate the dependency on Go choco install golang -y --version 1.13.1 --limit-output +@rem Install Python 3.8.0+powershell -File tools\internal_ci\helper_scripts\install_python38.ps1","if the order of installing golang and python38 matters (it sounds like it does), we have to add a comment. Otherwise in the future someone will add something after python installation sooner or later and will run into exactly the same problem.Thinking forward, a while ago I've split the kokoro jobs into language specific jobs so that with some extra work the build scripts become a bit more selfcontained - the python jobs could only install python dependencies and C# jobs could only install C# dependencies. If we figure out a good way of doing that, it could make our build more stable by installing fewer dependencies and reducing the chance that one languages will be broken due to dependency changes introduced by another language.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21202,353233449,2019-12-03T15:08:20Z,src/csharp/Grpc.Core/Internal/AsyncCall.cs,"@@ -626,6 +626,14 @@ private void HandleFinished(bool success, ClientSideStatus receivedStatus)             if (status.StatusCode != StatusCode.OK)             {                 streamingResponseCallFinishedTcs.SetException(new RpcException(status, receivedStatus.Trailers));+                if (status.StatusCode == StatusCode.Cancelled)","To clarify:`call.Dispose()` for `Async*Call` objects exposed publicly actually calls ""Cancel"" under the hood, which should address your concern.https://github.com/grpc/grpc/blob/5817f6287d758ead304668ea6e7819fd9a4c3dcb/src/csharp/Grpc.Core.Api/AsyncServerStreamingCall.cs#L109And as you can see, the dispose action is `asyncCall.Cancel`.https://github.com/grpc/grpc/blob/5817f6287d758ead304668ea6e7819fd9a4c3dcb/src/csharp/Grpc.Core/Calls.cs#L119",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21120,353267334,2019-12-03T16:01:15Z,src/csharp/Grpc.HealthCheck/HealthServiceImpl.cs,"@@ -86,17 +123,147 @@ public void ClearAll()         /// <returns>The asynchronous response.</returns>         public override Task<HealthCheckResponse> Check(HealthCheckRequest request, ServerCallContext context)         {-            lock (myLock)+            HealthCheckResponse response = GetHealthCheckResponse(request.Service, throwOnNotFound: true);++            return Task.FromResult(response);+        }++#if GRPC_SUPPORT_WATCH+        /// <summary>+        /// Performs a watch for the serving status of the requested service.+        /// The server will immediately send back a message indicating the current+        /// serving status.  It will then subsequently send a new message whenever+        /// the service's serving status changes.+        ///+        /// If the requested service is unknown when the call is received, the+        /// server will send a message setting the serving status to+        /// SERVICE_UNKNOWN but will *not* terminate the call.  If at some+        /// future point, the serving status of the service becomes known, the+        /// server will send a new message with the service's serving status.+        ///+        /// If the call terminates with status UNIMPLEMENTED, then clients+        /// should assume this method is not supported and should not retry the+        /// call.  If the call terminates with any other status (including OK),+        /// clients should retry the call with appropriate exponential backoff.+        /// </summary>+        /// <param name=""request"">The request received from the client.</param>+        /// <param name=""responseStream"">Used for sending responses back to the client.</param>+        /// <param name=""context"">The context of the server-side call handler being invoked.</param>+        /// <returns>A task indicating completion of the handler.</returns>+        public override async Task Watch(HealthCheckRequest request, IServerStreamWriter<HealthCheckResponse> responseStream, ServerCallContext context)+        {+            string service = request.Service;++            HealthCheckResponse response = GetHealthCheckResponse(service, throwOnNotFound: false);+            await responseStream.WriteAsync(response);++            // Channel is used to to marshall multiple callers updating status into a single queue.+            // This is required because IServerStreamWriter is not thread safe.+            // The channel will buffer up to XXX messages, after which it will drop the oldest messages.+            Channel<HealthCheckResponse> channel = Channel.CreateBounded<HealthCheckResponse>(new BoundedChannelOptions(capacity: 5) {","add a comment what's the semantics of ""dropping the oldest messages"" in terms of the health check protocol.What happens if the service status is changing more quickly than the Watch method can send them out?It looks like the intermediate states will be forgotten and it will try to send out the most recent statuses instead?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21120,353268002,2019-12-03T16:02:20Z,src/csharp/Grpc.HealthCheck/HealthServiceImpl.cs,"@@ -86,17 +123,147 @@ public void ClearAll()         /// <returns>The asynchronous response.</returns>         public override Task<HealthCheckResponse> Check(HealthCheckRequest request, ServerCallContext context)         {-            lock (myLock)+            HealthCheckResponse response = GetHealthCheckResponse(request.Service, throwOnNotFound: true);++            return Task.FromResult(response);+        }++#if GRPC_SUPPORT_WATCH+        /// <summary>+        /// Performs a watch for the serving status of the requested service.+        /// The server will immediately send back a message indicating the current+        /// serving status.  It will then subsequently send a new message whenever+        /// the service's serving status changes.+        ///+        /// If the requested service is unknown when the call is received, the+        /// server will send a message setting the serving status to+        /// SERVICE_UNKNOWN but will *not* terminate the call.  If at some+        /// future point, the serving status of the service becomes known, the+        /// server will send a new message with the service's serving status.+        ///+        /// If the call terminates with status UNIMPLEMENTED, then clients+        /// should assume this method is not supported and should not retry the+        /// call.  If the call terminates with any other status (including OK),+        /// clients should retry the call with appropriate exponential backoff.+        /// </summary>+        /// <param name=""request"">The request received from the client.</param>+        /// <param name=""responseStream"">Used for sending responses back to the client.</param>+        /// <param name=""context"">The context of the server-side call handler being invoked.</param>+        /// <returns>A task indicating completion of the handler.</returns>+        public override async Task Watch(HealthCheckRequest request, IServerStreamWriter<HealthCheckResponse> responseStream, ServerCallContext context)+        {+            string service = request.Service;++            HealthCheckResponse response = GetHealthCheckResponse(service, throwOnNotFound: false);+            await responseStream.WriteAsync(response);++            // Channel is used to to marshall multiple callers updating status into a single queue.+            // This is required because IServerStreamWriter is not thread safe.+            // The channel will buffer up to XXX messages, after which it will drop the oldest messages.+            Channel<HealthCheckResponse> channel = Channel.CreateBounded<HealthCheckResponse>(new BoundedChannelOptions(capacity: 5) {",nit: declare a const  with the default capacity and add a comment on what the semantics is.,
28123841,guantaol,https://api.github.com/repos/grpc/grpc/pulls/21216,353496954,2019-12-04T00:37:51Z,src/core/lib/iomgr/poller/eventmanager_libuv.cc,"@@ -0,0 +1,77 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include ""src/core/lib/iomgr/poller/eventmanager_libuv.h""++#include <grpc/support/time.h>++grpc::experimental::LibuvEventManager::Options::Options() : num_workers_(-1) {}+grpc::experimental::LibuvEventManager::Options::Options(int num_workers)+    : num_workers_(num_workers) {}++grpc::experimental::LibuvEventManager::LibuvEventManager(const Options& options)+    : options_(options), should_stop_(0), shutdown_refcount_(0) {+  int num_workers = options_.num_workers();+  // TODO(guantaol): replaces the hard-coded number with a flag.+  if (num_workers < 0) num_workers = 32;++  for (int i = 0; i < num_workers; i++) {+    workers_.emplace_back(options_.thread_name_prefix().c_str(),+                          &grpc::experimental::LibuvEventManager::RunWorkerLoop,+                          this);+    workers_.back().Start();+  }+}++grpc::experimental::LibuvEventManager::~LibuvEventManager() {+  Shutdown();+  for (auto it = workers_.begin(); it != workers_.end(); it++) {+    it->Join();+  }+}++void grpc::experimental::LibuvEventManager::RunWorkerLoop(void* manager) {+  LibuvEventManager* event_manager = static_cast<LibuvEventManager*>(manager);+  while (true) {+    if (event_manager->ShouldStop()) return;+    gpr_sleep_until(gpr_time_add(gpr_now(GPR_CLOCK_MONOTONIC),+                                 gpr_time_from_micros(10, GPR_TIMESPAN)));+  }+}++bool grpc::experimental::LibuvEventManager::ShouldStop() {+  return should_stop_.Load(grpc_core::MemoryOrder::ACQUIRE) != 0;+}++void grpc::experimental::LibuvEventManager::Shutdown() {+  if (should_stop_.Load(grpc_core::MemoryOrder::ACQUIRE))+    return;  // Already shut down.+  while (shutdown_refcount_.Load(grpc_core::MemoryOrder::ACQUIRE) > 0)","Sorry that my knowledge about fiber is minimal, but why will this never complete with fibers? Won't the two fibers run in parallel?",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/20530,353523008,2019-12-04T02:33:37Z,src/core/tsi/ssl_transport_security.cc,"@@ -1048,9 +1048,11 @@ static tsi_result ssl_handshaker_result_extract_peer(                                    &alpn_selected_len);   } +  STACK_OF(X509) *peer_chain = SSL_get0_verified_chain(impl->ssl);",Do we still get an unverified certificate chain if client or server does not validate the peer's certificate during the handshake?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21232,353767568,2019-12-04T14:16:23Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/socket.pyx.pxi,"@@ -125,23 +125,34 @@ cdef class _AsyncioSocket:     cdef void read(self, char * buffer_, size_t length, grpc_custom_read_callback grpc_read_cb):         assert not self._task_read -        self._task_read = asyncio.ensure_future(+        self._task_read = self._loop.create_task(             self._reader.read(n=length)         )         self._grpc_read_cb = grpc_read_cb         self._task_read.add_done_callback(self._read_cb)         self._read_buffer = buffer_++    async def _async_write(self, bytearray buffer):+        self._writer.write(buffer)+        await self._writer.drain()++        self._grpc_write_cb(+            <grpc_custom_socket*>self._grpc_socket,+            <grpc_error*>0+        )       cdef void write(self, grpc_slice_buffer * g_slice_buffer, grpc_custom_write_callback grpc_write_cb):+        # For each socket, C-Core guarantees there'll be only one ongoing write+        self._grpc_write_cb = grpc_write_cb","is the class instance attribute `self._grpc_write_cb` used in another place? If no, I do not see the need of having to save it every time that the `write` is being called. ",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21232,353777316,2019-12-04T14:33:26Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,335 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Virtual cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation_future, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation_future.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:+        pass -    async def initial_metadata(self):-        raise NotImplementedError()+    def is_active(self) -> bool:+        return self.done() -    async def trailing_metadata(self):-        raise NotImplementedError()+    def time_remaining(self) -> float:+        pass -    async def code(self) -> grpc.StatusCode:-        """"""Returns the `grpc.StatusCode` if the RPC is finished,-        otherwise first waits until the RPC finishes.+    async def initial_metadata(self) -> MetadataType:+        return await self._initial_metadata -        Returns:-          The `grpc.StatusCode` status code.-        """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+    async def trailing_metadata(self) -> MetadataType:+        return (await self._status).trailing_metadata() +    async def code(self) -> grpc.StatusCode:+        await self._status         return self._code      async def details(self) -> str:-        """"""Returns the details if the RPC is finished, otherwise first waits till the-        RPC finishes.+        return (await self._status).details() -        Returns:-          The details.+    async def debug_error_string(self) -> str:+        return (await self._status).debug_error_string()++    def _set_initial_metadata(self, metadata: MetadataType) -> None:+        self._initial_metadata.set_result(metadata)++    def _set_status(self, status: cygrpc.AioRpcStatus) -> None:+        """"""Private method to set final status of the RPC.++        This method may be called multiple time due to data race between local+        cancellation (by application) and C-Core receiving status from peer. We+        make no promise here which one will win.         """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+        if self._status.done():+            return+        else:+            self._status.set_result(status)+            self._code = _common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[+                status.code()]++    async def _raise_rpc_error_if_not_ok(self) -> None:+        if self._code != grpc.StatusCode.OK:+            raise _create_rpc_error(await self.initial_metadata(),+                                    self._status.result())++    def _repr(self) -> str:+        """"""Assembles the RPC representation string.""""""+        if not self._status.done():+            return '<{} object>'.format(self.__class__.__name__)+        if self._code is grpc.StatusCode.OK:+            return _OK_CALL_REPRESENTATION.format(+                self.__class__.__name__, self._code,+                self._status.result().self._status.result().details())+        else:+            return _NON_OK_CALL_REPRESENTATION.format(+                self.__class__.__name__, self._code,+                self._status.result().details(),+                self._status.result().debug_error_string())++    def __repr__(self) -> str:+        return self._repr()++    def __str__(self) -> str:+        return self._repr()+++class UnaryUnaryCall(Call, _base_call.UnaryUnaryCall):+    """"""Object for managing unary-unary RPC calls.++    Returned when an instance of `UnaryUnaryMultiCallable` object is called.+    """"""+    _loop: asyncio.AbstractEventLoop+    _request: RequestType+    _deadline: Optional[float]+    _channel: cygrpc.AioChannel+    _method: bytes+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction+    _call: asyncio.Task -        return self._details+    def __init__(self, request: RequestType, deadline: Optional[float],+                 channel: cygrpc.AioChannel, method: bytes,+                 request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__()+        self._loop = asyncio.get_event_loop()",`loop` is already part of the base class `Call`.,X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/21216,353783176,2019-12-04T14:43:15Z,src/core/lib/iomgr/poller/eventmanager_libuv.cc,"@@ -0,0 +1,77 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include ""src/core/lib/iomgr/poller/eventmanager_libuv.h""++#include <grpc/support/time.h>++grpc::experimental::LibuvEventManager::Options::Options() : num_workers_(-1) {}+grpc::experimental::LibuvEventManager::Options::Options(int num_workers)+    : num_workers_(num_workers) {}++grpc::experimental::LibuvEventManager::LibuvEventManager(const Options& options)+    : options_(options), should_stop_(0), shutdown_refcount_(0) {+  int num_workers = options_.num_workers();+  // TODO(guantaol): replaces the hard-coded number with a flag.+  if (num_workers < 0) num_workers = 32;++  for (int i = 0; i < num_workers; i++) {+    workers_.emplace_back(options_.thread_name_prefix().c_str(),+                          &grpc::experimental::LibuvEventManager::RunWorkerLoop,+                          this);+    workers_.back().Start();+  }+}++grpc::experimental::LibuvEventManager::~LibuvEventManager() {+  Shutdown();+  for (auto it = workers_.begin(); it != workers_.end(); it++) {+    it->Join();+  }+}++void grpc::experimental::LibuvEventManager::RunWorkerLoop(void* manager) {+  LibuvEventManager* event_manager = static_cast<LibuvEventManager*>(manager);+  while (true) {+    if (event_manager->ShouldStop()) return;+    gpr_sleep_until(gpr_time_add(gpr_now(GPR_CLOCK_MONOTONIC),+                                 gpr_time_from_micros(10, GPR_TIMESPAN)));+  }+}++bool grpc::experimental::LibuvEventManager::ShouldStop() {+  return should_stop_.Load(grpc_core::MemoryOrder::ACQUIRE) != 0;+}++void grpc::experimental::LibuvEventManager::Shutdown() {+  if (should_stop_.Load(grpc_core::MemoryOrder::ACQUIRE))+    return;  // Already shut down.+  while (shutdown_refcount_.Load(grpc_core::MemoryOrder::ACQUIRE) > 0)","Fibers use coöperative multithreading and they only yield when they perform a blocking operation (syscall, mutex, etc), so there's no guarantee that they run in parallel. Technically, even in standard C++ threads, there is no such guarantee of forward progress (somewhere deep in the heart of the spec). This problem is what forced us to disable the `channel_idle` filter, btw.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21232,353793366,2019-12-04T14:59:56Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,335 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Virtual cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation_future, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation_future.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:+        pass -    async def initial_metadata(self):-        raise NotImplementedError()+    def is_active(self) -> bool:+        return self.done() -    async def trailing_metadata(self):-        raise NotImplementedError()+    def time_remaining(self) -> float:+        pass -    async def code(self) -> grpc.StatusCode:-        """"""Returns the `grpc.StatusCode` if the RPC is finished,-        otherwise first waits until the RPC finishes.+    async def initial_metadata(self) -> MetadataType:+        return await self._initial_metadata -        Returns:-          The `grpc.StatusCode` status code.-        """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+    async def trailing_metadata(self) -> MetadataType:+        return (await self._status).trailing_metadata() +    async def code(self) -> grpc.StatusCode:+        await self._status         return self._code      async def details(self) -> str:-        """"""Returns the details if the RPC is finished, otherwise first waits till the-        RPC finishes.+        return (await self._status).details() -        Returns:-          The details.+    async def debug_error_string(self) -> str:+        return (await self._status).debug_error_string()++    def _set_initial_metadata(self, metadata: MetadataType) -> None:+        self._initial_metadata.set_result(metadata)++    def _set_status(self, status: cygrpc.AioRpcStatus) -> None:+        """"""Private method to set final status of the RPC.++        This method may be called multiple time due to data race between local+        cancellation (by application) and C-Core receiving status from peer. We+        make no promise here which one will win.         """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+        if self._status.done():+            return+        else:+            self._status.set_result(status)+            self._code = _common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[+                status.code()]++    async def _raise_rpc_error_if_not_ok(self) -> None:+        if self._code != grpc.StatusCode.OK:+            raise _create_rpc_error(await self.initial_metadata(),+                                    self._status.result())++    def _repr(self) -> str:+        """"""Assembles the RPC representation string.""""""+        if not self._status.done():+            return '<{} object>'.format(self.__class__.__name__)+        if self._code is grpc.StatusCode.OK:+            return _OK_CALL_REPRESENTATION.format(+                self.__class__.__name__, self._code,+                self._status.result().self._status.result().details())+        else:+            return _NON_OK_CALL_REPRESENTATION.format(+                self.__class__.__name__, self._code,+                self._status.result().details(),+                self._status.result().debug_error_string())++    def __repr__(self) -> str:+        return self._repr()++    def __str__(self) -> str:+        return self._repr()+++class UnaryUnaryCall(Call, _base_call.UnaryUnaryCall):+    """"""Object for managing unary-unary RPC calls.++    Returned when an instance of `UnaryUnaryMultiCallable` object is called.+    """"""+    _loop: asyncio.AbstractEventLoop+    _request: RequestType+    _deadline: Optional[float]+    _channel: cygrpc.AioChannel+    _method: bytes+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction+    _call: asyncio.Task -        return self._details+    def __init__(self, request: RequestType, deadline: Optional[float],+                 channel: cygrpc.AioChannel, method: bytes,+                 request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__()+        self._loop = asyncio.get_event_loop()+        self._request = request+        self._deadline = deadline+        self._channel = channel+        self._method = method+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer+        self._call = self._loop.create_task(self._invoke())++    def __del__(self) -> None:+        if not self._call.done():+            self._cancel(+                cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,+                                    _GC_CANCELLATION_DETAILS, None, None))++    async def _invoke(self) -> ResponseType:+        serialized_request = _common.serialize(self._request,+                                               self._request_serializer)++        # NOTE(lidiz) asyncio.CancelledError is not a good transport for+        # status, since the Task class do not cache the exact+        # asyncio.CancelledError object. So, the solution is catching the error+        # in Cython layer, then cancel the RPC and update the status, finally+        # re-raise the CancelledError.+        serialized_response = await self._channel.unary_unary(+            self._method,+            serialized_request,+            self._deadline,+            self._cancellation_future,+            self._set_initial_metadata,+            self._set_status,+        )+        await self._raise_rpc_error_if_not_ok()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:+        """"""Forwards the application cancellation reasoning.""""""+        if not self._status.done() and not self._cancellation_future.done():+            self._cancellation_future.set_result(status)+            self._call.cancel()+            return True+        else:+            return False -    def __await__(self):+    def cancel(self) -> bool:+        return self._cancel(+            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,+                                _LOCAL_CANCELLATION_DETAILS, None, None))++    def __await__(self) -> ResponseType:         """"""Wait till the ongoing RPC request finishes.          Returns:           Response of the RPC call.          Raises:-          AioRpcError: Indicating that the RPC terminated with non-OK status.+          RpcError: Indicating that the RPC terminated with non-OK status.           asyncio.CancelledError: Indicating that the RPC was canceled.         """"""-        # We can not relay on the `done()` method since some exceptions-        # might be pending to be catched, like `asyncio.CancelledError`.-        if self._response:-            return self._response-        elif self._exception:-            raise self._exception--        try:-            buffer_ = yield from self._call.__await__()-        except cygrpc.AioRpcError as aio_rpc_error:-            self._state = _RpcState.ABORT-            self._code = _common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[-                aio_rpc_error.code()]-            self._details = aio_rpc_error.details()-            self._initial_metadata = aio_rpc_error.initial_metadata()-            self._trailing_metadata = aio_rpc_error.trailing_metadata()--            # Propagates the pure Python class-            self._exception = AioRpcError(self._code, self._details,-                                          self._initial_metadata,-                                          self._trailing_metadata)-            raise self._exception from aio_rpc_error-        except asyncio.CancelledError as cancel_error:-            # _state, _code, _details are managed in the `cancel` method-            self._exception = cancel_error-            raise--        self._response = _common.deserialize(buffer_,-                                             self._response_deserializer)-        self._code = grpc.StatusCode.OK-        self._state = _RpcState.FINISHED-        return self._response+        response = yield from self._call+        return response+++class UnaryStreamCall(Call, _base_call.UnaryStreamCall):+    """"""Object for managing unary-stream RPC calls.++    Returned when an instance of `UnaryStreamMultiCallable` object is called.+    """"""+    _loop: asyncio.AbstractEventLoop+    _request: RequestType+    _deadline: Optional[float]+    _channel: cygrpc.AioChannel+    _method: bytes+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction+    _call: AsyncIterable[ResponseType]++    def __init__(self, request: RequestType, deadline: Optional[float],+                 channel: cygrpc.AioChannel, method: bytes,+                 request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__()+        self._loop = asyncio.get_event_loop()",same as before `_loop` is an already existing attribute of the base class.,X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21232,353817794,2019-12-04T15:40:17Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,335 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()",For the sake of consistency with the `_status` and `_initial_metadata` would make sense on naming it just `_cancellation`?,X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21232,353821847,2019-12-04T15:46:52Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,335 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Virtual cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation_future, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation_future.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:+        pass -    async def initial_metadata(self):-        raise NotImplementedError()+    def is_active(self) -> bool:+        return self.done() -    async def trailing_metadata(self):-        raise NotImplementedError()+    def time_remaining(self) -> float:+        pass -    async def code(self) -> grpc.StatusCode:-        """"""Returns the `grpc.StatusCode` if the RPC is finished,-        otherwise first waits until the RPC finishes.+    async def initial_metadata(self) -> MetadataType:+        return await self._initial_metadata -        Returns:-          The `grpc.StatusCode` status code.-        """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+    async def trailing_metadata(self) -> MetadataType:+        return (await self._status).trailing_metadata() +    async def code(self) -> grpc.StatusCode:+        await self._status         return self._code      async def details(self) -> str:-        """"""Returns the details if the RPC is finished, otherwise first waits till the-        RPC finishes.+        return (await self._status).details() -        Returns:-          The details.+    async def debug_error_string(self) -> str:+        return (await self._status).debug_error_string()++    def _set_initial_metadata(self, metadata: MetadataType) -> None:+        self._initial_metadata.set_result(metadata)++    def _set_status(self, status: cygrpc.AioRpcStatus) -> None:","Correct me if I'm wrong but this callback strategy might not be needed, at the moment that you are using the future for sync both layers it's just a matter of using the future for checking if the status is already there by making an `await` or by checking the `done()` status of the future.Indeed, the _code which seems to be the unique attribute that is being transalted could be translated in execution time, so doing like:```pythonasync def code(self):    status = await self._status    return _common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()]```Having the feeling that callback pattern implies some cognitive understanding that if can be avoided better ",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21232,353839265,2019-12-04T16:14:29Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -57,93 +62,204 @@ cdef class _AioCall:             self._channel.cq.c_ptr(),             method_slice,             NULL,-            deadline,+            c_deadline,             NULL         )         grpc_slice_unref(method_slice)      cdef void _destroy_grpc_call(self):         """"""Destroys the corresponding Core object for this RPC.""""""-        grpc_call_unref(self._grpc_call_wrapper.call)+        if self._grpc_call_wrapper.call != NULL:+            grpc_call_unref(self._grpc_call_wrapper.call) -    async def unary_unary(self, bytes method, bytes request, object timeout, AioCancelStatus cancel_status):-        cdef object loop = asyncio.get_event_loop()+    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):+        """"""Cancels the RPC in C-Core, and return the final RPC status.""""""+        cdef AioRpcStatus status+        cdef object details+        cdef char *c_details+        # Try to fetch application layer cancellation details in the future.+        # * If calcellation details present, cancel with status;+        # * If details not present, cancel with unknown reason.+        if cancellation_future.done():+            status = cancellation_future.result()+            details = str_to_bytes(status.details())+            self._references.append(details)+            c_details = <char *>details+            # By implementation, grpc_call_cancel_with_status always return OK+            grpc_call_cancel_with_status(+                self._grpc_call_wrapper.call,+                status.c_code(),+                c_details,+                NULL,+            )+            return status+        else:+            # By implementation, grpc_call_cancel always return OK+            grpc_call_cancel(self._grpc_call_wrapper.call, NULL)+            return AioRpcStatus(+                StatusCode.cancelled,+                _UNKNOWN_CANCELLATION_DETAILS,+                None,+                None,+            ) -        cdef tuple operations-        cdef Operation initial_metadata_operation-        cdef Operation send_message_operation-        cdef Operation send_close_from_client_operation-        cdef Operation receive_initial_metadata_operation-        cdef Operation receive_message_operation-        cdef Operation receive_status_on_client_operation+    async def unary_unary(self,+                          bytes method,+                          bytes request,+                          object deadline,+                          object cancellation_future,+                          object initial_metadata_observer,+                          object status_observer):+        """"""Performs a unary unary RPC.+        +        Args:+          method: name of the calling method in bytes.+          request: the serialized requests in bytes.+          deadline: optional deadline of the RPC in float.+          cancellation_future: the future that meant to transport the+            cancellation reason from the application layer.+          initial_metadata_observer: a callback for received initial metadata.+          status_observer: a callback for received final status.+        """"""+        cdef tuple ops -        cdef char *c_details = NULL+        cdef SendInitialMetadataOperation initial_metadata_op = SendInitialMetadataOperation(+            _EMPTY_METADATA,+            GRPC_INITIAL_METADATA_USED_MASK)+        cdef SendMessageOperation send_message_op = SendMessageOperation(request, _EMPTY_FLAGS)+        cdef SendCloseFromClientOperation send_close_op = SendCloseFromClientOperation(_EMPTY_FLAGS)+        cdef ReceiveInitialMetadataOperation receive_initial_metadata_op = ReceiveInitialMetadataOperation(_EMPTY_FLAGS)+        cdef ReceiveMessageOperation receive_message_op = ReceiveMessageOperation(_EMPTY_FLAGS)+        cdef ReceiveStatusOnClientOperation receive_status_on_client_op = ReceiveStatusOnClientOperation(_EMPTY_FLAGS) -        initial_metadata_operation = SendInitialMetadataOperation(_EMPTY_METADATA, GRPC_INITIAL_METADATA_USED_MASK)-        initial_metadata_operation.c()+        ops = (initial_metadata_op, send_message_op, send_close_op,+               receive_initial_metadata_op, receive_message_op,+               receive_status_on_client_op) -        send_message_operation = SendMessageOperation(request, _EMPTY_FLAGS)-        send_message_operation.c()+        try:+            self._create_grpc_call(deadline, method)+            try:+                await callback_start_batch(self._grpc_call_wrapper,+                                           ops,+                                           self._loop)+            except asyncio.CancelledError:+                status = self._cancel_and_create_status(cancellation_future)+                status_observer(status)+                raise","I'm still skeptic on this pattern, it means that callers and interceptors would need to do so for observing the code:```pythoncall = hi(...)try:   await callexcept grpc.RpcError as err:    status = err.code()    raiseexcept CancelledError as cancellation:    status = await call.code()    raiseelse:    status = await call.done()```So the way of getting the canceled error is by checking the status of the call since the exception is not reported as a generic `RpcError`.  And with the implementation of interceptors, because a cancellation can happen after and before the real RPC call, the code returned in the context of a `CancelledError` could be different than the `rpc.StatusCode.CANCELLED`As I said, I'm skeptic but I do not have any other good solution :(. And with this, we provide the way of not masking the `CanclledError` and giving the chance to the user for gathering the status code.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21232,353850048,2019-12-04T16:32:18Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -11,186 +11,288 @@ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.+""""""Tests behavior of the grpc.aio.UnaryUnaryCall class.""""""+ import asyncio import logging import unittest+import datetime  import grpc  from grpc.experimental import aio from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc from tests.unit.framework.common import test_constants from tests_aio.unit._test_server import start_test_server from tests_aio.unit._test_base import AioTestBase --class TestAioRpcError(unittest.TestCase):-    _TEST_INITIAL_METADATA = (""initial metadata"",)-    _TEST_TRAILING_METADATA = (""trailing metadata"",)--    def test_attributes(self):-        aio_rpc_error = aio.AioRpcError(-            grpc.StatusCode.CANCELLED,-            ""details"",-            initial_metadata=self._TEST_INITIAL_METADATA,-            trailing_metadata=self._TEST_TRAILING_METADATA)-        self.assertEqual(aio_rpc_error.code(), grpc.StatusCode.CANCELLED)-        self.assertEqual(aio_rpc_error.details(), ""details"")-        self.assertEqual(aio_rpc_error.initial_metadata(),-                         self._TEST_INITIAL_METADATA)-        self.assertEqual(aio_rpc_error.trailing_metadata(),-                         self._TEST_TRAILING_METADATA)+_NUM_STREAM_RESPONSES = 5+_RESPONSE_PAYLOAD_SIZE = 42+_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+# _RESPONSE_INTERVAL_US = test_constants.SHORT_TIMEOUT * 1000 * 1000+_RESPONSE_INTERVAL_US = 200 * 1000   class TestCall(AioTestBase): -    def test_call_ok(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.done())--                response = await call--                self.assertTrue(call.done())-                self.assertEqual(type(response), messages_pb2.SimpleResponse)-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--                # Response is cached at call object level, reentrance-                # returns again the same response-                response_retry = await call-                self.assertIs(response, response_retry)--        self.loop.run_until_complete(coro())--    def test_call_rpc_error(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                empty_call_with_sleep = channel.unary_unary(-                    ""/grpc.testing.TestService/EmptyCall"",-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.-                    FromString,-                )-                timeout = test_constants.SHORT_TIMEOUT / 2-                # TODO(https://github.com/grpc/grpc/issues/20869-                # Update once the async server is ready, change the-                # synchronization mechanism by removing the sleep(<timeout>)-                # as both components (client & server) will be on the same-                # process.-                call = empty_call_with_sleep(-                    messages_pb2.SimpleRequest(), timeout=timeout)--                with self.assertRaises(grpc.RpcError) as exception_context:-                    await call--                self.assertTrue(call.done())-                self.assertEqual(await call.code(),-                                 grpc.StatusCode.DEADLINE_EXCEEDED)--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        grpc.RpcError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())--    def test_call_code_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--        self.loop.run_until_complete(coro())--    def test_call_details_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.details(), None)--        self.loop.run_until_complete(coro())--    def test_cancel(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.cancelled())--                # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.-                # Force the loop to execute the RPC task.-                await asyncio.sleep(0)--                self.assertTrue(call.cancel())-                self.assertTrue(call.cancelled())-                self.assertFalse(call.cancel())--                with self.assertRaises(-                        asyncio.CancelledError) as exception_context:-                    await call--                self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)-                self.assertEqual(await call.details(),-                                 'Locally cancelled by application!')--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        asyncio.CancelledError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())+    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_call_ok(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.done())++            response = await call++            self.assertTrue(call.done())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++            # Response is cached at call object level, reentrance+            # returns again the same response+            response_retry = await call+            self.assertIs(response, response_retry)++    async def test_call_rpc_error(self):+        async with aio.insecure_channel(self._server_target) as channel:+            empty_call_with_sleep = channel.unary_unary(+                ""/grpc.testing.TestService/EmptyCall"",+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString,+            )+            timeout = test_constants.SHORT_TIMEOUT / 2+            # TODO(https://github.com/grpc/grpc/issues/20869+            # Update once the async server is ready, change the+            # synchronization mechanism by removing the sleep(<timeout>)+            # as both components (client & server) will be on the same+            # process.+            call = empty_call_with_sleep(+                messages_pb2.SimpleRequest(), timeout=timeout)++            with self.assertRaises(grpc.RpcError) as exception_context:+                await call++            self.assertTrue(call.done())+            self.assertEqual(await call.code(),+                             grpc.StatusCode.DEADLINE_EXCEEDED)++            # Exception is cached at call object level, reentrance+            # returns again the same exception+            with self.assertRaises(grpc.RpcError) as exception_context_retry:+                await call++            self.assertIs(exception_context.exception,+                          exception_context_retry.exception)++    async def test_call_code_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++    async def test_call_details_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual('', await call.details())++    async def test_cancel_unary_unary(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.cancelled())++            # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.+            # Force the loop to execute the RPC task.+            await asyncio.sleep(0)++            self.assertTrue(call.cancel())+            self.assertFalse(call.cancel())++            with self.assertRaises(asyncio.CancelledError) as exception_context:+                await call++            self.assertTrue(call.cancelled())+            self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+            self.assertEqual(await call.details(),+                             'Locally cancelled by application!')++            # NOTE(lidiz) The CancelledError is almost always re-created,+            # so we might not want to use it to transmit data.+            # https://github.com/python/cpython/blob/master/Lib/asyncio/tasks.py#L785","Oks, so we follow the same criteria and we never save a CancelledError exception.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21232,353851207,2019-12-04T16:34:16Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -11,186 +11,288 @@ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.+""""""Tests behavior of the grpc.aio.UnaryUnaryCall class.""""""+ import asyncio import logging import unittest+import datetime  import grpc  from grpc.experimental import aio from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc from tests.unit.framework.common import test_constants from tests_aio.unit._test_server import start_test_server from tests_aio.unit._test_base import AioTestBase --class TestAioRpcError(unittest.TestCase):-    _TEST_INITIAL_METADATA = (""initial metadata"",)-    _TEST_TRAILING_METADATA = (""trailing metadata"",)--    def test_attributes(self):-        aio_rpc_error = aio.AioRpcError(-            grpc.StatusCode.CANCELLED,-            ""details"",-            initial_metadata=self._TEST_INITIAL_METADATA,-            trailing_metadata=self._TEST_TRAILING_METADATA)-        self.assertEqual(aio_rpc_error.code(), grpc.StatusCode.CANCELLED)-        self.assertEqual(aio_rpc_error.details(), ""details"")-        self.assertEqual(aio_rpc_error.initial_metadata(),-                         self._TEST_INITIAL_METADATA)-        self.assertEqual(aio_rpc_error.trailing_metadata(),-                         self._TEST_TRAILING_METADATA)+_NUM_STREAM_RESPONSES = 5+_RESPONSE_PAYLOAD_SIZE = 42+_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+# _RESPONSE_INTERVAL_US = test_constants.SHORT_TIMEOUT * 1000 * 1000+_RESPONSE_INTERVAL_US = 200 * 1000   class TestCall(AioTestBase): -    def test_call_ok(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.done())--                response = await call--                self.assertTrue(call.done())-                self.assertEqual(type(response), messages_pb2.SimpleResponse)-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--                # Response is cached at call object level, reentrance-                # returns again the same response-                response_retry = await call-                self.assertIs(response, response_retry)--        self.loop.run_until_complete(coro())--    def test_call_rpc_error(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                empty_call_with_sleep = channel.unary_unary(-                    ""/grpc.testing.TestService/EmptyCall"",-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.-                    FromString,-                )-                timeout = test_constants.SHORT_TIMEOUT / 2-                # TODO(https://github.com/grpc/grpc/issues/20869-                # Update once the async server is ready, change the-                # synchronization mechanism by removing the sleep(<timeout>)-                # as both components (client & server) will be on the same-                # process.-                call = empty_call_with_sleep(-                    messages_pb2.SimpleRequest(), timeout=timeout)--                with self.assertRaises(grpc.RpcError) as exception_context:-                    await call--                self.assertTrue(call.done())-                self.assertEqual(await call.code(),-                                 grpc.StatusCode.DEADLINE_EXCEEDED)--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        grpc.RpcError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())--    def test_call_code_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--        self.loop.run_until_complete(coro())--    def test_call_details_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.details(), None)--        self.loop.run_until_complete(coro())--    def test_cancel(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.cancelled())--                # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.-                # Force the loop to execute the RPC task.-                await asyncio.sleep(0)--                self.assertTrue(call.cancel())-                self.assertTrue(call.cancelled())-                self.assertFalse(call.cancel())--                with self.assertRaises(-                        asyncio.CancelledError) as exception_context:-                    await call--                self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)-                self.assertEqual(await call.details(),-                                 'Locally cancelled by application!')--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        asyncio.CancelledError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())+    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_call_ok(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.done())++            response = await call++            self.assertTrue(call.done())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++            # Response is cached at call object level, reentrance+            # returns again the same response+            response_retry = await call+            self.assertIs(response, response_retry)++    async def test_call_rpc_error(self):+        async with aio.insecure_channel(self._server_target) as channel:+            empty_call_with_sleep = channel.unary_unary(+                ""/grpc.testing.TestService/EmptyCall"",+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString,+            )+            timeout = test_constants.SHORT_TIMEOUT / 2+            # TODO(https://github.com/grpc/grpc/issues/20869+            # Update once the async server is ready, change the+            # synchronization mechanism by removing the sleep(<timeout>)+            # as both components (client & server) will be on the same+            # process.+            call = empty_call_with_sleep(+                messages_pb2.SimpleRequest(), timeout=timeout)++            with self.assertRaises(grpc.RpcError) as exception_context:+                await call++            self.assertTrue(call.done())+            self.assertEqual(await call.code(),+                             grpc.StatusCode.DEADLINE_EXCEEDED)++            # Exception is cached at call object level, reentrance+            # returns again the same exception+            with self.assertRaises(grpc.RpcError) as exception_context_retry:+                await call++            self.assertIs(exception_context.exception,+                          exception_context_retry.exception)++    async def test_call_code_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++    async def test_call_details_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual('', await call.details())++    async def test_cancel_unary_unary(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.cancelled())++            # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.+            # Force the loop to execute the RPC task.+            await asyncio.sleep(0)++            self.assertTrue(call.cancel())+            self.assertFalse(call.cancel())++            with self.assertRaises(asyncio.CancelledError) as exception_context:+                await call++            self.assertTrue(call.cancelled())+            self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+            self.assertEqual(await call.details(),+                             'Locally cancelled by application!')++            # NOTE(lidiz) The CancelledError is almost always re-created,+            # so we might not want to use it to transmit data.+            # https://github.com/python/cpython/blob/master/Lib/asyncio/tasks.py#L785++    async def test_cancel_unary_stream(self):",Since `UnaryStreamCall` has its own implementation we could move all of these test cases to its own test class.,
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21374,353906405,2019-12-04T18:22:30Z,src/core/ext/filters/client_channel/xds/xds_bootstrap.cc,"@@ -58,23 +58,23 @@ XdsBootstrap::XdsBootstrap(grpc_slice contents, grpc_error** error)     return;   }   InlinedVector<grpc_error*, 1> error_list;-  bool seen_xds_server = false;+  bool seen_xds_servers = false;   bool seen_node = false;   for (grpc_json* child = tree_->child; child != nullptr; child = child->next) {     if (child->key == nullptr) {       error_list.push_back(           GRPC_ERROR_CREATE_FROM_STATIC_STRING(""JSON key is null""));-    } else if (strcmp(child->key, ""xds_server"") == 0) {",Just to confirm: is it true that `xds_server` is no longer possible to be seen in the bootstrap file?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,353965892,2019-12-04T20:29:04Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -57,93 +62,204 @@ cdef class _AioCall:             self._channel.cq.c_ptr(),             method_slice,             NULL,-            deadline,+            c_deadline,             NULL         )         grpc_slice_unref(method_slice)      cdef void _destroy_grpc_call(self):         """"""Destroys the corresponding Core object for this RPC.""""""-        grpc_call_unref(self._grpc_call_wrapper.call)+        if self._grpc_call_wrapper.call != NULL:+            grpc_call_unref(self._grpc_call_wrapper.call) -    async def unary_unary(self, bytes method, bytes request, object timeout, AioCancelStatus cancel_status):-        cdef object loop = asyncio.get_event_loop()+    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):+        """"""Cancels the RPC in C-Core, and return the final RPC status.""""""+        cdef AioRpcStatus status+        cdef object details+        cdef char *c_details+        # Try to fetch application layer cancellation details in the future.+        # * If calcellation details present, cancel with status;+        # * If details not present, cancel with unknown reason.+        if cancellation_future.done():+            status = cancellation_future.result()+            details = str_to_bytes(status.details())+            self._references.append(details)+            c_details = <char *>details+            # By implementation, grpc_call_cancel_with_status always return OK+            grpc_call_cancel_with_status(+                self._grpc_call_wrapper.call,+                status.c_code(),+                c_details,+                NULL,+            )+            return status+        else:+            # By implementation, grpc_call_cancel always return OK+            grpc_call_cancel(self._grpc_call_wrapper.call, NULL)+            return AioRpcStatus(+                StatusCode.cancelled,+                _UNKNOWN_CANCELLATION_DETAILS,+                None,+                None,+            ) -        cdef tuple operations-        cdef Operation initial_metadata_operation-        cdef Operation send_message_operation-        cdef Operation send_close_from_client_operation-        cdef Operation receive_initial_metadata_operation-        cdef Operation receive_message_operation-        cdef Operation receive_status_on_client_operation+    async def unary_unary(self,+                          bytes method,+                          bytes request,+                          object deadline,+                          object cancellation_future,+                          object initial_metadata_observer,+                          object status_observer):+        """"""Performs a unary unary RPC.+        +        Args:+          method: name of the calling method in bytes.+          request: the serialized requests in bytes.+          deadline: optional deadline of the RPC in float.+          cancellation_future: the future that meant to transport the+            cancellation reason from the application layer.+          initial_metadata_observer: a callback for received initial metadata.+          status_observer: a callback for received final status.+        """"""+        cdef tuple ops -        cdef char *c_details = NULL+        cdef SendInitialMetadataOperation initial_metadata_op = SendInitialMetadataOperation(+            _EMPTY_METADATA,+            GRPC_INITIAL_METADATA_USED_MASK)+        cdef SendMessageOperation send_message_op = SendMessageOperation(request, _EMPTY_FLAGS)+        cdef SendCloseFromClientOperation send_close_op = SendCloseFromClientOperation(_EMPTY_FLAGS)+        cdef ReceiveInitialMetadataOperation receive_initial_metadata_op = ReceiveInitialMetadataOperation(_EMPTY_FLAGS)+        cdef ReceiveMessageOperation receive_message_op = ReceiveMessageOperation(_EMPTY_FLAGS)+        cdef ReceiveStatusOnClientOperation receive_status_on_client_op = ReceiveStatusOnClientOperation(_EMPTY_FLAGS) -        initial_metadata_operation = SendInitialMetadataOperation(_EMPTY_METADATA, GRPC_INITIAL_METADATA_USED_MASK)-        initial_metadata_operation.c()+        ops = (initial_metadata_op, send_message_op, send_close_op,+               receive_initial_metadata_op, receive_message_op,+               receive_status_on_client_op) -        send_message_operation = SendMessageOperation(request, _EMPTY_FLAGS)-        send_message_operation.c()+        try:+            self._create_grpc_call(deadline, method)+            try:+                await callback_start_batch(self._grpc_call_wrapper,+                                           ops,+                                           self._loop)+            except asyncio.CancelledError:+                status = self._cancel_and_create_status(cancellation_future)+                status_observer(status)+                raise","In the very first version, I tried to extend `CancelledError`. But I found the CPython implementation is banning this path (see [code](https://github.com/python/cpython/blob/master/Lib/asyncio/tasks.py#L269)) that using `CancelledError` to transmit messages.I agree, in our first version, we can cache the exception ourself. There is still a risk that an application tries to continue our underlying Task, then seeing the default `CancelledError`.I would say these two patterns generates similar code:```Pythoncall = hi(...)try:   await callexcept grpc.RpcError as rpc_error:    status = rpc_error.code()    raiseexcept CancelledError as rpc_error:    status = rpc_error.code()    raiseelse:    status = await call.done()```WDYT?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,353967755,2019-12-04T20:33:17Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/socket.pyx.pxi,"@@ -125,23 +125,34 @@ cdef class _AsyncioSocket:     cdef void read(self, char * buffer_, size_t length, grpc_custom_read_callback grpc_read_cb):         assert not self._task_read -        self._task_read = asyncio.ensure_future(+        self._task_read = self._loop.create_task(             self._reader.read(n=length)         )         self._grpc_read_cb = grpc_read_cb         self._task_read.add_done_callback(self._read_cb)         self._read_buffer = buffer_++    async def _async_write(self, bytearray buffer):+        self._writer.write(buffer)+        await self._writer.drain()++        self._grpc_write_cb(+            <grpc_custom_socket*>self._grpc_socket,+            <grpc_error*>0+        )       cdef void write(self, grpc_slice_buffer * g_slice_buffer, grpc_custom_write_callback grpc_write_cb):+        # For each socket, C-Core guarantees there'll be only one ongoing write+        self._grpc_write_cb = grpc_write_cb","Good catch! Removed!It is used for flow control, which eventually I decided to drop out of this complex PR.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,353968414,2019-12-04T20:34:45Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/socket.pyx.pxi,"@@ -125,23 +125,34 @@ cdef class _AsyncioSocket:     cdef void read(self, char * buffer_, size_t length, grpc_custom_read_callback grpc_read_cb):         assert not self._task_read -        self._task_read = asyncio.ensure_future(+        self._task_read = self._loop.create_task(             self._reader.read(n=length)         )         self._grpc_read_cb = grpc_read_cb         self._task_read.add_done_callback(self._read_cb)         self._read_buffer = buffer_++    async def _async_write(self, bytearray buffer):+        self._writer.write(buffer)+        await self._writer.drain()++        self._grpc_write_cb(+            <grpc_custom_socket*>self._grpc_socket,+            <grpc_error*>0+        )       cdef void write(self, grpc_slice_buffer * g_slice_buffer, grpc_custom_write_callback grpc_write_cb):+        # For each socket, C-Core guarantees there'll be only one ongoing write+        self._grpc_write_cb = grpc_write_cb+         cdef char* start-        buffer_ = bytearray()+        buffer = bytearray()","My bad, I wasn't aware that `buffer` is a keyword. Renamed to `outbound_buffer`.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,353968952,2019-12-04T20:36:02Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/rpc_status.pyx.pxi,"@@ -14,16 +14,19 @@ """"""Exceptions for the aio version of the RPC calls.""""""  -cdef class AioRpcError(Exception):+cdef class AioRpcStatus(Exception): -    def __cinit__(self, tuple initial_metadata, int code, str details, tuple trailing_metadata):-        self._initial_metadata = initial_metadata+    # The final status of gRPC is represented by three trailing metadata:+    # `grpc-status`, `grpc-status-message`, abd `grpc-status-details`.+    def __cinit__(self,+                  int code,","Changed to `grpc_status_code`.`grpc_status_code` is a C enum, and `StatusCode` is a Python class. I thought Cython only allows Cython class or primitive types here. I'm wrong, thanks!",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,353971368,2019-12-04T20:41:46Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,335 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()","I don't have strong opinion here, let's discuss the semantic meaning of both variable names.My reason for using `_cancellation_future` instead of `_cancellation` is that status and metadata are terms that referring to a data structure or data type, but cancellation does not refer to anything and it is ambiguous what will be the result. So, I choose `_cancellation_future` to better reflect the usage of this variable.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,353975105,2019-12-04T20:50:03Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,335 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Virtual cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation_future, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation_future.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:+        pass -    async def initial_metadata(self):-        raise NotImplementedError()+    def is_active(self) -> bool:+        return self.done() -    async def trailing_metadata(self):-        raise NotImplementedError()+    def time_remaining(self) -> float:+        pass -    async def code(self) -> grpc.StatusCode:-        """"""Returns the `grpc.StatusCode` if the RPC is finished,-        otherwise first waits until the RPC finishes.+    async def initial_metadata(self) -> MetadataType:+        return await self._initial_metadata -        Returns:-          The `grpc.StatusCode` status code.-        """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+    async def trailing_metadata(self) -> MetadataType:+        return (await self._status).trailing_metadata() +    async def code(self) -> grpc.StatusCode:+        await self._status         return self._code      async def details(self) -> str:-        """"""Returns the details if the RPC is finished, otherwise first waits till the-        RPC finishes.+        return (await self._status).details() -        Returns:-          The details.+    async def debug_error_string(self) -> str:+        return (await self._status).debug_error_string()++    def _set_initial_metadata(self, metadata: MetadataType) -> None:+        self._initial_metadata.set_result(metadata)++    def _set_status(self, status: cygrpc.AioRpcStatus) -> None:","Syncing messages between two layers, I could think a couple ways: 1) AsyncIO futures; 2) pass down a context data structure; 3) pumping through some global data structure. For method 2, the drawback is no explicit `await`. And for method 3, the drawback is too much complexity.I agree using future and callback is adding complexity, but it is my best solution to trade off between performance, complexity and ease-of-use.Also, can you elaborate more about your suggestion? The `_set_status` is mainly used to set the final status, but the code snippet you provided is about accessing the final status. So, what is your recommended way of transmitting final status?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,353976681,2019-12-04T20:53:25Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,335 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Virtual cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation_future, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation_future.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:+        pass -    async def initial_metadata(self):-        raise NotImplementedError()+    def is_active(self) -> bool:+        return self.done() -    async def trailing_metadata(self):-        raise NotImplementedError()+    def time_remaining(self) -> float:+        pass -    async def code(self) -> grpc.StatusCode:-        """"""Returns the `grpc.StatusCode` if the RPC is finished,-        otherwise first waits until the RPC finishes.+    async def initial_metadata(self) -> MetadataType:+        return await self._initial_metadata -        Returns:-          The `grpc.StatusCode` status code.-        """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+    async def trailing_metadata(self) -> MetadataType:+        return (await self._status).trailing_metadata() +    async def code(self) -> grpc.StatusCode:+        await self._status         return self._code      async def details(self) -> str:-        """"""Returns the details if the RPC is finished, otherwise first waits till the-        RPC finishes.+        return (await self._status).details() -        Returns:-          The details.+    async def debug_error_string(self) -> str:+        return (await self._status).debug_error_string()++    def _set_initial_metadata(self, metadata: MetadataType) -> None:+        self._initial_metadata.set_result(metadata)++    def _set_status(self, status: cygrpc.AioRpcStatus) -> None:+        """"""Private method to set final status of the RPC.++        This method may be called multiple time due to data race between local+        cancellation (by application) and C-Core receiving status from peer. We+        make no promise here which one will win.         """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+        if self._status.done():+            return+        else:+            self._status.set_result(status)+            self._code = _common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[+                status.code()]++    async def _raise_rpc_error_if_not_ok(self) -> None:+        if self._code != grpc.StatusCode.OK:+            raise _create_rpc_error(await self.initial_metadata(),+                                    self._status.result())++    def _repr(self) -> str:+        """"""Assembles the RPC representation string.""""""+        if not self._status.done():+            return '<{} object>'.format(self.__class__.__name__)+        if self._code is grpc.StatusCode.OK:+            return _OK_CALL_REPRESENTATION.format(+                self.__class__.__name__, self._code,+                self._status.result().self._status.result().details())+        else:+            return _NON_OK_CALL_REPRESENTATION.format(+                self.__class__.__name__, self._code,+                self._status.result().details(),+                self._status.result().debug_error_string())++    def __repr__(self) -> str:+        return self._repr()++    def __str__(self) -> str:+        return self._repr()+++class UnaryUnaryCall(Call, _base_call.UnaryUnaryCall):+    """"""Object for managing unary-unary RPC calls.++    Returned when an instance of `UnaryUnaryMultiCallable` object is called.+    """"""+    _loop: asyncio.AbstractEventLoop+    _request: RequestType+    _deadline: Optional[float]+    _channel: cygrpc.AioChannel+    _method: bytes+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction+    _call: asyncio.Task -        return self._details+    def __init__(self, request: RequestType, deadline: Optional[float],+                 channel: cygrpc.AioChannel, method: bytes,+                 request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__()+        self._loop = asyncio.get_event_loop()+        self._request = request+        self._deadline = deadline+        self._channel = channel+        self._method = method+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer+        self._call = self._loop.create_task(self._invoke())++    def __del__(self) -> None:+        if not self._call.done():+            self._cancel(+                cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,+                                    _GC_CANCELLATION_DETAILS, None, None))++    async def _invoke(self) -> ResponseType:+        serialized_request = _common.serialize(self._request,+                                               self._request_serializer)++        # NOTE(lidiz) asyncio.CancelledError is not a good transport for+        # status, since the Task class do not cache the exact+        # asyncio.CancelledError object. So, the solution is catching the error+        # in Cython layer, then cancel the RPC and update the status, finally+        # re-raise the CancelledError.+        serialized_response = await self._channel.unary_unary(+            self._method,+            serialized_request,+            self._deadline,+            self._cancellation_future,+            self._set_initial_metadata,+            self._set_status,+        )+        await self._raise_rpc_error_if_not_ok()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:","Yes. I'm trying to unify varieties of similar classes. To me, semantically, cancellation is also an RPC's final status.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/21374,353985450,2019-12-04T21:12:56Z,src/core/ext/filters/client_channel/xds/xds_bootstrap.cc,"@@ -116,11 +140,11 @@ grpc_error* XdsBootstrap::ParseXdsServer(grpc_json* json) {         error_list.push_back(GRPC_ERROR_CREATE_FROM_STATIC_STRING(             ""\""server_uri\"" field is not a string""));       }-      if (server_uri_ != nullptr) {+      if (server.server_uri != nullptr) {         error_list.push_back(GRPC_ERROR_CREATE_FROM_STATIC_STRING(","It won't really matter, because the config parsing will fail anyway.  The only reason that we don't return immediately here is that it's more user-friendly to return *all* errors in the data rather than stopping at the first error, so that the user doesn't have to constantly iterate, fixing only one problem at a time.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/21374,353985606,2019-12-04T21:13:18Z,src/core/ext/filters/client_channel/xds/xds_bootstrap.cc,"@@ -116,11 +140,11 @@ grpc_error* XdsBootstrap::ParseXdsServer(grpc_json* json) {         error_list.push_back(GRPC_ERROR_CREATE_FROM_STATIC_STRING(             ""\""server_uri\"" field is not a string""));       }-      if (server_uri_ != nullptr) {+      if (server.server_uri != nullptr) {         error_list.push_back(GRPC_ERROR_CREATE_FROM_STATIC_STRING(             ""duplicate \""server_uri\"" field""));       }-      server_uri_ = child->value;+      server.server_uri = child->value;     } else if (strcmp(child->key, ""channel_creds"") == 0) {       if (child->type != GRPC_JSON_ARRAY) {         error_list.push_back(GRPC_ERROR_CREATE_FROM_STATIC_STRING(","See above.  It doesn't matter, because the config parsing will fail as long as there's at least one error.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21374,353992278,2019-12-04T21:28:36Z,src/core/ext/filters/client_channel/xds/xds_bootstrap.cc,"@@ -103,9 +103,33 @@ XdsBootstrap::~XdsBootstrap() {   grpc_slice_unref_internal(contents_); } -grpc_error* XdsBootstrap::ParseXdsServer(grpc_json* json) {+grpc_error* XdsBootstrap::ParseXdsServerList(grpc_json* json) {   InlinedVector<grpc_error*, 1> error_list;-  server_uri_ = nullptr;+  size_t idx = 0;+  for (grpc_json *child = json->child; child != nullptr;+       child = child->next, ++idx) {+    if (child->key != nullptr) {+      char* msg;+      gpr_asprintf(&msg, ""array element %"" PRIuPTR "" key is not null"", idx);+      error_list.push_back(GRPC_ERROR_CREATE_FROM_COPIED_STRING(msg));","In general I don't think assertion is a bad thing; maybe we should have an assertion that's effective in Debug mode but not in Release mode so we can feel free to use it more. I'm not going to push hard though.I usually find it easier to debug when we assert rather than letting things go on, so that when such a bug really occurs, it is easy to find out that the bug comes from JSON instead of needing to trace back the logics. ",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,354052672,2019-12-05T00:12:44Z,src/python/grpcio/grpc/experimental/aio/_base_call.py,"@@ -0,0 +1,129 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Abstract base classes for client-side Call objects.++Call objects represents the RPC itself, and offer methods to access / modify+its information. They also offer methods to manipulate the life-cycle of the+RPC, e.g. cancellation.+""""""++from abc import ABCMeta, abstractmethod+from typing import AsyncIterable, Awaitable, Generic, Text++import grpc++from ._typing import MetadataType, RequestType, ResponseType++__all__ = 'Call', 'UnaryUnaryCall', 'UnaryStreamCall'+++class Call(grpc.RpcContext, metaclass=ABCMeta):","The `grpc.RpcContext` interface is entirely synchronous. While this might work from the perspective of the Python interpreter, I think this will result in applications *depending* on particular methods being async. I think we need to introduce a `grpc.aio.RpcContext` class that makes the appropriate methods async. That way, we're guaranteed that any application coding against the `grpc.aio.RpcContext` will work regardless of the particular implementation.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,354053324,2019-12-05T00:15:10Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,331 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:","This method is sync here, but async in `grpc.aio.Call` above. While this class doesn't inherit from `grpc.RpcContext`, it implements all of its methods. I think it might be nice for this class and `grpc.aio.Call` to agree on the synchronicity/asynchronicity of each method.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,354057211,2019-12-05T00:30:34Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,335 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()","Because of the type annotations at the top of the class, this is less of an issue than it would be in untyped Python. I agree with Pau that it's better to drop the ""_future"" suffix.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,354058042,2019-12-05T00:33:52Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,331 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Virtual cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation_future, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation_future.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:",Is this method necessary for an async class? It seems like we should be able to use `Future.add_done_callback` instead somehow.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354079693,2019-12-05T02:09:02Z,src/python/grpcio/grpc/experimental/aio/_base_call.py,"@@ -0,0 +1,129 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Abstract base classes for client-side Call objects.++Call objects represents the RPC itself, and offer methods to access / modify+its information. They also offer methods to manipulate the life-cycle of the+RPC, e.g. cancellation.+""""""++from abc import ABCMeta, abstractmethod+from typing import AsyncIterable, Awaitable, Generic, Text++import grpc++from ._typing import MetadataType, RequestType, ResponseType++__all__ = 'Call', 'UnaryUnaryCall', 'UnaryStreamCall'+++class Call(grpc.RpcContext, metaclass=ABCMeta):","The [`grpc.RpcContext`](https://grpc.github.io/grpc/python/grpc.html#grpc.RpcContext) has 4 methods, which are all sync methods even in the Async API. That's why I didn't create a new `grpc.aio.RpcContext` here.On the other hand, I kind of want to create a new class to get rid of `add_callback` API which is not user friendly.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354080560,2019-12-05T02:13:11Z,src/python/grpcio/grpc/experimental/aio/_base_call.py,"@@ -0,0 +1,129 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Abstract base classes for client-side Call objects.++Call objects represents the RPC itself, and offer methods to access / modify+its information. They also offer methods to manipulate the life-cycle of the+RPC, e.g. cancellation.+""""""++from abc import ABCMeta, abstractmethod+from typing import AsyncIterable, Awaitable, Generic, Text++import grpc++from ._typing import MetadataType, RequestType, ResponseType++__all__ = 'Call', 'UnaryUnaryCall', 'UnaryStreamCall'+++class Call(grpc.RpcContext, metaclass=ABCMeta):+    """"""The abstract base class of an RPC on the client-side.""""""++    @abstractmethod+    def cancelled(self) -> bool:+        """"""Return True if the RPC is cancelled.++        The RPC is cancelled when the cancellation was requested with cancel().++        Returns:+          A bool indicates whether the RPC is cancelled or not.+        """"""++    @abstractmethod+    def done(self) -> bool:+        """"""Return True if the RPC is done.++        An RPC is done if the RPC is completed, cancelled or aborted.++        Returns:+          A bool indicates if the RPC is done.+        """"""++    @abstractmethod+    async def initial_metadata(self) -> MetadataType:+        """"""Accesses the initial metadata sent by the server.++        Coroutine continues once the value is available.++        Returns:+          The initial :term:`metadata`.+        """"""++    @abstractmethod+    async def trailing_metadata(self) -> MetadataType:+        """"""Accesses the trailing metadata sent by the server.++        Coroutine continues once the value is available.++        Returns:+          The trailing :term:`metadata`.+        """"""++    @abstractmethod+    async def code(self) -> grpc.StatusCode:+        """"""Accesses the status code sent by the server.++        Coroutine continues once the value is available.++        Returns:+          The StatusCode value for the RPC.+        """"""++    @abstractmethod+    async def details(self) -> Text:+        """"""Accesses the details sent by the server.++        Coroutine continues once the value is available.++        Returns:+          The details string of the RPC.+        """"""+++class UnaryUnaryCall(+        Generic[RequestType, ResponseType], Call, metaclass=ABCMeta):+    """"""The abstract base class of an unary-unary RPC on the client-side.""""""++    @abstractmethod+    def __await__(self) -> Awaitable[ResponseType]:+        """"""Await the response message to be ready.++        Returns:+          The response message of the RPC.+        """"""+++class UnaryStreamCall(+        Generic[RequestType, ResponseType], Call, metaclass=ABCMeta):++    @abstractmethod+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        """"""Returns the async iterable representation that yields messages.++        Under the hood, it is calling the ""read"" method.++        Returns:+          An async iterable object that yields messages.+        """"""++    @abstractmethod+    async def read(self) -> ResponseType:+        """"""Reads one message from the RPC.++        Parallel read operations is not allowed.","Updated to:```        Concurrent reads in multiple coroutines are not allowed. If you want to        perform read in multiple coroutines, you needs synchronization. So, you        can start another read after current read is finished.```Alternatively, we could simply throw an exception if user trying to read concurrently, what do you think?",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354081156,2019-12-05T02:16:03Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,331 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:","AioRpcError does not implement `grpc.aio.Call`, see `class AioRpcError(grpc.RpcError):`. I'm torn about this topic, because on the one hand, I hope the API to be consistent as you do, on the other hand, those informations are ready because this is the final state so there is no need for `async def`. WDYT?",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354081583,2019-12-05T02:18:06Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,331 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Virtual cancellation method.","Changed to `Placeholder`. My intention is to remind readers that this method needs to be implemented, and here are expectations. Also, `@abstractmethod` only works if this is an abstract base class.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354082276,2019-12-05T02:21:49Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,331 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Virtual cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation_future, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation_future.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:","I'm torn about this, this callback design is not ideal. I have seen many users asking how to use it, because it only accepts functions like `Callable[[None], None]`. If users are not familiar with factory pattern, then they will have a hard time using it.On the contrary, the `Future.add_done_callback` supplies the future itself to callbacks, which means the callback have directly access to the `grpc.aio.Call` object. It would be much easier to use.However, this method is supported in our existing API, I'm not sure if we can simply get rid of it.",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21232,354121832,2019-12-05T05:56:51Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,335 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Virtual cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation_future, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation_future.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:+        pass -    async def initial_metadata(self):-        raise NotImplementedError()+    def is_active(self) -> bool:+        return self.done() -    async def trailing_metadata(self):-        raise NotImplementedError()+    def time_remaining(self) -> float:+        pass -    async def code(self) -> grpc.StatusCode:-        """"""Returns the `grpc.StatusCode` if the RPC is finished,-        otherwise first waits until the RPC finishes.+    async def initial_metadata(self) -> MetadataType:+        return await self._initial_metadata -        Returns:-          The `grpc.StatusCode` status code.-        """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+    async def trailing_metadata(self) -> MetadataType:+        return (await self._status).trailing_metadata() +    async def code(self) -> grpc.StatusCode:+        await self._status         return self._code      async def details(self) -> str:-        """"""Returns the details if the RPC is finished, otherwise first waits till the-        RPC finishes.+        return (await self._status).details() -        Returns:-          The details.+    async def debug_error_string(self) -> str:+        return (await self._status).debug_error_string()++    def _set_initial_metadata(self, metadata: MetadataType) -> None:+        self._initial_metadata.set_result(metadata)++    def _set_status(self, status: cygrpc.AioRpcStatus) -> None:","Most likely I'm missing something, but my point is that the `self._set_status` callback that is being given as a parameter when the Cython `unary_unary` is invoked could be replaced by the future, the same for the metadata, so we would have the following```python        serialized_response = await self._channel.unary_unary(            self._method,            serialized_request,            self._deadline,            self._cancellation,            self._initial_metadata,            self._status        )```At that point both layers, the Cython one and the Python one, just use the Future for knowing if it was finished and for gathering or setting the result. Other `Call` methods, like the `code()` one,  can just directly use the future for gathering the result",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21232,354122620,2019-12-05T06:00:37Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,335 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Virtual cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation_future, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation_future.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:+        pass -    async def initial_metadata(self):-        raise NotImplementedError()+    def is_active(self) -> bool:+        return self.done() -    async def trailing_metadata(self):-        raise NotImplementedError()+    def time_remaining(self) -> float:+        pass -    async def code(self) -> grpc.StatusCode:-        """"""Returns the `grpc.StatusCode` if the RPC is finished,-        otherwise first waits until the RPC finishes.+    async def initial_metadata(self) -> MetadataType:+        return await self._initial_metadata -        Returns:-          The `grpc.StatusCode` status code.-        """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+    async def trailing_metadata(self) -> MetadataType:+        return (await self._status).trailing_metadata() +    async def code(self) -> grpc.StatusCode:+        await self._status         return self._code      async def details(self) -> str:-        """"""Returns the details if the RPC is finished, otherwise first waits till the-        RPC finishes.+        return (await self._status).details() -        Returns:-          The details.+    async def debug_error_string(self) -> str:+        return (await self._status).debug_error_string()++    def _set_initial_metadata(self, metadata: MetadataType) -> None:+        self._initial_metadata.set_result(metadata)++    def _set_status(self, status: cygrpc.AioRpcStatus) -> None:+        """"""Private method to set final status of the RPC.++        This method may be called multiple time due to data race between local+        cancellation (by application) and C-Core receiving status from peer. We+        make no promise here which one will win.         """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+        if self._status.done():+            return+        else:+            self._status.set_result(status)+            self._code = _common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[+                status.code()]++    async def _raise_rpc_error_if_not_ok(self) -> None:+        if self._code != grpc.StatusCode.OK:+            raise _create_rpc_error(await self.initial_metadata(),+                                    self._status.result())++    def _repr(self) -> str:+        """"""Assembles the RPC representation string.""""""+        if not self._status.done():+            return '<{} object>'.format(self.__class__.__name__)+        if self._code is grpc.StatusCode.OK:+            return _OK_CALL_REPRESENTATION.format(+                self.__class__.__name__, self._code,+                self._status.result().self._status.result().details())+        else:+            return _NON_OK_CALL_REPRESENTATION.format(+                self.__class__.__name__, self._code,+                self._status.result().details(),+                self._status.result().debug_error_string())++    def __repr__(self) -> str:+        return self._repr()++    def __str__(self) -> str:+        return self._repr()+++class UnaryUnaryCall(Call, _base_call.UnaryUnaryCall):+    """"""Object for managing unary-unary RPC calls.++    Returned when an instance of `UnaryUnaryMultiCallable` object is called.+    """"""+    _loop: asyncio.AbstractEventLoop+    _request: RequestType+    _deadline: Optional[float]+    _channel: cygrpc.AioChannel+    _method: bytes+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction+    _call: asyncio.Task -        return self._details+    def __init__(self, request: RequestType, deadline: Optional[float],+                 channel: cygrpc.AioChannel, method: bytes,+                 request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__()+        self._loop = asyncio.get_event_loop()+        self._request = request+        self._deadline = deadline+        self._channel = channel+        self._method = method+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer+        self._call = self._loop.create_task(self._invoke())++    def __del__(self) -> None:+        if not self._call.done():+            self._cancel(+                cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,+                                    _GC_CANCELLATION_DETAILS, None, None))++    async def _invoke(self) -> ResponseType:+        serialized_request = _common.serialize(self._request,+                                               self._request_serializer)++        # NOTE(lidiz) asyncio.CancelledError is not a good transport for+        # status, since the Task class do not cache the exact+        # asyncio.CancelledError object. So, the solution is catching the error+        # in Cython layer, then cancel the RPC and update the status, finally+        # re-raise the CancelledError.+        serialized_response = await self._channel.unary_unary(+            self._method,+            serialized_request,+            self._deadline,+            self._cancellation_future,+            self._set_initial_metadata,+            self._set_status,+        )+        await self._raise_rpc_error_if_not_ok()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:","But here you are using the `AioRpcStatus` for notifying to the Cython layer how you would like to cancel the RPC, this would be used later for the Cython layer for canceling the RPC and for building the final `AioRpcStatus`.I'm still thinking that both use cases are different and would need different classes.In any case, Im not block this PR for this small thing.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,354458702,2019-12-05T17:50:41Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,331 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:","The case that I worry about is someone who is used to, e.g., using `await call.metadata()` does `await error.metadata()`. The error would like like```TypeError: object dict can't be used in 'await' expression```I suppose this is a pretty clear indication that you shouldn't be calling `await`.",
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/21385,354552332,2019-12-05T21:19:24Z,src/cpp/client/generic_stub.cc,"@@ -59,6 +59,10 @@ GenericStub::PrepareUnaryCall(grpc::ClientContext* context,                               const grpc::string& method,                               const grpc::ByteBuffer& request,                               CompletionQueue* cq) {+  // Unary rpcs must send a payload+  if (!request.Valid()) {+    return nullptr;","@vjpai Originally, I thought it was only a misuse to send an invalid byte buffer in the unary case, since streaming is allowed to send ""no message"" ~~and an invalid byte buffer is equivalent to ""no message"".~~ But your comment prompted me to think about how it should probably be invalid to do a streaming Write with no message... so yeah, that would imply that we should assert that we're not sending invalid byte buffers for Send messages in general.However, I think I need to discuss with you further whether that responsibility should be in call_op_set.h or elsewhere. Currently, we have:https://github.com/grpc/grpc/blob/bebd20b126303a1f5d6cbb31d0463460b59c5074/include/grpcpp/impl/codegen/call_op_set.h#L312-L313The invalidity of the ByteBuffer is currently part of how we determine if a Send op is needed.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354574439,2019-12-05T22:13:13Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,335 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation_future: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation_future = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Virtual cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation_future, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation_future.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:+        pass -    async def initial_metadata(self):-        raise NotImplementedError()+    def is_active(self) -> bool:+        return self.done() -    async def trailing_metadata(self):-        raise NotImplementedError()+    def time_remaining(self) -> float:+        pass -    async def code(self) -> grpc.StatusCode:-        """"""Returns the `grpc.StatusCode` if the RPC is finished,-        otherwise first waits until the RPC finishes.+    async def initial_metadata(self) -> MetadataType:+        return await self._initial_metadata -        Returns:-          The `grpc.StatusCode` status code.-        """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+    async def trailing_metadata(self) -> MetadataType:+        return (await self._status).trailing_metadata() +    async def code(self) -> grpc.StatusCode:+        await self._status         return self._code      async def details(self) -> str:-        """"""Returns the details if the RPC is finished, otherwise first waits till the-        RPC finishes.+        return (await self._status).details() -        Returns:-          The details.+    async def debug_error_string(self) -> str:+        return (await self._status).debug_error_string()++    def _set_initial_metadata(self, metadata: MetadataType) -> None:+        self._initial_metadata.set_result(metadata)++    def _set_status(self, status: cygrpc.AioRpcStatus) -> None:","Personally, I found callback is slightly more decoupled than using `Future`. Exposing a callback function to lower layer, the lower layer logic can only interact with it in the only way (calling it). On the other hand, if we expose the future interface, our lower layer logic are going to have a dozen more methods to interact with the object.To keep it simple and prevent our future error, I still want to speak for callback pattern here.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21394,354586953,2019-12-05T22:46:29Z,src/core/ext/filters/client_channel/xds/xds_api.cc,"@@ -105,56 +106,59 @@ bool XdsDropConfig::ShouldDrop( namespace {  void PopulateMetadataValue(upb_arena* arena, google_protobuf_Value* value_pb,-                           const XdsBootstrap::MetadataValue& value);+                           const Json& value);",Probably a dumb question: `PopulateMetadataValue` seems to be defined outside the anonymous namespace. Why would the compiler be happy with declaring it here?,
41815,tmatsuo,https://api.github.com/repos/grpc/grpc/pulls/21324,354600133,2019-12-05T23:28:02Z,src/core/lib/security/credentials/oauth2/oauth2_credentials.h,"@@ -90,6 +92,10 @@ class grpc_oauth2_token_fetcher_credentials : public grpc_call_credentials {                             grpc_httpcli_context* httpcli_context,                             grpc_polling_entity* pollent, grpc_iomgr_cb_func cb,                             grpc_millis deadline) = 0;+  virtual void add_additional_metadata(grpc_credentials_mdelem_array*) {",Making it a pure virtual means that we need to have implementation on every sub classes. Maybe we want to have default implementation as it has?,
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/21375,354602711,2019-12-05T23:37:07Z,include/grpcpp/alts_context.h,"@@ -0,0 +1,75 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPCPP_ALTS_CONTEXT_H+#define GRPCPP_ALTS_CONTEXT_H++#include <grpcpp/impl/codegen/security/auth_context.h>++#include <memory>+// might need forward declaration+#include ""src/core/tsi/alts/handshaker/alts_tsi_handshaker.h""+#include ""src/proto/grpc/gcp/altscontext.upb.h""++namespace grpc {++enum SecurityLevel {+  SECURITY_NONE = 0,+  INTEGRITY_ONLY = 1,+  INTEGRITY_AND_PRIVACY = 2+};++typedef struct Versions {+  int major_version;+  int minor_version;+} Versions;++typedef struct RpcProtocolVersions {+  Versions max_rpc_versions;+  Versions min_rpc_versions;+} RpcProtocolVersions;++class AltsContext {+ public:+  explicit AltsContext(const AuthContext& auth_context);","Given AuthContext, it does not necessarily have AltsContext. My suggestion would be1. AltsContext class takes grpc_gcp_AltsContext as constructor2. Use ```std::unique_ptr<AltsContext> GetAltsContextFromAuthContext(    const AuthContext& auth_context);```If there is AltsContext inside AuthContext, it will return a unique_ptr on the object, otherwise, return nullptr",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,354628169,2019-12-06T01:24:22Z,src/python/grpcio_tests/tests_aio/unit/_test_base.py,"@@ -12,18 +12,54 @@ # See the License for the specific language governing permissions and # limitations under the License. +import logging+import functools import asyncio+from typing import Callable import unittest from grpc.experimental import aio +__all__ = 'AioTestBase' -class AioTestBase(unittest.TestCase):+_COROUTINE_FUNCTION_ALLOWLIST = ['setUp', 'tearDown']+++def _async_to_sync_decorator(f: Callable, loop: asyncio.AbstractEventLoop):++    @functools.wraps(f)+    def wrapper(*args, **kwargs):+        return loop.run_until_complete(f(*args, **kwargs))++    return wrapper+++def _get_default_loop(debug=True):+    try:+        loop = asyncio.get_event_loop()+    except:+        loop = asyncio.new_event_loop()+        asyncio.set_event_loop(loop)+    finally:+        loop.set_debug(debug)+        return loop -    def setUp(self):-        self._loop = asyncio.new_event_loop()-        asyncio.set_event_loop(self._loop)-        aio.init_grpc_aio()++class AioTestBase(unittest.TestCase):      @property     def loop(self):-        return self._loop+        return _get_default_loop()++    def __getattribute__(self, name):+        """"""Overrides the loading logic to support coroutine functions.""""""+        attr = super().__getattribute__(name)++        # If possible, converts the coroutine into a sync function.+        if name.startswith('test_') or name in _COROUTINE_FUNCTION_ALLOWLIST:",Nit: I think a [metaclass](https://docs.python.org/3/reference/datamodel.html#metaclasses) would be a slightly more conventional way of implementing this. But this way is also fine.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,354630638,2019-12-06T01:36:48Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -11,186 +11,296 @@ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.+""""""Tests behavior of the grpc.aio.UnaryUnaryCall class.""""""+ import asyncio import logging import unittest+import datetime  import grpc  from grpc.experimental import aio from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc from tests.unit.framework.common import test_constants from tests_aio.unit._test_server import start_test_server from tests_aio.unit._test_base import AioTestBase --class TestAioRpcError(unittest.TestCase):-    _TEST_INITIAL_METADATA = (""initial metadata"",)-    _TEST_TRAILING_METADATA = (""trailing metadata"",)--    def test_attributes(self):-        aio_rpc_error = aio.AioRpcError(-            grpc.StatusCode.CANCELLED,-            ""details"",-            initial_metadata=self._TEST_INITIAL_METADATA,-            trailing_metadata=self._TEST_TRAILING_METADATA)-        self.assertEqual(aio_rpc_error.code(), grpc.StatusCode.CANCELLED)-        self.assertEqual(aio_rpc_error.details(), ""details"")-        self.assertEqual(aio_rpc_error.initial_metadata(),-                         self._TEST_INITIAL_METADATA)-        self.assertEqual(aio_rpc_error.trailing_metadata(),-                         self._TEST_TRAILING_METADATA)---class TestCall(AioTestBase):--    def test_call_ok(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.done())--                response = await call--                self.assertTrue(call.done())-                self.assertEqual(type(response), messages_pb2.SimpleResponse)-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--                # Response is cached at call object level, reentrance-                # returns again the same response-                response_retry = await call-                self.assertIs(response, response_retry)--        self.loop.run_until_complete(coro())--    def test_call_rpc_error(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                empty_call_with_sleep = channel.unary_unary(-                    ""/grpc.testing.TestService/EmptyCall"",-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.-                    FromString,-                )-                timeout = test_constants.SHORT_TIMEOUT / 2-                # TODO(https://github.com/grpc/grpc/issues/20869-                # Update once the async server is ready, change the-                # synchronization mechanism by removing the sleep(<timeout>)-                # as both components (client & server) will be on the same-                # process.-                call = empty_call_with_sleep(-                    messages_pb2.SimpleRequest(), timeout=timeout)--                with self.assertRaises(grpc.RpcError) as exception_context:-                    await call--                self.assertTrue(call.done())-                self.assertEqual(await call.code(),-                                 grpc.StatusCode.DEADLINE_EXCEEDED)--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        grpc.RpcError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())--    def test_call_code_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--        self.loop.run_until_complete(coro())--    def test_call_details_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.details(), None)--        self.loop.run_until_complete(coro())--    def test_cancel(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.cancelled())--                # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.-                # Force the loop to execute the RPC task.-                await asyncio.sleep(0)--                self.assertTrue(call.cancel())-                self.assertTrue(call.cancelled())-                self.assertFalse(call.cancel())--                with self.assertRaises(-                        asyncio.CancelledError) as exception_context:-                    await call--                self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)-                self.assertEqual(await call.details(),-                                 'Locally cancelled by application!')--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        asyncio.CancelledError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())+_NUM_STREAM_RESPONSES = 5+_RESPONSE_PAYLOAD_SIZE = 42+_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+_RESPONSE_INTERVAL_US = test_constants.SHORT_TIMEOUT * 1000 * 1000+++class TestUnaryUnaryCall(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_call_ok(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.done())++            response = await call++            self.assertTrue(call.done())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++            # Response is cached at call object level, reentrance+            # returns again the same response+            response_retry = await call+            self.assertIs(response, response_retry)++    async def test_call_rpc_error(self):+        async with aio.insecure_channel(self._server_target) as channel:+            empty_call_with_sleep = channel.unary_unary(+                ""/grpc.testing.TestService/EmptyCall"",+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString,+            )+            timeout = test_constants.SHORT_TIMEOUT / 2+            # TODO(https://github.com/grpc/grpc/issues/20869+            # Update once the async server is ready, change the+            # synchronization mechanism by removing the sleep(<timeout>)+            # as both components (client & server) will be on the same+            # process.+            call = empty_call_with_sleep(+                messages_pb2.SimpleRequest(), timeout=timeout)++            with self.assertRaises(grpc.RpcError) as exception_context:+                await call+",Can we make any assertions about the contents of the exception?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,354632080,2019-12-06T01:43:34Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -11,186 +11,296 @@ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.+""""""Tests behavior of the grpc.aio.UnaryUnaryCall class.""""""+ import asyncio import logging import unittest+import datetime  import grpc  from grpc.experimental import aio from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc from tests.unit.framework.common import test_constants from tests_aio.unit._test_server import start_test_server from tests_aio.unit._test_base import AioTestBase --class TestAioRpcError(unittest.TestCase):-    _TEST_INITIAL_METADATA = (""initial metadata"",)-    _TEST_TRAILING_METADATA = (""trailing metadata"",)--    def test_attributes(self):-        aio_rpc_error = aio.AioRpcError(-            grpc.StatusCode.CANCELLED,-            ""details"",-            initial_metadata=self._TEST_INITIAL_METADATA,-            trailing_metadata=self._TEST_TRAILING_METADATA)-        self.assertEqual(aio_rpc_error.code(), grpc.StatusCode.CANCELLED)-        self.assertEqual(aio_rpc_error.details(), ""details"")-        self.assertEqual(aio_rpc_error.initial_metadata(),-                         self._TEST_INITIAL_METADATA)-        self.assertEqual(aio_rpc_error.trailing_metadata(),-                         self._TEST_TRAILING_METADATA)---class TestCall(AioTestBase):--    def test_call_ok(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.done())--                response = await call--                self.assertTrue(call.done())-                self.assertEqual(type(response), messages_pb2.SimpleResponse)-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--                # Response is cached at call object level, reentrance-                # returns again the same response-                response_retry = await call-                self.assertIs(response, response_retry)--        self.loop.run_until_complete(coro())--    def test_call_rpc_error(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                empty_call_with_sleep = channel.unary_unary(-                    ""/grpc.testing.TestService/EmptyCall"",-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.-                    FromString,-                )-                timeout = test_constants.SHORT_TIMEOUT / 2-                # TODO(https://github.com/grpc/grpc/issues/20869-                # Update once the async server is ready, change the-                # synchronization mechanism by removing the sleep(<timeout>)-                # as both components (client & server) will be on the same-                # process.-                call = empty_call_with_sleep(-                    messages_pb2.SimpleRequest(), timeout=timeout)--                with self.assertRaises(grpc.RpcError) as exception_context:-                    await call--                self.assertTrue(call.done())-                self.assertEqual(await call.code(),-                                 grpc.StatusCode.DEADLINE_EXCEEDED)--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        grpc.RpcError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())--    def test_call_code_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--        self.loop.run_until_complete(coro())--    def test_call_details_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.details(), None)--        self.loop.run_until_complete(coro())--    def test_cancel(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.cancelled())--                # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.-                # Force the loop to execute the RPC task.-                await asyncio.sleep(0)--                self.assertTrue(call.cancel())-                self.assertTrue(call.cancelled())-                self.assertFalse(call.cancel())--                with self.assertRaises(-                        asyncio.CancelledError) as exception_context:-                    await call--                self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)-                self.assertEqual(await call.details(),-                                 'Locally cancelled by application!')--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        asyncio.CancelledError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())+_NUM_STREAM_RESPONSES = 5+_RESPONSE_PAYLOAD_SIZE = 42+_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+_RESPONSE_INTERVAL_US = test_constants.SHORT_TIMEOUT * 1000 * 1000+++class TestUnaryUnaryCall(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_call_ok(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.done())++            response = await call++            self.assertTrue(call.done())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++            # Response is cached at call object level, reentrance+            # returns again the same response+            response_retry = await call+            self.assertIs(response, response_retry)++    async def test_call_rpc_error(self):+        async with aio.insecure_channel(self._server_target) as channel:+            empty_call_with_sleep = channel.unary_unary(+                ""/grpc.testing.TestService/EmptyCall"",+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString,+            )+            timeout = test_constants.SHORT_TIMEOUT / 2+            # TODO(https://github.com/grpc/grpc/issues/20869+            # Update once the async server is ready, change the+            # synchronization mechanism by removing the sleep(<timeout>)+            # as both components (client & server) will be on the same+            # process.+            call = empty_call_with_sleep(+                messages_pb2.SimpleRequest(), timeout=timeout)++            with self.assertRaises(grpc.RpcError) as exception_context:+                await call++            self.assertTrue(call.done())+            self.assertEqual(await call.code(),+                             grpc.StatusCode.DEADLINE_EXCEEDED)++            # Exception is cached at call object level, reentrance+            # returns again the same exception+            with self.assertRaises(grpc.RpcError) as exception_context_retry:+                await call++            self.assertIs(exception_context.exception,+                          exception_context_retry.exception)++    async def test_call_code_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++    async def test_call_details_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual('', await call.details())++    async def test_cancel_unary_unary(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.cancelled())++            # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.+            # Force the loop to execute the RPC task.+            await asyncio.sleep(0)++            self.assertTrue(call.cancel())+            self.assertFalse(call.cancel())++            with self.assertRaises(asyncio.CancelledError) as exception_context:+                await call++            self.assertTrue(call.cancelled())+            self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+            self.assertEqual(await call.details(),+                             'Locally cancelled by application!')++            # NOTE(lidiz) The CancelledError is almost always re-created,+            # so we might not want to use it to transmit data.+            # https://github.com/python/cpython/blob/master/Lib/asyncio/tasks.py#L785",Please use a permalink when linking to a file on Github. The link should contain a commit hash instead of a branch name. This will keep the comment from turning into a dead link.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,354632830,2019-12-06T01:47:14Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -11,186 +11,296 @@ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.+""""""Tests behavior of the grpc.aio.UnaryUnaryCall class.""""""+ import asyncio import logging import unittest+import datetime  import grpc  from grpc.experimental import aio from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc from tests.unit.framework.common import test_constants from tests_aio.unit._test_server import start_test_server from tests_aio.unit._test_base import AioTestBase --class TestAioRpcError(unittest.TestCase):-    _TEST_INITIAL_METADATA = (""initial metadata"",)-    _TEST_TRAILING_METADATA = (""trailing metadata"",)--    def test_attributes(self):-        aio_rpc_error = aio.AioRpcError(-            grpc.StatusCode.CANCELLED,-            ""details"",-            initial_metadata=self._TEST_INITIAL_METADATA,-            trailing_metadata=self._TEST_TRAILING_METADATA)-        self.assertEqual(aio_rpc_error.code(), grpc.StatusCode.CANCELLED)-        self.assertEqual(aio_rpc_error.details(), ""details"")-        self.assertEqual(aio_rpc_error.initial_metadata(),-                         self._TEST_INITIAL_METADATA)-        self.assertEqual(aio_rpc_error.trailing_metadata(),-                         self._TEST_TRAILING_METADATA)---class TestCall(AioTestBase):--    def test_call_ok(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.done())--                response = await call--                self.assertTrue(call.done())-                self.assertEqual(type(response), messages_pb2.SimpleResponse)-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--                # Response is cached at call object level, reentrance-                # returns again the same response-                response_retry = await call-                self.assertIs(response, response_retry)--        self.loop.run_until_complete(coro())--    def test_call_rpc_error(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                empty_call_with_sleep = channel.unary_unary(-                    ""/grpc.testing.TestService/EmptyCall"",-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.-                    FromString,-                )-                timeout = test_constants.SHORT_TIMEOUT / 2-                # TODO(https://github.com/grpc/grpc/issues/20869-                # Update once the async server is ready, change the-                # synchronization mechanism by removing the sleep(<timeout>)-                # as both components (client & server) will be on the same-                # process.-                call = empty_call_with_sleep(-                    messages_pb2.SimpleRequest(), timeout=timeout)--                with self.assertRaises(grpc.RpcError) as exception_context:-                    await call--                self.assertTrue(call.done())-                self.assertEqual(await call.code(),-                                 grpc.StatusCode.DEADLINE_EXCEEDED)--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        grpc.RpcError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())--    def test_call_code_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--        self.loop.run_until_complete(coro())--    def test_call_details_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.details(), None)--        self.loop.run_until_complete(coro())--    def test_cancel(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.cancelled())--                # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.-                # Force the loop to execute the RPC task.-                await asyncio.sleep(0)--                self.assertTrue(call.cancel())-                self.assertTrue(call.cancelled())-                self.assertFalse(call.cancel())--                with self.assertRaises(-                        asyncio.CancelledError) as exception_context:-                    await call--                self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)-                self.assertEqual(await call.details(),-                                 'Locally cancelled by application!')--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        asyncio.CancelledError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())+_NUM_STREAM_RESPONSES = 5+_RESPONSE_PAYLOAD_SIZE = 42+_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+_RESPONSE_INTERVAL_US = test_constants.SHORT_TIMEOUT * 1000 * 1000+++class TestUnaryUnaryCall(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_call_ok(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.done())++            response = await call++            self.assertTrue(call.done())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++            # Response is cached at call object level, reentrance+            # returns again the same response+            response_retry = await call+            self.assertIs(response, response_retry)++    async def test_call_rpc_error(self):+        async with aio.insecure_channel(self._server_target) as channel:+            empty_call_with_sleep = channel.unary_unary(+                ""/grpc.testing.TestService/EmptyCall"",+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString,+            )+            timeout = test_constants.SHORT_TIMEOUT / 2+            # TODO(https://github.com/grpc/grpc/issues/20869+            # Update once the async server is ready, change the+            # synchronization mechanism by removing the sleep(<timeout>)+            # as both components (client & server) will be on the same+            # process.+            call = empty_call_with_sleep(+                messages_pb2.SimpleRequest(), timeout=timeout)++            with self.assertRaises(grpc.RpcError) as exception_context:+                await call++            self.assertTrue(call.done())+            self.assertEqual(await call.code(),+                             grpc.StatusCode.DEADLINE_EXCEEDED)++            # Exception is cached at call object level, reentrance+            # returns again the same exception+            with self.assertRaises(grpc.RpcError) as exception_context_retry:+                await call++            self.assertIs(exception_context.exception,+                          exception_context_retry.exception)++    async def test_call_code_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++    async def test_call_details_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual('', await call.details())++    async def test_cancel_unary_unary(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.cancelled())++            # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.+            # Force the loop to execute the RPC task.+            await asyncio.sleep(0)++            self.assertTrue(call.cancel())+            self.assertFalse(call.cancel())++            with self.assertRaises(asyncio.CancelledError) as exception_context:+                await call++            self.assertTrue(call.cancelled())+            self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+            self.assertEqual(await call.details(),+                             'Locally cancelled by application!')++            # NOTE(lidiz) The CancelledError is almost always re-created,+            # so we might not want to use it to transmit data.+            # https://github.com/python/cpython/blob/master/Lib/asyncio/tasks.py#L785+++class TestUnaryStreamCall(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)+","I only see non-sunny-day cases in this class. Can you add a test without cancellation or error that verifies all applicable call methods, e.g. `initial_metadata` and `trailing_metadata`?Edit: I see now that they're in `test_channel.py`. It seems there's some overlap in what these two files are testing. What is the organizational division between these two files in your mind?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,354635330,2019-12-06T01:59:05Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -58,63 +61,70 @@ def test_unary_unary(self):              self.assertIs(type(response), messages_pb2.SimpleResponse) -            await channel.close()--        self.loop.run_until_complete(coro())--    def test_unary_call_times_out(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                empty_call_with_sleep = channel.unary_unary(-                    _EMPTY_CALL_METHOD,-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.-                    FromString,-                )-                timeout = test_constants.SHORT_TIMEOUT / 2-                # TODO(https://github.com/grpc/grpc/issues/20869)-                # Update once the async server is ready, change the-                # synchronization mechanism by removing the sleep(<timeout>)-                # as both components (client & server) will be on the same-                # process.-                with self.assertRaises(grpc.RpcError) as exception_context:-                    await empty_call_with_sleep(-                        messages_pb2.SimpleRequest(), timeout=timeout)--                _, details = grpc.StatusCode.DEADLINE_EXCEEDED.value  # pylint: disable=unused-variable-                self.assertEqual(exception_context.exception.code(),-                                 grpc.StatusCode.DEADLINE_EXCEEDED)-                self.assertEqual(exception_context.exception.details(),-                                 details.title())-                self.assertIsNotNone(-                    exception_context.exception.initial_metadata())-                self.assertIsNotNone(-                    exception_context.exception.trailing_metadata())--        self.loop.run_until_complete(coro())+    async def test_unary_call_times_out(self):+        async with aio.insecure_channel(self._server_target) as channel:+            empty_call_with_sleep = channel.unary_unary(+                _EMPTY_CALL_METHOD,+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString,+            )+            timeout = test_constants.SHORT_TIMEOUT / 2+            # TODO(https://github.com/grpc/grpc/issues/20869)+            # Update once the async server is ready, change the+            # synchronization mechanism by removing the sleep(<timeout>)+            # as both components (client & server) will be on the same+            # process.+            with self.assertRaises(grpc.RpcError) as exception_context:+                await empty_call_with_sleep(+                    messages_pb2.SimpleRequest(), timeout=timeout)++            _, details = grpc.StatusCode.DEADLINE_EXCEEDED.value  # pylint: disable=unused-variable+            self.assertEqual(grpc.StatusCode.DEADLINE_EXCEEDED,+                             exception_context.exception.code())+            self.assertEqual(details.title(),+                             exception_context.exception.details())+            self.assertIsNotNone(exception_context.exception.initial_metadata())+            self.assertIsNotNone(+                exception_context.exception.trailing_metadata())      @unittest.skip('https://github.com/grpc/grpc/issues/20818')-    def test_call_to_the_void(self):+    async def test_call_to_the_void(self):+        channel = aio.insecure_channel('0.1.1.1:1111')+        hi = channel.unary_unary(+            _UNARY_CALL_METHOD,+            request_serializer=messages_pb2.SimpleRequest.SerializeToString,+            response_deserializer=messages_pb2.SimpleResponse.FromString)+        response = await hi(messages_pb2.SimpleRequest()) -        async def coro():-            channel = aio.insecure_channel('0.1.1.1:1111')-            hi = channel.unary_unary(-                _UNARY_CALL_METHOD,-                request_serializer=messages_pb2.SimpleRequest.SerializeToString,-                response_deserializer=messages_pb2.SimpleResponse.FromString)-            response = await hi(messages_pb2.SimpleRequest())+        self.assertIs(type(response), messages_pb2.SimpleResponse) -            self.assertIs(type(response), messages_pb2.SimpleResponse)+        await channel.close()++    async def test_unary_stream(self):+        channel = aio.insecure_channel(self._server_target)+        stub = test_pb2_grpc.TestServiceStub(channel)++        # Prepares the request+        request = messages_pb2.StreamingOutputCallRequest()+        for _ in range(_NUM_STREAM_RESPONSES):+            request.response_parameters.append(+                messages_pb2.ResponseParameters(size=_RESPONSE_PAYLOAD_SIZE))++        # Invokes the actual RPC+        call = stub.StreamingOutputCall(request) -            await channel.close()+        # Validates the responses+        response_cnt = 0+        async for response in call:+            response_cnt += 1+            self.assertIs(+                type(response), messages_pb2.StreamingOutputCallResponse)+            self.assertEqual(_RESPONSE_PAYLOAD_SIZE, len(response.payload.body)) -        self.loop.run_until_complete(coro())+        self.assertEqual(_NUM_STREAM_RESPONSES, response_cnt)",Any assertions about `code()`? `initial_metadata()`? `trailing_metadata()`?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,354636171,2019-12-06T02:03:18Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -43,18 +46,32 @@ def __init__(self):     async def _block_forever(self, unused_request, unused_context):         await asyncio.get_event_loop().create_future() -    async def _BLOCK_BRIEFLY(self, unused_request, unused_context):+    async def _block_briefly(self, unused_request, unused_context):         await asyncio.sleep(test_constants.SHORT_TIMEOUT / 2)         return _RESPONSE +    async def _unary_stream_async_gen(self, unused_request, unused_context):+        for _ in range(_NUM_STREAM_RESPONSES):+            yield _RESPONSE++    async def _unary_stream_reader_writer(self, unused_request, context):",Can we test that there's an exception when you try to mix the async generator style with the `read`/`write` style?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354643561,2019-12-06T02:40:59Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,336 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Placeholder cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:+        pass -    async def initial_metadata(self):-        raise NotImplementedError()+    def is_active(self) -> bool:+        return self.done()","Removed as we introducing `grpc.aio.RpcContext`, it is duplicated with `done()`.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354644430,2019-12-06T02:45:40Z,src/python/grpcio_tests/tests_aio/unit/_test_base.py,"@@ -12,18 +12,54 @@ # See the License for the specific language governing permissions and # limitations under the License. +import logging+import functools import asyncio+from typing import Callable import unittest from grpc.experimental import aio +__all__ = 'AioTestBase' -class AioTestBase(unittest.TestCase):+_COROUTINE_FUNCTION_ALLOWLIST = ['setUp', 'tearDown']+++def _async_to_sync_decorator(f: Callable, loop: asyncio.AbstractEventLoop):++    @functools.wraps(f)+    def wrapper(*args, **kwargs):+        return loop.run_until_complete(f(*args, **kwargs))++    return wrapper+++def _get_default_loop(debug=True):+    try:+        loop = asyncio.get_event_loop()+    except:+        loop = asyncio.new_event_loop()+        asyncio.set_event_loop(loop)+    finally:+        loop.set_debug(debug)+        return loop -    def setUp(self):-        self._loop = asyncio.new_event_loop()-        asyncio.set_event_loop(self._loop)-        aio.init_grpc_aio()++class AioTestBase(unittest.TestCase):      @property     def loop(self):-        return self._loop+        return _get_default_loop()++    def __getattribute__(self, name):+        """"""Overrides the loading logic to support coroutine functions.""""""+        attr = super().__getattribute__(name)++        # If possible, converts the coroutine into a sync function.+        if name.startswith('test_') or name in _COROUTINE_FUNCTION_ALLOWLIST:","Added a line of note: `# NOTE(gnossen) this test class can also be implemented with metaclass.`Yes, it could be done with metaclass, I feel `__getattribute__` is easier to read with slight performance overhead. This PR is pretty stable with this implementation, we could change it in another PR.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354645154,2019-12-06T02:49:52Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -11,186 +11,296 @@ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.+""""""Tests behavior of the grpc.aio.UnaryUnaryCall class.""""""+ import asyncio import logging import unittest+import datetime  import grpc  from grpc.experimental import aio from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc from tests.unit.framework.common import test_constants from tests_aio.unit._test_server import start_test_server from tests_aio.unit._test_base import AioTestBase --class TestAioRpcError(unittest.TestCase):-    _TEST_INITIAL_METADATA = (""initial metadata"",)-    _TEST_TRAILING_METADATA = (""trailing metadata"",)--    def test_attributes(self):-        aio_rpc_error = aio.AioRpcError(-            grpc.StatusCode.CANCELLED,-            ""details"",-            initial_metadata=self._TEST_INITIAL_METADATA,-            trailing_metadata=self._TEST_TRAILING_METADATA)-        self.assertEqual(aio_rpc_error.code(), grpc.StatusCode.CANCELLED)-        self.assertEqual(aio_rpc_error.details(), ""details"")-        self.assertEqual(aio_rpc_error.initial_metadata(),-                         self._TEST_INITIAL_METADATA)-        self.assertEqual(aio_rpc_error.trailing_metadata(),-                         self._TEST_TRAILING_METADATA)---class TestCall(AioTestBase):--    def test_call_ok(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.done())--                response = await call--                self.assertTrue(call.done())-                self.assertEqual(type(response), messages_pb2.SimpleResponse)-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--                # Response is cached at call object level, reentrance-                # returns again the same response-                response_retry = await call-                self.assertIs(response, response_retry)--        self.loop.run_until_complete(coro())--    def test_call_rpc_error(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                empty_call_with_sleep = channel.unary_unary(-                    ""/grpc.testing.TestService/EmptyCall"",-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.-                    FromString,-                )-                timeout = test_constants.SHORT_TIMEOUT / 2-                # TODO(https://github.com/grpc/grpc/issues/20869-                # Update once the async server is ready, change the-                # synchronization mechanism by removing the sleep(<timeout>)-                # as both components (client & server) will be on the same-                # process.-                call = empty_call_with_sleep(-                    messages_pb2.SimpleRequest(), timeout=timeout)--                with self.assertRaises(grpc.RpcError) as exception_context:-                    await call--                self.assertTrue(call.done())-                self.assertEqual(await call.code(),-                                 grpc.StatusCode.DEADLINE_EXCEEDED)--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        grpc.RpcError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())--    def test_call_code_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--        self.loop.run_until_complete(coro())--    def test_call_details_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.details(), None)--        self.loop.run_until_complete(coro())--    def test_cancel(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.cancelled())--                # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.-                # Force the loop to execute the RPC task.-                await asyncio.sleep(0)--                self.assertTrue(call.cancel())-                self.assertTrue(call.cancelled())-                self.assertFalse(call.cancel())--                with self.assertRaises(-                        asyncio.CancelledError) as exception_context:-                    await call--                self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)-                self.assertEqual(await call.details(),-                                 'Locally cancelled by application!')--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        asyncio.CancelledError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())+_NUM_STREAM_RESPONSES = 5+_RESPONSE_PAYLOAD_SIZE = 42+_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+_RESPONSE_INTERVAL_US = test_constants.SHORT_TIMEOUT * 1000 * 1000+++class TestUnaryUnaryCall(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_call_ok(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.done())",`is_active` is removed due to duplication with `done`.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354646355,2019-12-06T02:56:22Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -11,186 +11,296 @@ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.+""""""Tests behavior of the grpc.aio.UnaryUnaryCall class.""""""+ import asyncio import logging import unittest+import datetime  import grpc  from grpc.experimental import aio from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc from tests.unit.framework.common import test_constants from tests_aio.unit._test_server import start_test_server from tests_aio.unit._test_base import AioTestBase --class TestAioRpcError(unittest.TestCase):-    _TEST_INITIAL_METADATA = (""initial metadata"",)-    _TEST_TRAILING_METADATA = (""trailing metadata"",)--    def test_attributes(self):-        aio_rpc_error = aio.AioRpcError(-            grpc.StatusCode.CANCELLED,-            ""details"",-            initial_metadata=self._TEST_INITIAL_METADATA,-            trailing_metadata=self._TEST_TRAILING_METADATA)-        self.assertEqual(aio_rpc_error.code(), grpc.StatusCode.CANCELLED)-        self.assertEqual(aio_rpc_error.details(), ""details"")-        self.assertEqual(aio_rpc_error.initial_metadata(),-                         self._TEST_INITIAL_METADATA)-        self.assertEqual(aio_rpc_error.trailing_metadata(),-                         self._TEST_TRAILING_METADATA)---class TestCall(AioTestBase):--    def test_call_ok(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.done())--                response = await call--                self.assertTrue(call.done())-                self.assertEqual(type(response), messages_pb2.SimpleResponse)-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--                # Response is cached at call object level, reentrance-                # returns again the same response-                response_retry = await call-                self.assertIs(response, response_retry)--        self.loop.run_until_complete(coro())--    def test_call_rpc_error(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                empty_call_with_sleep = channel.unary_unary(-                    ""/grpc.testing.TestService/EmptyCall"",-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.-                    FromString,-                )-                timeout = test_constants.SHORT_TIMEOUT / 2-                # TODO(https://github.com/grpc/grpc/issues/20869-                # Update once the async server is ready, change the-                # synchronization mechanism by removing the sleep(<timeout>)-                # as both components (client & server) will be on the same-                # process.-                call = empty_call_with_sleep(-                    messages_pb2.SimpleRequest(), timeout=timeout)--                with self.assertRaises(grpc.RpcError) as exception_context:-                    await call--                self.assertTrue(call.done())-                self.assertEqual(await call.code(),-                                 grpc.StatusCode.DEADLINE_EXCEEDED)--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        grpc.RpcError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())--    def test_call_code_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--        self.loop.run_until_complete(coro())--    def test_call_details_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.details(), None)--        self.loop.run_until_complete(coro())--    def test_cancel(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.cancelled())--                # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.-                # Force the loop to execute the RPC task.-                await asyncio.sleep(0)--                self.assertTrue(call.cancel())-                self.assertTrue(call.cancelled())-                self.assertFalse(call.cancel())--                with self.assertRaises(-                        asyncio.CancelledError) as exception_context:-                    await call--                self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)-                self.assertEqual(await call.details(),-                                 'Locally cancelled by application!')--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        asyncio.CancelledError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())+_NUM_STREAM_RESPONSES = 5+_RESPONSE_PAYLOAD_SIZE = 42+_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+_RESPONSE_INTERVAL_US = test_constants.SHORT_TIMEOUT * 1000 * 1000+++class TestUnaryUnaryCall(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_call_ok(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.done())++            response = await call++            self.assertTrue(call.done())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++            # Response is cached at call object level, reentrance+            # returns again the same response+            response_retry = await call+            self.assertIs(response, response_retry)++    async def test_call_rpc_error(self):+        async with aio.insecure_channel(self._server_target) as channel:+            empty_call_with_sleep = channel.unary_unary(+                ""/grpc.testing.TestService/EmptyCall"",+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString,+            )+            timeout = test_constants.SHORT_TIMEOUT / 2+            # TODO(https://github.com/grpc/grpc/issues/20869+            # Update once the async server is ready, change the+            # synchronization mechanism by removing the sleep(<timeout>)+            # as both components (client & server) will be on the same+            # process.+            call = empty_call_with_sleep(+                messages_pb2.SimpleRequest(), timeout=timeout)++            with self.assertRaises(grpc.RpcError) as exception_context:+                await call++            self.assertTrue(call.done())+            self.assertEqual(await call.code(),+                             grpc.StatusCode.DEADLINE_EXCEEDED)++            # Exception is cached at call object level, reentrance+            # returns again the same exception+            with self.assertRaises(grpc.RpcError) as exception_context_retry:+                await call++            self.assertIs(exception_context.exception,+                          exception_context_retry.exception)++    async def test_call_code_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++    async def test_call_details_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual('', await call.details())++    async def test_cancel_unary_unary(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.cancelled())++            # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.+            # Force the loop to execute the RPC task.+            await asyncio.sleep(0)++            self.assertTrue(call.cancel())+            self.assertFalse(call.cancel())++            with self.assertRaises(asyncio.CancelledError) as exception_context:+                await call++            self.assertTrue(call.cancelled())+            self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+            self.assertEqual(await call.details(),+                             'Locally cancelled by application!')++            # NOTE(lidiz) The CancelledError is almost always re-created,+            # so we might not want to use it to transmit data.+            # https://github.com/python/cpython/blob/master/Lib/asyncio/tasks.py#L785",Changed to: https://github.com/python/cpython/blob/edad4d89e357c92f70c0324b937845d652b20afd/Lib/asyncio/tasks.py#L785,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354647061,2019-12-06T03:00:00Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -11,186 +11,296 @@ # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.+""""""Tests behavior of the grpc.aio.UnaryUnaryCall class.""""""+ import asyncio import logging import unittest+import datetime  import grpc  from grpc.experimental import aio from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc from tests.unit.framework.common import test_constants from tests_aio.unit._test_server import start_test_server from tests_aio.unit._test_base import AioTestBase --class TestAioRpcError(unittest.TestCase):-    _TEST_INITIAL_METADATA = (""initial metadata"",)-    _TEST_TRAILING_METADATA = (""trailing metadata"",)--    def test_attributes(self):-        aio_rpc_error = aio.AioRpcError(-            grpc.StatusCode.CANCELLED,-            ""details"",-            initial_metadata=self._TEST_INITIAL_METADATA,-            trailing_metadata=self._TEST_TRAILING_METADATA)-        self.assertEqual(aio_rpc_error.code(), grpc.StatusCode.CANCELLED)-        self.assertEqual(aio_rpc_error.details(), ""details"")-        self.assertEqual(aio_rpc_error.initial_metadata(),-                         self._TEST_INITIAL_METADATA)-        self.assertEqual(aio_rpc_error.trailing_metadata(),-                         self._TEST_TRAILING_METADATA)---class TestCall(AioTestBase):--    def test_call_ok(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.done())--                response = await call--                self.assertTrue(call.done())-                self.assertEqual(type(response), messages_pb2.SimpleResponse)-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--                # Response is cached at call object level, reentrance-                # returns again the same response-                response_retry = await call-                self.assertIs(response, response_retry)--        self.loop.run_until_complete(coro())--    def test_call_rpc_error(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                empty_call_with_sleep = channel.unary_unary(-                    ""/grpc.testing.TestService/EmptyCall"",-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.-                    FromString,-                )-                timeout = test_constants.SHORT_TIMEOUT / 2-                # TODO(https://github.com/grpc/grpc/issues/20869-                # Update once the async server is ready, change the-                # synchronization mechanism by removing the sleep(<timeout>)-                # as both components (client & server) will be on the same-                # process.-                call = empty_call_with_sleep(-                    messages_pb2.SimpleRequest(), timeout=timeout)--                with self.assertRaises(grpc.RpcError) as exception_context:-                    await call--                self.assertTrue(call.done())-                self.assertEqual(await call.code(),-                                 grpc.StatusCode.DEADLINE_EXCEEDED)--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        grpc.RpcError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())--    def test_call_code_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.code(), grpc.StatusCode.OK)--        self.loop.run_until_complete(coro())--    def test_call_details_awaitable(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())-                self.assertEqual(await call.details(), None)--        self.loop.run_until_complete(coro())--    def test_cancel(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                hi = channel.unary_unary(-                    '/grpc.testing.TestService/UnaryCall',-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.FromString-                )-                call = hi(messages_pb2.SimpleRequest())--                self.assertFalse(call.cancelled())--                # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.-                # Force the loop to execute the RPC task.-                await asyncio.sleep(0)--                self.assertTrue(call.cancel())-                self.assertTrue(call.cancelled())-                self.assertFalse(call.cancel())--                with self.assertRaises(-                        asyncio.CancelledError) as exception_context:-                    await call--                self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)-                self.assertEqual(await call.details(),-                                 'Locally cancelled by application!')--                # Exception is cached at call object level, reentrance-                # returns again the same exception-                with self.assertRaises(-                        asyncio.CancelledError) as exception_context_retry:-                    await call--                self.assertIs(exception_context.exception,-                              exception_context_retry.exception)--        self.loop.run_until_complete(coro())+_NUM_STREAM_RESPONSES = 5+_RESPONSE_PAYLOAD_SIZE = 42+_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+_RESPONSE_INTERVAL_US = test_constants.SHORT_TIMEOUT * 1000 * 1000+++class TestUnaryUnaryCall(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_call_ok(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.done())++            response = await call++            self.assertTrue(call.done())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++            # Response is cached at call object level, reentrance+            # returns again the same response+            response_retry = await call+            self.assertIs(response, response_retry)++    async def test_call_rpc_error(self):+        async with aio.insecure_channel(self._server_target) as channel:+            empty_call_with_sleep = channel.unary_unary(+                ""/grpc.testing.TestService/EmptyCall"",+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString,+            )+            timeout = test_constants.SHORT_TIMEOUT / 2+            # TODO(https://github.com/grpc/grpc/issues/20869+            # Update once the async server is ready, change the+            # synchronization mechanism by removing the sleep(<timeout>)+            # as both components (client & server) will be on the same+            # process.+            call = empty_call_with_sleep(+                messages_pb2.SimpleRequest(), timeout=timeout)++            with self.assertRaises(grpc.RpcError) as exception_context:+                await call++            self.assertTrue(call.done())+            self.assertEqual(await call.code(),+                             grpc.StatusCode.DEADLINE_EXCEEDED)++            # Exception is cached at call object level, reentrance+            # returns again the same exception+            with self.assertRaises(grpc.RpcError) as exception_context_retry:+                await call++            self.assertIs(exception_context.exception,+                          exception_context_retry.exception)++    async def test_call_code_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual(await call.code(), grpc.StatusCode.OK)++    async def test_call_details_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual('', await call.details())++    async def test_cancel_unary_unary(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())++            self.assertFalse(call.cancelled())++            # TODO(https://github.com/grpc/grpc/issues/20869) remove sleep.+            # Force the loop to execute the RPC task.+            await asyncio.sleep(0)++            self.assertTrue(call.cancel())+            self.assertFalse(call.cancel())++            with self.assertRaises(asyncio.CancelledError) as exception_context:+                await call++            self.assertTrue(call.cancelled())+            self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+            self.assertEqual(await call.details(),+                             'Locally cancelled by application!')++            # NOTE(lidiz) The CancelledError is almost always re-created,+            # so we might not want to use it to transmit data.+            # https://github.com/python/cpython/blob/master/Lib/asyncio/tasks.py#L785+++class TestUnaryStreamCall(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)+","For the tests I'm touching, I'm hoping the `channel_test` is testing on higher level behaviors, and `call_test` is testing the intricate behavior that we expect the call object to do.Metadata needs to be implemented in both the client side and server side. Otherwise, we are asserting blank stuffs.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354647941,2019-12-06T03:04:52Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -58,63 +61,70 @@ def test_unary_unary(self):              self.assertIs(type(response), messages_pb2.SimpleResponse) -            await channel.close()--        self.loop.run_until_complete(coro())--    def test_unary_call_times_out(self):--        async def coro():-            server_target, _ = await start_test_server()  # pylint: disable=unused-variable--            async with aio.insecure_channel(server_target) as channel:-                empty_call_with_sleep = channel.unary_unary(-                    _EMPTY_CALL_METHOD,-                    request_serializer=messages_pb2.SimpleRequest.-                    SerializeToString,-                    response_deserializer=messages_pb2.SimpleResponse.-                    FromString,-                )-                timeout = test_constants.SHORT_TIMEOUT / 2-                # TODO(https://github.com/grpc/grpc/issues/20869)-                # Update once the async server is ready, change the-                # synchronization mechanism by removing the sleep(<timeout>)-                # as both components (client & server) will be on the same-                # process.-                with self.assertRaises(grpc.RpcError) as exception_context:-                    await empty_call_with_sleep(-                        messages_pb2.SimpleRequest(), timeout=timeout)--                _, details = grpc.StatusCode.DEADLINE_EXCEEDED.value  # pylint: disable=unused-variable-                self.assertEqual(exception_context.exception.code(),-                                 grpc.StatusCode.DEADLINE_EXCEEDED)-                self.assertEqual(exception_context.exception.details(),-                                 details.title())-                self.assertIsNotNone(-                    exception_context.exception.initial_metadata())-                self.assertIsNotNone(-                    exception_context.exception.trailing_metadata())--        self.loop.run_until_complete(coro())+    async def test_unary_call_times_out(self):+        async with aio.insecure_channel(self._server_target) as channel:+            empty_call_with_sleep = channel.unary_unary(+                _EMPTY_CALL_METHOD,+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString,+            )+            timeout = test_constants.SHORT_TIMEOUT / 2+            # TODO(https://github.com/grpc/grpc/issues/20869)+            # Update once the async server is ready, change the+            # synchronization mechanism by removing the sleep(<timeout>)+            # as both components (client & server) will be on the same+            # process.+            with self.assertRaises(grpc.RpcError) as exception_context:+                await empty_call_with_sleep(+                    messages_pb2.SimpleRequest(), timeout=timeout)++            _, details = grpc.StatusCode.DEADLINE_EXCEEDED.value  # pylint: disable=unused-variable+            self.assertEqual(grpc.StatusCode.DEADLINE_EXCEEDED,+                             exception_context.exception.code())+            self.assertEqual(details.title(),+                             exception_context.exception.details())+            self.assertIsNotNone(exception_context.exception.initial_metadata())+            self.assertIsNotNone(+                exception_context.exception.trailing_metadata())      @unittest.skip('https://github.com/grpc/grpc/issues/20818')-    def test_call_to_the_void(self):+    async def test_call_to_the_void(self):+        channel = aio.insecure_channel('0.1.1.1:1111')+        hi = channel.unary_unary(+            _UNARY_CALL_METHOD,+            request_serializer=messages_pb2.SimpleRequest.SerializeToString,+            response_deserializer=messages_pb2.SimpleResponse.FromString)+        response = await hi(messages_pb2.SimpleRequest()) -        async def coro():-            channel = aio.insecure_channel('0.1.1.1:1111')-            hi = channel.unary_unary(-                _UNARY_CALL_METHOD,-                request_serializer=messages_pb2.SimpleRequest.SerializeToString,-                response_deserializer=messages_pb2.SimpleResponse.FromString)-            response = await hi(messages_pb2.SimpleRequest())+        self.assertIs(type(response), messages_pb2.SimpleResponse) -            self.assertIs(type(response), messages_pb2.SimpleResponse)+        await channel.close()++    async def test_unary_stream(self):+        channel = aio.insecure_channel(self._server_target)+        stub = test_pb2_grpc.TestServiceStub(channel)++        # Prepares the request+        request = messages_pb2.StreamingOutputCallRequest()+        for _ in range(_NUM_STREAM_RESPONSES):+            request.response_parameters.append(+                messages_pb2.ResponseParameters(size=_RESPONSE_PAYLOAD_SIZE))++        # Invokes the actual RPC+        call = stub.StreamingOutputCall(request) -            await channel.close()+        # Validates the responses+        response_cnt = 0+        async for response in call:+            response_cnt += 1+            self.assertIs(+                type(response), messages_pb2.StreamingOutputCallResponse)+            self.assertEqual(_RESPONSE_PAYLOAD_SIZE, len(response.payload.body)) -        self.loop.run_until_complete(coro())+        self.assertEqual(_NUM_STREAM_RESPONSES, response_cnt)","Added a assertion for `code()` to be `OK`. As for metadata, they are not implemented yet for both sides.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,354654278,2019-12-06T03:41:29Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,336 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Placeholder cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:+        pass -    async def initial_metadata(self):-        raise NotImplementedError()+    def is_active(self) -> bool:+        return self.done() -    async def trailing_metadata(self):-        raise NotImplementedError()+    def time_remaining(self) -> float:+        pass","This is kind of funny that if I raise `NotImplementedError` here, the smart PyLint will complain that I'm not implementing the abstract method:```src/python/grpcio/grpc/experimental/aio/_call.py:241:0: W0223: Method 'add_done_callback' is abstract in class 'Call' but is not overridden (abstract-method)src/python/grpcio/grpc/experimental/aio/_call.py:241:0: W0223: Method 'time_remaining' is abstract in class 'Call' but is not overridden (abstract-method)src/python/grpcio/grpc/experimental/aio/_call.py:323:0: W0223: Method 'add_done_callback' is abstract in class 'Call' but is not overridden (abstract-method)src/python/grpcio/grpc/experimental/aio/_call.py:323:0: W0223: Method 'time_remaining' is abstract in class 'Call' but is not overridden (abstract-method)```We still want to respect this error from PyLint, so I use `pass` here. Feel free to suggest a better solution.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21411,354884852,2019-12-06T15:23:01Z,cmake/cares.cmake,"@@ -28,10 +28,6 @@ if(gRPC_CARES_PROVIDER STREQUAL ""module"")     set(_gRPC_CARES_LIBRARIES c-ares)   endif() -  if(gRPC_INSTALL)-    message(WARNING ""gRPC_INSTALL will be forced to FALSE because gRPC_CARES_PROVIDER is \""module\"""")","Two concerns- I remember this check was added because we had trouble making the installation work when importing dependencies via add_subdirectory() (see https://github.com/grpc/grpc/pull/11140/files)- qq: If you install with the dependencies added via add_subdirectory(), isn't that an antipattern? It means that all the dependencies will be installed as part of gRPC (statically linked). Besides other things, it can influence the correctness of e.g. our pkgconfig files?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/21394,354932232,2019-12-06T16:56:50Z,src/core/ext/filters/client_channel/xds/xds_api.cc,"@@ -105,56 +106,59 @@ bool XdsDropConfig::ShouldDrop( namespace {  void PopulateMetadataValue(upb_arena* arena, google_protobuf_Value* value_pb,-                           const XdsBootstrap::MetadataValue& value);+                           const Json& value);","I don't see where `PopulateMetadataValue()` is declared outside of the anonymous namespace.  It looks to me like it's declared and defined in the anonymous namespace.Putting a function in an anonymous namespace is the same thing as declaring it `static`, but it's a more idiomatic C++ way of doing it.  (`static` is the old C style.)",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21394,354935179,2019-12-06T17:03:13Z,src/core/ext/filters/client_channel/xds/xds_api.cc,"@@ -105,56 +106,59 @@ bool XdsDropConfig::ShouldDrop( namespace {  void PopulateMetadataValue(upb_arena* arena, google_protobuf_Value* value_pb,-                           const XdsBootstrap::MetadataValue& value);+                           const Json& value);",Sorry my bad... Somehow I mismatched the braces.,
224720,zackgalbreath,https://api.github.com/repos/grpc/grpc/pulls/21411,354936205,2019-12-06T17:05:37Z,cmake/cares.cmake,"@@ -28,10 +28,6 @@ if(gRPC_CARES_PROVIDER STREQUAL ""module"")     set(_gRPC_CARES_LIBRARIES c-ares)   endif() -  if(gRPC_INSTALL)-    message(WARNING ""gRPC_INSTALL will be forced to FALSE because gRPC_CARES_PROVIDER is \""module\"""")","> * isn't that an antipattern?I wouldn't necessarily say so. A ""batteries included"" installation can be much more convenient for users.> * Besides other things, it can influence the correctness of e.g. our pkgconfig files?To test this out, I locally modified the pkgconfig distribtest to install a newer version of CMake and do an install-from-submodule. ""helloworld"" still built fine using the resulting pkgconfig files.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,355068163,2019-12-06T23:12:14Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,336 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Placeholder cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING+    def add_callback(self, unused_callback) -> None:+        pass -    async def initial_metadata(self):-        raise NotImplementedError()+    def is_active(self) -> bool:+        return self.done() -    async def trailing_metadata(self):-        raise NotImplementedError()+    def time_remaining(self) -> float:+        pass",I would prefer to suppress that linter error. Returning `None` for this method is simply incorrect behavior from the perspective of a user of the library.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,355068524,2019-12-06T23:14:02Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -43,18 +46,32 @@ def __init__(self):     async def _block_forever(self, unused_request, unused_context):         await asyncio.get_event_loop().create_future() -    async def _BLOCK_BRIEFLY(self, unused_request, unused_context):+    async def _block_briefly(self, unused_request, unused_context):         await asyncio.sleep(test_constants.SHORT_TIMEOUT / 2)         return _RESPONSE +    async def _unary_stream_async_gen(self, unused_request, unused_context):+        for _ in range(_NUM_STREAM_RESPONSES):+            yield _RESPONSE++    async def _unary_stream_reader_writer(self, unused_request, context):",Nice! Being able to mix the two styles is great as long we make sure to thoroughly test it.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,355082616,2019-12-07T00:31:16Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -57,93 +65,213 @@ cdef class _AioCall:             self._channel.cq.c_ptr(),             method_slice,             NULL,-            deadline,+            c_deadline,             NULL         )         grpc_slice_unref(method_slice)      cdef void _destroy_grpc_call(self):-        """"""Destroys the corresponding Core object for this RPC.""""""-        grpc_call_unref(self._grpc_call_wrapper.call)--    async def unary_unary(self, bytes method, bytes request, object timeout, AioCancelStatus cancel_status):-        cdef object loop = asyncio.get_event_loop()+        """"""Destroys the corresponding Core object for this RPC. -        cdef tuple operations-        cdef Operation initial_metadata_operation-        cdef Operation send_message_operation-        cdef Operation send_close_from_client_operation-        cdef Operation receive_initial_metadata_operation-        cdef Operation receive_message_operation-        cdef Operation receive_status_on_client_operation+        This method is idempotent. Multiple calls should not result in crashes.+        """"""+        if self._grpc_call_wrapper.call != NULL:+            grpc_call_unref(self._grpc_call_wrapper.call)+            self._grpc_call_wrapper.call = NULL -        cdef char *c_details = NULL+    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):+        """"""Cancels the RPC in C-Core, and return the final RPC status.""""""+        cdef AioRpcStatus status+        cdef object details+        cdef char *c_details+        # Try to fetch application layer cancellation details in the future.+        # * If calcellation details present, cancel with status;+        # * If details not present, cancel with unknown reason.+        if cancellation_future.done():+            status = cancellation_future.result()+            details = str_to_bytes(status.details())+            self._references.append(details)+            c_details = <char *>details+            # By implementation, grpc_call_cancel_with_status always return OK+            grpc_call_cancel_with_status(+                self._grpc_call_wrapper.call,+                status.c_code(),+                c_details,+                NULL,+            )+            return status+        else:+            # By implementation, grpc_call_cancel always return OK+            grpc_call_cancel(self._grpc_call_wrapper.call, NULL)+            status = AioRpcStatus(+                StatusCode.cancelled,+                _UNKNOWN_CANCELLATION_DETAILS,+                None,+                None,+            )+            cancellation_future.set_result(status)+            return status -        initial_metadata_operation = SendInitialMetadataOperation(_EMPTY_METADATA, GRPC_INITIAL_METADATA_USED_MASK)-        initial_metadata_operation.c()+    async def unary_unary(self,+                          bytes method,+                          bytes request,+                          object deadline,+                          object cancellation_future,+                          object initial_metadata_observer,+                          object status_observer):+        """"""Performs a unary unary RPC.+        +        Args:+          method: name of the calling method in bytes.+          request: the serialized requests in bytes.+          deadline: optional deadline of the RPC in float.+          cancellation_future: the future that meant to transport the+            cancellation reason from the application layer.+          initial_metadata_observer: a callback for received initial metadata.+          status_observer: a callback for received final status.+        """"""+        cdef tuple ops -        send_message_operation = SendMessageOperation(request, _EMPTY_FLAGS)-        send_message_operation.c()+        cdef SendInitialMetadataOperation initial_metadata_op = SendInitialMetadataOperation(+            _EMPTY_METADATA,+            GRPC_INITIAL_METADATA_USED_MASK)+        cdef SendMessageOperation send_message_op = SendMessageOperation(request, _EMPTY_FLAGS)+        cdef SendCloseFromClientOperation send_close_op = SendCloseFromClientOperation(_EMPTY_FLAGS)+        cdef ReceiveInitialMetadataOperation receive_initial_metadata_op = ReceiveInitialMetadataOperation(_EMPTY_FLAGS)+        cdef ReceiveMessageOperation receive_message_op = ReceiveMessageOperation(_EMPTY_FLAGS)+        cdef ReceiveStatusOnClientOperation receive_status_on_client_op = ReceiveStatusOnClientOperation(_EMPTY_FLAGS) -        send_close_from_client_operation = SendCloseFromClientOperation(_EMPTY_FLAGS)-        send_close_from_client_operation.c()+        ops = (initial_metadata_op, send_message_op, send_close_op,+               receive_initial_metadata_op, receive_message_op,+               receive_status_on_client_op) -        receive_initial_metadata_operation = ReceiveInitialMetadataOperation(_EMPTY_FLAGS)-        receive_initial_metadata_operation.c()+        try:+            self._create_grpc_call(deadline, method)+            try:+                await callback_start_batch(self._grpc_call_wrapper,+                                           ops,+                                           self._loop)+            except asyncio.CancelledError:+                status = self._cancel_and_create_status(cancellation_future)+                status_observer(status)+                raise+        finally:+            # If the RPC failed, this method will return None instead of crash.",Which method? `receive_initial_metadata_op.initial_metadata()`?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21232,355126337,2019-12-07T15:05:50Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -56,207 +81,336 @@ def __init__(self,         self._details = details         self._initial_metadata = initial_metadata         self._trailing_metadata = trailing_metadata+        self._debug_error_string = debug_error_string      def code(self) -> grpc.StatusCode:-        """"""+        """"""Accesses the status code sent by the server.+         Returns:           The `grpc.StatusCode` status code.         """"""         return self._code      def details(self) -> Optional[str]:-        """"""+        """"""Accesses the details sent by the server.+         Returns:           The description of the error.         """"""         return self._details      def initial_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the initial metadata sent by the server.+         Returns:-          The inital metadata received.+          The initial metadata received.         """"""         return self._initial_metadata      def trailing_metadata(self) -> Optional[Dict]:-        """"""+        """"""Accesses the trailing metadata sent by the server.+         Returns:           The trailing metadata received.         """"""         return self._trailing_metadata +    def debug_error_string(self) -> str:+        """"""Accesses the debug error string sent by the server. -@enum.unique-class _RpcState(enum.Enum):-    """"""Identifies the state of the RPC.""""""-    ONGOING = 1-    CANCELLED = 2-    FINISHED = 3-    ABORT = 4+        Returns:+          The debug error string received.+        """"""+        return self._debug_error_string +    def _repr(self) -> str:+        """"""Assembles the error string for the RPC error.""""""+        return _NON_OK_CALL_REPRESENTATION.format(self.__class__.__name__,+                                                  self._code, self._details,+                                                  self._debug_error_string) -class Call:-    """"""Object for managing RPC calls,-    returned when an instance of `UnaryUnaryMultiCallable` object is called.-    """"""+    def __repr__(self) -> str:+        return self._repr() -    _cancellation_details: ClassVar[str] = 'Locally cancelled by application!'+    def __str__(self) -> str:+        return self._repr() -    _state: _RpcState-    _exception: Optional[Exception]-    _response: Optional[bytes]-    _code: grpc.StatusCode-    _details: Optional[str]-    _initial_metadata: Optional[Dict]-    _trailing_metadata: Optional[Dict]-    _call: asyncio.Task-    _call_cancel_status: cygrpc.AioCancelStatus-    _response_deserializer: DeserializingFunction -    def __init__(self, call: asyncio.Task,-                 response_deserializer: DeserializingFunction,-                 call_cancel_status: cygrpc.AioCancelStatus) -> None:-        """"""Constructor.+def _create_rpc_error(initial_metadata: Optional[MetadataType],+                      status: cygrpc.AioRpcStatus) -> AioRpcError:+    return AioRpcError(_common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[status.code()],+                       status.details(), initial_metadata,+                       status.trailing_metadata()) -        Args:-          call: Asyncio Task that holds the RPC execution.-          response_deserializer: Deserializer used for parsing the reponse.-          call_cancel_status: A cygrpc.AioCancelStatus used for giving a-            specific error when the RPC is canceled.-        """""" -        self._state = _RpcState.ONGOING-        self._exception = None-        self._response = None-        self._code = grpc.StatusCode.UNKNOWN-        self._details = None-        self._initial_metadata = None-        self._trailing_metadata = None-        self._call = call-        self._call_cancel_status = call_cancel_status-        self._response_deserializer = response_deserializer+class Call(_base_call.Call):+    _loop: asyncio.AbstractEventLoop+    _code: grpc.StatusCode+    _status: Awaitable[cygrpc.AioRpcStatus]+    _initial_metadata: Awaitable[MetadataType]+    _cancellation: asyncio.Future -    def __del__(self):-        self.cancel()+    def __init__(self) -> None:+        self._loop = asyncio.get_event_loop()+        self._code = None+        self._status = self._loop.create_future()+        self._initial_metadata = self._loop.create_future()+        self._cancellation = self._loop.create_future()      def cancel(self) -> bool:-        """"""Cancels the ongoing RPC request.+        """"""Placeholder cancellation method. -        Returns:-          True if the RPC can be canceled, False if was already cancelled or terminated.+        The implementation of this method needs to pass the cancellation reason+        into self._cancellation, using `set_result` instead of+        `set_exception`.         """"""-        if self.cancelled() or self.done():-            return False--        code = grpc.StatusCode.CANCELLED-        self._call_cancel_status.cancel(-            _common.STATUS_CODE_TO_CYGRPC_STATUS_CODE[code],-            details=Call._cancellation_details)-        self._call.cancel()-        self._details = Call._cancellation_details-        self._code = code-        self._state = _RpcState.CANCELLED-        return True+        raise NotImplementedError()      def cancelled(self) -> bool:-        """"""Returns if the RPC was cancelled.--        Returns:-          True if the requests was cancelled, False if not.-        """"""-        return self._state is _RpcState.CANCELLED--    def running(self) -> bool:-        """"""Returns if the RPC is running.--        Returns:-          True if the requests is running, False if it already terminated.-        """"""-        return not self.done()+        return self._cancellation.done(+        ) or self._code == grpc.StatusCode.CANCELLED      def done(self) -> bool:-        """"""Returns if the RPC has finished.+        return self._status.done() -        Returns:-          True if the requests has finished, False is if still ongoing.-        """"""-        return self._state is not _RpcState.ONGOING--    async def initial_metadata(self):+    def add_done_callback(self, unused_callback) -> None:         raise NotImplementedError() -    async def trailing_metadata(self):+    def time_remaining(self) -> Optional[float]:         raise NotImplementedError() -    async def code(self) -> grpc.StatusCode:-        """"""Returns the `grpc.StatusCode` if the RPC is finished,-        otherwise first waits until the RPC finishes.+    async def initial_metadata(self) -> MetadataType:+        return await self._initial_metadata -        Returns:-          The `grpc.StatusCode` status code.-        """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+    async def trailing_metadata(self) -> MetadataType:+        return (await self._status).trailing_metadata() +    async def code(self) -> grpc.StatusCode:+        await self._status         return self._code      async def details(self) -> str:-        """"""Returns the details if the RPC is finished, otherwise first waits till the-        RPC finishes.+        return (await self._status).details() -        Returns:-          The details.+    async def debug_error_string(self) -> str:+        return (await self._status).debug_error_string()++    def _set_initial_metadata(self, metadata: MetadataType) -> None:+        self._initial_metadata.set_result(metadata)++    def _set_status(self, status: cygrpc.AioRpcStatus) -> None:+        """"""Private method to set final status of the RPC.++        This method may be called multiple time due to data race between local+        cancellation (by application) and C-Core receiving status from peer. We+        make no promise here which one will win.         """"""-        if not self.done():-            try:-                await self-            except (asyncio.CancelledError, AioRpcError):-                pass+        if self._status.done():+            return+        else:+            self._status.set_result(status)+            self._code = _common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[+                status.code()]++    async def _raise_rpc_error_if_not_ok(self) -> None:+        if self._code != grpc.StatusCode.OK:+            raise _create_rpc_error(await self.initial_metadata(),+                                    self._status.result())++    def _repr(self) -> str:+        """"""Assembles the RPC representation string.""""""+        if not self._status.done():+            return '<{} object>'.format(self.__class__.__name__)+        if self._code is grpc.StatusCode.OK:+            return _OK_CALL_REPRESENTATION.format(+                self.__class__.__name__, self._code,+                self._status.result().self._status.result().details())+        else:+            return _NON_OK_CALL_REPRESENTATION.format(+                self.__class__.__name__, self._code,+                self._status.result().details(),+                self._status.result().debug_error_string())++    def __repr__(self) -> str:+        return self._repr()++    def __str__(self) -> str:+        return self._repr()+++# pylint: disable=abstract-method+class UnaryUnaryCall(Call, _base_call.UnaryUnaryCall):",Since the interface `_base_call.UnaryUnaryCall` is a derivated class of `Call` should we narrow the inheritance for the implementation to exclusively the interface `_base_call.UnaryUnaryCall`?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,355138049,2019-12-07T19:02:33Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -13,42 +13,111 @@ # limitations under the License. """"""Invocation-side implementation of gRPC Asyncio Python."""""" import asyncio-from typing import Callable, Optional+from typing import Any, Optional, Sequence, Text, Tuple +import grpc from grpc import _common from grpc._cython import cygrpc+from ._call import Call, UnaryUnaryCall, UnaryStreamCall+from ._typing import (DeserializingFunction, MetadataType, SerializingFunction) -from ._call import Call -SerializingFunction = Callable[[str], bytes]-DeserializingFunction = Callable[[bytes], str]+def _timeout_to_deadline(loop: asyncio.AbstractEventLoop,+                         timeout: Optional[float]) -> Optional[float]:+    if timeout is None:+        return None+    return loop.time() + timeout   class UnaryUnaryMultiCallable:-    """"""Afford invoking a unary-unary RPC from client-side in an asynchronous way.""""""+    """"""Factory an asynchronous unary-unary RPC stub call from client-side.""""""      def __init__(self, channel: cygrpc.AioChannel, method: bytes,                  request_serializer: SerializingFunction,                  response_deserializer: DeserializingFunction) -> None:+        self._loop = asyncio.get_event_loop()         self._channel = channel         self._method = method         self._request_serializer = request_serializer         self._response_deserializer = response_deserializer-        self._loop = asyncio.get_event_loop() -    def _timeout_to_deadline(self, timeout: int) -> Optional[int]:-        if timeout is None:-            return None-        return self._loop.time() + timeout+    def __call__(self,+                 request: Any,+                 *,+                 timeout: Optional[float] = None,+                 metadata: Optional[MetadataType] = None,+                 credentials: Optional[grpc.CallCredentials] = None,+                 wait_for_ready: Optional[bool] = None,+                 compression: Optional[grpc.Compression] = None) -> Call:",Good catch!Changed to the abstract class of `UnaryUnaryCall`. Also changed for `UnaryStreamCall`.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21421,355672731,2019-12-09T20:41:16Z,templates/config.m4.template,"@@ -38,13 +38,14 @@     PHP_SUBST(GRPC_SHARED_LIBADD)   <%     srcs = []-    for src in php_config_m4.src:-      srcs.append(src)-    for lib in libs:-      if lib.name in php_config_m4.get('deps', []) and lib.name != 'z':-        for src in lib.src:-          srcs.append(src)-    srcs = sorted(srcs)+    srcs.extend(php_config_m4.src)+    php_deps = php_config_m4.get('deps', [])+    lib_maps = {lib.name: lib for lib in libs}+    for dep in php_deps[:]:+      php_deps.extend(lib_maps[dep].transitive_deps)+    for dep in list(set(php_deps) - set(('z',))):",Nit: The temporary tuple isn't necessary. You can use `{'z'}` or `set('z')`.,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21421,355673635,2019-12-09T20:43:28Z,templates/config.m4.template,"@@ -38,13 +38,14 @@     PHP_SUBST(GRPC_SHARED_LIBADD)   <%     srcs = []-    for src in php_config_m4.src:-      srcs.append(src)-    for lib in libs:-      if lib.name in php_config_m4.get('deps', []) and lib.name != 'z':-        for src in lib.src:-          srcs.append(src)-    srcs = sorted(srcs)+    srcs.extend(php_config_m4.src)+    php_deps = php_config_m4.get('deps', [])+    lib_maps = {lib.name: lib for lib in libs}+    for dep in php_deps[:]:+      php_deps.extend(lib_maps[dep].transitive_deps)+    for dep in list(set(php_deps) - set(('z',))):",Nit: Sets are iterable. Is there any particular reason you're wrapping this in a list?,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21421,355675044,2019-12-09T20:46:37Z,templates/config.m4.template,"@@ -38,13 +38,14 @@     PHP_SUBST(GRPC_SHARED_LIBADD)   <%     srcs = []-    for src in php_config_m4.src:-      srcs.append(src)-    for lib in libs:-      if lib.name in php_config_m4.get('deps', []) and lib.name != 'z':-        for src in lib.src:-          srcs.append(src)-    srcs = sorted(srcs)+    srcs.extend(php_config_m4.src)+    php_deps = php_config_m4.get('deps', [])+    lib_maps = {lib.name: lib for lib in libs}+    for dep in php_deps[:]:",Nit: It's a bit odd that you're iterating over a temporary copy of `php_deps` and then extending the original `php_deps`. Why not create a new list called something like `php_full_deps` and build that up instead?,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21421,355679314,2019-12-09T20:55:29Z,templates/config.w32.template,"@@ -32,32 +34,20 @@       ""/I""+configure_module_dirname+""\\third_party\\upb ""+       ""/I""+configure_module_dirname+""\\third_party\\zlib "");   <%-    dirs = {}-    for lib in libs:-      if lib.name in php_config_m4.get('deps', []) and lib.name != 'ares':-        for source in lib.src:-          tmp = source-          prev = ''-          while (True):-            idx = tmp.find('/');-            if (idx == -1):-              break-            dirs[prev + '\\\\' + tmp[:idx]] = 1-            prev += ('\\\\' + tmp[:idx]);-            tmp = tmp[idx+1:]-            -    dirs['\\\\src'] = 1;-    dirs['\\\\src\\\\php'] = 1;-    dirs['\\\\src\\\\php\\\\ext'] = 1;-    dirs['\\\\src\\\\php\\\\ext\\\\grpc'] = 1;-    dirs = dirs.keys()-    dirs.sort()+    dirset = set()+    for src in srcs:+      dirset.add(src[:src.rfind('/')])",Do we have access to the `os.path` module from here? This just seems like a less portable version of [`os.path.dirname`](https://docs.python.org/3/library/os.path.html#os.path.dirname).,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21421,355680589,2019-12-09T20:58:13Z,templates/config.w32.template,"@@ -32,32 +34,20 @@       ""/I""+configure_module_dirname+""\\third_party\\upb ""+       ""/I""+configure_module_dirname+""\\third_party\\zlib "");   <%-    dirs = {}-    for lib in libs:-      if lib.name in php_config_m4.get('deps', []) and lib.name != 'ares':-        for source in lib.src:-          tmp = source-          prev = ''-          while (True):-            idx = tmp.find('/');-            if (idx == -1):-              break-            dirs[prev + '\\\\' + tmp[:idx]] = 1-            prev += ('\\\\' + tmp[:idx]);-            tmp = tmp[idx+1:]-            -    dirs['\\\\src'] = 1;-    dirs['\\\\src\\\\php'] = 1;-    dirs['\\\\src\\\\php\\\\ext'] = 1;-    dirs['\\\\src\\\\php\\\\ext\\\\grpc'] = 1;-    dirs = dirs.keys()-    dirs.sort()+    dirset = set()+    for src in srcs:+      dirset.add(src[:src.rfind('/')])+    for dir in list(dirset):+      frags = dir.split('/')+      for i in range(1, len(frags)):+        dirset.add('/'.join(frags[:i]))+    dirs = [d.replace('/', '\\\\') for d in sorted(list(dirset))]",Nit: `sorted` can take a list directly.,
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/21423,355690450,2019-12-09T21:20:33Z,include/grpc/grpc_security_constants.h,"@@ -105,6 +106,28 @@ typedef enum {   GRPC_SSL_REQUEST_AND_REQUIRE_CLIENT_CERTIFICATE_AND_VERIFY } grpc_ssl_client_certificate_request_type; ++typedef enum {+  /** Default option: performs server certificate verification and hostname+     verification */+  GRPC_SSL_SERVER_VERIFICATION,+  /** Performs server certificate verification, but skips hostname verification+   */+  GRPC_SSL_SKIP_HOSTNAME_VERIFICATION,+  /** Performs hostname name verification, but skips server certificate+     verification */+  GRPC_SSL_SKIP_SERVER_CERTIFICATE_VERIFICATION,+  /** Skips both server certificate and hostname verification */+  GRPC_SSL_SKIP_ALL_SERVER_VERIFICATION+} grpc_ssl_server_verification_option;++typedef enum {+  /** Get peer leaf certificate */+  GRPC_SSL_PEER_LEAF_CERTIFICATE,+  /** Get peer full chain */+  GRPC_SSL_PEER_FULL_CHAIN+} grpc_ssl_peer_cert_request_type;",Is it used anywhere? Another option is to provide two TSI properties: one for full cert chain and the other for leaf cert and pass both of them to server authorization check where you can do your own custom check.,
28269509,akshayku,https://api.github.com/repos/grpc/grpc/pulls/21423,355692813,2019-12-09T21:25:39Z,include/grpc/grpc_security_constants.h,"@@ -105,6 +106,28 @@ typedef enum {   GRPC_SSL_REQUEST_AND_REQUIRE_CLIENT_CERTIFICATE_AND_VERIFY } grpc_ssl_client_certificate_request_type; ++typedef enum {+  /** Default option: performs server certificate verification and hostname+     verification */+  GRPC_SSL_SERVER_VERIFICATION,+  /** Performs server certificate verification, but skips hostname verification+   */+  GRPC_SSL_SKIP_HOSTNAME_VERIFICATION,+  /** Performs hostname name verification, but skips server certificate+     verification */+  GRPC_SSL_SKIP_SERVER_CERTIFICATE_VERIFICATION,+  /** Skips both server certificate and hostname verification */+  GRPC_SSL_SKIP_ALL_SERVER_VERIFICATION+} grpc_ssl_server_verification_option;++typedef enum {+  /** Get peer leaf certificate */+  GRPC_SSL_PEER_LEAF_CERTIFICATE,+  /** Get peer full chain */+  GRPC_SSL_PEER_FULL_CHAIN+} grpc_ssl_peer_cert_request_type;","Actually, it's not needed in experimental namespace as we will always return the full chain as reflected in peer_pem documentation of server authorization check. It's an artifact from earlier PR where we were trying to have a backwards compatibility. I will remove it. ",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,355698969,2019-12-09T21:38:57Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -57,93 +65,213 @@ cdef class _AioCall:             self._channel.cq.c_ptr(),             method_slice,             NULL,-            deadline,+            c_deadline,             NULL         )         grpc_slice_unref(method_slice)      cdef void _destroy_grpc_call(self):-        """"""Destroys the corresponding Core object for this RPC.""""""-        grpc_call_unref(self._grpc_call_wrapper.call)--    async def unary_unary(self, bytes method, bytes request, object timeout, AioCancelStatus cancel_status):-        cdef object loop = asyncio.get_event_loop()+        """"""Destroys the corresponding Core object for this RPC. -        cdef tuple operations-        cdef Operation initial_metadata_operation-        cdef Operation send_message_operation-        cdef Operation send_close_from_client_operation-        cdef Operation receive_initial_metadata_operation-        cdef Operation receive_message_operation-        cdef Operation receive_status_on_client_operation+        This method is idempotent. Multiple calls should not result in crashes.+        """"""+        if self._grpc_call_wrapper.call != NULL:+            grpc_call_unref(self._grpc_call_wrapper.call)+            self._grpc_call_wrapper.call = NULL -        cdef char *c_details = NULL+    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):+        """"""Cancels the RPC in C-Core, and return the final RPC status.""""""+        cdef AioRpcStatus status+        cdef object details+        cdef char *c_details+        # Try to fetch application layer cancellation details in the future.+        # * If calcellation details present, cancel with status;+        # * If details not present, cancel with unknown reason.+        if cancellation_future.done():+            status = cancellation_future.result()+            details = str_to_bytes(status.details())+            self._references.append(details)+            c_details = <char *>details+            # By implementation, grpc_call_cancel_with_status always return OK+            grpc_call_cancel_with_status(+                self._grpc_call_wrapper.call,+                status.c_code(),+                c_details,+                NULL,+            )+            return status+        else:+            # By implementation, grpc_call_cancel always return OK+            grpc_call_cancel(self._grpc_call_wrapper.call, NULL)+            status = AioRpcStatus(+                StatusCode.cancelled,+                _UNKNOWN_CANCELLATION_DETAILS,+                None,+                None,+            )+            cancellation_future.set_result(status)+            return status -        initial_metadata_operation = SendInitialMetadataOperation(_EMPTY_METADATA, GRPC_INITIAL_METADATA_USED_MASK)-        initial_metadata_operation.c()+    async def unary_unary(self,+                          bytes method,+                          bytes request,+                          object deadline,+                          object cancellation_future,+                          object initial_metadata_observer,+                          object status_observer):+        """"""Performs a unary unary RPC.+        +        Args:+          method: name of the calling method in bytes.+          request: the serialized requests in bytes.+          deadline: optional deadline of the RPC in float.+          cancellation_future: the future that meant to transport the+            cancellation reason from the application layer.+          initial_metadata_observer: a callback for received initial metadata.+          status_observer: a callback for received final status.+        """"""+        cdef tuple ops -        send_message_operation = SendMessageOperation(request, _EMPTY_FLAGS)-        send_message_operation.c()+        cdef SendInitialMetadataOperation initial_metadata_op = SendInitialMetadataOperation(+            _EMPTY_METADATA,+            GRPC_INITIAL_METADATA_USED_MASK)+        cdef SendMessageOperation send_message_op = SendMessageOperation(request, _EMPTY_FLAGS)+        cdef SendCloseFromClientOperation send_close_op = SendCloseFromClientOperation(_EMPTY_FLAGS)+        cdef ReceiveInitialMetadataOperation receive_initial_metadata_op = ReceiveInitialMetadataOperation(_EMPTY_FLAGS)+        cdef ReceiveMessageOperation receive_message_op = ReceiveMessageOperation(_EMPTY_FLAGS)+        cdef ReceiveStatusOnClientOperation receive_status_on_client_op = ReceiveStatusOnClientOperation(_EMPTY_FLAGS) -        send_close_from_client_operation = SendCloseFromClientOperation(_EMPTY_FLAGS)-        send_close_from_client_operation.c()+        ops = (initial_metadata_op, send_message_op, send_close_op,+               receive_initial_metadata_op, receive_message_op,+               receive_status_on_client_op) -        receive_initial_metadata_operation = ReceiveInitialMetadataOperation(_EMPTY_FLAGS)-        receive_initial_metadata_operation.c()+        try:+            self._create_grpc_call(deadline, method)+            try:+                await callback_start_batch(self._grpc_call_wrapper,","Nit: It seems like `callback_start_batch` is no longer the greatest name for what's happening here. By the time this call completes, the batch has been not only started, but *completed* as well.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,355707845,2019-12-09T21:58:11Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi,"@@ -104,10 +109,67 @@ async def callback_start_batch(GrpcCallWrapper grpc_call_wrapper,         wrapper.c_functor(), NULL)      if error != GRPC_CALL_OK:-        raise RuntimeError(""Failed grpc_call_start_batch: {}"".format(error))+        raise CallbackStartBatchError(""Failed grpc_call_start_batch: {}"".format(error))      await future     cpython.Py_DECREF(wrapper)     cdef grpc_event c_event     # Tag.event must be called, otherwise messages won't be parsed from C     batch_operation_tag.event(c_event)+++async def _receive_message(GrpcCallWrapper grpc_call_wrapper,+                           object loop):+    """"""Retrives parsed messages from C-Core.+    +    The messages maybe already in C-Core's buffer, so there isn't a 1-to-1+    mapping between this and the underlying ""socket.read()"". Also, eventually,+    this function will end with an EOF, which reads empty message.+    """"""+    cdef ReceiveMessageOperation receive_op = ReceiveMessageOperation(_EMPTY_FLAG)+    cdef tuple ops = (receive_op,)+    try:+        await callback_start_batch(grpc_call_wrapper, ops, loop)+    except CallbackStartBatchError as e:+        # NOTE(lidiz) The receive message operation has two ways to indicate+        # finish state : 1) returns empty message due to EOF; 2) fails inside+        # the callback (e.g. cancelled).+        #+        # Since they all indicates finish, they are better be merged.+        _LOGGER.exception(e)",Logging instead of raising seems like it could be problematic. Under what conditions do we expect `grpc_call_start_batch` to return a status besides `GRPC_CALL_OK`?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,355710356,2019-12-09T22:03:51Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi,"@@ -104,10 +109,67 @@ async def callback_start_batch(GrpcCallWrapper grpc_call_wrapper,         wrapper.c_functor(), NULL)      if error != GRPC_CALL_OK:-        raise RuntimeError(""Failed grpc_call_start_batch: {}"".format(error))+        raise CallbackStartBatchError(""Failed grpc_call_start_batch: {}"".format(error))      await future     cpython.Py_DECREF(wrapper)     cdef grpc_event c_event     # Tag.event must be called, otherwise messages won't be parsed from C     batch_operation_tag.event(c_event)+++async def _receive_message(GrpcCallWrapper grpc_call_wrapper,+                           object loop):+    """"""Retrives parsed messages from C-Core.+    +    The messages maybe already in C-Core's buffer, so there isn't a 1-to-1+    mapping between this and the underlying ""socket.read()"". Also, eventually,+    this function will end with an EOF, which reads empty message.+    """"""+    cdef ReceiveMessageOperation receive_op = ReceiveMessageOperation(_EMPTY_FLAG)+    cdef tuple ops = (receive_op,)+    try:+        await callback_start_batch(grpc_call_wrapper, ops, loop)+    except CallbackStartBatchError as e:+        # NOTE(lidiz) The receive message operation has two ways to indicate+        # finish state : 1) returns empty message due to EOF; 2) fails inside+        # the callback (e.g. cancelled).+        #+        # Since they all indicates finish, they are better be merged.+        _LOGGER.exception(e)+    return receive_op.message()+++async def _send_message(GrpcCallWrapper grpc_call_wrapper,+                        bytes message,+                        bint metadata_sent,+                        object loop):+    cdef SendMessageOperation op = SendMessageOperation(message, _EMPTY_FLAG)+    cdef tuple ops+    if metadata_sent:+        ops = (op,)+    else:+        ops = (+            # Initial metadata must be sent before first outbound message.+            SendInitialMetadataOperation(None, _EMPTY_FLAG),",Interesting quirk of the Core API.Optional: We could also solve this problem by making Core more lenient and doing the equivalent of this in the case that the first `SendMessageOperation` comes without a preceding `SendInitialMetadataOperation`. Seems like this would be a mostly backwards compatible change.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,355712157,2019-12-09T22:08:17Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -61,64 +99,132 @@ async def _handle_unary_unary_rpc(object method_handler,                                   RPCState rpc_state,                                   object loop):     # Receives request message-    cdef tuple receive_ops = (-        ReceiveMessageOperation(_EMPTY_FLAGS),-    )-    await callback_start_batch(rpc_state, receive_ops, loop)+    cdef bytes request_raw = await _receive_message(rpc_state, loop)      # Deserializes the request message-    cdef bytes request_raw = receive_ops[0].message()-    cdef object request_message-    if method_handler.request_deserializer:-        request_message = method_handler.request_deserializer(request_raw)-    else:-        request_message = request_raw+    cdef object request_message = deserialize(+        method_handler.request_deserializer,+        request_raw,+    )      # Executes application logic-    cdef object response_message = await method_handler.unary_unary(request_message, _ServicerContextPlaceHolder())+    cdef object response_message = await method_handler.unary_unary(+        request_message,+        _ServicerContext(+            rpc_state,+            None,+            None,+            loop,+        ),+    )      # Serializes the response message-    cdef bytes response_raw-    if method_handler.response_serializer:-        response_raw = method_handler.response_serializer(response_message)-    else:-        response_raw = response_message+    cdef bytes response_raw = serialize(+        method_handler.response_serializer,+        response_message,+    )      # Sends response message     cdef tuple send_ops = (         SendStatusFromServerOperation(-        tuple(), StatusCode.ok, b'', _EMPTY_FLAGS),-        SendInitialMetadataOperation(tuple(), _EMPTY_FLAGS),+            tuple(),+            StatusCode.ok,+            b'',+            _EMPTY_FLAGS,+        ),+        SendInitialMetadataOperation(None, _EMPTY_FLAGS),         SendMessageOperation(response_raw, _EMPTY_FLAGS),     )     await callback_start_batch(rpc_state, send_ops, loop)  +async def _handle_unary_stream_rpc(object method_handler,+                                   RPCState rpc_state,+                                   object loop):+    # Receives request message+    cdef bytes request_raw = await _receive_message(rpc_state, loop)++    # Deserializes the request message+    cdef object request_message = deserialize(+        method_handler.request_deserializer,+        request_raw,+    )++    cdef _ServicerContext servicer_context = _ServicerContext(+        rpc_state,+        method_handler.request_deserializer,+        method_handler.response_serializer,+        loop,+    )++    cdef object async_response_generator+    cdef object response_message+    if inspect.iscoroutinefunction(method_handler.unary_stream):",What's the latency on this `inspect.iscoroutinefunction` call? Can this check be done ahead of time instead of in the data path?,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,355714529,2019-12-09T22:14:04Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -43,12 +44,49 @@ cdef class RPCState:             grpc_call_unref(self.call)  +cdef class _ServicerContext:+    cdef RPCState _rpc_state+    cdef object _loop+    cdef bint _metadata_sent+    cdef object _request_deserializer+    cdef object _response_serializer++    def __cinit__(self,+                  RPCState rpc_state,+                  object request_deserializer,+                  object response_serializer,+                  object loop):+        self._rpc_state = rpc_state+        self._request_deserializer = request_deserializer+        self._response_serializer = response_serializer+        self._loop = loop+        self._metadata_sent = False++    async def read(self):+        cdef bytes raw_message = await _receive_message(self._rpc_state, self._loop)+        return deserialize(self._request_deserializer,+                           raw_message)++    async def write(self, object message):+        await _send_message(self._rpc_state,+                            serialize(self._response_serializer, message),+                            self._metadata_sent,+                            self._loop)+        if not self._metadata_sent:+            self._metadata_sent = True++    async def send_initial_metadata(self, tuple metadata):+        if self._metadata_sent:+            raise ValueError('Send initial metadata failed: already sent')","I don't think `ValueError` is quite right here. It's not the value of an argument that's wrong. It's the usage of this function. If `UsageError` were a built-in exception, I would recommend that. Perhaps `RuntimeError` or a subclass of it?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21232,355716720,2019-12-09T22:19:21Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -221,18 +318,22 @@ cdef class AioServer:             if self._status != AIO_SERVER_STATUS_RUNNING:                 break +            # Accepts new request from C-Core             rpc_state = await _server_call_request_call(                 self._server,                 self._cq,                 self._loop) +            # Schedules the RPC as a separate coroutine             rpc_task = self._loop.create_task(                 _handle_rpc(                     self._generic_handlers,                     rpc_state,                     self._loop                 )             )++            # Fires off a task that listening on the cancellation from client.",Nit: s/that listening/that listens/,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,355747369,2019-12-09T23:47:46Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -43,12 +44,49 @@ cdef class RPCState:             grpc_call_unref(self.call)  +cdef class _ServicerContext:+    cdef RPCState _rpc_state+    cdef object _loop+    cdef bint _metadata_sent+    cdef object _request_deserializer+    cdef object _response_serializer++    def __cinit__(self,+                  RPCState rpc_state,+                  object request_deserializer,+                  object response_serializer,+                  object loop):+        self._rpc_state = rpc_state+        self._request_deserializer = request_deserializer+        self._response_serializer = response_serializer+        self._loop = loop+        self._metadata_sent = False++    async def read(self):+        cdef bytes raw_message = await _receive_message(self._rpc_state, self._loop)+        return deserialize(self._request_deserializer,+                           raw_message)++    async def write(self, object message):+        await _send_message(self._rpc_state,+                            serialize(self._response_serializer, message),+                            self._metadata_sent,+                            self._loop)+        if not self._metadata_sent:+            self._metadata_sent = True++    async def send_initial_metadata(self, tuple metadata):+        if self._metadata_sent:+            raise ValueError('Send initial metadata failed: already sent')","Changed to `RuntimeError`. I switched to `ValueError` when I saw https://github.com/grpc/grpc/blob/cd29d5d93597701fe52cd66b4653856f41ae61a3/src/python/grpcio/grpc/_server.py#L302. Since our API contract doesn't include the exception type, I think it should be fine to change to an exception type with better semantic.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,355756964,2019-12-10T00:14:11Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi,"@@ -104,10 +109,67 @@ async def callback_start_batch(GrpcCallWrapper grpc_call_wrapper,         wrapper.c_functor(), NULL)      if error != GRPC_CALL_OK:-        raise RuntimeError(""Failed grpc_call_start_batch: {}"".format(error))+        raise CallbackStartBatchError(""Failed grpc_call_start_batch: {}"".format(error))      await future     cpython.Py_DECREF(wrapper)     cdef grpc_event c_event     # Tag.event must be called, otherwise messages won't be parsed from C     batch_operation_tag.event(c_event)+++async def _receive_message(GrpcCallWrapper grpc_call_wrapper,+                           object loop):+    """"""Retrives parsed messages from C-Core.+    +    The messages maybe already in C-Core's buffer, so there isn't a 1-to-1+    mapping between this and the underlying ""socket.read()"". Also, eventually,+    this function will end with an EOF, which reads empty message.+    """"""+    cdef ReceiveMessageOperation receive_op = ReceiveMessageOperation(_EMPTY_FLAG)+    cdef tuple ops = (receive_op,)+    try:+        await callback_start_batch(grpc_call_wrapper, ops, loop)+    except CallbackStartBatchError as e:+        # NOTE(lidiz) The receive message operation has two ways to indicate+        # finish state : 1) returns empty message due to EOF; 2) fails inside+        # the callback (e.g. cancelled).+        #+        # Since they all indicates finish, they are better be merged.+        _LOGGER.exception(e)","I changed the logging level from `exception` to `debug`.The return value of `grpc_call_start_batch` is accountable for scheduling the operations, and it would fail if the cq is not in correct state, or the call is already cancelled, etc.. To obtain the final state of the operation, we need to wait until the callback is invoked with the `success` flag.I don't have a better solution than this one, since the raised exception is expected. Logging the actual exception helps debugging, I think lowering it to `debug` level should be more accurate.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21232,355763083,2019-12-10T00:23:13Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi,"@@ -104,10 +109,67 @@ async def callback_start_batch(GrpcCallWrapper grpc_call_wrapper,         wrapper.c_functor(), NULL)      if error != GRPC_CALL_OK:-        raise RuntimeError(""Failed grpc_call_start_batch: {}"".format(error))+        raise CallbackStartBatchError(""Failed grpc_call_start_batch: {}"".format(error))      await future     cpython.Py_DECREF(wrapper)     cdef grpc_event c_event     # Tag.event must be called, otherwise messages won't be parsed from C     batch_operation_tag.event(c_event)+++async def _receive_message(GrpcCallWrapper grpc_call_wrapper,+                           object loop):+    """"""Retrives parsed messages from C-Core.+    +    The messages maybe already in C-Core's buffer, so there isn't a 1-to-1+    mapping between this and the underlying ""socket.read()"". Also, eventually,+    this function will end with an EOF, which reads empty message.+    """"""+    cdef ReceiveMessageOperation receive_op = ReceiveMessageOperation(_EMPTY_FLAG)+    cdef tuple ops = (receive_op,)+    try:+        await callback_start_batch(grpc_call_wrapper, ops, loop)+    except CallbackStartBatchError as e:+        # NOTE(lidiz) The receive message operation has two ways to indicate+        # finish state : 1) returns empty message due to EOF; 2) fails inside+        # the callback (e.g. cancelled).+        #+        # Since they all indicates finish, they are better be merged.+        _LOGGER.exception(e)+    return receive_op.message()+++async def _send_message(GrpcCallWrapper grpc_call_wrapper,+                        bytes message,+                        bint metadata_sent,+                        object loop):+    cdef SendMessageOperation op = SendMessageOperation(message, _EMPTY_FLAG)+    cdef tuple ops+    if metadata_sent:+        ops = (op,)+    else:+        ops = (+            # Initial metadata must be sent before first outbound message.+            SendInitialMetadataOperation(None, _EMPTY_FLAG),","That is a doable solution with different tradeoffs. E.g. Core might need another layer of abstraction to insert certain operations, or the error handling path needs to indicate that `InitialMetadata` is sent automatically.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/21421,356267976,2019-12-10T20:45:40Z,templates/config.m4.template,"@@ -38,13 +38,14 @@     PHP_SUBST(GRPC_SHARED_LIBADD)   <%     srcs = []-    for src in php_config_m4.src:-      srcs.append(src)-    for lib in libs:-      if lib.name in php_config_m4.get('deps', []) and lib.name != 'z':-        for src in lib.src:-          srcs.append(src)-    srcs = sorted(srcs)+    srcs.extend(php_config_m4.src)+    php_deps = php_config_m4.get('deps', [])+    lib_maps = {lib.name: lib for lib in libs}+    for dep in php_deps[:]:+      php_deps.extend(lib_maps[dep].transitive_deps)+    for dep in list(set(php_deps) - set(('z',))):",Actually I just copied the same thing from the previous code and I guess that this is because php already is linked to zlib library.,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/21421,356271903,2019-12-10T20:54:37Z,templates/config.w32.template,"@@ -32,32 +34,20 @@       ""/I""+configure_module_dirname+""\\third_party\\upb ""+       ""/I""+configure_module_dirname+""\\third_party\\zlib "");   <%-    dirs = {}-    for lib in libs:-      if lib.name in php_config_m4.get('deps', []) and lib.name != 'ares':-        for source in lib.src:-          tmp = source-          prev = ''-          while (True):-            idx = tmp.find('/');-            if (idx == -1):-              break-            dirs[prev + '\\\\' + tmp[:idx]] = 1-            prev += ('\\\\' + tmp[:idx]);-            tmp = tmp[idx+1:]-            -    dirs['\\\\src'] = 1;-    dirs['\\\\src\\\\php'] = 1;-    dirs['\\\\src\\\\php\\\\ext'] = 1;-    dirs['\\\\src\\\\php\\\\ext\\\\grpc'] = 1;-    dirs = dirs.keys()-    dirs.sort()+    dirset = set()+    for src in srcs:+      dirset.add(src[:src.rfind('/')])",This path is technically not related with system path. (e.g. it doesn't uses backslash on windows) so `/` should be used instead of `os.sep`,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/21421,356273623,2019-12-10T20:58:26Z,templates/config.w32.template,"@@ -32,32 +34,20 @@       ""/I""+configure_module_dirname+""\\third_party\\upb ""+       ""/I""+configure_module_dirname+""\\third_party\\zlib "");   <%-    dirs = {}-    for lib in libs:-      if lib.name in php_config_m4.get('deps', []) and lib.name != 'ares':-        for source in lib.src:-          tmp = source-          prev = ''-          while (True):-            idx = tmp.find('/');-            if (idx == -1):-              break-            dirs[prev + '\\\\' + tmp[:idx]] = 1-            prev += ('\\\\' + tmp[:idx]);-            tmp = tmp[idx+1:]-            -    dirs['\\\\src'] = 1;-    dirs['\\\\src\\\\php'] = 1;-    dirs['\\\\src\\\\php\\\\ext'] = 1;-    dirs['\\\\src\\\\php\\\\ext\\\\grpc'] = 1;-    dirs = dirs.keys()-    dirs.sort()+    dirset = set()+    for src in srcs:+      dirset.add(src[:src.rfind('/')])+    for dir in list(dirset):","This is done to clone the dirset because dirset will be modified during iteration. Instead, I did the same change done for `config.w4`.",
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/21375,356321956,2019-12-10T22:52:47Z,include/grpcpp/alts_context.h,"@@ -0,0 +1,76 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPCPP_ALTS_CONTEXT_H+#define GRPCPP_ALTS_CONTEXT_H++#include <grpc/grpc_security_constants.h>+#include <grpcpp/impl/codegen/security/auth_context.h>++#include <memory>++struct grpc_gcp_AltsContext;++namespace grpc {++typedef struct Versions {+  int major_version;+  int minor_version;+} Versions;++typedef struct RpcProtocolVersions {+  Versions max_rpc_versions;+  Versions min_rpc_versions;+} RpcProtocolVersions;++// AltsContext is wrapper class for grpc_gcp_AltsContext.+// It should only be instantiated by calling GetAltsContextFromAuthContext.","If it should only be created by the `GetAltsContextFromAuthContext` function, then maybe you can make the ctor private and add that function as a friend to the class.",X
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/21375,356323324,2019-12-10T22:56:34Z,src/cpp/common/alts_context.cc,"@@ -0,0 +1,112 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc_security.h>+#include <grpcpp/alts_context.h>++#include ""src/core/lib/gprpp/memory.h""+#include ""src/core/tsi/alts/handshaker/alts_tsi_handshaker.h""+#include ""src/cpp/common/secure_auth_context.h""+#include ""src/proto/grpc/gcp/altscontext.upb.h""++namespace grpc {++AltsContext::AltsContext(const grpc_gcp_AltsContext* ctx) {+  upb_strview application_protocol =+      grpc_gcp_AltsContext_application_protocol(ctx);+  application_protocol_ =+      std::string(application_protocol.data, application_protocol.size);",Note if `application_protocol.data` is `nullptr` this is undefined behavior. I am not sure whether `data` will always be valid here. The same applies to the other string constructions.,X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/21324,356377845,2019-12-11T02:33:37Z,src/core/lib/security/credentials/oauth2/oauth2_credentials.cc,"@@ -252,6 +259,7 @@ void grpc_oauth2_token_fetcher_credentials::on_http_response(     if (status == GRPC_CREDENTIALS_OK) {       grpc_credentials_mdelem_array_add(pending_request->md_array,                                         access_token_md);+      add_additional_metadata(pending_request->md_array);","The fact that we need make sure to use it both when token is cached and when it's not cached is slightly subtle.Is it possible to unconditionally add the ""additional metadata"" e.g. at the top of `grpc_oauth2_token_fetcher_credentials::get_request_metadata` ?",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/21324,356457428,2019-12-11T08:30:19Z,src/core/lib/security/credentials/oauth2/oauth2_credentials.cc,"@@ -273,6 +280,7 @@ bool grpc_oauth2_token_fetcher_credentials::get_request_metadata(     grpc_polling_entity* pollent, grpc_auth_metadata_context /*context*/,     grpc_credentials_mdelem_array* md_array, grpc_closure* on_request_metadata,     grpc_error** /*error*/) {+  add_additional_metadata(md_array);","naming nit: can we prefix this with `maybe`, i.e. change to `maybe_add_additional_metadata`",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,356866452,2019-12-11T22:20:47Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -199,6 +211,23 @@ def __init__(self, target: Text,         if compression:             raise NotImplementedError(""TODO: compression not implemented yet"") +        if interceptors is None:+            self._unary_unary_interceptors = None",Can we annotate instance variables for `Channel` class? E.g. `_unary_unary_interceptors` and `_channel`,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,356868209,2019-12-11T22:25:03Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,270 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from typing import Any, Callable, Optional, Iterator, Sequence++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import RequestType, SerializingFunction, DeserializingFunction, MetadataType++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: bytes+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor:+    """"""Affords intercepting unary-unary invocations.""""""++    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, Any], Any],+            client_call_details: ClientCallDetails, request: Any) -> Any:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""","nit: a few space lines.```Python        """"""Intercepts a unary-unary invocation asynchronously.        Args:          continuation: A coroutine that proceeds with the invocation by            executing the next interceptor in chain or invoking the            actual RPC on the underlying Channel. It is the interceptor's            responsibility to call it if it decides to move the RPC forward.            The interceptor can use            `response_future = await continuation(client_call_details, request)`            to continue with the RPC. `continuation` returns the response of the            RPC.          client_call_details: A ClientCallDetails object describing the            outgoing RPC.          request: The request value for the RPC.        Returns:            An object with the RPC response.        Raises:          AioRpcError: Indicating that the RPC terminated with non-OK status.          asyncio.CancelledError: Indicating that the RPC was canceled.```",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,356868878,2019-12-11T22:26:37Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,270 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from typing import Any, Callable, Optional, Iterator, Sequence++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import RequestType, SerializingFunction, DeserializingFunction, MetadataType++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: bytes+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor:+    """"""Affords intercepting unary-unary invocations.""""""++    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, Any], Any],+            client_call_details: ClientCallDetails, request: Any) -> Any:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors, where",nit: The style guide suggests that the first line should be a quick summary of what this class do.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,356878182,2019-12-11T22:50:40Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,270 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from typing import Any, Callable, Optional, Iterator, Sequence++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import RequestType, SerializingFunction, DeserializingFunction, MetadataType++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: bytes+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor:+    """"""Affords intercepting unary-unary invocations.""""""++    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, Any], Any],+            client_call_details: ClientCallDetails, request: Any) -> Any:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors, where+    interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__(+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],  # pylint: disable=R0913+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._intercepted_call = None+        self._intercepted_call_created = asyncio.Event(loop=self._loop)+        self._cancelled_before_rpc = False+        self._invoke(interceptors, method, timeout, request, request_serializer,+                     response_deserializer)++    def __del__(self):+        self.cancel()++    def _invoke(self, interceptors: Sequence[UnaryUnaryClientInterceptor],+                method: bytes, timeout: Optional[float], request: RequestType,+                request_serializer: SerializingFunction,+                response_deserializer: DeserializingFunction) -> None:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:","If we design interceptors in this way, that means there will only be one `UnaryUnaryCall` object. The interceptors will lost the functionality of modifying responses / RPC final status, which is different from what current API supports. Some use case might be affected by this change, for example if users want to perform error handling:```Pythonclass InterceptorA(aio.UnaryUnaryClientInterceptor):    async def intercept_unary_unary(self, continuation,                                    client_call_details, request):        call = await continuation(client_call_details, request)        try:            response = await call        except _SUPPRESSIBLE_ERROR:            return callclass InterceptorB(aio.UnaryUnaryClientInterceptor):    async def intercept_unary_unary(self, continuation,                                    client_call_details, request):        call = await continuation(client_call_details, request)        # The same error will be raised here, and outside of interception        response = await call         return call```Or performing response caching:```Pythonclass CacheInterceptor(aio.UnaryUnaryClientInterceptor):    async def intercept_unary_unary(self, continuation,                                    client_call_details, request):        if _IS_CACHED(request):            response = _FIND_CACHED_RESPONSE(request)            # How can I return the cached response?        if _IS_PARTIALY_CACHED(request):            partial_response = _FIND_PARTIALY_CACHED_RESPONSE(request)            incremental_request = _GET_INCREMENTAL_REQUEST(                partial_response,                request            )            response = await await continuation(                client_call_details,                incremental_request            )            # How can I append the newly fetched results?        return await continuation(client_call_details, request)```Are we willing to trade this functionality for performance?Also, each interceptor needs to return `Call` object, which is a new pattern that new comers need to adapt.I might be a bit biased here since I wrote a [summary](https://github.com/grpc/proposal/blob/master/L13-python-interceptors.md) before, so I might focus too much on the detailed difference.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21455,356890415,2019-12-11T23:29:22Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,270 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from typing import Any, Callable, Optional, Iterator, Sequence++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import RequestType, SerializingFunction, DeserializingFunction, MetadataType++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: bytes+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor:+    """"""Affords intercepting unary-unary invocations.""""""++    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, Any], Any],+            client_call_details: ClientCallDetails, request: Any) -> Any:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""",Abstract class or `NotImplementedError`?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21455,356895373,2019-12-11T23:47:37Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -0,0 +1,340 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest++import grpc++from grpc.experimental import aio+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from src.proto.grpc.testing import messages_pb2+++class TestUnaryUnaryClientInterceptor(AioTestBase):++    def test_invalid_interceptor(self):++        class InvalidInterceptor:+            """"""Just an invalid Interceptor""""""++        with self.assertRaises(ValueError):+            aio.insecure_channel("""", interceptors=[InvalidInterceptor()])++    async def test_executed_right_order(self):++        interceptors_executed = []++        class Interceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for testing if the interceptor is being called""""""++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                interceptors_executed.append(self)+                call = await continuation(client_call_details, request)+                return call++        interceptors = [Interceptor() for i in range(2)]++        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=interceptors) as channel:+            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())+            response = await call++            # Check that all interceptors were executed, and were executed+            # in the right order.+            self.assertSequenceEqual(interceptors_executed, interceptors)++            self.assertIsInstance(response, messages_pb2.SimpleResponse)++    @unittest.expectedFailure+    # TODO(https://github.com/grpc/grpc/issues/20144) Once metadata support is+    # implemented in the client-side, this test must be implemented.+    def test_modify_metadata(self):+        raise NotImplementedError()++    @unittest.expectedFailure+    # TODO(https://github.com/grpc/grpc/issues/20532) Once credentials support is+    # implemented in the client-side, this test must be implemented.+    def test_modify_credentials(self):+        raise NotImplementedError()++    async def test_status_code_Ok(self):++        class StatusCodeOkInterceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for observing status code Ok returned by the RPC""""""++            def __init__(self):+                self.status_code_Ok_observed = False++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                code = await call.code()+                if code == grpc.StatusCode.OK:+                    self.status_code_Ok_observed = True++                return call++        interceptor = StatusCodeOkInterceptor()+        server_target, server = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=[interceptor]) as channel:++            # when no error StatusCode.OK must be observed+            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)++            await multicallable(messages_pb2.SimpleRequest())++            self.assertTrue(interceptor.status_code_Ok_observed)++    async def test_add_timeout(self):++        class TimeoutInterceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for adding a timeout to the RPC""""""++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                new_client_call_details = aio.ClientCallDetails(+                    method=client_call_details.method,+                    timeout=0.1,+                    metadata=client_call_details.metadata,+                    credentials=client_call_details.credentials)+                return await continuation(new_client_call_details, request)++        interceptor = TimeoutInterceptor()+        server_target, server = await start_test_server()++        async with aio.insecure_channel(+                server_target, interceptors=[interceptor]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)++            await server.stop(None)++            with self.assertRaises(grpc.RpcError) as exception_context:+                await multicallable(messages_pb2.SimpleRequest())++            self.assertEqual(exception_context.exception.code(),+                             grpc.StatusCode.DEADLINE_EXCEEDED)+++class TestInterceptedUnaryUnaryCall(AioTestBase):++    async def test_call_ok(self):++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                return call++        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=[Interceptor()]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())+            response = await call++            self.assertTrue(call.done())+            self.assertFalse(call.cancelled())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)+            self.assertEqual(await call.details(), '')+            self.assertEqual(await call.initial_metadata(), ())+            self.assertEqual(await call.trailing_metadata(), ())++    async def test_rpc_attributes_available_before_interceptor_ends(self):","This could be problematic. It should be a valid use case for an interceptor to modify the trailing metadata or change the code, right?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21458,356916864,2019-12-12T01:17:05Z,tools/distrib/python/grpcio_tools/grpc_tools/main.cc,"@@ -36,3 +49,137 @@ int protoc_main(int argc, char* argv[]) {    return cli.Run(argc, argv); }++namespace internal {++class GeneratorContextImpl+    : public ::google::protobuf::compiler::GeneratorContext {+ public:+  GeneratorContextImpl(+      const std::vector<const ::google::protobuf::FileDescriptor*>&+          parsed_files,+      std::vector<std::pair<std::string, std::string>>* files_out)+      : files_(files_out), parsed_files_(parsed_files) {}++  ::google::protobuf::io::ZeroCopyOutputStream* Open(+      const std::string& filename) {+    files_->emplace_back(filename, """");+    return new ::google::protobuf::io::StringOutputStream(+        &(files_->back().second));+  }++  // NOTE(rbellevi): Equivalent to Open, since all files start out empty.+  ::google::protobuf::io::ZeroCopyOutputStream* OpenForAppend(+      const std::string& filename) {+    return Open(filename);+  }++  // NOTE(rbellevi): Equivalent to Open, since all files start out empty.+  ::google::protobuf::io::ZeroCopyOutputStream* OpenForInsert(+      const std::string& filename, const std::string& insertion_point) {+    return Open(filename);+  }++  void ListParsedFiles(+      std::vector<const ::google::protobuf::FileDescriptor*>* output) {+    *output = parsed_files_;+  }++ private:+  std::vector<std::pair<std::string, std::string>>* files_;+  const std::vector<const ::google::protobuf::FileDescriptor*>& parsed_files_;+};++class ErrorCollectorImpl+    : public ::google::protobuf::compiler::MultiFileErrorCollector {+ public:+  ErrorCollectorImpl(std::vector<::grpc_tools::ProtocError>* errors,+                     std::vector<::grpc_tools::ProtocWarning>* warnings)+      : errors_(errors), warnings_(warnings) {}++  void AddError(const std::string& filename, int line, int column,+                const std::string& message) {+    errors_->emplace_back(filename, line, column, message);+  }++  void AddWarning(const std::string& filename, int line, int column,+                  const std::string& message) {+    warnings_->emplace_back(filename, line, column, message);+  }++ private:+  std::vector<::grpc_tools::ProtocError>* errors_;+  std::vector<::grpc_tools::ProtocWarning>* warnings_;+};++static void calculate_transitive_closure(+    const ::google::protobuf::FileDescriptor* descriptor,+    std::vector<const ::google::protobuf::FileDescriptor*>* transitive_closure,+    std::unordered_set<const ::google::protobuf::FileDescriptor*>* visited) {+  for (int i = 0; i < descriptor->dependency_count(); ++i) {+    const ::google::protobuf::FileDescriptor* dependency =+        descriptor->dependency(i);+    if (visited->find(dependency) == visited->end()) {+      calculate_transitive_closure(dependency, transitive_closure, visited);+    }+  }+  transitive_closure->push_back(descriptor);+  visited->insert(descriptor);+}",Does ProtoBuf implementation provide similar function of removing duplicates?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,357107978,2019-12-12T12:02:49Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -199,6 +211,23 @@ def __init__(self, target: Text,         if compression:             raise NotImplementedError(""TODO: compression not implemented yet"") +        if interceptors is None:+            self._unary_unary_interceptors = None+        else:+            self._unary_unary_interceptors = list(+                filter(+                    lambda interceptor: isinstance(interceptor, UnaryUnaryClientInterceptor),+                    interceptors)) or None","I thought that would be a none negligible performance impact on the evaluation of the empty list, that's the reason that I put the `ort None`.Testing it [1] seems that the impact is not so important as I thought, I would remove this useless optimization.```bash$ python /tmp/eval.pyTime op if None:  ~ 22.37415313720703 nanosecondTime op if []: ~ 24.283170700073242 nanosecond```[1] https://gist.github.com/pfreixes/e5417deba3d4235a39b93db5e0e83f50",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,357109440,2019-12-12T12:06:17Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -199,6 +211,23 @@ def __init__(self, target: Text,         if compression:             raise NotImplementedError(""TODO: compression not implemented yet"") +        if interceptors is None:+            self._unary_unary_interceptors = None+        else:+            self._unary_unary_interceptors = list(+                filter(+                    lambda interceptor: isinstance(interceptor, UnaryUnaryClientInterceptor),+                    interceptors)) or None++            invalid_interceptors = set(interceptors) - set(+                self._unary_unary_interceptors or [])++            if invalid_interceptors:","We will be extending later with other interceptors the previous expression, like for example```pythoninvalid_interceptors = set(interceptors) - set(                self._unary_unary_interceptors) - set(self._unary_stream_interceptors)```With this, we are only raising a `ValueError` if and only if one of the interceptors does not meet with one of the supported classes.Because we do not support for now any other interceptors, for now the check is only considering the unary unary ones.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,357116067,2019-12-12T12:23:15Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -0,0 +1,340 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest++import grpc++from grpc.experimental import aio+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from src.proto.grpc.testing import messages_pb2+++class TestUnaryUnaryClientInterceptor(AioTestBase):++    def test_invalid_interceptor(self):++        class InvalidInterceptor:+            """"""Just an invalid Interceptor""""""++        with self.assertRaises(ValueError):+            aio.insecure_channel("""", interceptors=[InvalidInterceptor()])++    async def test_executed_right_order(self):++        interceptors_executed = []++        class Interceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for testing if the interceptor is being called""""""++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                interceptors_executed.append(self)+                call = await continuation(client_call_details, request)+                return call++        interceptors = [Interceptor() for i in range(2)]++        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=interceptors) as channel:+            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())+            response = await call++            # Check that all interceptors were executed, and were executed+            # in the right order.+            self.assertSequenceEqual(interceptors_executed, interceptors)++            self.assertIsInstance(response, messages_pb2.SimpleResponse)++    @unittest.expectedFailure+    # TODO(https://github.com/grpc/grpc/issues/20144) Once metadata support is+    # implemented in the client-side, this test must be implemented.+    def test_modify_metadata(self):+        raise NotImplementedError()++    @unittest.expectedFailure+    # TODO(https://github.com/grpc/grpc/issues/20532) Once credentials support is+    # implemented in the client-side, this test must be implemented.+    def test_modify_credentials(self):+        raise NotImplementedError()++    async def test_status_code_Ok(self):++        class StatusCodeOkInterceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for observing status code Ok returned by the RPC""""""++            def __init__(self):+                self.status_code_Ok_observed = False++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                code = await call.code()+                if code == grpc.StatusCode.OK:+                    self.status_code_Ok_observed = True++                return call++        interceptor = StatusCodeOkInterceptor()+        server_target, server = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=[interceptor]) as channel:++            # when no error StatusCode.OK must be observed+            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)++            await multicallable(messages_pb2.SimpleRequest())++            self.assertTrue(interceptor.status_code_Ok_observed)++    async def test_add_timeout(self):++        class TimeoutInterceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for adding a timeout to the RPC""""""++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                new_client_call_details = aio.ClientCallDetails(+                    method=client_call_details.method,+                    timeout=0.1,+                    metadata=client_call_details.metadata,+                    credentials=client_call_details.credentials)+                return await continuation(new_client_call_details, request)++        interceptor = TimeoutInterceptor()+        server_target, server = await start_test_server()++        async with aio.insecure_channel(+                server_target, interceptors=[interceptor]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)++            await server.stop(None)++            with self.assertRaises(grpc.RpcError) as exception_context:+                await multicallable(messages_pb2.SimpleRequest())++            self.assertEqual(exception_context.exception.code(),+                             grpc.StatusCode.DEADLINE_EXCEEDED)+++class TestInterceptedUnaryUnaryCall(AioTestBase):++    async def test_call_ok(self):++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                return call++        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=[Interceptor()]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())+            response = await call++            self.assertTrue(call.done())+            self.assertFalse(call.cancelled())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)+            self.assertEqual(await call.details(), '')+            self.assertEqual(await call.initial_metadata(), ())+            self.assertEqual(await call.trailing_metadata(), ())++    async def test_rpc_attributes_available_before_interceptor_ends(self):","Agree with you, not only in the use case that you and @lidizheng are presenting [1] - mutating the response, also when there is reentrance in the interceptor.For example, an interceptor implementing a retry system might call many times the dependency - aka continuation - having different results/calls in each call. By providing the freedom to the caller for retrieving the RPC attributes before the interceptor work has finished we are opening the door to introduce weird bugs in the application.I would advocate for yes or yes for making the caller wait till the interceptor's work has finished.[1] https://github.com/grpc/grpc/pull/21455/files#r356878182",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,357150651,2019-12-12T13:43:09Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,270 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from typing import Any, Callable, Optional, Iterator, Sequence++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import RequestType, SerializingFunction, DeserializingFunction, MetadataType++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: bytes+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor:+    """"""Affords intercepting unary-unary invocations.""""""++    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, Any], Any],+            client_call_details: ClientCallDetails, request: Any) -> Any:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors, where+    interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__(+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],  # pylint: disable=R0913+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._intercepted_call = None+        self._intercepted_call_created = asyncio.Event(loop=self._loop)+        self._cancelled_before_rpc = False+        self._invoke(interceptors, method, timeout, request, request_serializer,+                     response_deserializer)++    def __del__(self):+        self.cancel()++    def _invoke(self, interceptors: Sequence[UnaryUnaryClientInterceptor],+                method: bytes, timeout: Optional[float], request: RequestType,+                request_serializer: SerializingFunction,+                response_deserializer: DeserializingFunction) -> None:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:","This is IMO related to the conversation that opened @gnossen here [1].I guess that I didn't express myself with enough clarity in the previous PR, but I would advocate for not allowing the caller for accessing to the RPC till the interceptor task is not finished. Without this address, the problem that you are presenting can not be solved in a consistent way, meaning that the caller might use a different call than the one returned by the last interceptor.We would need to make a call on that, as I said I'm in favor of plumbing the caller to the last call returned by the last interceptor, and not providing early access to the first call.Once this is solved we can start talking about the mutation problem that you are rising here. Technically, once the previous issue is solved, the interceptors would be free to return any call, for example, calls that have been cached previously. This would solve the `CacheInterceptor` example that you provided.The ""problem"" would come if we need more granularity, for example for mutating the metadata or changing the status code, which right now is not supported by the `Call` interface. TBH I'm wondering if this is really supported by the sync version. is this supported?Most likely I'm missing something, but having the feeling that for me the other examples that you have provided - the one for suppressing errors and the one for making a result composition - the way to go would be implementing that specific business logic in the application layer rather than trying to do it using an interceptor. [1] https://github.com/grpc/grpc/pull/21455/files#r356895373",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21445,357262408,2019-12-12T17:03:53Z,templates/gRPC-Core.podspec.template,"@@ -20,60 +20,66 @@   # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   # See the License for the specific language governing permissions and   # limitations under the License.--  <%!-  def grpc_lib_files(libs, expect_libs, groups):-    out = []-    for lib in libs:-      if lib.name in expect_libs:-        for group in groups:-          out += lib.get(group, [])-    return sorted(out)--  grpc_libs = (""grpc"", ""gpr"", ""upb"")--  def grpc_private_files(libs):-    out = grpc_lib_files(libs, grpc_libs, (""headers"", ""src""))-    return out--  def grpc_public_headers(libs):-    out = grpc_lib_files(libs, grpc_libs, (""public_headers"",))-    return out--  def grpc_private_headers(libs):-    out = grpc_lib_files(libs, grpc_libs, (""headers"",))-    return out--  def grpc_cronet_files(libs):-    out = grpc_lib_files(libs, (""grpc_cronet"",), (""src"", ""headers""))-    excl = grpc_private_files(libs)-    excl += [-        # We do not need cronet dedicated plugin registry-        ""src/core/ext/transport/cronet/plugin_registry/grpc_cronet_plugin_registry.cc"",-        # We do not need dummy cronet API for ObjC-        ""src/core/ext/transport/cronet/transport/cronet_api_dummy.cc"",-    ]-    return [file for file in out if not file in excl]--  def grpc_cronet_public_headers(libs):-    out = grpc_lib_files(libs, (""grpc_cronet"",), (""public_headers"",))-    excl = grpc_public_headers(libs)-    return [file for file in out if not file in excl]--  def grpc_test_util_files(libs):-    out = grpc_lib_files(libs, (""grpc_test_util"",), (""src"", ""headers""))-    excl = grpc_private_files(libs)-    # Subprocess is not supported in tvOS and not needed by our tests.-    excl += [""test/core/util/subprocess_posix.cc""]-    return [file for file in out if not file in excl]--  def end2end_tests_files(libs):-    out = grpc_lib_files(libs, (""end2end_tests"",), (""src"", ""headers""))-    excl = grpc_private_files(libs)-    return [file for file in out if not file in excl]+  <%+  lib_maps = {lib.name: lib for lib in libs}    def ruby_multiline_list(files, indent):     return (',\n' + indent*' ').join('\'%s\'' % f for f in files)++  def is_absl_lib(target_name):+    return target_name.startswith(""absl/"")++  def get_absl_spec_name(label):+    # e.g. //absl/apple:banana -> abseil/apple/banana+    return ""abseil/"" + label[7:].replace("":"", ""/"")++  def all_libs(lib):+    return list(sorted(set({lib} | lib_maps[lib].transitive_deps)))++  def all_grpc_libs_and_abseil_specs(lib):+    grpc_libs = []+    absl_specs = set()+    for lib_name in all_libs(lib):+      if is_absl_lib(lib_name): continue+      grpc_libs.append(lib_name)+      for dep in lib_maps[lib_name].deps:+        if is_absl_lib(dep):+          absl_specs.add(get_absl_spec_name(dep))+    return (grpc_libs, list(sorted(absl_specs)))++  def grpc_all_lib_files(lib, fields):","Have a similar comment to this function name. It sounds like you are getting all files of all libs. I would change it to something like `list_lib_files(lib, fields)`",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21445,357263270,2019-12-12T17:05:42Z,templates/gRPC-Core.podspec.template,"@@ -20,60 +20,66 @@   # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   # See the License for the specific language governing permissions and   # limitations under the License.--  <%!-  def grpc_lib_files(libs, expect_libs, groups):-    out = []-    for lib in libs:-      if lib.name in expect_libs:-        for group in groups:-          out += lib.get(group, [])-    return sorted(out)--  grpc_libs = (""grpc"", ""gpr"", ""upb"")--  def grpc_private_files(libs):-    out = grpc_lib_files(libs, grpc_libs, (""headers"", ""src""))-    return out--  def grpc_public_headers(libs):-    out = grpc_lib_files(libs, grpc_libs, (""public_headers"",))-    return out--  def grpc_private_headers(libs):-    out = grpc_lib_files(libs, grpc_libs, (""headers"",))-    return out--  def grpc_cronet_files(libs):-    out = grpc_lib_files(libs, (""grpc_cronet"",), (""src"", ""headers""))-    excl = grpc_private_files(libs)-    excl += [-        # We do not need cronet dedicated plugin registry-        ""src/core/ext/transport/cronet/plugin_registry/grpc_cronet_plugin_registry.cc"",-        # We do not need dummy cronet API for ObjC-        ""src/core/ext/transport/cronet/transport/cronet_api_dummy.cc"",-    ]-    return [file for file in out if not file in excl]--  def grpc_cronet_public_headers(libs):-    out = grpc_lib_files(libs, (""grpc_cronet"",), (""public_headers"",))-    excl = grpc_public_headers(libs)-    return [file for file in out if not file in excl]--  def grpc_test_util_files(libs):-    out = grpc_lib_files(libs, (""grpc_test_util"",), (""src"", ""headers""))-    excl = grpc_private_files(libs)-    # Subprocess is not supported in tvOS and not needed by our tests.-    excl += [""test/core/util/subprocess_posix.cc""]-    return [file for file in out if not file in excl]--  def end2end_tests_files(libs):-    out = grpc_lib_files(libs, (""end2end_tests"",), (""src"", ""headers""))-    excl = grpc_private_files(libs)-    return [file for file in out if not file in excl]+  <%+  lib_maps = {lib.name: lib for lib in libs}    def ruby_multiline_list(files, indent):     return (',\n' + indent*' ').join('\'%s\'' % f for f in files)++  def is_absl_lib(target_name):+    return target_name.startswith(""absl/"")++  def get_absl_spec_name(label):+    # e.g. //absl/apple:banana -> abseil/apple/banana+    return ""abseil/"" + label[7:].replace("":"", ""/"")++  def all_libs(lib):+    return list(sorted(set({lib} | lib_maps[lib].transitive_deps)))++  def all_grpc_libs_and_abseil_specs(lib):","You don't actually mean ""all grpc libs"" here IIUC; only the libs that's dependency of `lib`. Again, probably the name of the function can be tweaked a bit to make this clearer.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21445,357269012,2019-12-12T17:17:31Z,templates/gRPC-Core.podspec.template,"@@ -20,60 +20,66 @@   # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   # See the License for the specific language governing permissions and   # limitations under the License.--  <%!-  def grpc_lib_files(libs, expect_libs, groups):-    out = []-    for lib in libs:-      if lib.name in expect_libs:-        for group in groups:-          out += lib.get(group, [])-    return sorted(out)--  grpc_libs = (""grpc"", ""gpr"", ""upb"")--  def grpc_private_files(libs):-    out = grpc_lib_files(libs, grpc_libs, (""headers"", ""src""))-    return out--  def grpc_public_headers(libs):-    out = grpc_lib_files(libs, grpc_libs, (""public_headers"",))-    return out--  def grpc_private_headers(libs):-    out = grpc_lib_files(libs, grpc_libs, (""headers"",))-    return out--  def grpc_cronet_files(libs):-    out = grpc_lib_files(libs, (""grpc_cronet"",), (""src"", ""headers""))-    excl = grpc_private_files(libs)-    excl += [-        # We do not need cronet dedicated plugin registry-        ""src/core/ext/transport/cronet/plugin_registry/grpc_cronet_plugin_registry.cc"",-        # We do not need dummy cronet API for ObjC-        ""src/core/ext/transport/cronet/transport/cronet_api_dummy.cc"",-    ]-    return [file for file in out if not file in excl]--  def grpc_cronet_public_headers(libs):-    out = grpc_lib_files(libs, (""grpc_cronet"",), (""public_headers"",))-    excl = grpc_public_headers(libs)-    return [file for file in out if not file in excl]--  def grpc_test_util_files(libs):-    out = grpc_lib_files(libs, (""grpc_test_util"",), (""src"", ""headers""))-    excl = grpc_private_files(libs)-    # Subprocess is not supported in tvOS and not needed by our tests.-    excl += [""test/core/util/subprocess_posix.cc""]-    return [file for file in out if not file in excl]--  def end2end_tests_files(libs):-    out = grpc_lib_files(libs, (""end2end_tests"",), (""src"", ""headers""))-    excl = grpc_private_files(libs)-    return [file for file in out if not file in excl]+  <%+  lib_maps = {lib.name: lib for lib in libs}    def ruby_multiline_list(files, indent):     return (',\n' + indent*' ').join('\'%s\'' % f for f in files)++  def is_absl_lib(target_name):+    return target_name.startswith(""absl/"")++  def get_absl_spec_name(label):+    # e.g. //absl/apple:banana -> abseil/apple/banana+    return ""abseil/"" + label[7:].replace("":"", ""/"")++  def all_libs(lib):+    return list(sorted(set({lib} | lib_maps[lib].transitive_deps)))++  def all_grpc_libs_and_abseil_specs(lib):",And the abseil part probably does not need a `lib` parameter since you are only using it for `grpc`,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/21413,357309832,2019-12-12T18:47:10Z,src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc,"@@ -1064,114 +1050,114 @@ void GrpcLb::BalancerCallState::OnBalancerMessageReceivedLocked(   grpc_byte_buffer_reader_destroy(&bbr);   grpc_byte_buffer_destroy(lb_calld->recv_message_payload_);   lb_calld->recv_message_payload_ = nullptr;-  const grpc_grpclb_initial_response* initial_response;-  grpc_grpclb_serverlist* serverlist;+  GrpcLbResponse response;   upb::Arena arena;-  if (!lb_calld->seen_initial_response_ &&-      (initial_response = grpc_grpclb_initial_response_parse(-           response_slice, arena.ptr())) != nullptr) {-    // Have NOT seen initial response, look for initial response.-    const google_protobuf_Duration* client_stats_report_interval =-        grpc_lb_v1_InitialLoadBalanceResponse_client_stats_report_interval(-            initial_response);-    if (client_stats_report_interval != nullptr) {-      lb_calld->client_stats_report_interval_ =-          GPR_MAX(GPR_MS_PER_SEC,-                  grpc_grpclb_duration_to_millis(client_stats_report_interval));-      if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_glb_trace)) {-        gpr_log(GPR_INFO,-                ""[grpclb %p] lb_calld=%p: Received initial LB response ""-                ""message; client load reporting interval = %"" PRId64-                "" milliseconds"",-                grpclb_policy, lb_calld,-                lb_calld->client_stats_report_interval_);-      }-    } else if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_glb_trace)) {-      gpr_log(GPR_INFO,-              ""[grpclb %p] lb_calld=%p: Received initial LB response message; ""-              ""client load reporting NOT enabled"",-              grpclb_policy, lb_calld);-    }-    lb_calld->seen_initial_response_ = true;-  } else if ((serverlist = grpc_grpclb_response_parse_serverlist(-                  response_slice)) != nullptr) {-    // Have seen initial response, look for serverlist.-    GPR_ASSERT(lb_calld->lb_call_ != nullptr);-    auto serverlist_wrapper = MakeRefCounted<Serverlist>(serverlist);-    if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_glb_trace)) {-      grpc_core::UniquePtr<char> serverlist_text = serverlist_wrapper->AsText();-      gpr_log(GPR_INFO,-              ""[grpclb %p] lb_calld=%p: Serverlist with %"" PRIuPTR-              "" servers received:\n%s"",-              grpclb_policy, lb_calld, serverlist->num_servers,-              serverlist_text.get());-    }-    lb_calld->seen_serverlist_ = true;-    // Start sending client load report only after we start using the-    // serverlist returned from the current LB call.-    if (lb_calld->client_stats_report_interval_ > 0 &&-        lb_calld->client_stats_ == nullptr) {-      lb_calld->client_stats_ = MakeRefCounted<GrpcLbClientStats>();-      // Ref held by callback.-      lb_calld->Ref(DEBUG_LOCATION, ""client_load_report"").release();-      lb_calld->ScheduleNextClientLoadReportLocked();-    }-    // Check if the serverlist differs from the previous one.-    if (grpclb_policy->serverlist_ != nullptr &&-        *grpclb_policy->serverlist_ == *serverlist_wrapper) {-      if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_glb_trace)) {-        gpr_log(GPR_INFO,-                ""[grpclb %p] lb_calld=%p: Incoming server list identical to ""-                ""current, ignoring."",-                grpclb_policy, lb_calld);-      }-    } else {  // New serverlist.-      // Dispose of the fallback.-      // TODO(roth): Ideally, we should stay in fallback mode until we-      // know that we can reach at least one of the backends in the new-      // serverlist.  Unfortunately, we can't do that, since we need to-      // send the new addresses to the child policy in order to determine-      // if they are reachable, and if we don't exit fallback mode now,-      // CreateOrUpdateChildPolicyLocked() will use the fallback-      // addresses instead of the addresses from the new serverlist.-      // However, if we can't reach any of the servers in the new-      // serverlist, then the child policy will never switch away from-      // the fallback addresses, but the grpclb policy will still think-      // that we're not in fallback mode, which means that we won't send-      // updates to the child policy when the fallback addresses are-      // updated by the resolver.  This is sub-optimal, but the only way-      // to fix it is to maintain a completely separate child policy for-      // fallback mode, and that's more work than we want to put into-      // the grpclb implementation at this point, since we're deprecating-      // it in favor of the xds policy.  We will implement this the-      // right way in the xds policy instead.-      if (grpclb_policy->fallback_mode_) {-        gpr_log(GPR_INFO,-                ""[grpclb %p] Received response from balancer; exiting ""-                ""fallback mode"",-                grpclb_policy);-        grpclb_policy->fallback_mode_ = false;-      }-      if (grpclb_policy->fallback_at_startup_checks_pending_) {-        grpclb_policy->fallback_at_startup_checks_pending_ = false;-        grpc_timer_cancel(&grpclb_policy->lb_fallback_timer_);-        grpclb_policy->CancelBalancerChannelConnectivityWatchLocked();-      }-      // Update the serverlist in the GrpcLb instance. This serverlist-      // instance will be destroyed either upon the next update or when the-      // GrpcLb instance is destroyed.-      grpclb_policy->serverlist_ = std::move(serverlist_wrapper);-      grpclb_policy->CreateOrUpdateChildPolicyLocked();-    }-  } else {-    // No valid initial response or serverlist found.+  if (!GrpcLbResponseParse(response_slice, arena.ptr(), &response) ||+      (response.type == response.INITIAL && lb_calld->seen_initial_response_)) {","nit: outside of the scope of this PR since it looks like the pre-existing behavior, but can be considered:though we have a check for receiving ""initial"" response type after having already received one, we don't have a check for receiving a server list without yet having received an ""initial"" response.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/21413,357362479,2019-12-12T20:45:59Z,src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc,"@@ -1064,114 +1050,114 @@ void GrpcLb::BalancerCallState::OnBalancerMessageReceivedLocked(   grpc_byte_buffer_reader_destroy(&bbr);   grpc_byte_buffer_destroy(lb_calld->recv_message_payload_);   lb_calld->recv_message_payload_ = nullptr;-  const grpc_grpclb_initial_response* initial_response;-  grpc_grpclb_serverlist* serverlist;+  GrpcLbResponse response;   upb::Arena arena;-  if (!lb_calld->seen_initial_response_ &&-      (initial_response = grpc_grpclb_initial_response_parse(-           response_slice, arena.ptr())) != nullptr) {-    // Have NOT seen initial response, look for initial response.-    const google_protobuf_Duration* client_stats_report_interval =-        grpc_lb_v1_InitialLoadBalanceResponse_client_stats_report_interval(-            initial_response);-    if (client_stats_report_interval != nullptr) {-      lb_calld->client_stats_report_interval_ =-          GPR_MAX(GPR_MS_PER_SEC,-                  grpc_grpclb_duration_to_millis(client_stats_report_interval));-      if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_glb_trace)) {-        gpr_log(GPR_INFO,-                ""[grpclb %p] lb_calld=%p: Received initial LB response ""-                ""message; client load reporting interval = %"" PRId64-                "" milliseconds"",-                grpclb_policy, lb_calld,-                lb_calld->client_stats_report_interval_);-      }-    } else if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_glb_trace)) {-      gpr_log(GPR_INFO,-              ""[grpclb %p] lb_calld=%p: Received initial LB response message; ""-              ""client load reporting NOT enabled"",-              grpclb_policy, lb_calld);-    }-    lb_calld->seen_initial_response_ = true;-  } else if ((serverlist = grpc_grpclb_response_parse_serverlist(-                  response_slice)) != nullptr) {-    // Have seen initial response, look for serverlist.-    GPR_ASSERT(lb_calld->lb_call_ != nullptr);-    auto serverlist_wrapper = MakeRefCounted<Serverlist>(serverlist);-    if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_glb_trace)) {-      grpc_core::UniquePtr<char> serverlist_text = serverlist_wrapper->AsText();-      gpr_log(GPR_INFO,-              ""[grpclb %p] lb_calld=%p: Serverlist with %"" PRIuPTR-              "" servers received:\n%s"",-              grpclb_policy, lb_calld, serverlist->num_servers,-              serverlist_text.get());-    }-    lb_calld->seen_serverlist_ = true;-    // Start sending client load report only after we start using the-    // serverlist returned from the current LB call.-    if (lb_calld->client_stats_report_interval_ > 0 &&-        lb_calld->client_stats_ == nullptr) {-      lb_calld->client_stats_ = MakeRefCounted<GrpcLbClientStats>();-      // Ref held by callback.-      lb_calld->Ref(DEBUG_LOCATION, ""client_load_report"").release();-      lb_calld->ScheduleNextClientLoadReportLocked();-    }-    // Check if the serverlist differs from the previous one.-    if (grpclb_policy->serverlist_ != nullptr &&-        *grpclb_policy->serverlist_ == *serverlist_wrapper) {-      if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_glb_trace)) {-        gpr_log(GPR_INFO,-                ""[grpclb %p] lb_calld=%p: Incoming server list identical to ""-                ""current, ignoring."",-                grpclb_policy, lb_calld);-      }-    } else {  // New serverlist.-      // Dispose of the fallback.-      // TODO(roth): Ideally, we should stay in fallback mode until we-      // know that we can reach at least one of the backends in the new-      // serverlist.  Unfortunately, we can't do that, since we need to-      // send the new addresses to the child policy in order to determine-      // if they are reachable, and if we don't exit fallback mode now,-      // CreateOrUpdateChildPolicyLocked() will use the fallback-      // addresses instead of the addresses from the new serverlist.-      // However, if we can't reach any of the servers in the new-      // serverlist, then the child policy will never switch away from-      // the fallback addresses, but the grpclb policy will still think-      // that we're not in fallback mode, which means that we won't send-      // updates to the child policy when the fallback addresses are-      // updated by the resolver.  This is sub-optimal, but the only way-      // to fix it is to maintain a completely separate child policy for-      // fallback mode, and that's more work than we want to put into-      // the grpclb implementation at this point, since we're deprecating-      // it in favor of the xds policy.  We will implement this the-      // right way in the xds policy instead.-      if (grpclb_policy->fallback_mode_) {-        gpr_log(GPR_INFO,-                ""[grpclb %p] Received response from balancer; exiting ""-                ""fallback mode"",-                grpclb_policy);-        grpclb_policy->fallback_mode_ = false;-      }-      if (grpclb_policy->fallback_at_startup_checks_pending_) {-        grpclb_policy->fallback_at_startup_checks_pending_ = false;-        grpc_timer_cancel(&grpclb_policy->lb_fallback_timer_);-        grpclb_policy->CancelBalancerChannelConnectivityWatchLocked();-      }-      // Update the serverlist in the GrpcLb instance. This serverlist-      // instance will be destroyed either upon the next update or when the-      // GrpcLb instance is destroyed.-      grpclb_policy->serverlist_ = std::move(serverlist_wrapper);-      grpclb_policy->CreateOrUpdateChildPolicyLocked();-    }-  } else {-    // No valid initial response or serverlist found.+  if (!GrpcLbResponseParse(response_slice, arena.ptr(), &response) ||+      (response.type == response.INITIAL && lb_calld->seen_initial_response_)) {","Yeah, this is for historical reasons: I think the original grpclb implementation predated adding the initial response to the protocol, and grpclb_end2end_test depends on this behavior.  There's an existing TODO to clean this up, but we probably won't actually do it unless it starts causing some problem.https://github.com/grpc/grpc/blob/1550e10fd6178ee452b75c1f4038ab9cbe8d83de/test/cpp/end2end/grpclb_end2end_test.cc#L232",
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/21456,357430687,2019-12-13T00:02:22Z,include/grpcpp/alts_context.h,"@@ -0,0 +1,73 @@+/*+ *+ * Copyright 2019 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPCPP_ALTS_CONTEXT_H+#define GRPCPP_ALTS_CONTEXT_H++#include <grpc/grpc_security_constants.h>+#include <grpcpp/impl/codegen/security/auth_context.h>++#include <memory>++struct grpc_gcp_AltsContext;++namespace grpc {++typedef struct Versions {","Sorry for missing this before. But you are defining something like `grpc::Versions` and `grpc::RpcProtocolVersions`, which are too broad.One thing you can do is to do something like this (note I renamed places such as `Versions` to `Version`)```class AltsContext { public:   struct RpcProtocolVersions {     struct Version { int major_version; int minor_version; };     Version max_rpc_version;     Version min_rpc_version;  };  // Other stuff of the class};```",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/21445,357438910,2019-12-13T00:38:42Z,templates/gRPC-Core.podspec.template,"@@ -20,60 +20,66 @@   # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   # See the License for the specific language governing permissions and   # limitations under the License.--  <%!-  def grpc_lib_files(libs, expect_libs, groups):-    out = []-    for lib in libs:-      if lib.name in expect_libs:-        for group in groups:-          out += lib.get(group, [])-    return sorted(out)--  grpc_libs = (""grpc"", ""gpr"", ""upb"")--  def grpc_private_files(libs):-    out = grpc_lib_files(libs, grpc_libs, (""headers"", ""src""))-    return out--  def grpc_public_headers(libs):-    out = grpc_lib_files(libs, grpc_libs, (""public_headers"",))-    return out--  def grpc_private_headers(libs):-    out = grpc_lib_files(libs, grpc_libs, (""headers"",))-    return out--  def grpc_cronet_files(libs):-    out = grpc_lib_files(libs, (""grpc_cronet"",), (""src"", ""headers""))-    excl = grpc_private_files(libs)-    excl += [-        # We do not need cronet dedicated plugin registry-        ""src/core/ext/transport/cronet/plugin_registry/grpc_cronet_plugin_registry.cc"",-        # We do not need dummy cronet API for ObjC-        ""src/core/ext/transport/cronet/transport/cronet_api_dummy.cc"",-    ]-    return [file for file in out if not file in excl]--  def grpc_cronet_public_headers(libs):-    out = grpc_lib_files(libs, (""grpc_cronet"",), (""public_headers"",))-    excl = grpc_public_headers(libs)-    return [file for file in out if not file in excl]--  def grpc_test_util_files(libs):-    out = grpc_lib_files(libs, (""grpc_test_util"",), (""src"", ""headers""))-    excl = grpc_private_files(libs)-    # Subprocess is not supported in tvOS and not needed by our tests.-    excl += [""test/core/util/subprocess_posix.cc""]-    return [file for file in out if not file in excl]--  def end2end_tests_files(libs):-    out = grpc_lib_files(libs, (""end2end_tests"",), (""src"", ""headers""))-    excl = grpc_private_files(libs)-    return [file for file in out if not file in excl]+  <%+  lib_maps = {lib.name: lib for lib in libs}    def ruby_multiline_list(files, indent):     return (',\n' + indent*' ').join('\'%s\'' % f for f in files)++  def is_absl_lib(target_name):+    return target_name.startswith(""absl/"")++  def get_absl_spec_name(label):+    # e.g. //absl/apple:banana -> abseil/apple/banana+    return ""abseil/"" + label[7:].replace("":"", ""/"")++  def all_libs(lib):+    return list(sorted(set({lib} | lib_maps[lib].transitive_deps)))++  def all_grpc_libs_and_abseil_specs(lib):",It's not easy to split them because this functions iterates all dependency and separate them into gRPC libraries and Abseil libraries.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,357790354,2019-12-13T19:09:11Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,270 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from typing import Any, Callable, Optional, Iterator, Sequence++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import RequestType, SerializingFunction, DeserializingFunction, MetadataType++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: bytes+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor:+    """"""Affords intercepting unary-unary invocations.""""""++    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, Any], Any],+            client_call_details: ClientCallDetails, request: Any) -> Any:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors, where+    interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__(+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],  # pylint: disable=R0913+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._intercepted_call = None+        self._intercepted_call_created = asyncio.Event(loop=self._loop)+        self._cancelled_before_rpc = False+        self._invoke(interceptors, method, timeout, request, request_serializer,+                     response_deserializer)++    def __del__(self):+        self.cancel()++    def _invoke(self, interceptors: Sequence[UnaryUnaryClientInterceptor],+                method: bytes, timeout: Optional[float], request: RequestType,+                request_serializer: SerializingFunction,+                response_deserializer: DeserializingFunction) -> None:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:","> The ""problem"" would come if we need more granularity, for example for mutating the metadata or changing the status code, which right now is not supported by the Call interface. TBH I'm wondering if this is really supported by the sync version. is this supported?For the sync version, applications can mutate the sending initial metadata by changing the `client_call_details`. As for trailing metadata, you are right, there is no way to mutate it.> Technically, once the previous issue is solved, the interceptors would be free to return any call, for example, calls that have been cached previously.To make caching data structure efficient, applications might want to store the data itself without the abstraction. Can you try to write a short snippet for a simple `CacheInterceptor`? Or add it as a unit test.> ...the problem that you are presenting can not be solved in a consistent way, meaning that the caller might use a different call than the one returned by the last interceptor.If we can solve the common use cases, I'm fine with either design, in fact the one call design is simpler than the multiple call design.",
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/21465,357815761,2019-12-13T20:16:21Z,test/cpp/end2end/end2end_test.cc,"@@ -1131,24 +1131,34 @@ TEST_P(End2endTest, CancelRpcBeforeStart) {   } } -// TODO(https://github.com/grpc/grpc/issues/21263): stop using timed sleeps to-// synchronize cancellation semantics. TEST_P(End2endTest, CancelDelayedRpc) {","Nit: For precision and to be consistent with the naming of the test directly above, we could name this CancelRpcAfterStart",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,357851983,2019-12-13T22:07:33Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,270 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from typing import Any, Callable, Optional, Iterator, Sequence++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import RequestType, SerializingFunction, DeserializingFunction, MetadataType++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: bytes+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor:+    """"""Affords intercepting unary-unary invocations.""""""++    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, Any], Any],+            client_call_details: ClientCallDetails, request: Any) -> Any:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors, where+    interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__(+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],  # pylint: disable=R0913+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._intercepted_call = None+        self._intercepted_call_created = asyncio.Event(loop=self._loop)+        self._cancelled_before_rpc = False+        self._invoke(interceptors, method, timeout, request, request_serializer,+                     response_deserializer)++    def __del__(self):+        self.cancel()++    def _invoke(self, interceptors: Sequence[UnaryUnaryClientInterceptor],+                method: bytes, timeout: Optional[float], request: RequestType,+                request_serializer: SerializingFunction,+                response_deserializer: DeserializingFunction) -> None:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:","I found there isn't an example for cached interceptor. I wrote my short version:```Pythonimport grpcimport helloworld_pb2import helloworld_pb2_grpcimport loggingclass CacheInterceptor(grpc.UnaryUnaryClientInterceptor):    def __init__(self):        self._cached = {}    def intercept_unary_unary(self,                              continuation,                              client_call_details,                              request):        if client_call_details.method != '/helloworld.Greeter/SayHello':            # Not-cacheable methods, just proceed.            return continuation(client_call_details, request)        hash_key = request.name        if hash_key in self._cached:            logging.debug('Cache hit for request [%s]', request)            return self._cached[hash_key]        else:            logging.debug('Cache miss for request [%s]', request)            response = continuation(client_call_details, request)            self._cached[hash_key] = response            return responsedef main():    channel = grpc.insecure_channel('localhost:50051')    intercepted_channel = grpc.intercept_channel(channel, CacheInterceptor())    stub = helloworld_pb2_grpc.GreeterStub(intercepted_channel)    stub.SayHello(helloworld_pb2.HelloRequest(name='First'))    stub.SayHello(helloworld_pb2.HelloRequest(name='First'))    stub.SayHello(helloworld_pb2.HelloRequest(name='First'))    stub.SayHello(helloworld_pb2.HelloRequest(name='Second'))    stub.SayHello(helloworld_pb2.HelloRequest(name='Third'))    channel.close()if __name__ == ""__main__"":    logging.basicConfig(level=logging.DEBUG)    main()```",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,357857130,2019-12-13T22:27:08Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,270 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from typing import Any, Callable, Optional, Iterator, Sequence++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import RequestType, SerializingFunction, DeserializingFunction, MetadataType++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: bytes+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor:+    """"""Affords intercepting unary-unary invocations.""""""++    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, Any], Any],+            client_call_details: ClientCallDetails, request: Any) -> Any:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors, where+    interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__(+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],  # pylint: disable=R0913+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._intercepted_call = None+        self._intercepted_call_created = asyncio.Event(loop=self._loop)+        self._cancelled_before_rpc = False+        self._invoke(interceptors, method, timeout, request, request_serializer,+                     response_deserializer)++    def __del__(self):+        self.cancel()++    def _invoke(self, interceptors: Sequence[UnaryUnaryClientInterceptor],+                method: bytes, timeout: Optional[float], request: RequestType,+                request_serializer: SerializingFunction,+                response_deserializer: DeserializingFunction) -> None:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                return await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)+            else:+                self._intercepted_call = UnaryUnaryCall(+                    request,+                    _timeout_to_deadline(self._loop,+                                         client_call_details.timeout),+                    self._channel, client_call_details.method,+                    request_serializer, response_deserializer)","Based on your design, alternative 2 requires a small change in the logic, and could simplify the logic here. By encapsulate the `UnaryUnaryCall` creation as the last continuation, we now have one task that representing the call the `self. _interceptors_task`.It utilized the cancellation mechanism of `asyncio.Future` which generates an `CancelledError` exception in the deepest coroutine, then propagate upward to cancel all other coroutines.#### Case 1: Before-RPC cancellationIn this case, the RPC is not initiated, once the cancel is called. The entire interception should stop. We could do that by calling `self._interceptors_task.cancel()` in side of `InterceptedUnaryUnaryCall`'s `cancel` method. In that way, the cancel will halt any ongoing interception, and let the `CancelledError` exception propagate.#### Case 2: Ongoing-RPC cancellationOnce the RPC is running, it might pending on read or write. The `CancelledError` will be propagated to the Cython layer, and the RPC will be cancelled normally.#### Case 3: After-RPC cancellationAfter the RPC is finished, the final status is set, but the interception is not over. There will be a slightly strange behavior here, that the cancellation might appear success, but the final status is `StatusCode.OK`. The cancellation still effectively cancelled the second half of interception, but not setting the final status. To me, this is acceptable.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,357866488,2019-12-13T23:09:42Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,270 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from typing import Any, Callable, Optional, Iterator, Sequence++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import RequestType, SerializingFunction, DeserializingFunction, MetadataType++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: bytes+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor:+    """"""Affords intercepting unary-unary invocations.""""""++    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, Any], Any],+            client_call_details: ClientCallDetails, request: Any) -> Any:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors, where+    interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__(+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],  # pylint: disable=R0913+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._intercepted_call = None+        self._intercepted_call_created = asyncio.Event(loop=self._loop)+        self._cancelled_before_rpc = False+        self._invoke(interceptors, method, timeout, request, request_serializer,+                     response_deserializer)++    def __del__(self):+        self.cancel()++    def _invoke(self, interceptors: Sequence[UnaryUnaryClientInterceptor],+                method: bytes, timeout: Optional[float], request: RequestType,+                request_serializer: SerializingFunction,+                response_deserializer: DeserializingFunction) -> None:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:","Oks, let try to summarize the key points of the talk that we had ""off linee"" regarding the changes that would need to be done in this PR 1 - Do not allow the caller to access before the code interceptors are finished to the RPC attributes, the caller would need to see consistently the result of the last call returned by the last interceptor (well indeed is the first one :))2 - Considering that the synchronous client allows the interceptors to return a raw response, rather than returning always a call. The interceptor interface for Aio would give also that freedom. If a response - proto - is returned instead of a `UnaryUnaryCall`, behind the scenes and before returning the value to the prev interceptor the response will be wrapped into a new `UnaryUnaryCall` with default values.3 - In some scenarios, the interceptor could be interested in just mutating the response returned by a real RPC. For that, we could provide a new method like `call.with_response(proto)` for building a new `UnaryUnaryCall` that would inherit all of the RPC attributes by producing a new call instance where the response would be replaced for the one given as a parameter. Option 1 and 2 would be a must for having feature parity with the sync interceptors, while Option 3 could be a nice to have that could be implemented in the future if the users ask for it.  If nobody opposes I will be doing the proper changes in this PR for implementing option 1 and option 2. /cc @lidizheng @gnossen ",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21445,357875283,2019-12-13T23:57:24Z,templates/gRPC-C++.podspec.template,"@@ -20,108 +20,100 @@   # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   # See the License for the specific language governing permissions and   # limitations under the License.+  <%+  lib_maps = {lib.name: lib for lib in libs}+  filegroup_maps = {filegroup.name: filegroup for filegroup in filegroups} -  <%!-  def grpc_lib_files(libs, expect_libs, groups):+  def ruby_multiline_list(files, indent):+    return (',\n' + indent*' ').join('\'%s\'' % f for f in files)++  def is_absl_lib(target_name):+    return target_name.startswith(""absl/"")++  def get_absl_spec_name(label):+    # e.g. //absl/apple:banana -> abseil/apple/banana+    return ""abseil/"" + label[7:].replace("":"", ""/"")++  def lib_and_transitive_deps(lib):+    return list(sorted(set({lib} | lib_maps[lib].transitive_deps)))++  def regular_libs_and_abseil_specs(lib):+    regular_libs = []+    absl_specs = set()+    for lib_name in lib_and_transitive_deps(lib):+      if is_absl_lib(lib_name): continue+      regular_libs.append(lib_name)+      for dep in lib_maps[lib_name].deps:+        if is_absl_lib(dep):+          absl_specs.add(get_absl_spec_name(dep))+    return (regular_libs, list(sorted(absl_specs)))++  def list_lib_files(lib, fields):+    files = set()+    for lib_name in regular_libs_and_abseil_specs(lib)[0]:+      lib = lib_maps[lib_name]+      for field in fields:+        files.update(lib.get(field, []))+    return list(sorted(files))++  def grpc_filegroup_files(expect_filegroups, groups):",`list_filegroup_files`?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/21478,358296461,2019-12-16T15:28:29Z,include/grpcpp/server_impl.h,"@@ -182,7 +179,7 @@ class Server : public grpc::ServerInterface, private grpc::GrpcLibraryCodegen {   ///   /// \param sync_cq_timeout_msec The timeout to use when calling AsyncNext() on   /// server completion queues passed via sync_server_cqs param.-  Server(int max_message_size, ChannelArguments* args,+  Server(ChannelArguments* args,","I think this is clearly the right thing to do -- I'm not sure why this extra arg was ever here in the first place -- but this is technically a non-backward-compatible API change.  We can probably get away with this, because users should be using ServerBuilder instead of directly instantiating Server.  But let's get a review from @vjpai, just to make sure this shouldn't break anything for anyone.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/21478,358299409,2019-12-16T15:33:24Z,include/grpcpp/server_impl.h,"@@ -306,7 +303,7 @@ class Server : public grpc::ServerInterface, private grpc::GrpcLibraryCodegen {       std::unique_ptr<grpc::experimental::ServerInterceptorFactoryInterface>>       interceptor_creators_; -  const int max_receive_message_size_;+  int max_receive_message_size_;","Why is this member even needed?  It looks like the only thing it ever gets used for is populating a similar member inside of `grpc::internal::Call`, but I don't see any code in that class that actually uses that member for anything ether.  Maybe we can just remove all of this plumbing?",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21467,358327687,2019-12-16T16:20:21Z,src/cpp/README.md,"@@ -30,20 +25,75 @@ To add gRPC as a dependency in bazel:   grpc_deps()   ``` -## cmake+## CMake++`cmake` is your best option if you cannot use bazel. It supports building on Linux,+MacOS and Windows (official support) but also has a good chance of working on+other platforms (no promises!). `cmake` has good support for crosscompiling and+can be used for targeting the Android platform. -`cmake` is your best option if you cannot use bazel. It supports building on Linux, MacOS and Windows (official support) but also has a good chance of working on other platforms (no promises!). `cmake` has good-support for crosscompiling and can be used for targeting Android platform.+To build gRPC C++ from source, follow the [BUILDING guide](../../BUILDING.md). -If your project is using cmake, there are several ways to add gRPC dependency.-- install gRPC via cmake first and then locate it with `find_package(gRPC CONFIG)`. [Example](../../examples/cpp/helloworld/CMakeLists.txt)-- via cmake's `ExternalProject_Add` using a technique called ""superbuild"". [Example](../../examples/cpp/helloworld/cmake_externalproject/CMakeLists.txt)-- add gRPC source tree to your project (preferably as a git submodule) and add it to your CMake project with `add_subdirectory`. [Example](../../examples/cpp/helloworld/CMakeLists.txt)+### find_package -If your project is not using CMake (e.g. you're using `make` directly), you can first install gRPC C++ using CMake,-and have your non-CMake project rely on the `pkgconfig` files which are provided by gRPC installation. [Example](../../test/distrib/cpp/run_distrib_test_cmake_pkgconfig.sh)+The canonical way to discover dependencies in CMake is the+[`find_package` command](https://cmake.org/cmake/help/latest/command/find_package.html).++```cmake+find_package(gRPC CONFIG REQUIRED)+add_executable(my_exe my_exe.cc)+target_link_libraries(my_exe gRPC::grpc++)+```+[Full example](../../examples/cpp/helloworld/CMakeLists.txt)++`find_package` can only find software that has already been installed on your+system. The following sections describe strategies to automatically build gRPC+as part of your project.++### FetchContent+If you are using CMake v3.11 or newer you should use CMake's+[FetchContent module](https://cmake.org/cmake/help/latest/module/FetchContent.html).+This will download gRPC's source code when you initially configure your project.++```cmake+include(FetchContent)+FetchContent_Declare(+  gRPC+  GIT_REPOSITORY https://github.com/grpc/grpc+)+FetchContent_MakeAvailable(gRPC)","I am not very familiar with FetchContent, so I have a few questions:- Looks like this will download gRPC without all the submodules, so I assume all the dependencies will be missing if you don't also do FetchContent_MakeAvailable() for gRPC's dependencies first?Under the hood, does FetchContent_MakeAvailable basically do ""add_subdirectory()"" (=import grpc's CMakeLists.txt in the project's CMakeLists.txt) or does it basically build and install it into a scratch area?The advantage of performing an actual installation under a prefix is that it's the ""right C++ way"" of using a dependency. If you just import the gRPC's build file in the top level build file, it can lead to some problems because the two CMakeLists.txt are not properly isolated (in which case I'm not sure if FetchContent is as great as it looks).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21467,358335058,2019-12-16T16:32:53Z,src/cpp/README.md,"@@ -1,17 +1,12 @@--# Overview--A C++ implementation of gRPC--# To start using gRPC C+++# Add gRPC as a dependency to your project","nit: keep the ""To start using gRPC C++"" header and make first sentence ""This section describes how to add gRPC as a dependency to your C++ project"".",
224720,zackgalbreath,https://api.github.com/repos/grpc/grpc/pulls/21467,358370444,2019-12-16T17:38:25Z,src/cpp/README.md,"@@ -30,20 +25,75 @@ To add gRPC as a dependency in bazel:   grpc_deps()   ``` -## cmake+## CMake++`cmake` is your best option if you cannot use bazel. It supports building on Linux,+MacOS and Windows (official support) but also has a good chance of working on+other platforms (no promises!). `cmake` has good support for crosscompiling and+can be used for targeting the Android platform. -`cmake` is your best option if you cannot use bazel. It supports building on Linux, MacOS and Windows (official support) but also has a good chance of working on other platforms (no promises!). `cmake` has good-support for crosscompiling and can be used for targeting Android platform.+To build gRPC C++ from source, follow the [BUILDING guide](../../BUILDING.md). -If your project is using cmake, there are several ways to add gRPC dependency.-- install gRPC via cmake first and then locate it with `find_package(gRPC CONFIG)`. [Example](../../examples/cpp/helloworld/CMakeLists.txt)-- via cmake's `ExternalProject_Add` using a technique called ""superbuild"". [Example](../../examples/cpp/helloworld/cmake_externalproject/CMakeLists.txt)-- add gRPC source tree to your project (preferably as a git submodule) and add it to your CMake project with `add_subdirectory`. [Example](../../examples/cpp/helloworld/CMakeLists.txt)+### find_package -If your project is not using CMake (e.g. you're using `make` directly), you can first install gRPC C++ using CMake,-and have your non-CMake project rely on the `pkgconfig` files which are provided by gRPC installation. [Example](../../test/distrib/cpp/run_distrib_test_cmake_pkgconfig.sh)+The canonical way to discover dependencies in CMake is the+[`find_package` command](https://cmake.org/cmake/help/latest/command/find_package.html).++```cmake+find_package(gRPC CONFIG REQUIRED)+add_executable(my_exe my_exe.cc)+target_link_libraries(my_exe gRPC::grpc++)+```+[Full example](../../examples/cpp/helloworld/CMakeLists.txt)++`find_package` can only find software that has already been installed on your+system. The following sections describe strategies to automatically build gRPC+as part of your project.++### FetchContent+If you are using CMake v3.11 or newer you should use CMake's+[FetchContent module](https://cmake.org/cmake/help/latest/module/FetchContent.html).+This will download gRPC's source code when you initially configure your project.++```cmake+include(FetchContent)+FetchContent_Declare(+  gRPC+  GIT_REPOSITORY https://github.com/grpc/grpc+)+FetchContent_MakeAvailable(gRPC)","> download gRPC without all the submodulesFetchContent clones the submodules too.> Under the hood, does FetchContent_MakeAvailable basically do ""add_subdirectory()"" (=import grpc's CMakeLists.txt in the project's CMakeLists.txt) or does it basically build and install it into a scratch area?The former. In that regard it's very similar to setting up a git submodule for grpc.> it would be good to add a few sentences that summarize what fetchContent does👍 ",
224720,zackgalbreath,https://api.github.com/repos/grpc/grpc/pulls/21467,358370940,2019-12-16T17:39:29Z,src/cpp/README.md,"@@ -30,20 +25,75 @@ To add gRPC as a dependency in bazel:   grpc_deps()   ``` -## cmake+## CMake++`cmake` is your best option if you cannot use bazel. It supports building on Linux,+MacOS and Windows (official support) but also has a good chance of working on","Adding gRPC as a dependency to your CMake project won't be different on Visual Studio vs. some other generator. We could add some docs about [Visual Studio + CMake integration](https://docs.microsoft.com/en-us/cpp/build/cmake-projects-in-visual-studio?view=vs-2019) in another, more developer-centric section if you think that would be worthwhile.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21467,358391095,2019-12-16T18:24:05Z,src/cpp/README.md,"@@ -30,20 +25,75 @@ To add gRPC as a dependency in bazel:   grpc_deps()   ``` -## cmake+## CMake++`cmake` is your best option if you cannot use bazel. It supports building on Linux,+MacOS and Windows (official support) but also has a good chance of working on+other platforms (no promises!). `cmake` has good support for crosscompiling and+can be used for targeting the Android platform. -`cmake` is your best option if you cannot use bazel. It supports building on Linux, MacOS and Windows (official support) but also has a good chance of working on other platforms (no promises!). `cmake` has good-support for crosscompiling and can be used for targeting Android platform.+To build gRPC C++ from source, follow the [BUILDING guide](../../BUILDING.md). -If your project is using cmake, there are several ways to add gRPC dependency.-- install gRPC via cmake first and then locate it with `find_package(gRPC CONFIG)`. [Example](../../examples/cpp/helloworld/CMakeLists.txt)-- via cmake's `ExternalProject_Add` using a technique called ""superbuild"". [Example](../../examples/cpp/helloworld/cmake_externalproject/CMakeLists.txt)-- add gRPC source tree to your project (preferably as a git submodule) and add it to your CMake project with `add_subdirectory`. [Example](../../examples/cpp/helloworld/CMakeLists.txt)+### find_package -If your project is not using CMake (e.g. you're using `make` directly), you can first install gRPC C++ using CMake,-and have your non-CMake project rely on the `pkgconfig` files which are provided by gRPC installation. [Example](../../test/distrib/cpp/run_distrib_test_cmake_pkgconfig.sh)+The canonical way to discover dependencies in CMake is the+[`find_package` command](https://cmake.org/cmake/help/latest/command/find_package.html).++```cmake+find_package(gRPC CONFIG REQUIRED)+add_executable(my_exe my_exe.cc)+target_link_libraries(my_exe gRPC::grpc++)+```+[Full example](../../examples/cpp/helloworld/CMakeLists.txt)++`find_package` can only find software that has already been installed on your+system. The following sections describe strategies to automatically build gRPC+as part of your project.++### FetchContent+If you are using CMake v3.11 or newer you should use CMake's+[FetchContent module](https://cmake.org/cmake/help/latest/module/FetchContent.html).+This will download gRPC's source code when you initially configure your project.++```cmake+include(FetchContent)+FetchContent_Declare(+  gRPC+  GIT_REPOSITORY https://github.com/grpc/grpc+)+FetchContent_MakeAvailable(gRPC)","Ok, so IIUC, FetchContent does:1. download grpc and all its submodules2. calls ""cmake install"" for gRPC's CMakeLists.txt (which is only useful with cmake 3.13+ because otherwise installation of gRPC's dependencies of from submodules doesn't work).3. calls find_package(gRPC ....) to import installed gRPC's targets.I assume you could also use FetchContent with submodules off and import all the gRPC's dependencies separately (e.g. FetchContent(c-ares), FetchContent(protobuf) etc...) and install all the deps one-by-one? The problem of installing everything as a monolith could be if user's project wanted also use some of gRPC's dependencies (in which case it's kind of annoying that gRPC's build and installation with gRPC_*_PROVIDER=module bundles all of them).Overall I like the simplicity of FetchContent, but what I'm unsure about is whether offering too many different kind of installation options (which are kind of similar but not quite the same) will lead to confusion among users (and also on our team because it's hard to give advice when X different mode with subtle differences are supported).IMHO the only really ""clean"" way of installing (from the purist perspective) is to install all the dependencies separately and then install gRPC (all under a prefix) and then use `find_package(gRPC ...)`, but it looks like `FetchContent` can't quite achieve that?Superbuild can achieve that, but it's very annoying to use and uses weird kind of build layering. So I'm not sure what the approach here is.  ",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/21478,358404519,2019-12-16T18:53:06Z,include/grpcpp/server_impl.h,"@@ -182,7 +179,7 @@ class Server : public grpc::ServerInterface, private grpc::GrpcLibraryCodegen {   ///   /// \param sync_cq_timeout_msec The timeout to use when calling AsyncNext() on   /// server completion queues passed via sync_server_cqs param.-  Server(int max_message_size, ChannelArguments* args,+  Server(ChannelArguments* args,",It's not API. The constructors of this class are private and only accessed by a friend. I have no concerns.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21467,358669951,2019-12-17T09:01:08Z,src/cpp/README.md,"@@ -30,20 +31,86 @@ To add gRPC as a dependency in bazel:   grpc_deps()   ``` -## cmake+## CMake++`cmake` is your best option if you cannot use bazel. It supports building on Linux,+MacOS and Windows (official support) but also has a good chance of working on+other platforms (no promises!). `cmake` has good support for crosscompiling and+can be used for targeting the Android platform. -`cmake` is your best option if you cannot use bazel. It supports building on Linux, MacOS and Windows (official support) but also has a good chance of working on other platforms (no promises!). `cmake` has good-support for crosscompiling and can be used for targeting Android platform.+To build gRPC C++ from source, follow the [BUILDING guide](../../BUILDING.md). -If your project is using cmake, there are several ways to add gRPC dependency.-- install gRPC via cmake first and then locate it with `find_package(gRPC CONFIG)`. [Example](../../examples/cpp/helloworld/CMakeLists.txt)-- via cmake's `ExternalProject_Add` using a technique called ""superbuild"". [Example](../../examples/cpp/helloworld/cmake_externalproject/CMakeLists.txt)-- add gRPC source tree to your project (preferably as a git submodule) and add it to your CMake project with `add_subdirectory`. [Example](../../examples/cpp/helloworld/CMakeLists.txt)+### find_package -If your project is not using CMake (e.g. you're using `make` directly), you can first install gRPC C++ using CMake,-and have your non-CMake project rely on the `pkgconfig` files which are provided by gRPC installation. [Example](../../test/distrib/cpp/run_distrib_test_cmake_pkgconfig.sh)+The canonical way to discover dependencies in CMake is the+[`find_package` command](https://cmake.org/cmake/help/latest/command/find_package.html).++```cmake+find_package(gRPC CONFIG REQUIRED)+add_executable(my_exe my_exe.cc)+target_link_libraries(my_exe gRPC::grpc++)+```+[Full example](../../examples/cpp/helloworld/CMakeLists.txt)++`find_package` can only find software that has already been installed on your+system. In practice that means you'll need to install gRPC using cmake first.+gRPC's cmake support provides the option to install gRPC either system-wide+(not recommended) or under a directory prefix in a way that you can later+easily use it with the `find_package(gRPC CONFIG REQUIRED)` command.++The following sections describe strategies to automatically build gRPC+as part of your project.++### FetchContent+If you are using CMake v3.11 or newer you should use CMake's+[FetchContent module](https://cmake.org/cmake/help/latest/module/FetchContent.html).+The first time you run CMake in a given build directory, FetchContent will+clone the gRPC repository and its submodules. `FetchContent_MakeAvailable()`+also sets up an `add_subdirectory()` rule for you. This causes gRPC to be+built as part of your project.++```cmake+include(FetchContent)+FetchContent_Declare(+  gRPC+  GIT_REPOSITORY https://github.com/grpc/grpc+  GIT_TAG        v1.25.0+)+FetchContent_MakeAvailable(gRPC)++add_executable(my_exe my_exe.cc)+target_link_libraries(my_exe grpc++)+``` -## make+### git submodule+If you cannot use FetchContent, another approach is to add the gRPC source tree+to your project as a+[git submodule](https://git-scm.com/book/en/v2/Git-Tools-Submodules).+You can then add it to your CMake project with `add_subdirectory()`.+[Example](../../examples/cpp/helloworld/CMakeLists.txt)+","Can we mention what are some of the downsides of using add_subdirectory()?My impression is that this works fine for smaller projects without many other dependencies, but it's not the canonical way, so it may or may not work for projects with complicated builds that have many dependencies etc. Basically add_subdirectory leads can lead to insufficient isolation of the CMakeLists.txt build files, and that can be good or bad thing depending on the circumstances.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21194,358719952,2019-12-17T10:44:28Z,tools/distrib/upgrade_boringssl_objc.sh,"@@ -0,0 +1,44 @@+#!/bin/bash+# Copyright 2018 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# Generate the list of boringssl symbols that need to be shadowed based on the","nit: the comment is out of date - looks like this is intended to be a script to perform the full update of boringssl version, not just updating the shadow list.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21194,358753356,2019-12-17T12:03:42Z,tools/distrib/upgrade_boringssl_objc.sh,"@@ -0,0 +1,44 @@+#!/bin/bash+# Copyright 2018 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# Generate the list of boringssl symbols that need to be shadowed based on the+# current boringssl submodule. Requires local toolchain to build boringssl.++set -e++cd ""$(dirname $0)""+cd ../../third_party/boringssl++BORINGSSL_COMMIT=$(git rev-parse HEAD)+BORINGSSL_PREFIX_HEADERS_DIR=src/boringssl++# Do the following in grpc root directory+cd ../..++docker build tools/dockerfile/grpc_objc/generate_boringssl_prefix_header -t grpc/boringssl_prefix_header+mkdir -p $BORINGSSL_PREFIX_HEADERS_DIR+docker run -it --rm -v $(pwd)/$BORINGSSL_PREFIX_HEADERS_DIR:/output grpc/boringssl_prefix_header $BORINGSSL_COMMIT++# Increase the minor version by 1","I don't like that this script is not idempotent - if you run it multiple times, it will keep increasing the version number.IMHO just having a robust .sh that regenerates the boringssl_prefix_symbols.h is enough for this script.If you want to add a script that automates the whole boringssl update process, we can look into that but there are questions to be answered first and it doesn't belong to this PR.  I'd recommend turning this script into something that just generates the header file (with use of docker if you will).",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21194,358753762,2019-12-17T12:04:50Z,tools/distrib/upgrade_boringssl_objc.sh,"@@ -0,0 +1,44 @@+#!/bin/bash+# Copyright 2018 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# Generate the list of boringssl symbols that need to be shadowed based on the+# current boringssl submodule. Requires local toolchain to build boringssl.++set -e++cd ""$(dirname $0)""+cd ../../third_party/boringssl++BORINGSSL_COMMIT=$(git rev-parse HEAD)+BORINGSSL_PREFIX_HEADERS_DIR=src/boringssl++# Do the following in grpc root directory+cd ../..++docker build tools/dockerfile/grpc_objc/generate_boringssl_prefix_header -t grpc/boringssl_prefix_header+mkdir -p $BORINGSSL_PREFIX_HEADERS_DIR+docker run -it --rm -v $(pwd)/$BORINGSSL_PREFIX_HEADERS_DIR:/output grpc/boringssl_prefix_header $BORINGSSL_COMMIT","I suspect without changing the UID, the generated file will end up being owned by root.https://github.com/grpc/grpc/blob/2e4ebd7478c58d119210b5b68e929e7098282f9c/tools/distrib/clang_format_code.sh#L30",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21488,358783101,2019-12-17T13:16:54Z,examples/cpp/helloworld/CMakeLists.txt,"@@ -55,6 +55,23 @@ if(GRPC_AS_SUBMODULE)   set(_PROTOBUF_PROTOC $<TARGET_FILE:protoc>)   set(_GRPC_GRPCPP_UNSECURE grpc++_unsecure)   set(_GRPC_CPP_PLUGIN_EXECUTABLE $<TARGET_FILE:grpc_cpp_plugin>)+elseif(GRPC_FETCHCONTENT)+  # Another way is to use CMake's FetchContent module to clone gRPC at+  # configure time. This makes gRPC's source code available to your project,+  # similar to a git submodule.+  include(FetchContent)+  FetchContent_Declare(+    grpc+    GIT_REPOSITORY https://github.com/grpc/grpc.git+    GIT_TAG        v1.25.0)","when used in a distribtest, setting the GIT_TAG makes no sense. Nor does master. The distribtest should test with whatever gRPC commit is being under test (e.g. for PRs, it must test the version from that PR - otherwise it's useless).",
224720,zackgalbreath,https://api.github.com/repos/grpc/grpc/pulls/21467,358850356,2019-12-17T15:18:04Z,src/cpp/README.md,"@@ -30,20 +31,86 @@ To add gRPC as a dependency in bazel:   grpc_deps()   ``` -## cmake+## CMake++`cmake` is your best option if you cannot use bazel. It supports building on Linux,+MacOS and Windows (official support) but also has a good chance of working on+other platforms (no promises!). `cmake` has good support for crosscompiling and+can be used for targeting the Android platform. -`cmake` is your best option if you cannot use bazel. It supports building on Linux, MacOS and Windows (official support) but also has a good chance of working on other platforms (no promises!). `cmake` has good-support for crosscompiling and can be used for targeting Android platform.+To build gRPC C++ from source, follow the [BUILDING guide](../../BUILDING.md). -If your project is using cmake, there are several ways to add gRPC dependency.-- install gRPC via cmake first and then locate it with `find_package(gRPC CONFIG)`. [Example](../../examples/cpp/helloworld/CMakeLists.txt)-- via cmake's `ExternalProject_Add` using a technique called ""superbuild"". [Example](../../examples/cpp/helloworld/cmake_externalproject/CMakeLists.txt)-- add gRPC source tree to your project (preferably as a git submodule) and add it to your CMake project with `add_subdirectory`. [Example](../../examples/cpp/helloworld/CMakeLists.txt)+### find_package -If your project is not using CMake (e.g. you're using `make` directly), you can first install gRPC C++ using CMake,-and have your non-CMake project rely on the `pkgconfig` files which are provided by gRPC installation. [Example](../../test/distrib/cpp/run_distrib_test_cmake_pkgconfig.sh)+The canonical way to discover dependencies in CMake is the+[`find_package` command](https://cmake.org/cmake/help/latest/command/find_package.html).++```cmake+find_package(gRPC CONFIG REQUIRED)+add_executable(my_exe my_exe.cc)+target_link_libraries(my_exe gRPC::grpc++)+```+[Full example](../../examples/cpp/helloworld/CMakeLists.txt)++`find_package` can only find software that has already been installed on your+system. In practice that means you'll need to install gRPC using cmake first.+gRPC's cmake support provides the option to install gRPC either system-wide+(not recommended) or under a directory prefix in a way that you can later+easily use it with the `find_package(gRPC CONFIG REQUIRED)` command.++The following sections describe strategies to automatically build gRPC+as part of your project.++### FetchContent+If you are using CMake v3.11 or newer you should use CMake's+[FetchContent module](https://cmake.org/cmake/help/latest/module/FetchContent.html).+The first time you run CMake in a given build directory, FetchContent will+clone the gRPC repository and its submodules. `FetchContent_MakeAvailable()`+also sets up an `add_subdirectory()` rule for you. This causes gRPC to be+built as part of your project.++```cmake+include(FetchContent)+FetchContent_Declare(+  gRPC+  GIT_REPOSITORY https://github.com/grpc/grpc+  GIT_TAG        v1.25.0+)+FetchContent_MakeAvailable(gRPC)++add_executable(my_exe my_exe.cc)+target_link_libraries(my_exe grpc++)+``` -## make+### git submodule+If you cannot use FetchContent, another approach is to add the gRPC source tree+to your project as a+[git submodule](https://git-scm.com/book/en/v2/Git-Tools-Submodules).+You can then add it to your CMake project with `add_subdirectory()`.+[Example](../../examples/cpp/helloworld/CMakeLists.txt)",You mean this script?https://github.com/grpc/grpc/blob/master/test/distrib/cpp/run_distrib_test_cmake_as_submodule.shIMO it doesn't provide much insight. All it really does is set an example-specific configuration variable.,
224720,zackgalbreath,https://api.github.com/repos/grpc/grpc/pulls/21467,358856244,2019-12-17T15:27:14Z,src/cpp/README.md,"@@ -30,20 +31,86 @@ To add gRPC as a dependency in bazel:   grpc_deps()   ``` -## cmake+## CMake++`cmake` is your best option if you cannot use bazel. It supports building on Linux,+MacOS and Windows (official support) but also has a good chance of working on+other platforms (no promises!). `cmake` has good support for crosscompiling and+can be used for targeting the Android platform. -`cmake` is your best option if you cannot use bazel. It supports building on Linux, MacOS and Windows (official support) but also has a good chance of working on other platforms (no promises!). `cmake` has good-support for crosscompiling and can be used for targeting Android platform.+To build gRPC C++ from source, follow the [BUILDING guide](../../BUILDING.md). -If your project is using cmake, there are several ways to add gRPC dependency.-- install gRPC via cmake first and then locate it with `find_package(gRPC CONFIG)`. [Example](../../examples/cpp/helloworld/CMakeLists.txt)-- via cmake's `ExternalProject_Add` using a technique called ""superbuild"". [Example](../../examples/cpp/helloworld/cmake_externalproject/CMakeLists.txt)-- add gRPC source tree to your project (preferably as a git submodule) and add it to your CMake project with `add_subdirectory`. [Example](../../examples/cpp/helloworld/CMakeLists.txt)+### find_package -If your project is not using CMake (e.g. you're using `make` directly), you can first install gRPC C++ using CMake,-and have your non-CMake project rely on the `pkgconfig` files which are provided by gRPC installation. [Example](../../test/distrib/cpp/run_distrib_test_cmake_pkgconfig.sh)+The canonical way to discover dependencies in CMake is the+[`find_package` command](https://cmake.org/cmake/help/latest/command/find_package.html).++```cmake+find_package(gRPC CONFIG REQUIRED)+add_executable(my_exe my_exe.cc)+target_link_libraries(my_exe gRPC::grpc++)+```+[Full example](../../examples/cpp/helloworld/CMakeLists.txt)++`find_package` can only find software that has already been installed on your+system. In practice that means you'll need to install gRPC using cmake first.+gRPC's cmake support provides the option to install gRPC either system-wide+(not recommended) or under a directory prefix in a way that you can later+easily use it with the `find_package(gRPC CONFIG REQUIRED)` command.++The following sections describe strategies to automatically build gRPC+as part of your project.++### FetchContent+If you are using CMake v3.11 or newer you should use CMake's+[FetchContent module](https://cmake.org/cmake/help/latest/module/FetchContent.html).+The first time you run CMake in a given build directory, FetchContent will+clone the gRPC repository and its submodules. `FetchContent_MakeAvailable()`+also sets up an `add_subdirectory()` rule for you. This causes gRPC to be+built as part of your project.++```cmake+include(FetchContent)+FetchContent_Declare(+  gRPC+  GIT_REPOSITORY https://github.com/grpc/grpc+  GIT_TAG        v1.25.0+)+FetchContent_MakeAvailable(gRPC)++add_executable(my_exe my_exe.cc)+target_link_libraries(my_exe grpc++)+``` -## make+### git submodule+If you cannot use FetchContent, another approach is to add the gRPC source tree+to your project as a+[git submodule](https://git-scm.com/book/en/v2/Git-Tools-Submodules).+You can then add it to your CMake project with `add_subdirectory()`.+[Example](../../examples/cpp/helloworld/CMakeLists.txt)+","I don't know if I would necessarily call it a downside of `add_subdirectory`, but the main complication you run into with more complex dependency webs is what we ran into with opencensus recently: the need to support both these use cases:* Build my dependencies for me* Find the dependencies that are already installed on my systemWe already demonstrate how to do this with the `helloworld` cpp example, but perhaps another sentence or two in the documentation mentioning this issue would be worthwhile.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21476,358893065,2019-12-17T16:27:02Z,cmake/protobuf.cmake,"@@ -39,7 +39,9 @@ if(gRPC_PROTOBUF_PROVIDER STREQUAL ""module"")     endif()     if(TARGET protoc)       set(_gRPC_PROTOBUF_PROTOC protoc)-      set(_gRPC_PROTOBUF_PROTOC_EXECUTABLE $<TARGET_FILE:protoc>)+      if(NOT DEFINED _gRPC_PROTOBUF_PROTOC_EXECUTABLE)+        set(_gRPC_PROTOBUF_PROTOC_EXECUTABLE $<TARGET_FILE:protoc>)","Btw, this is a different logic than we use for _gRPC_CPP_PLUGINhttps://github.com/grpc/grpc/blob/cd56d6664395f93dcc3ff1564df2d0d034275190/templates/CMakeLists.txt.template#L311we should probably unify. Can you rely on CMAKE_CROSSCOMPILING here?",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,358893738,2019-12-17T16:27:58Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -0,0 +1,340 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest++import grpc++from grpc.experimental import aio+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from src.proto.grpc.testing import messages_pb2+++class TestUnaryUnaryClientInterceptor(AioTestBase):++    def test_invalid_interceptor(self):++        class InvalidInterceptor:+            """"""Just an invalid Interceptor""""""++        with self.assertRaises(ValueError):+            aio.insecure_channel("""", interceptors=[InvalidInterceptor()])++    async def test_executed_right_order(self):++        interceptors_executed = []++        class Interceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for testing if the interceptor is being called""""""++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                interceptors_executed.append(self)+                call = await continuation(client_call_details, request)+                return call++        interceptors = [Interceptor() for i in range(2)]++        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=interceptors) as channel:+            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())+            response = await call++            # Check that all interceptors were executed, and were executed+            # in the right order.+            self.assertSequenceEqual(interceptors_executed, interceptors)++            self.assertIsInstance(response, messages_pb2.SimpleResponse)++    @unittest.expectedFailure+    # TODO(https://github.com/grpc/grpc/issues/20144) Once metadata support is+    # implemented in the client-side, this test must be implemented.+    def test_modify_metadata(self):+        raise NotImplementedError()++    @unittest.expectedFailure+    # TODO(https://github.com/grpc/grpc/issues/20532) Once credentials support is+    # implemented in the client-side, this test must be implemented.+    def test_modify_credentials(self):+        raise NotImplementedError()++    async def test_status_code_Ok(self):++        class StatusCodeOkInterceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for observing status code Ok returned by the RPC""""""++            def __init__(self):+                self.status_code_Ok_observed = False++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                code = await call.code()+                if code == grpc.StatusCode.OK:+                    self.status_code_Ok_observed = True++                return call++        interceptor = StatusCodeOkInterceptor()+        server_target, server = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=[interceptor]) as channel:++            # when no error StatusCode.OK must be observed+            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)++            await multicallable(messages_pb2.SimpleRequest())++            self.assertTrue(interceptor.status_code_Ok_observed)++    async def test_add_timeout(self):++        class TimeoutInterceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for adding a timeout to the RPC""""""++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                new_client_call_details = aio.ClientCallDetails(+                    method=client_call_details.method,+                    timeout=0.1,+                    metadata=client_call_details.metadata,+                    credentials=client_call_details.credentials)+                return await continuation(new_client_call_details, request)++        interceptor = TimeoutInterceptor()+        server_target, server = await start_test_server()++        async with aio.insecure_channel(+                server_target, interceptors=[interceptor]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)++            await server.stop(None)++            with self.assertRaises(grpc.RpcError) as exception_context:+                await multicallable(messages_pb2.SimpleRequest())++            self.assertEqual(exception_context.exception.code(),+                             grpc.StatusCode.DEADLINE_EXCEEDED)+++class TestInterceptedUnaryUnaryCall(AioTestBase):++    async def test_call_ok(self):++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                return call++        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=[Interceptor()]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())+            response = await call++            self.assertTrue(call.done())+            self.assertFalse(call.cancelled())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)+            self.assertEqual(await call.details(), '')+            self.assertEqual(await call.initial_metadata(), ())+            self.assertEqual(await call.trailing_metadata(), ())++    async def test_rpc_attributes_available_before_interceptor_ends(self):",This test has been removed because this logic is no longer supported.,X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,358904256,2019-12-17T16:45:33Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,345 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from abc import ABCMeta, abstractmethod+from typing import Callable, Optional, Iterator, Sequence, Text, Union++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import (+    RequestType, SerializingFunction, DeserializingFunction,+    MetadataType, ResponseType+)++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: Text+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor(metaclass=ABCMeta):+    """"""Affords intercepting unary-unary invocations.""""""++    @abstractmethod+    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, RequestType],+                                         UnaryUnaryCall],+            client_call_details: ClientCallDetails,+            request: RequestType) -> Union[UnaryUnaryCall, ResponseType]:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    Interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__( # pylint: disable=R0913+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._interceptors_task = asyncio.ensure_future(+            self._invoke(interceptors, method, timeout, request,+                         request_serializer, response_deserializer))++    def __del__(self):+        self.cancel()++    async def _invoke(+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            method: bytes, timeout: Optional[float], request: RequestType,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                try:+                    call_or_response = await interceptor.intercept_unary_unary(+                        continuation, client_call_details, request)+                except grpc.RpcError as err:+                    # gRPC error is masked inside an artificial call,+                    # caller will see this error if and only+                    # if runs an `await call` operation+                    return UnaryUnaryCallRpcError(err)+                except asyncio.CancelledError:","@lidizheng this is one of the main pain points having to deal with local cancellations masked as simple and generic `asyncio.CancelledError`.For making them available to the previous interceptor or to the caller implementing the following flow:```pythoncall = multicallable(...)try:    await callexcept asyncio.CancelledError    print(await call.code()) ```We have been forced to create an artificial `UnaryUnaryCancelledError` which masks the `asyncio.CancelledError`, which IMO is debatable.Now this artificial `UnaryUnaryCancelledError` class makes its best effort and just returns alwaySthe `grpc.StatusCode.CANCELLED` status code, which might be totally false since the RPC could not even be started.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21495,358959311,2019-12-17T18:35:29Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -319,33 +329,26 @@ cdef class AioServer:                 break              # Accepts new request from Core-            rpc_state = await _server_call_request_call(-                self._server,-                self._cq,-                self._loop)--            # Schedules the RPC as a separate coroutine-            rpc_task = self._loop.create_task(-                _handle_rpc(-                    self._generic_handlers,-                    rpc_state,-                    self._loop-                )-            )+            rpc_state = await self._request_call()++            # Creates the dedicated RPC coroutine. If we schedule it right now,","Nice job catching this race condition! Was this causing any tests to fail/flake? If not, can we add one?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,358959722,2019-12-17T18:36:19Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,345 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from abc import ABCMeta, abstractmethod+from typing import Callable, Optional, Iterator, Sequence, Text, Union++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import (+    RequestType, SerializingFunction, DeserializingFunction,+    MetadataType, ResponseType+)++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: Text+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor(metaclass=ABCMeta):+    """"""Affords intercepting unary-unary invocations.""""""++    @abstractmethod+    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, RequestType],+                                         UnaryUnaryCall],+            client_call_details: ClientCallDetails,+            request: RequestType) -> Union[UnaryUnaryCall, ResponseType]:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    Interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__( # pylint: disable=R0913+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._interceptors_task = asyncio.ensure_future(+            self._invoke(interceptors, method, timeout, request,+                         request_serializer, response_deserializer))++    def __del__(self):+        self.cancel()++    async def _invoke(+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            method: bytes, timeout: Optional[float], request: RequestType,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                try:+                    call_or_response = await interceptor.intercept_unary_unary(+                        continuation, client_call_details, request)+                except grpc.RpcError as err:+                    # gRPC error is masked inside an artificial call,+                    # caller will see this error if and only+                    # if runs an `await call` operation+                    return UnaryUnaryCallRpcError(err)+                except asyncio.CancelledError:","The solution you have is similar to what sync stack have (see [code](https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_interceptor.py#L150). They have a `FailureOutcome` class and a `UnaryOutcome` class.However, about the `UnaryUnaryCancelledError`, I have several thoughts to discuss:1. If `asyncio.CancelledError` is explicitly injected by the application, should we abort the interception?2. Should we give users the power to decide whether to abort or not? They can use following script to stop `asyncio.CancelledError` bottom-up propagation.3. Is there a reason `UnaryUnaryCancelledError` can not merge with `UnaryUnaryCallRpcError`?```Pythonasync def intercept_unary_unary(self, continuation, client_call_details, request):    try:        call = await continuation(client_call_details, request)    except asyncio.CancelledError:        print('RPC cancelled locally.')    except grpc.RpcError as rpc_error:        if rpc_error.code() == grpc.StatusCode.CANCELLED:            print('RPC cancelled remotely.')    return call```",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,358964087,2019-12-17T18:45:57Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,345 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from abc import ABCMeta, abstractmethod+from typing import Callable, Optional, Iterator, Sequence, Text, Union++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import (+    RequestType, SerializingFunction, DeserializingFunction,+    MetadataType, ResponseType+)++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: Text+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor(metaclass=ABCMeta):+    """"""Affords intercepting unary-unary invocations.""""""++    @abstractmethod+    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, RequestType],+                                         UnaryUnaryCall],+            client_call_details: ClientCallDetails,+            request: RequestType) -> Union[UnaryUnaryCall, ResponseType]:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    Interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__( # pylint: disable=R0913+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._interceptors_task = asyncio.ensure_future(+            self._invoke(interceptors, method, timeout, request,+                         request_serializer, response_deserializer))++    def __del__(self):+        self.cancel()++    async def _invoke(+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            method: bytes, timeout: Optional[float], request: RequestType,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                try:+                    call_or_response = await interceptor.intercept_unary_unary(+                        continuation, client_call_details, request)+                except grpc.RpcError as err:+                    # gRPC error is masked inside an artificial call,+                    # caller will see this error if and only+                    # if runs an `await call` operation+                    return UnaryUnaryCallRpcError(err)","Why do we need to mask the `grpc.RpcError` as `UnaryUnaryCallRpcError`? Instead, if the motivation is to let other interceptors able to handle the `RpcError`, we could simply let the exception propagate. You are using recursion here, the stack should do the trick for us.",
224720,zackgalbreath,https://api.github.com/repos/grpc/grpc/pulls/21488,358970680,2019-12-17T18:59:32Z,examples/cpp/helloworld/CMakeLists.txt,"@@ -55,6 +55,23 @@ if(GRPC_AS_SUBMODULE)   set(_PROTOBUF_PROTOC $<TARGET_FILE:protoc>)   set(_GRPC_GRPCPP_UNSECURE grpc++_unsecure)   set(_GRPC_CPP_PLUGIN_EXECUTABLE $<TARGET_FILE:grpc_cpp_plugin>)+elseif(GRPC_FETCHCONTENT)+  # Another way is to use CMake's FetchContent module to clone gRPC at+  # configure time. This makes gRPC's source code available to your project,+  # similar to a git submodule.+  include(FetchContent)+  FetchContent_Declare(+    grpc+    GIT_REPOSITORY https://github.com/grpc/grpc.git+    GIT_TAG        v1.25.0)","👍 I came up with what I think is a ""best of both worlds"" approach. The example still specifies v1.25.0, but we override it in the distribtest by setting `FETCHCONTENT_SOURCE_DIR_GRPC` to the existing clone of GRPC.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21495,358974336,2019-12-17T19:07:10Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -188,13 +195,19 @@ async def _handle_unary_stream_rpc(object method_handler,     await execute_batch(rpc_state, ops, loop)  -async def _handle_cancellation_from_core(object rpc_task,-                                         RPCState rpc_state,-                                         object loop):+async def _schedule_rpc_coro_and_handle_cancellation(object rpc_coro,","Split into `_schedule_rpc_coro` and `_handle_cancellation_from_core`, and the former function is calling the latter function. The relationship between this two functions shouldn't be parallel, otherwise the race condition still exists.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21495,358976478,2019-12-17T19:11:34Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -319,33 +329,26 @@ cdef class AioServer:                 break              # Accepts new request from Core-            rpc_state = await _server_call_request_call(-                self._server,-                self._cq,-                self._loop)--            # Schedules the RPC as a separate coroutine-            rpc_task = self._loop.create_task(-                _handle_rpc(-                    self._generic_handlers,-                    rpc_state,-                    self._loop-                )-            )+            rpc_state = await self._request_call()++            # Creates the dedicated RPC coroutine. If we schedule it right now,","It is covered by any early cancellation test cases, e.g. [`test_early_cancel_unary_stream`](https://github.com/grpc/grpc/blob/master/src/python/grpcio_tests/tests_aio/unit/call_test.py#L217). After firing the RPC, sends cancel from the client-side immediately. If the RPC coroutine is running and the cancellation coroutine is not, the test will hang.This race condition is more likely to happen in unit tests because both the client and the server are running in the same event loop.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21455,358984053,2019-12-17T19:27:00Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -135,89 +135,194 @@ class TimeoutInterceptor(aio.UnaryUnaryClientInterceptor):                 request_serializer=messages_pb2.SimpleRequest.SerializeToString,                 response_deserializer=messages_pb2.SimpleResponse.FromString) +            call = multicallable(messages_pb2.SimpleRequest())+             await server.stop(None)              with self.assertRaises(grpc.RpcError) as exception_context:-                await multicallable(messages_pb2.SimpleRequest())+                await call              self.assertEqual(exception_context.exception.code(),                              grpc.StatusCode.DEADLINE_EXCEEDED) +            self.assertTrue(call.done())+            self.assertEqual(grpc.StatusCode.DEADLINE_EXCEEDED, await+                             call.code()) -class TestInterceptedUnaryUnaryCall(AioTestBase):+    async def test_retry(self): -    async def test_call_ok(self):+        class RetryInterceptor(aio.UnaryUnaryClientInterceptor):+            """"""Simulates a Retry Interceptor which ends up by making +            two RPC calls."""""" -        class Interceptor(aio.UnaryUnaryClientInterceptor):+            def __init__(self):+                self.calls = []              async def intercept_unary_unary(self, continuation,                                             client_call_details, request):-                call = await continuation(client_call_details, request)++                new_client_call_details = aio.ClientCallDetails(+                    method=client_call_details.method,+                    timeout=0.1,+                    metadata=client_call_details.metadata,+                    credentials=client_call_details.credentials)++                try:+                    call = await continuation(new_client_call_details, request)+                    await call+                except grpc.RpcError:+                    pass++                self.calls.append(call)++                new_client_call_details = aio.ClientCallDetails(+                    method=client_call_details.method,+                    timeout=None,+                    metadata=client_call_details.metadata,+                    credentials=client_call_details.credentials)++                call = await continuation(new_client_call_details, request)+                self.calls.append(call)                 return call -        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        interceptor = RetryInterceptor()+        server_target, server = await start_test_server()          async with aio.insecure_channel(-                server_target, interceptors=[Interceptor()]) as channel:+                server_target, interceptors=[interceptor]) as channel:              multicallable = channel.unary_unary(                 '/grpc.testing.TestService/UnaryCall',                 request_serializer=messages_pb2.SimpleRequest.SerializeToString,                 response_deserializer=messages_pb2.SimpleResponse.FromString)+             call = multicallable(messages_pb2.SimpleRequest())-            response = await call +            await call++            self.assertEqual(grpc.StatusCode.OK, await+                             call.code())++            # Check that two calls were made, first one finishing with+            # a deadline and second one finishing ok..+            self.assertEqual(len(interceptor.calls), 2)+            self.assertEqual(await interceptor.calls[0].code(), grpc.StatusCode.DEADLINE_EXCEEDED)+            self.assertEqual(await interceptor.calls[1].code(), grpc.StatusCode.OK)++    async def test_rpcerror_raised_when_call_is_awaited(self):++        class Interceptor(aio.UnaryUnaryClientInterceptor):+            """"""RpcErrors are only seen when the call is awaited""""""++            def __init__(self):+                self.deadline_seen = []",Nit: Looks like `deadline_seen` changes from `List` to `bool`?,X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,359028226,2019-12-17T21:06:46Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,345 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from abc import ABCMeta, abstractmethod+from typing import Callable, Optional, Iterator, Sequence, Text, Union++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import (+    RequestType, SerializingFunction, DeserializingFunction,+    MetadataType, ResponseType+)++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: Text+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor(metaclass=ABCMeta):+    """"""Affords intercepting unary-unary invocations.""""""++    @abstractmethod+    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, RequestType],+                                         UnaryUnaryCall],+            client_call_details: ClientCallDetails,+            request: RequestType) -> Union[UnaryUnaryCall, ResponseType]:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    Interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__( # pylint: disable=R0913+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._interceptors_task = asyncio.ensure_future(+            self._invoke(interceptors, method, timeout, request,+                         request_serializer, response_deserializer))++    def __del__(self):+        self.cancel()++    async def _invoke(+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            method: bytes, timeout: Optional[float], request: RequestType,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                try:+                    call_or_response = await interceptor.intercept_unary_unary(+                        continuation, client_call_details, request)+                except grpc.RpcError as err:+                    # gRPC error is masked inside an artificial call,+                    # caller will see this error if and only+                    # if runs an `await call` operation+                    return UnaryUnaryCallRpcError(err)","I'm doing this for the sake of consistency, remember that the pattern that we are prescribing for the caller is the following one:```pythoncall = mutlicallable(...)try:    await callexcept grpc.RpcError as err:    print(await call.code())except asyncio.CancelledError:    print(await call.code())```Basically we are asking for following the next rules:1 - The way of raising any issue is by awaiting the call, this would let you know if there was an error by raising an exception or returning a response if everything went fine.2 - Even raising an exception call will keep and could be used for gathering the RPC attributes3- This is especially relevant when an `asyncio.CancelledErrorr` happens, where the only place where RPC attributes can be gathered is by using the callBy masking the the `grpc.RpcErrror` and using a `UnaryUnaryCallRpcError` instead, we are making the previous pattern explicit in the interceptors side.Without doing this the rules that we already set up for the caller could be easily unfollowed by the interceptors, so ending up with something like this:```pythontry:    call = await continuation(....)except grpc.RpcError as err:    print(await call.code())except asyncio.CancelledError:    print(await call.code())```This code won't be following the prescription that we made for the caller, since the call will never be awaited, and in case of an error, the `call` will never get any value since the exception happened during the continuation execution.So if the interceptor needs to take care of the outcome of a call this would need to follow the following pattern:```pythontry:    call = await continuation(....)    await callexcept grpc.RpcError as err:    print(await call.code())except asyncio.CancelledError:    print(await call.code())```Remember that the first await when the continuation is called only relates to the coroutine call, so is also needed a second await on the call returned. Hence, is when the except statements make sense since the call would have all of the RPC context.",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,359036085,2019-12-17T21:24:45Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,345 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from abc import ABCMeta, abstractmethod+from typing import Callable, Optional, Iterator, Sequence, Text, Union++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import (+    RequestType, SerializingFunction, DeserializingFunction,+    MetadataType, ResponseType+)++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: Text+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor(metaclass=ABCMeta):+    """"""Affords intercepting unary-unary invocations.""""""++    @abstractmethod+    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, RequestType],+                                         UnaryUnaryCall],+            client_call_details: ClientCallDetails,+            request: RequestType) -> Union[UnaryUnaryCall, ResponseType]:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    Interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__( # pylint: disable=R0913+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._interceptors_task = asyncio.ensure_future(+            self._invoke(interceptors, method, timeout, request,+                         request_serializer, response_deserializer))++    def __del__(self):+        self.cancel()++    async def _invoke(+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            method: bytes, timeout: Optional[float], request: RequestType,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                try:+                    call_or_response = await interceptor.intercept_unary_unary(+                        continuation, client_call_details, request)+                except grpc.RpcError as err:+                    # gRPC error is masked inside an artificial call,+                    # caller will see this error if and only+                    # if runs an `await call` operation+                    return UnaryUnaryCallRpcError(err)+                except asyncio.CancelledError:","One comment about your snippet, for completing the RPC you would need to await the call that has been returned by the continuation. Otherwise, you won't get any response or exceptionRegarding the questions that you made>If asyncio.CancelledError is explicitly injected by the application, should we abort the interception?You mean but having another part of the app canceling with the Asyncio interface a task that might be executing an RPC? In that case, the cancellation path should be the same. I could add a new test for that scenario. > Should we give users the power to decide whether to abort or not? They can use following script to stop asyncio.CancelledError bottom-up propagation.If the user wants to swallow the local cancellation and try it again has total freedom on makeing a new call, for example:```pythonasync def intercept_unary_unary(self, continuation, client_call_details, request):    for i in range(ATTEMPTS):        try:            call = await continuation(client_call_details, request)            await call            break        except asyncio.CancelledError:            print('RPC cancelled locally.')            print('Trying again')        except grpc.RpcError as rpc_error:            if rpc_error.code() == grpc.StatusCode.CANCELLED:                print('RPC cancelled remotely.')           print('Trying again')    return call```> Is there a reason UnaryUnaryCancelledError can not merge with UnaryUnaryCallRpcErrorJust an implementation detail, while in the `UnaryUnaryCallRpcError` RPC attributes depend on the RPC error, for the `UnaryUnaryCancelledError` everything is already pre-backed in the class.",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,359037357,2019-12-17T21:27:28Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -0,0 +1,340 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest++import grpc++from grpc.experimental import aio+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from src.proto.grpc.testing import messages_pb2+++class TestUnaryUnaryClientInterceptor(AioTestBase):++    def test_invalid_interceptor(self):++        class InvalidInterceptor:+            """"""Just an invalid Interceptor""""""++        with self.assertRaises(ValueError):+            aio.insecure_channel("""", interceptors=[InvalidInterceptor()])++    async def test_executed_right_order(self):++        interceptors_executed = []++        class Interceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for testing if the interceptor is being called""""""++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                interceptors_executed.append(self)+                call = await continuation(client_call_details, request)+                return call++        interceptors = [Interceptor() for i in range(2)]++        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=interceptors) as channel:+            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())+            response = await call++            # Check that all interceptors were executed, and were executed+            # in the right order.+            self.assertSequenceEqual(interceptors_executed, interceptors)++            self.assertIsInstance(response, messages_pb2.SimpleResponse)++    @unittest.expectedFailure+    # TODO(https://github.com/grpc/grpc/issues/20144) Once metadata support is+    # implemented in the client-side, this test must be implemented.+    def test_modify_metadata(self):+        raise NotImplementedError()++    @unittest.expectedFailure+    # TODO(https://github.com/grpc/grpc/issues/20532) Once credentials support is+    # implemented in the client-side, this test must be implemented.+    def test_modify_credentials(self):+        raise NotImplementedError()++    async def test_status_code_Ok(self):++        class StatusCodeOkInterceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for observing status code Ok returned by the RPC""""""++            def __init__(self):+                self.status_code_Ok_observed = False++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                code = await call.code()+                if code == grpc.StatusCode.OK:+                    self.status_code_Ok_observed = True++                return call++        interceptor = StatusCodeOkInterceptor()+        server_target, server = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=[interceptor]) as channel:++            # when no error StatusCode.OK must be observed+            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)++            await multicallable(messages_pb2.SimpleRequest())++            self.assertTrue(interceptor.status_code_Ok_observed)++    async def test_add_timeout(self):++        class TimeoutInterceptor(aio.UnaryUnaryClientInterceptor):+            """"""Interceptor used for adding a timeout to the RPC""""""++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                new_client_call_details = aio.ClientCallDetails(+                    method=client_call_details.method,+                    timeout=0.1,+                    metadata=client_call_details.metadata,+                    credentials=client_call_details.credentials)+                return await continuation(new_client_call_details, request)++        interceptor = TimeoutInterceptor()+        server_target, server = await start_test_server()++        async with aio.insecure_channel(+                server_target, interceptors=[interceptor]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)++            await server.stop(None)++            with self.assertRaises(grpc.RpcError) as exception_context:+                await multicallable(messages_pb2.SimpleRequest())++            self.assertEqual(exception_context.exception.code(),+                             grpc.StatusCode.DEADLINE_EXCEEDED)+++class TestInterceptedUnaryUnaryCall(AioTestBase):++    async def test_call_ok(self):++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                return call++        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=[Interceptor()]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())+            response = await call++            self.assertTrue(call.done())+            self.assertFalse(call.cancelled())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)+            self.assertEqual(await call.details(), '')+            self.assertEqual(await call.initial_metadata(), ())+            self.assertEqual(await call.trailing_metadata(), ())++    async def test_rpc_attributes_available_before_interceptor_ends(self):++        interceptor_lock = asyncio.Event()++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                await interceptor_lock.wait()+                return call++        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=[Interceptor()]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())++            # Following RPC response attributes can be accessed without+            # having to wait for the finalitzation of the interceptor code+            self.assertEqual(await call.code(), grpc.StatusCode.OK)+            self.assertEqual(await call.details(), '')+            self.assertEqual(await call.initial_metadata(), ())+            self.assertEqual(await call.trailing_metadata(), ())++            interceptor_lock.set()+            await call++    async def test_rpc_response_available_after_interceptor_ends(self):++        interceptor_lock = asyncio.Event()++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                await interceptor_lock.wait()+                return call++        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(+                server_target, interceptors=[Interceptor()]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())++            response_read = False++            async def read_response():+                nonlocal response_read+                await call+                response_read = True++            task = asyncio.ensure_future(read_response())+            await asyncio.sleep(0)+            self.assertFalse(response_read)++            interceptor_lock.set()+            await task+            self.assertTrue(response_read)++    async def test_cancel_before_rpc(self):","If I'm not missing something, in both test cases, cancellation happens just in the middle of the intercept functions.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,359057934,2019-12-17T22:15:47Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,345 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from abc import ABCMeta, abstractmethod+from typing import Callable, Optional, Iterator, Sequence, Text, Union++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import (+    RequestType, SerializingFunction, DeserializingFunction,+    MetadataType, ResponseType+)++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: Text+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor(metaclass=ABCMeta):+    """"""Affords intercepting unary-unary invocations.""""""++    @abstractmethod+    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, RequestType],+                                         UnaryUnaryCall],+            client_call_details: ClientCallDetails,+            request: RequestType) -> Union[UnaryUnaryCall, ResponseType]:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    Interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__( # pylint: disable=R0913+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._interceptors_task = asyncio.ensure_future(+            self._invoke(interceptors, method, timeout, request,+                         request_serializer, response_deserializer))++    def __del__(self):+        self.cancel()++    async def _invoke(+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            method: bytes, timeout: Optional[float], request: RequestType,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                try:+                    call_or_response = await interceptor.intercept_unary_unary(+                        continuation, client_call_details, request)+                except grpc.RpcError as err:+                    # gRPC error is masked inside an artificial call,+                    # caller will see this error if and only+                    # if runs an `await call` operation+                    return UnaryUnaryCallRpcError(err)","> 1 - The way of raising any issue is by awaiting the call, this would let you know if there was an error by raising an exception or returning a response if everything went fine.> 2 - Even raising an exception call will keep and could be used for gathering the RPC attributes> 3- This is especially relevant when an asyncio.CancelledErrorr happens, where the only place where RPC attributes can be gathered is by using the callYes. These rules are ones we are following for normal calls.> This code won't be following the prescription that we made for the caller, since the call will never be awaited, and in case of an error, the call will never get any value since the exception happened during the continuation execution.Good argument! This is indeed a problem I neglected. I think current solution is self-contained enough. Before I LGTM, I still want to have a discussion about another alternative:Can we advice users to only access RPC's error status through `RpcError`?I felt like most of the pain comes from a design that is too idealistic, which we are trying to support both `asyncio.CancelledError` and `RpcError` with `CANCELLED` status code. Since the `cancel` method is implemented by ourselves, we can make sure the `asyncio.CancelledError` is handled in the Cython layer and then we can wrap it with `RpcError`. So, users can only observe `RpcError` being raised from `await call`.In this way, the error-handling section of code is unified in all scenarios.```Python# Normal Unary Calltry:    response = await callexcept grpc.RpcError as err:    print(err.code())# Interceptor only await continuationtry:    call = await continuation(....)except grpc.RpcError as err:    print(err.code())# Interceptor awaits RPCtry:    call = await continuation(....)    await callexcept grpc.RpcError as err:    print(err.code())```",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,359060495,2019-12-17T22:22:33Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,334 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from abc import ABCMeta, abstractmethod+from typing import Callable, Optional, Iterator, Sequence, Text, Union++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import (RequestType, SerializingFunction, DeserializingFunction,+                      MetadataType, ResponseType)+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: Text+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor(metaclass=ABCMeta):+    """"""Affords intercepting unary-unary invocations.""""""++    @abstractmethod+    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, RequestType],+                                         UnaryUnaryCall],+            client_call_details: ClientCallDetails,+            request: RequestType) -> Union[UnaryUnaryCall, ResponseType]:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    Interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__(  # pylint: disable=R0913+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            request: RequestType, timeout: Optional[float],+            channel: cygrpc.AioChannel, method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._interceptors_task = asyncio.ensure_future(+            self._invoke(interceptors, method, timeout, request,+                         request_serializer, response_deserializer))++    def __del__(self):+        self.cancel()++    async def _invoke(+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            method: bytes, timeout: Optional[float], request: RequestType,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                try:+                    call_or_response = await interceptor.intercept_unary_unary(+                        continuation, client_call_details, request)+                except grpc.RpcError as err:+                    # gRPC error is masked inside an artificial call,+                    # caller will see this error if and only+                    # if runs an `await call` operation+                    return UnaryUnaryCallRpcError(err)+                except asyncio.CancelledError:+                    # Cancellation is masked inside an artificial call,+                    # caller will see this error if and only+                    # if runs an `await call` operation+                    return UnaryUnaryCancelledError()++                if isinstance(call_or_response, _base_call.UnaryUnaryCall):+                    return call_or_response+                else:+                    return UnaryUnaryCallResponse(call_or_response)++            else:+                return UnaryUnaryCall(request,+                                      _timeout_to_deadline(+                                          self._loop,+                                          client_call_details.timeout),+                                      self._channel, client_call_details.method,+                                      request_serializer, response_deserializer)++        client_call_details = ClientCallDetails(method, timeout, None, None)+        return await _run_interceptor(+            iter(interceptors), client_call_details, request)++    def cancel(self) -> bool:+        if self._interceptors_task.done():+            return False++        return self._interceptors_task.cancel()++    def cancelled(self) -> bool:+        if not self._interceptors_task.done():+            return False++        call = self._interceptors_task.result()+        return call.cancelled()++    def done(self) -> bool:+        if not self._interceptors_task.done():+            return False++        return True++    def add_done_callback(self, unused_callback) -> None:+        raise NotImplementedError()++    def time_remaining(self) -> Optional[float]:+        raise NotImplementedError()++    async def initial_metadata(self) -> Optional[MetadataType]:+        return await (await self._interceptors_task).initial_metadata()++    async def trailing_metadata(self) -> Optional[MetadataType]:+        return await (await self._interceptors_task).trailing_metadata()++    async def code(self) -> grpc.StatusCode:+        return await (await self._interceptors_task).code()++    async def details(self) -> str:+        return await (await self._interceptors_task).details()++    async def debug_error_string(self) -> Optional[str]:+        return await (await self._interceptors_task).debug_error_string()++    def __await__(self):+        call = yield from self._interceptors_task.__await__()+        response = yield from call.__await__()+        return response+++class UnaryUnaryCallRpcError(_base_call.UnaryUnaryCall):+    """"""Final UnaryUnaryCall class finished with an RpcError.""""""+    _error: grpc.RpcError++    def __init__(self, error: grpc.RpcError) -> None:+        self._error = error++    def cancel(self) -> bool:+        return False++    def cancelled(self) -> bool:+        return False++    def done(self) -> bool:+        return True++    def add_done_callback(self, unused_callback) -> None:+        raise NotImplementedError()++    def time_remaining(self) -> Optional[float]:+        raise NotImplementedError()++    async def initial_metadata(self) -> Optional[MetadataType]:+        return None++    async def trailing_metadata(self) -> Optional[MetadataType]:+        return self._error.initial_metadata()++    async def code(self) -> grpc.StatusCode:+        return self._error.code()++    async def details(self) -> str:+        return self._error.details()++    async def debug_error_string(self) -> Optional[str]:+        return self._error.debug_error_string()++    def __await__(self):+        raise self._error+++class UnaryUnaryCallResponse(_base_call.UnaryUnaryCall):+    """"""Final UnaryUnaryCall class finished with a response.""""""+    _response: ResponseType++    def __init__(self, response: ResponseType) -> None:+        self._response = response++    def cancel(self) -> bool:+        return False++    def cancelled(self) -> bool:+        return False++    def done(self) -> bool:+        return True++    def add_done_callback(self, unused_callback) -> None:+        raise NotImplementedError()++    def time_remaining(self) -> Optional[float]:+        raise NotImplementedError()++    async def initial_metadata(self) -> Optional[MetadataType]:+        return None++    async def trailing_metadata(self) -> Optional[MetadataType]:+        return None++    async def code(self) -> grpc.StatusCode:+        return grpc.StatusCode.OK++    async def details(self) -> str:+        return ''++    async def debug_error_string(self) -> Optional[str]:+        return None++    def __await__(self):+        if False:  # pylint: disable=W0125+            # This code path is never used, but a yield statement is needed+            # for telling the interpreterthat __await__ is a generator.+            yield None","nit: Can we add a TODO here to remove this trick? Potentially, we could make this class a generator. It doesn't block this PR.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,359061806,2019-12-17T22:26:03Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,334 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from abc import ABCMeta, abstractmethod+from typing import Callable, Optional, Iterator, Sequence, Text, Union++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import (RequestType, SerializingFunction, DeserializingFunction,+                      MetadataType, ResponseType)+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: Text+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor(metaclass=ABCMeta):+    """"""Affords intercepting unary-unary invocations.""""""++    @abstractmethod+    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, RequestType],+                                         UnaryUnaryCall],+            client_call_details: ClientCallDetails,+            request: RequestType) -> Union[UnaryUnaryCall, ResponseType]:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    Interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__(  # pylint: disable=R0913+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            request: RequestType, timeout: Optional[float],+            channel: cygrpc.AioChannel, method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._interceptors_task = asyncio.ensure_future(+            self._invoke(interceptors, method, timeout, request,+                         request_serializer, response_deserializer))++    def __del__(self):+        self.cancel()++    async def _invoke(+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            method: bytes, timeout: Optional[float], request: RequestType,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                try:+                    call_or_response = await interceptor.intercept_unary_unary(+                        continuation, client_call_details, request)+                except grpc.RpcError as err:+                    # gRPC error is masked inside an artificial call,+                    # caller will see this error if and only+                    # if runs an `await call` operation+                    return UnaryUnaryCallRpcError(err)+                except asyncio.CancelledError:+                    # Cancellation is masked inside an artificial call,+                    # caller will see this error if and only+                    # if runs an `await call` operation+                    return UnaryUnaryCancelledError()++                if isinstance(call_or_response, _base_call.UnaryUnaryCall):+                    return call_or_response+                else:+                    return UnaryUnaryCallResponse(call_or_response)++            else:+                return UnaryUnaryCall(request,+                                      _timeout_to_deadline(+                                          self._loop,+                                          client_call_details.timeout),+                                      self._channel, client_call_details.method,+                                      request_serializer, response_deserializer)++        client_call_details = ClientCallDetails(method, timeout, None, None)+        return await _run_interceptor(+            iter(interceptors), client_call_details, request)++    def cancel(self) -> bool:+        if self._interceptors_task.done():+            return False++        return self._interceptors_task.cancel()++    def cancelled(self) -> bool:+        if not self._interceptors_task.done():+            return False++        call = self._interceptors_task.result()+        return call.cancelled()++    def done(self) -> bool:+        if not self._interceptors_task.done():+            return False++        return True++    def add_done_callback(self, unused_callback) -> None:+        raise NotImplementedError()++    def time_remaining(self) -> Optional[float]:+        raise NotImplementedError()++    async def initial_metadata(self) -> Optional[MetadataType]:+        return await (await self._interceptors_task).initial_metadata()++    async def trailing_metadata(self) -> Optional[MetadataType]:+        return await (await self._interceptors_task).trailing_metadata()++    async def code(self) -> grpc.StatusCode:+        return await (await self._interceptors_task).code()++    async def details(self) -> str:+        return await (await self._interceptors_task).details()++    async def debug_error_string(self) -> Optional[str]:+        return await (await self._interceptors_task).debug_error_string()++    def __await__(self):+        call = yield from self._interceptors_task.__await__()+        response = yield from call.__await__()+        return response+++class UnaryUnaryCallRpcError(_base_call.UnaryUnaryCall):+    """"""Final UnaryUnaryCall class finished with an RpcError.""""""+    _error: grpc.RpcError++    def __init__(self, error: grpc.RpcError) -> None:+        self._error = error++    def cancel(self) -> bool:+        return False++    def cancelled(self) -> bool:+        return False++    def done(self) -> bool:+        return True++    def add_done_callback(self, unused_callback) -> None:+        raise NotImplementedError()++    def time_remaining(self) -> Optional[float]:+        raise NotImplementedError()++    async def initial_metadata(self) -> Optional[MetadataType]:+        return None++    async def trailing_metadata(self) -> Optional[MetadataType]:+        return self._error.initial_metadata()++    async def code(self) -> grpc.StatusCode:+        return self._error.code()++    async def details(self) -> str:+        return self._error.details()++    async def debug_error_string(self) -> Optional[str]:+        return self._error.debug_error_string()++    def __await__(self):+        raise self._error+++class UnaryUnaryCallResponse(_base_call.UnaryUnaryCall):+    """"""Final UnaryUnaryCall class finished with a response.""""""+    _response: ResponseType++    def __init__(self, response: ResponseType) -> None:+        self._response = response++    def cancel(self) -> bool:+        return False++    def cancelled(self) -> bool:+        return False++    def done(self) -> bool:+        return True++    def add_done_callback(self, unused_callback) -> None:+        raise NotImplementedError()++    def time_remaining(self) -> Optional[float]:+        raise NotImplementedError()++    async def initial_metadata(self) -> Optional[MetadataType]:+        return None++    async def trailing_metadata(self) -> Optional[MetadataType]:+        return None++    async def code(self) -> grpc.StatusCode:+        return grpc.StatusCode.OK++    async def details(self) -> str:+        return ''++    async def debug_error_string(self) -> Optional[str]:+        return None++    def __await__(self):+        if False:  # pylint: disable=W0125+            # This code path is never used, but a yield statement is needed+            # for telling the interpreterthat __await__ is a generator.+            yield None+        return self._response+++class UnaryUnaryCancelledError(_base_call.UnaryUnaryCall):+    """"""Final UnaryUnaryCall class finished with an asyncio.CancelledError.""""""++    def cancel(self) -> bool:+        return False++    def cancelled(self) -> bool:+        return True++    def done(self) -> bool:+        return True++    def add_done_callback(self, unused_callback) -> None:+        raise NotImplementedError()++    def time_remaining(self) -> Optional[float]:+        raise NotImplementedError()++    async def initial_metadata(self) -> Optional[MetadataType]:+        return None++    async def trailing_metadata(self) -> Optional[MetadataType]:+        return None++    async def code(self) -> grpc.StatusCode:+        return grpc.StatusCode.CANCELLED++    async def details(self) -> str:+        return ''","nit: If we want to keep this class, we need the details to have some content like ""cancelled locally"".",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21506,359205734,2019-12-18T08:15:23Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -78,17 +77,21 @@ cdef class _AioCall:         """"""Destroys the corresponding Core object for this RPC.""""""         grpc_call_unref(self._grpc_call_wrapper.call) -    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):-        """"""Cancels the RPC in Core, and return the final RPC status.""""""-        cdef AioRpcStatus status+    def cancel(self, AioRpcStatus status):","Im wondering if it makes sense to pass the whole `status`, because it will be yes or yes  - if Im not missing something - filled during the happy path here [1].So maybe we would need to just give as a parameters the status and the details.[1] https://github.com/grpc/grpc/pull/21506/files#diff-d07d1d5928e4bc4defeb2e1218c57186R152",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21506,359209028,2019-12-18T08:24:50Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -275,28 +273,29 @@ def __del__(self) -> None:         serialized_request = _common.serialize(self._request,                                                self._request_serializer) -        # NOTE(lidiz) asyncio.CancelledError is not a good transport for-        # status, since the Task class do not cache the exact-        # asyncio.CancelledError object. So, the solution is catching the error-        # in Cython layer, then cancel the RPC and update the status, finally-        # re-raise the CancelledError.-        serialized_response = await self._channel.unary_unary(-            self._method,-            serialized_request,-            self._deadline,-            self._cancellation,-            self._set_initial_metadata,-            self._set_status,-        )+        # NOTE(lidiz) asyncio.CancelledError is not a good transport for status,+        # because the asyncio.Task class do not cache the exception object.+        # https://github.com/python/cpython/blob/edad4d89e357c92f70c0324b937845d652b20afd/Lib/asyncio/tasks.py#L785+        try:+            serialized_response = await self._cython_call.unary_unary(+                serialized_request,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:",question when are we making the assignation of a `self._code = grpc.StatusCode.CANCELLED` when a `asyncio.CancelledError` exception happens?,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21476,359410023,2019-12-18T15:31:17Z,test/distrib/cpp/run_distrib_test_raspberry_pi.sh,"@@ -0,0 +1,93 @@+#!/bin/bash+# Copyright 2017 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++cd ""$(dirname ""$0"")/../../..""+grpc_dir=$(pwd)++echo ""deb http://archive.debian.org/debian jessie-backports main"" | tee /etc/apt/sources.list.d/jessie-backports.list+echo 'Acquire::Check-Valid-Until ""false"";' > /etc/apt/apt.conf+sed -i '/deb http:\/\/deb.debian.org\/debian jessie-updates main/d' /etc/apt/sources.list+apt-get update+apt-get install -t jessie-backports -y git libssl-dev wget++# Install CMake 3.16+wget -q -O cmake-linux.sh https://github.com/Kitware/CMake/releases/download/v3.16.1/cmake-3.16.1-Linux-x86_64.sh+sh cmake-linux.sh -- --skip-license --prefix=/usr+rm cmake-linux.sh++# Build and install gRPC for the host architecture.+# We do this because we need to be able to run protoc and grpc_cpp_plugin+# while cross-compiling.+mkdir -p ""cmake/build""+pushd ""cmake/build""+cmake \+  -DCMAKE_BUILD_TYPE=Release \+  -DgRPC_INSTALL=ON \+  -DgRPC_BUILD_TESTS=OFF \+  -DgRPC_SSL_PROVIDER=package \+  ../..+make -j4 install+popd++# Download raspberry pi toolchain.+mkdir -p ""/tmp/raspberrypi_root""+pushd ""/tmp/raspberrypi_root""+git clone https://github.com/raspberrypi/tools raspberrypi-tools+cd raspberrypi-tools && git checkout 4a335520900ce55e251ac4f420f52bf0b2ab6b1f && cd ..++# Write a toolchain file to use for cross-compiling.+cat > toolchain.cmake <<'EOT'+SET(CMAKE_SYSTEM_NAME Linux)",qq: don't the raspberry pi tools provide  a ready-to-use toolchain?,
224720,zackgalbreath,https://api.github.com/repos/grpc/grpc/pulls/21476,359424773,2019-12-18T15:56:04Z,test/distrib/cpp/run_distrib_test_raspberry_pi.sh,"@@ -0,0 +1,93 @@+#!/bin/bash+# Copyright 2017 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++cd ""$(dirname ""$0"")/../../..""+grpc_dir=$(pwd)++echo ""deb http://archive.debian.org/debian jessie-backports main"" | tee /etc/apt/sources.list.d/jessie-backports.list+echo 'Acquire::Check-Valid-Until ""false"";' > /etc/apt/apt.conf+sed -i '/deb http:\/\/deb.debian.org\/debian jessie-updates main/d' /etc/apt/sources.list+apt-get update+apt-get install -t jessie-backports -y git libssl-dev wget++# Install CMake 3.16+wget -q -O cmake-linux.sh https://github.com/Kitware/CMake/releases/download/v3.16.1/cmake-3.16.1-Linux-x86_64.sh+sh cmake-linux.sh -- --skip-license --prefix=/usr+rm cmake-linux.sh++# Build and install gRPC for the host architecture.+# We do this because we need to be able to run protoc and grpc_cpp_plugin+# while cross-compiling.+mkdir -p ""cmake/build""+pushd ""cmake/build""+cmake \+  -DCMAKE_BUILD_TYPE=Release \+  -DgRPC_INSTALL=ON \+  -DgRPC_BUILD_TESTS=OFF \+  -DgRPC_SSL_PROVIDER=package \+  ../..+make -j4 install+popd++# Download raspberry pi toolchain.+mkdir -p ""/tmp/raspberrypi_root""+pushd ""/tmp/raspberrypi_root""+git clone https://github.com/raspberrypi/tools raspberrypi-tools+cd raspberrypi-tools && git checkout 4a335520900ce55e251ac4f420f52bf0b2ab6b1f && cd ..++# Write a toolchain file to use for cross-compiling.+cat > toolchain.cmake <<'EOT'+SET(CMAKE_SYSTEM_NAME Linux)","They do, that's what we're cloning from github in this distrib test.OTOH, CMake toolchain files are really meant to be machine-specific. As you can see in this example, they tend to have hardcoded paths to compilers & such in them.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21506,359526367,2019-12-18T19:25:23Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -78,17 +77,21 @@ cdef class _AioCall:         """"""Destroys the corresponding Core object for this RPC.""""""         grpc_call_unref(self._grpc_call_wrapper.call) -    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):-        """"""Cancels the RPC in Core, and return the final RPC status.""""""-        cdef AioRpcStatus status+    def cancel(self, AioRpcStatus status):","About the arguments, I'm more in favor of a structured data class. It is easier to be pass around in Cython layer and in Python layer. To me, the handling of the a failed status and a cancelled status are sharing same logic. I'm open to changes, if there is a good reasoning.Also, `status` is supplied not only in normal path, but also in Python layer's `UnaryUnaryCall.cancel()` and `UnaryUnaryCall.__del__`.---Another tricky part of this implementation is that there is a potential data race between Core setting final status and application calling cancel.There was a protection for the race condition in `_set_status`: https://github.com/grpc/grpc/pull/21506/files/d30866beb92c8e01d5a82f312b0fa5a4e74c6daa#diff-802047f7d320089cb1d9c7de77c9cae5L208In this PR, I decided to do it right by completely resolving this race condition. Now, no matter the status arrived first or cancel called first, it would cancel the other one.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21515,359596461,2019-12-18T22:15:38Z,tools/buildgen/plugins/transitive_dependencies.py,"@@ -16,43 +16,49 @@ This takes the list of libs, node_modules, and targets from our yaml dictionary, and adds to each the transitive closure of the list of dependencies.- """"""  -def get_lib(libs, name):-    try:-        return next(lib for lib in libs if lib['name'] == name)-    except StopIteration:-        return None+def transitive_deps(lib_map, node):+    """"""Returns a list of transitive dependencies from node.++    Recursively iterate all dependent node in a depth-first fashion and+    list a result using a topological sorting.+    """"""+    result = []+    seen = set()+    start = node +    def recursive_helper(node):+        if node is None:+            return+        for dep in node.get(""deps"", []):+            if dep not in seen:+                seen.add(dep)+                next_node = lib_map.get(dep)+                if next_node:","Based on your base case, I think this conditional is redundant.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21508,359600508,2019-12-18T22:26:45Z,src/abseil-cpp/preprocessed_builds.yaml.gen.py,"@@ -63,38 +64,101 @@ def parse_rule(elem, package):       testonly=get_elem_value(elem, ""testonly"") or False)  -def read_build(package):+def read_bazel_build(package):   """"""Runs bazel query on given package file and returns all cc rules.""""""   result = subprocess.check_output(       [""bazel"", ""query"", package + "":all"", ""--output"", ""xml""])   root = ET.fromstring(result)   return [-      parse_rule(elem, package)+      parse_bazel_rule(elem, package)       for elem in root       if elem.tag == ""rule"" and elem.attrib[""class""].startswith(""cc_"")   ]  -def collect_rules(root_path):-  """"""Collects and returns all rules from root path recursively.""""""+def collect_bazel_rules(root_path):+  """"""Collects and returns all bazel rules from root path recursively.""""""   rules = []   for cur, _, _ in os.walk(root_path):     build_path = os.path.join(cur, ""BUILD.bazel"")     if os.path.exists(build_path):-      rules.extend(read_build(""//"" + cur))+      rules.extend(read_bazel_build(""//"" + cur))   return rules  +def parse_cmake_rule(str, package):+  """"""Returns a rule from absl cmake rule.+     Reference: https://github.com/abseil/abseil-cpp/blob/master/CMake/AbseilHelpers.cmake+  """"""+  kv = {}+  bucket = None+  lines = str.splitlines()+  for line in lines[1:-1]:+    if re.match(""[A-Z]+"", line.strip()):+      bucket = kv.setdefault(line.strip(), [])+    else:+      if bucket is not None:+        bucket.append(line.strip())+      else:+        assert ""Illegal syntax: "" + str+  return Rule(+      type=lines[0].rstrip(""(""),+      name=""absl::"" + kv[""NAME""][0],+      package=package,+      srcs=[package + ""/"" + f.strip('""') for f in kv.get(""SRCS"", [])],+      hdrs=[package + ""/"" + f.strip('""') for f in kv.get(""HDRS"", [])],+      textual_hdrs=[],+      deps=kv.get(""DEPS"", []),+      visibility=""PUBLIC"" in kv,+      testonly=""TESTONLY"" in kv,+  )+++def read_cmake_build(build_path, package):+  """"""Parses given CMakeLists.txt file and returns all cc rules.""""""+  rules = []+  with open(build_path, ""r"") as f:+    src = f.read()+    for mo in re.finditer(""^absl_cc_.*\("", src, re.MULTILINE):+      mo2 = re.search(""^\)"", src[mo.start(0):], re.MULTILINE)+      expr = src[mo.start(0):mo.start(0) + mo2.start(0) + 1]+      rules.append(parse_cmake_rule(expr, package))+  return rules+++def collect_cmake_rules(root_path):+  """"""Collects and returns all cmake rules from root path recursively.""""""+  rules = []+  for cur, _, _ in os.walk(root_path):+    build_path = os.path.join(cur, ""CMakeLists.txt"")+    if os.path.exists(build_path):+      rules.extend(read_cmake_build(build_path, cur))+  return rules+++def pairing_bazel_and_cmake_rules(bazel_rules, cmake_rules):+  """"""Returns a pair map between bazel rules and cmake rules based on+     the similarity of the file list in the rule.""""""+  pair_map = {}+  for rule in bazel_rules:+    best_crule, best_s = None, 0+    for crule in cmake_rules:",Looks like this is O(n^2) (assuming there are roughly as many bazel rules as cmake rules). Is that going to be a maintainability problem? How long does this script currently take to run?,X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/21508,359626305,2019-12-18T23:50:46Z,src/abseil-cpp/preprocessed_builds.yaml.gen.py,"@@ -63,38 +64,101 @@ def parse_rule(elem, package):       testonly=get_elem_value(elem, ""testonly"") or False)  -def read_build(package):+def read_bazel_build(package):   """"""Runs bazel query on given package file and returns all cc rules.""""""   result = subprocess.check_output(       [""bazel"", ""query"", package + "":all"", ""--output"", ""xml""])   root = ET.fromstring(result)   return [-      parse_rule(elem, package)+      parse_bazel_rule(elem, package)       for elem in root       if elem.tag == ""rule"" and elem.attrib[""class""].startswith(""cc_"")   ]  -def collect_rules(root_path):-  """"""Collects and returns all rules from root path recursively.""""""+def collect_bazel_rules(root_path):+  """"""Collects and returns all bazel rules from root path recursively.""""""   rules = []   for cur, _, _ in os.walk(root_path):     build_path = os.path.join(cur, ""BUILD.bazel"")     if os.path.exists(build_path):-      rules.extend(read_build(""//"" + cur))+      rules.extend(read_bazel_build(""//"" + cur))   return rules  +def parse_cmake_rule(str, package):+  """"""Returns a rule from absl cmake rule.+     Reference: https://github.com/abseil/abseil-cpp/blob/master/CMake/AbseilHelpers.cmake+  """"""+  kv = {}+  bucket = None+  lines = str.splitlines()+  for line in lines[1:-1]:+    if re.match(""[A-Z]+"", line.strip()):+      bucket = kv.setdefault(line.strip(), [])+    else:+      if bucket is not None:+        bucket.append(line.strip())+      else:+        assert ""Illegal syntax: "" + str+  return Rule(+      type=lines[0].rstrip(""(""),+      name=""absl::"" + kv[""NAME""][0],+      package=package,+      srcs=[package + ""/"" + f.strip('""') for f in kv.get(""SRCS"", [])],+      hdrs=[package + ""/"" + f.strip('""') for f in kv.get(""HDRS"", [])],+      textual_hdrs=[],+      deps=kv.get(""DEPS"", []),+      visibility=""PUBLIC"" in kv,+      testonly=""TESTONLY"" in kv,+  )+++def read_cmake_build(build_path, package):+  """"""Parses given CMakeLists.txt file and returns all cc rules.""""""+  rules = []+  with open(build_path, ""r"") as f:+    src = f.read()+    for mo in re.finditer(""^absl_cc_.*\("", src, re.MULTILINE):",`mo` is a common (lazy) variable like `i` for `MatchObject`. You can see the same thing in the example of python [manual](https://docs.python.org/3/library/re.html).,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21506,359806532,2019-12-19T11:08:12Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -308,16 +308,17 @@ def cancel(self) -> bool:                                 _LOCAL_CANCELLATION_DETAILS, None, None))      def __await__(self) -> ResponseType:-        """"""Wait till the ongoing RPC request finishes.--        Returns:-          Response of the RPC call.--        Raises:-          RpcError: Indicating that the RPC terminated with non-OK status.-          asyncio.CancelledError: Indicating that the RPC was canceled.-        """"""-        response = yield from self._call+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            # Even if we converted all other CancelledError, there is still+            # this corner case. If the application cancels immediately after+            # the Call object is created, we will observe this+            # `CancelledError`.+            if not self.cancelled():+                self.cancel()+            raise _create_rpc_error(_EMPTY_METADATA, self._status.result())","With that, we would be masking `asyncio.CancelledError`,  which IMO could be nocive for the users. So, not allowing them to react upon an `asyncio.CancelledError` exception or even worst, having them reacting to a `RpcError` which was originally caused because by an `asyncio.CancelledError` exception.My opinion here is that graceful cancelations - the ones explicitly asked by the caller using the `call.cancel()` interface - would need to end up as simple `RpcErrors`, while implicit cancellations because of an `asyncio.CancelledError` would not be treated as RPC errors and do our best.Do our best could mean either of the following things:1- Cancel the ongoing RPC here [1] and do nothing else, the call object would end up in a none finished state.2 - The same as 1 but reacting upon a `asyncio.CancelledError` for leaving the call object in a finished state but reraising always the original exception.Theoretically from the point of view of the caller, if there are no cleanup tasks to do they should not take care of `asyncio.CancelledError` exceptions, so having something like this:```pythontry:   await callexcept grpc.RpcError as err:   assert await call.code() == err.code()   if err.code() == CANCELLED:       # should I retry?``` [1] https://github.com/grpc/grpc/pull/21506/files#diff-d07d1d5928e4bc4defeb2e1218c57186R154",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21506,359812072,2019-12-19T11:23:36Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -78,17 +77,21 @@ cdef class _AioCall:         """"""Destroys the corresponding Core object for this RPC.""""""         grpc_call_unref(self._grpc_call_wrapper.call) -    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):-        """"""Cancels the RPC in Core, and return the final RPC status.""""""-        cdef AioRpcStatus status+    def cancel(self, AioRpcStatus status):","Maybe I'm too naive on that, but my idea would be that once we would start canceling explicitly the RPC calls by using the cancel method provided by the `AioCall` would be Core who would tell you the status of finished RPC.With this, you would be reducing the code paths where the status is being managed, so having only one place the code executed after awaiting the Core.Having the code path for managing the status reduced to only one place, the `AioCall.cancel` method would have more sense of accepting a strict list of parameters related only to cancellation attributes.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21506,360114859,2019-12-19T21:01:18Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -78,17 +77,26 @@ cdef class _AioCall:         """"""Destroys the corresponding Core object for this RPC.""""""         grpc_call_unref(self._grpc_call_wrapper.call) -    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):-        """"""Cancels the RPC in Core, and return the final RPC status.""""""-        cdef AioRpcStatus status+    @property+    def locally_cancelled(self):","I would like to make semantic as simple as possible, users or us should not worry about whether an RPC is cancelled locally or remotely. Sorry, I wasn't clear about that design before. I'm open to expose the `_is_locally_cancelled` flag, but in a way that is less invasive.The goal of the `_is_locally_cancelled` flag is only be used in order to free resources. For cancellation detection, the final status code is the ground truth.So, I temporarily removed this flag until there is a valid use case for it.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21506,360123810,2019-12-19T21:24:35Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -78,17 +77,21 @@ cdef class _AioCall:         """"""Destroys the corresponding Core object for this RPC.""""""         grpc_call_unref(self._grpc_call_wrapper.call) -    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):-        """"""Cancels the RPC in Core, and return the final RPC status.""""""-        cdef AioRpcStatus status+    def cancel(self, AioRpcStatus status):","There are three cancellation behaviors we need to handle:1. Cancellation from Core;2. Task cancelled;3. Injected `CancelledError` from parent coroutine (see [test case](https://github.com/grpc/grpc/pull/21506/files#diff-ebe6c75bf1347f8032019b99fbf4a8bdR149-R170)).Based on the comment, your proposal is that we convert behavior 2 into behavior 1. But that will make us vulnerable to behavior 3, which is not a rare pattern. On the other hand, if we use the try-catch clause and explicit conversion, I'm certain we should cover all the cases.The cancellation mechanism I implemented in last streaming API PR is a complex version of your proposal (due to cancellation_future). It is stable and working until I found it doesn't handle behavior 3.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21506,360129783,2019-12-19T21:38:43Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -275,28 +274,29 @@ def __del__(self) -> None:         serialized_request = _common.serialize(self._request,                                                self._request_serializer) -        # NOTE(lidiz) asyncio.CancelledError is not a good transport for-        # status, since the Task class do not cache the exact-        # asyncio.CancelledError object. So, the solution is catching the error-        # in Cython layer, then cancel the RPC and update the status, finally-        # re-raise the CancelledError.-        serialized_response = await self._channel.unary_unary(-            self._method,-            serialized_request,-            self._deadline,-            self._cancellation,-            self._set_initial_metadata,-            self._set_status,-        )+        # NOTE(lidiz) asyncio.CancelledError is not a good transport for status,+        # because the asyncio.Task class do not cache the exception object.+        # https://github.com/python/cpython/blob/edad4d89e357c92f70c0324b937845d652b20afd/Lib/asyncio/tasks.py#L785+        try:+            serialized_response = await self._cython_call.unary_unary(+                serialized_request,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if self._code != grpc.StatusCode.CANCELLED:+                self.cancel()++        # Raises RpcError here if RPC failed or cancelled         await self._raise_rpc_error_if_not_ok()          return _common.deserialize(serialized_response,                                    self._response_deserializer)      def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:         """"""Forwards the application cancellation reasoning.""""""-        if not self._status.done() and not self._cancellation.done():-            self._cancellation.set_result(status)+        if not self._status.done():+            self._set_status(status)","Good catch! I'm adding it back due to a potential usage, that the user may want to send the RST_STREAM frame right after message body. But in gRPC, currently, we don't support truncating messages. It is quite un-deterministic that how will lower level behave during the cancellation in different phases of RPC.It is not a good idea to only depend on the Core cancellation mechanism. The problem is the delay. If we pass the cancel into Core and then parse it sometime later. The final status of the RPC becomes uncertain again. The Core could already received the final status, hence the cancellation failed, and application are seeing OK.On the other hand, if we enforced the cancellation top-down, we are certain about the final status, and no race condition.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21506,360137228,2019-12-19T21:57:41Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -308,16 +308,17 @@ def cancel(self) -> bool:                                 _LOCAL_CANCELLATION_DETAILS, None, None))      def __await__(self) -> ResponseType:-        """"""Wait till the ongoing RPC request finishes.--        Returns:-          Response of the RPC call.--        Raises:-          RpcError: Indicating that the RPC terminated with non-OK status.-          asyncio.CancelledError: Indicating that the RPC was canceled.-        """"""-        response = yield from self._call+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            # Even if we converted all other CancelledError, there is still+            # this corner case. If the application cancels immediately after+            # the Call object is created, we will observe this+            # `CancelledError`.+            if not self.cancelled():+                self.cancel()+            raise _create_rpc_error(_EMPTY_METADATA, self._status.result())","I agree with you that this solution does have a drawback that it disabled users to react upon `asyncio.CancelledError`. Also, it affects the logging, since `CancelledError` might be silent in certain cases but `RpcError` never get silent automatically.I understand your proposal, but there is a topic of predicability. If we allow `asyncio.CancelledError` to be raised from our package, then users need to always remember handling both the `asyncio.CancelledError` and the `RpcError`. For RPC in different state, the outcome exception will be different. I think it is not an ideal design.As discussed before, no matter the cancellation is originated locally or remotely, the retry logic always require spawning a retry RPC. So, by unifying the exceptions, we make it less error-prone (one less try-catch clause).```Pythontry:   await callexcept grpc.RpcError as err:   if err.code() == CANCELLED:       # apply retry logic```",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21506,360149977,2019-12-19T22:26:32Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -78,17 +77,21 @@ cdef class _AioCall:         """"""Destroys the corresponding Core object for this RPC.""""""         grpc_call_unref(self._grpc_call_wrapper.call) -    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):-        """"""Cancels the RPC in Core, and return the final RPC status.""""""-        cdef AioRpcStatus status+    def cancel(self, AioRpcStatus status):","Yes, I'm suggesting implementing the use case 2 as 1, but IMO the important thing is what to do with use case 3, do we need to mask the `asyncio.CanceledError` as a `RpcError` I'm inclined to believe that better no, maybe Im missing something. So in this assert [1], I would expect to catch an `asyncio.CancelledError`.By raising a generic `RpcError` we are opening the box for having the following two problems:- Local coroutine within the task reacting upon the `RpcError` as it was a normal `RpcError` while the parent task asked explicitly for canceling it. - Having the parent task the necessity of understanding what `RpcError` semantically means.Implementing the use case 2 as 1 is an implementation detail, as long as we are able to detect that is a local cancellation so converting the exception to a `RpcError` I would say that is not gonna be a big deal for me. [1] https://github.com/grpc/grpc/pull/21506/files#diff-ebe6c75bf1347f8032019b99fbf4a8bdR168",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21506,360192837,2019-12-20T01:21:45Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -78,17 +77,21 @@ cdef class _AioCall:         """"""Destroys the corresponding Core object for this RPC.""""""         grpc_call_unref(self._grpc_call_wrapper.call) -    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):-        """"""Cancels the RPC in Core, and return the final RPC status.""""""-        cdef AioRpcStatus status+    def cancel(self, AioRpcStatus status):","It make sense that masking `CancelledError` as `RpcError` might work against the user's mind. If a user is calling `cancel` method explicitly, I believe they probably don't want to retry. In that case, `CancelledError` does bring value by early exiting the RPC. I would like to explore this idea a bit, that we convert all local cancellation `RpcError` to `CancelledError`.About cancellation from parent coroutine, we might not able to differentiate a call cancellation and a parent coroutine cancellation. So, if retry is needed, the retry logic will apply to both cases or none of them.```Pythonasync def coro_1(stub, request):    call = stub.UnaryCall(request)    async def cancel_soon():        await asyncio.sleep(1)  # Or other business logic        call.cancel()        asyncio.create_task(cancel_soon())    await call    # 1. Are we expecting `CancelledError` or `RpcError`?    # 2. Is the `CancelledError` generate by `cancel()` or parent?    # 3. Should this coroutine end silently?async def coro_2(stub, request):    task = asyncio.create_task(coro_1(stub, request))    await asyncio.wait_for(task, 1)```> Having the parent task the necessity of understanding what RpcError semantically means.Cancellation is not the only trigger for `RpcError` exception. Application needs to be aware of it.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21530,360270202,2019-12-20T08:30:32Z,tools/run_tests/artifacts/artifact_targets.py,"@@ -271,7 +271,7 @@ def build_jobspec(self):                     cmake_arch_option = '-DOPENSSL_NO_ASM=ON'                 return create_docker_jobspec(                     self.name,-                    'tools/dockerfile/grpc_artifact_linux_%s' % self.arch,+                    'tools/dockerfile/grpc_artifact_centos6_%s' % self.arch,",should we get rid of https://github.com/grpc/grpc/blob/master/tools/dockerfile/grpc_artifact_linux_x64/Dockerfile entirely and build all the artifacts (that are currently built with it) using manylinux2010 based image?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21506,360467180,2019-12-20T16:53:24Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -78,17 +77,21 @@ cdef class _AioCall:         """"""Destroys the corresponding Core object for this RPC.""""""         grpc_call_unref(self._grpc_call_wrapper.call) -    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):-        """"""Cancels the RPC in Core, and return the final RPC status.""""""-        cdef AioRpcStatus status+    def cancel(self, AioRpcStatus status):","> It make sense that masking CancelledError as RpcError might work against the user's mind. If a user is calling cancel method explicitly, I believe they probably don't want to retry. In that case, CancelledError does bring value by early exiting the RPC. I would like to explore this idea a bit, that we convert all local cancellation RpcError to CancelledErrorI'm starting to believe that this is the way to go> About cancellation from parent coroutine, we might not able to differentiate a call cancellation and a parent coroutine cancellation. So, if retry is needed, the retry logic will apply to both cases or none of them.I would say that most of the retries would come because of a server cancelation, not because of a local one. But you know, will be a moment where some pattern would need this... I will start with something and later on, we can improve it while the API is still experimental.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/21530,360470181,2019-12-20T17:01:20Z,tools/run_tests/artifacts/artifact_targets.py,"@@ -271,7 +271,7 @@ def build_jobspec(self):                     cmake_arch_option = '-DOPENSSL_NO_ASM=ON'                 return create_docker_jobspec(                     self.name,-                    'tools/dockerfile/grpc_artifact_linux_%s' % self.arch,+                    'tools/dockerfile/grpc_artifact_centos6_%s' % self.arch,","Not really because it's also being used for packaging and [build_package_python](https://github.com/grpc/grpc/blob/master/tools/run_tests/artifacts/build_package_python.sh) depends on `apt-get` which is not available on CentOS. But this can be revised not to rely upon `apt-get`, though.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21506,360588815,2019-12-20T21:57:53Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -78,17 +77,21 @@ cdef class _AioCall:         """"""Destroys the corresponding Core object for this RPC.""""""         grpc_call_unref(self._grpc_call_wrapper.call) -    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):-        """"""Cancels the RPC in Core, and return the final RPC status.""""""-        cdef AioRpcStatus status+    def cancel(self, AioRpcStatus status):","@lidizheng I think it would be really beneficial if there were some way to distinguish between local cancellation (regardless of whether that's done via task cancellation or call cancellation) and remote cancellation by the server. As you've already pointed out, it's likely that people will want to retry on the latter, but not on the former.If we're using your numbered list from earlier in the chain, I see a lot of value in distinguishing between case 1 and the others. However, I don't see as much value in distinguishing between 2 and 3. It seems like the best way to go about that is to raise `RpcError` for case 1 and `CancelledError` for the other two.The drawback in converting local call cancellation to `asyncio.CancelledError` is that you lose the ability to distinguish between intentional cancellation by the application and (inadvertent) cancellation by something else interacting with the event loop. In the former case, you probably don't want to retry. In the latter case, you might.",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/21423,360724781,2019-12-22T19:39:49Z,src/core/lib/security/security_connector/tls/tls_security_connector.cc,"@@ -185,7 +185,7 @@ void TlsChannelSecurityConnector::check_peer(    * server authorization check. */   if (config != nullptr) {     const tsi_peer_property* p =-        tsi_peer_get_property_by_name(&peer, TSI_X509_PEM_CERT_PROPERTY);+        tsi_peer_get_property_by_name(&peer, TSI_X509_PEM_CERT_CHAIN_PROPERTY);","Instead of modifying the existing TSI peer property (`TSI_X509_PEM_CERT_PROPERTY`), I suggest to add a new property for `TSI_X509_PEM_CERT_CHAIN_PROPERTY` so that users who want to use a leaf certificate do not need to re-parse the certificate chain.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21506,360756207,2019-12-23T03:25:53Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -78,17 +77,21 @@ cdef class _AioCall:         """"""Destroys the corresponding Core object for this RPC.""""""         grpc_call_unref(self._grpc_call_wrapper.call) -    cdef AioRpcStatus _cancel_and_create_status(self, object cancellation_future):-        """"""Cancels the RPC in Core, and return the final RPC status.""""""-        cdef AioRpcStatus status+    def cancel(self, AioRpcStatus status):","@pfreixes @gnossen Sounds like the new proposal is the most sensible one. I implemented the discussed cancellation mechanism in https://github.com/grpc/grpc/pull/21506/commits/4c251f704b84084942fa467e02ae25b0262b79b7.However, there is a drawback in implementation. We can't remove most try-catch clauses for `CancelledError`, because other coroutine might explicitly pending on the final status. If we let `CancelledError` pass freely, the RPC will be in an ""undead"" state that it is cancelled but will deadlock if another coroutine called `await call.code()` or `await call.initial_metadata()` etc..Besides the verbosity of the implementation, I think all local cancellations (scenario 2 & 3) are converted into `CancelledError`, and scenario 1 remains `RpcError`.",
28269509,akshayku,https://api.github.com/repos/grpc/grpc/pulls/20316,360933148,2019-12-23T16:08:42Z,test/core/end2end/e_passthrough.c,"@@ -0,0 +1,62 @@+#include <stdio.h>",Renaming to engine_passthrough.**_c_** as openssl is **_c_** based. Also added comments.,
28269509,akshayku,https://api.github.com/repos/grpc/grpc/pulls/20316,360935494,2019-12-23T16:17:05Z,test/core/end2end/h2_ssl_cert_test.cc,"@@ -369,12 +381,38 @@ int main(int argc, char** argv) {    grpc_init();   ::testing::InitGoogleTest(&argc, argv);+  while (--argc > 0) {+      if (**++argv == '-') {","Yes, you have to pass ""engine"" and then the engine name. `h2_ssl_cert_test -engine e_passthrough`The reason I kept it seperate instead of doing it automatically as I am unsure of how gRPC tests work. As engine is not part of boringSSL, it will fail. So wanted to test this only in condition when gRPC is built with openSSL. Plus not sure what different kind of platforms gRPC test framework runs on. Hence the conservative route. ",
28269509,akshayku,https://api.github.com/repos/grpc/grpc/pulls/20316,360935810,2019-12-23T16:18:14Z,src/core/tsi/ssl_transport_security.cc,"@@ -568,21 +570,87 @@ static tsi_result ssl_ctx_use_private_key(SSL_CTX* context, const char* pem_key,   EVP_PKEY* private_key = nullptr;   BIO* pem;   GPR_ASSERT(pem_key_size <= INT_MAX);-  pem = BIO_new_mem_buf((void*)pem_key, static_cast<int>(pem_key_size));-  if (pem == nullptr) return TSI_OUT_OF_RESOURCES;-  do {-    private_key = PEM_read_bio_PrivateKey(pem, nullptr, nullptr, (void*)"""");-    if (private_key == nullptr) {-      result = TSI_INVALID_ARGUMENT;-      break;-    }-    if (!SSL_CTX_use_PrivateKey(context, private_key)) {-      result = TSI_INVALID_ARGUMENT;-      break;-    }-  } while (0);-  if (private_key != nullptr) EVP_PKEY_free(private_key);-  BIO_free(pem);++// BoringSSL does not have ENGINE_load_dynamic, ENGINE_by_id+// support.+#ifndef OPENSSL_IS_BORINGSSL+  if (strncmp(pem_key, kSslEnginePrefix,+              GPR_ARRAY_SIZE(kSslEnginePrefix) - 1) == 0)+  {+    ENGINE* engine;+    do {+      char *p, *last;+      gpr_log(GPR_INFO, ""ENGINE key specified"");+      p = (char*)pem_key + sizeof(""engine:"") - 1;+      last = (char*)strchr(p, ':');+      if (last == nullptr) {+        gpr_log(GPR_ERROR, ""No engine defined %s"", p);+        result = TSI_INVALID_ARGUMENT;+        break;+      }+      ENGINE_load_dynamic();+      *last = '\0';+      engine = ENGINE_by_id(p);+      if (engine == nullptr) {+        // If not available at ENGINE_DIR, use dynamic to load from+        // current working directory.+        engine = ENGINE_by_id(""dynamic"");+        if (engine == nullptr) {+          gpr_log(GPR_ERROR, ""Cannot load dynamic engine"");+          result = TSI_INVALID_ARGUMENT;+          break;+        }+        if (!ENGINE_ctrl_cmd_string(engine, ""SO_PATH"", p, 0) ||+            !ENGINE_ctrl_cmd_string(engine, ""LOAD"", NULL, 0)) {+          gpr_log(GPR_ERROR, ""Cannot load engine %s"", p);+          result = TSI_INVALID_ARGUMENT;+          break;+        }+      }+      *last++ = ':';+      if (!ENGINE_set_default(engine, ENGINE_METHOD_ALL)) {+        gpr_log(GPR_ERROR, ""Cannot set default methods for engine"");+        result = TSI_INVALID_ARGUMENT;+        break;+      }+      if (!ENGINE_init(engine)) {+        gpr_log(GPR_ERROR, ""ENGINE Initialization failed"");+        result = TSI_INVALID_ARGUMENT;+        break;+      }+      private_key = ENGINE_load_private_key(engine, last, 0, 0);+      if (private_key == nullptr) {+        gpr_log(GPR_ERROR, ""ENGINE load private key() failed"");+        result = TSI_INVALID_ARGUMENT;+        break;+      }+      if (!SSL_CTX_use_PrivateKey(context, private_key)) {+        result = TSI_INVALID_ARGUMENT;+        break;+      }+    } while (0);",It is same pattern currently existing in this function where you only loop once and breaks out of the loop in error condition. Just following the existing pattern.,
28269509,akshayku,https://api.github.com/repos/grpc/grpc/pulls/21423,360951585,2019-12-23T17:16:37Z,include/grpcpp/security/tls_credentials_options.h,"@@ -317,6 +321,11 @@ class TlsCredentialsOptions {    * goes unused when creating channel credentials, and the user can set it to    * GRPC_SSL_DONT_REQUEST_CLIENT_CERTIFICATE. **/   grpc_ssl_client_certificate_request_type cert_request_type_;+  /** The server_verification_option_ flag is only relevant when the+   * TlsCredentialsOptions are used to instantiate client credentials; the flag+   * goes unused when creating server credentials, and the user can set it to+   * GRPC_SSL_SERVER_VERIFICATION. **/",Yes. Was following the similar pattern used for grpc_ssl_client_certificate_request_type,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21543,361002283,2019-12-23T21:08:33Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -62,21 +61,15 @@ class TestChannel(AioTestBase):             self.assertIsInstance(response, messages_pb2.SimpleResponse)      async def test_unary_call_times_out(self):","The only ""problem"" is that our test suite for the client is quite coupled to the basic proto stub `test_pb2_grpc.py`, so any change in functionality for any method could change the desired semantic, the same that happened with the `EmptyCall` method.This is a recurrent question, should we decouple the test suite from that file, so no longer using it, and use our own one? or even better, use our one server fixture based on a programmatic RPC handlers builder,  the same that is being used for testing the current Aio server. ",X
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/21423,361724598,2019-12-27T18:50:24Z,src/core/lib/security/security_connector/tls/tls_security_connector.cc,"@@ -202,6 +202,19 @@ void TlsChannelSecurityConnector::check_peer(                                     : check_arg_->target_name;       on_peer_checked_ = on_peer_checked;       gpr_free(peer_pem);+      const tsi_peer_property* chain =+        tsi_peer_get_property_by_name(&peer, TSI_X509_PEM_CERT_CHAIN_PROPERTY);+      if (chain != nullptr) {+        char* peer_pem_chain =+          static_cast<char*>(gpr_malloc(chain->value.length + 1));",s/`gpr_malloc`/`gpr_zalloc` and remove `peer_pem_chain[chain->value.length] = '\0';`,X
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/21455,361807669,2019-12-28T18:27:04Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,334 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from abc import ABCMeta, abstractmethod+from typing import Callable, Optional, Iterator, Sequence, Text, Union++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import (RequestType, SerializingFunction, DeserializingFunction,+                      MetadataType, ResponseType)+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: Text+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor(metaclass=ABCMeta):+    """"""Affords intercepting unary-unary invocations.""""""++    @abstractmethod+    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, RequestType],+                                         UnaryUnaryCall],+            client_call_details: ClientCallDetails,+            request: RequestType) -> Union[UnaryUnaryCall, ResponseType]:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    Interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__(  # pylint: disable=R0913+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            request: RequestType, timeout: Optional[float],+            channel: cygrpc.AioChannel, method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._interceptors_task = asyncio.ensure_future(+            self._invoke(interceptors, method, timeout, request,+                         request_serializer, response_deserializer))++    def __del__(self):+        self.cancel()++    async def _invoke(+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            method: bytes, timeout: Optional[float], request: RequestType,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:+            try:+                interceptor = next(interceptors)","Minor suggestion, but this can be done in a single instruction by setting the default second parameter```pythoninterceptor = next(interceptor, None)```",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/20316,362579609,2020-01-02T18:35:11Z,build.yaml,"@@ -1682,6 +1682,14 @@ libs:   deps:   - grpc   secure: true+- name: e_passthrough",Did you add this library as dependency of h2_ssl_cert_test?,
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/21423,362607134,2020-01-02T19:53:41Z,src/core/tsi/ssl_transport_security.cc,"@@ -1047,18 +1070,25 @@ static tsi_result ssl_handshaker_result_extract_peer(     SSL_get0_next_proto_negotiated(impl->ssl, &alpn_selected,                                    &alpn_selected_len);   }-+  STACK_OF(X509) *peer_chain = SSL_get_peer_cert_chain(impl->ssl);","Nit. Could you please add some comments about `SSL_get_peer_cert_chain` API w.r.t. its different behavior at client and server side? That is, `peer_chain` will not include a leaf certificate if the API is called at the server side.  ",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21577,362816540,2020-01-03T13:56:35Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -27,12 +27,14 @@ cdef class _AioCall:     def __cinit__(self,                   AioChannel channel,                   object deadline,+                  tuple metadata,                   bytes method):         self._channel = channel         self._references = []         self._grpc_call_wrapper = GrpcCallWrapper()         self._loop = asyncio.get_event_loop()         self._create_grpc_call(deadline, method)+        self._initial_metadata = metadata",I would say that this attribute was initially added here [1] for managing the initial metadata sent by the server when `UnaryStreamCall` is used. Reading into the code I would say that is not even used right now since the initial metadata is transmitted to the downstream layer through a callback (correct me if Im wrong @lidizheng) Instead of using this attribute I would create a new one called only `_metdata` that could be used for any of the arities for sending the metadata by the client-side.[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pxd.pxi#L30,X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21577,362822958,2020-01-03T14:17:21Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -126,16 +132,28 @@ async def _handle_unary_unary_rpc(object method_handler,     )      # Sends response message-    cdef tuple send_ops = (-        SendStatusFromServerOperation(-            tuple(),-            StatusCode.ok,-            b'',-            _EMPTY_FLAGS,-        ),-        SendInitialMetadataOperation(None, _EMPTY_FLAGS),-        SendMessageOperation(response_raw, _EMPTY_FLAGS),-    )+    cdef tuple send_ops+    if servicer_context._metadata_sent:",I'm a bit lost with this conditional is this fixing a bug that existed with the current code? so having to send yes or yes always the initial metadata if this hasn't been sent previously?,X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21577,362826599,2020-01-03T14:27:40Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -80,9 +84,15 @@ cdef class _ServicerContext:         if self._metadata_sent:             raise RuntimeError('Send initial metadata failed: already sent')         else:-            _send_initial_metadata(self._rpc_state, self._loop)+            await _send_initial_metadata(self._rpc_state, metadata, self._loop)","An open question, how much we would need to care about race conditions here? Theoretically, if we won't lock the whole `send_initial_metadata` function the user might call `_send_initial_metadata` more than once.But as I said, just an open question. For me wouldn't be a blocker right now. //cc @lidizheng @gnossen ",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21577,362829105,2020-01-03T14:34:13Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -284,6 +286,7 @@ def __del__(self) -> None:             self._method,             serialized_request,             self._deadline,+            self._metadata,","From what I can undestand here we would need to provide yes or yes a tuple, so a `None` object won't be valid since the signature for the function says that a `tuple` parameter is expeted.am I missing something ? or is there a place where the `None` default value is convderted to an empty tuple?",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21577,362915590,2020-01-03T18:38:47Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -109,6 +109,24 @@ class TestUnaryUnaryCall(AioTestBase):             call = hi(messages_pb2.SimpleRequest())             self.assertEqual('', await call.details()) +    async def test_call_initial_metadata_awaitable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = hi(messages_pb2.SimpleRequest())+            self.assertEqual((), await call.initial_metadata())","Can we add one more test for non-empty initial_metadata and trailing_metadata? Also we might need tests for invalid metadata, see `_invalid_metadata_test.py`.Optional: I felt like we could create a separate file for metadata, since their content are kind of isolated.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21577,362917854,2020-01-03T18:45:49Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -27,12 +27,14 @@ cdef class _AioCall:     def __cinit__(self,                   AioChannel channel,                   object deadline,+                  tuple metadata,                   bytes method):         self._channel = channel         self._references = []         self._grpc_call_wrapper = GrpcCallWrapper()         self._loop = asyncio.get_event_loop()         self._create_grpc_call(deadline, method)+        self._initial_metadata = metadata","Yes, it does collide with received initial metadata.We could change the naming of the metadata sent by client from `initial_metadata` to a more distinguishable name `outbound_initial_metadata` or `invocation_metadata` or `client_initial_metadata`.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21577,362919775,2020-01-03T18:51:47Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -80,9 +84,15 @@ cdef class _ServicerContext:         if self._metadata_sent:             raise RuntimeError('Send initial metadata failed: already sent')         else:-            _send_initial_metadata(self._rpc_state, self._loop)+            await _send_initial_metadata(self._rpc_state, metadata, self._loop)","The race condition is indeed a problem. We could 1) enforce the synchronization ourselves; 2) or educate users in docstring. In other implementations of gRPC, usually it is enforced by users (Golang, C++). For Python, I will vote for adding locks to make debugging simpler for our users.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21577,362920114,2020-01-03T18:52:52Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -80,9 +84,15 @@ cdef class _ServicerContext:         if self._metadata_sent:             raise RuntimeError('Send initial metadata failed: already sent')         else:-            _send_initial_metadata(self._rpc_state, self._loop)+            await _send_initial_metadata(self._rpc_state, metadata, self._loop)             self._metadata_sent = True +    def set_trailing_metadata(self, tuple metadata):+        self._rpc_state.trailing_metadata = metadata++    def invocation_metadata(self):+        return self._rpc_state.invocation_metadata()+  cdef _find_method_handler(str method, list generic_handlers):     # TODO(lidiz) connects Metadata to call details",Don't forget to pass-in the invocation metadata here.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21577,362920783,2020-01-03T18:54:59Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -126,16 +132,28 @@ async def _handle_unary_unary_rpc(object method_handler,     )      # Sends response message-    cdef tuple send_ops = (-        SendStatusFromServerOperation(-            tuple(),-            StatusCode.ok,-            b'',-            _EMPTY_FLAGS,-        ),-        SendInitialMetadataOperation(None, _EMPTY_FLAGS),-        SendMessageOperation(response_raw, _EMPTY_FLAGS),-    )+    cdef tuple send_ops+    if servicer_context._metadata_sent:+        send_ops = (+            SendStatusFromServerOperation(+                rpc_state.trailing_metadata,+                StatusCode.ok,+                b'',+                _EMPTY_FLAGS,+            ),+            SendMessageOperation(response_raw, _EMPTY_FLAGS),+        )+    else:+        send_ops = (+            SendStatusFromServerOperation(+                rpc_state.trailing_metadata,+                StatusCode.ok,+                b'',+                _EMPTY_FLAGS,+            ),+            SendInitialMetadataOperation(None, _EMPTY_FLAGS),","Let's be consistent about empty metadata, either all `None` or all `tuple()`.",X
5279114,ZHmao,https://api.github.com/repos/grpc/grpc/pulls/21577,363257994,2020-01-06T11:32:44Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -126,16 +132,28 @@ async def _handle_unary_unary_rpc(object method_handler,     )      # Sends response message-    cdef tuple send_ops = (-        SendStatusFromServerOperation(-            tuple(),-            StatusCode.ok,-            b'',-            _EMPTY_FLAGS,-        ),-        SendInitialMetadataOperation(None, _EMPTY_FLAGS),-        SendMessageOperation(response_raw, _EMPTY_FLAGS),-    )+    cdef tuple send_ops+    if servicer_context._metadata_sent:","Yes, it fixed the wait forever issue when we echo metadata from server-side. https://grpcpython.slack.com/archives/CLZ49CDNK/p1576553904001300",
5279114,ZHmao,https://api.github.com/repos/grpc/grpc/pulls/21577,363262060,2020-01-06T11:46:58Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -27,12 +27,14 @@ cdef class _AioCall:     def __cinit__(self,                   AioChannel channel,                   object deadline,+                  tuple metadata,                   bytes method):         self._channel = channel         self._references = []         self._grpc_call_wrapper = GrpcCallWrapper()         self._loop = asyncio.get_event_loop()         self._create_grpc_call(deadline, method)+        self._initial_metadata = metadata","OK, I prefer to use `invocation_metadata` .",
5279114,ZHmao,https://api.github.com/repos/grpc/grpc/pulls/21577,363265275,2020-01-06T11:57:23Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -33,9 +33,13 @@ cdef class RPCState:         self.server = server         grpc_metadata_array_init(&self.request_metadata)         grpc_call_details_init(&self.details)+        self.trailing_metadata = tuple()",You mean like this?:_EMPTY_META = tuple()self.trailing_metadata = _EMPTY_META,
5279114,ZHmao,https://api.github.com/repos/grpc/grpc/pulls/21577,363265731,2020-01-06T11:58:59Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -126,16 +132,28 @@ async def _handle_unary_unary_rpc(object method_handler,     )      # Sends response message-    cdef tuple send_ops = (-        SendStatusFromServerOperation(-            tuple(),-            StatusCode.ok,-            b'',-            _EMPTY_FLAGS,-        ),-        SendInitialMetadataOperation(None, _EMPTY_FLAGS),-        SendMessageOperation(response_raw, _EMPTY_FLAGS),-    )+    cdef tuple send_ops+    if servicer_context._metadata_sent:+        send_ops = (+            SendStatusFromServerOperation(+                rpc_state.trailing_metadata,+                StatusCode.ok,+                b'',+                _EMPTY_FLAGS,+            ),+            SendMessageOperation(response_raw, _EMPTY_FLAGS),+        )+    else:+        send_ops = (+            SendStatusFromServerOperation(+                rpc_state.trailing_metadata,+                StatusCode.ok,+                b'',+                _EMPTY_FLAGS,+            ),+            SendInitialMetadataOperation(None, _EMPTY_FLAGS),","OK, I will make all the empty metadata to tuple().",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21577,363483619,2020-01-06T21:08:17Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -126,16 +132,28 @@ async def _handle_unary_unary_rpc(object method_handler,     )      # Sends response message-    cdef tuple send_ops = (-        SendStatusFromServerOperation(-            tuple(),-            StatusCode.ok,-            b'',-            _EMPTY_FLAGS,-        ),-        SendInitialMetadataOperation(None, _EMPTY_FLAGS),-        SendMessageOperation(response_raw, _EMPTY_FLAGS),-    )+    cdef tuple send_ops+    if servicer_context._metadata_sent:+        send_ops = (+            SendStatusFromServerOperation(+                rpc_state.trailing_metadata,+                StatusCode.ok,+                b'',+                _EMPTY_FLAGS,+            ),+            SendMessageOperation(response_raw, _EMPTY_FLAGS),+        )+    else:+        send_ops = (+            SendStatusFromServerOperation(+                rpc_state.trailing_metadata,+                StatusCode.ok,+                b'',+                _EMPTY_FLAGS,+            ),+            SendInitialMetadataOperation(None, _EMPTY_FLAGS),","Core may sort these operations into the proper order, but it seems somewhat unintuitive that this tuple isn't in chronological order. Can we move the `SendInitialMetadataOperation` to the top of the list?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21577,363483931,2020-01-06T21:08:59Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -126,16 +132,28 @@ async def _handle_unary_unary_rpc(object method_handler,     )      # Sends response message-    cdef tuple send_ops = (-        SendStatusFromServerOperation(-            tuple(),-            StatusCode.ok,-            b'',-            _EMPTY_FLAGS,-        ),-        SendInitialMetadataOperation(None, _EMPTY_FLAGS),-        SendMessageOperation(response_raw, _EMPTY_FLAGS),-    )+    cdef tuple send_ops+    if servicer_context._metadata_sent:+        send_ops = (+            SendStatusFromServerOperation(",Seems like this block of the conditional has a fair bit in common with the next block. Can we DRY this up a bit?,X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,363950545,2020-01-07T21:05:44Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -0,0 +1,345 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Interceptors implementation of gRPC Asyncio Python.""""""+import asyncio+import collections+import functools+from abc import ABCMeta, abstractmethod+from typing import Callable, Optional, Iterator, Sequence, Text, Union++import grpc+from grpc._cython import cygrpc++from . import _base_call+from ._call import UnaryUnaryCall+from ._utils import _timeout_to_deadline+from ._typing import (+    RequestType, SerializingFunction, DeserializingFunction,+    MetadataType, ResponseType+)++_LOCAL_CANCELLATION_BEFORE_RPC_DETAILS = 'Locally cancelled by application before starting the RPC!'+++class ClientCallDetails(+        collections.namedtuple(+            'ClientCallDetails',+            ('method', 'timeout', 'metadata', 'credentials')),+        grpc.ClientCallDetails):++    method: Text+    timeout: Optional[float]+    metadata: Optional[MetadataType]+    credentials: Optional[grpc.CallCredentials]+++class UnaryUnaryClientInterceptor(metaclass=ABCMeta):+    """"""Affords intercepting unary-unary invocations.""""""++    @abstractmethod+    async def intercept_unary_unary(+            self, continuation: Callable[[ClientCallDetails, RequestType],+                                         UnaryUnaryCall],+            client_call_details: ClientCallDetails,+            request: RequestType) -> Union[UnaryUnaryCall, ResponseType]:+        """"""Intercepts a unary-unary invocation asynchronously.+        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `response_future = await continuation(client_call_details, request)`+            to continue with the RPC. `continuation` returns the response of the+            RPC.+          client_call_details: A ClientCallDetails object describing the+            outgoing RPC.+          request: The request value for the RPC.+        Returns:+            An object with the RPC response.+        Raises:+          AioRpcError: Indicating that the RPC terminated with non-OK status.+          asyncio.CancelledError: Indicating that the RPC was canceled.+        """"""+++class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    Interceptors might have some work to do before the RPC invocation with+    the capacity of changing the invocation parameters, and some work to do+    after the RPC invocation with the capacity for accessing to the wrapped+    `UnaryUnaryCall`.++    It handles also early and later cancellations, when the RPC has not even+    started and the execution is still held by the interceptors or when the+    RPC has finished but again the execution is still held by the interceptors.++    Once the RPC is finally executed, all methods are finally done against the+    intercepted call, being at the same time the same call returned to the+    interceptors.++    For most of the methods, like `initial_metadata()` the caller does not need+    to wait until the interceptors task is finished, once the RPC is done the+    caller will have the freedom for accessing to the results.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _cancelled_before_rpc: bool+    _intercepted_call: Optional[_base_call.UnaryUnaryCall]+    _intercepted_call_created: asyncio.Event+    _interceptors_task: asyncio.Task++    def __init__( # pylint: disable=R0913+            self,+            interceptors: Sequence[UnaryUnaryClientInterceptor],+            request: RequestType,+            timeout: Optional[float],+            channel: cygrpc.AioChannel,+            method: bytes,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> None:+        self._channel = channel+        self._loop = asyncio.get_event_loop()+        self._interceptors_task = asyncio.ensure_future(+            self._invoke(interceptors, method, timeout, request,+                         request_serializer, response_deserializer))++    def __del__(self):+        self.cancel()++    async def _invoke(+            self, interceptors: Sequence[UnaryUnaryClientInterceptor],+            method: bytes, timeout: Optional[float], request: RequestType,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:+            try:+                interceptor = next(interceptors)+            except StopIteration:+                interceptor = None++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)+                try:+                    call_or_response = await interceptor.intercept_unary_unary(+                        continuation, client_call_details, request)+                except grpc.RpcError as err:+                    # gRPC error is masked inside an artificial call,+                    # caller will see this error if and only+                    # if runs an `await call` operation+                    return UnaryUnaryCallRpcError(err)","Finally, I will follow your advice. So not masking the `RpcError` and `CancelledError` exceptions as fake calls. Could be some down-side effects of doing so, especially for the use case of the `CancelledError` where implicit task cancellations could remain unnoticeable at interceptor, as the following example shows:```pythoncall = continuation(...)await a_long_internal_asynchronous_coroutine()....```The `a_long_internal_asynchronous_coroutine` would never be affected by a `CancelledError` exception, so not giving up on this execution even thought a cancellation was propagated by the down layers.IMO this could be a good reason for not masking the `CancelledError`, the same for the`RpcError`.So, in that case, as you already mentioned we would need to advise the users for expecting both exceptions at continuation call and when the _call_ is being awaited.  ",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,363997023,2020-01-07T23:12:15Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -176,14 +166,25 @@ def cancelled(self) -> bool:         if not self._interceptors_task.done():             return False -        call = self._interceptors_task.result()-        return call.cancelled()+        try:+            call = self._interceptors_task.result()","If `result()` is available for an `asyncio.Task`, the value of `done()` should be `True` (see [doc](https://docs.python.org/3/library/asyncio-task.html#asyncio.Task.done)). Hence, I don't think this line can be reached when the Task is in error state. So, the cancellation result returned might be wrong.On the other hand, if the Task is not finished yet, this call will throw an `InvalidStateError`.We might need to think this over. Same argument apply to other try-catch sections in this file.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21455,363998431,2020-01-07T23:17:23Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -348,6 +294,106 @@ class Interceptor(aio.UnaryUnaryClientInterceptor):             self.assertEqual(await call.initial_metadata(), ())             self.assertEqual(await call.trailing_metadata(), ()) +    async def test_call_ok_awaited(self):++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                await call+                return call++        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(server_target,+                                        interceptors=[Interceptor()+                                                     ]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())+            response = await call++            self.assertTrue(call.done())+            self.assertFalse(call.cancelled())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)+            self.assertEqual(await call.details(), '')+            self.assertEqual(await call.initial_metadata(), ())+            self.assertEqual(await call.trailing_metadata(), ())++    async def test_call_rpcerror(self):++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                return call++        server_target, server = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(server_target,+                                        interceptors=[Interceptor()+                                                     ]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)++            await server.stop(None)++            call = multicallable(messages_pb2.SimpleRequest(), timeout=0.1)","0.1 second is a large amount of time, I didn't spot a sleep on both client-side and server-side. So, the RPC seems should not timeout. If this is due to https://github.com/grpc/grpc/issues/21605, we should note it down. This test case might not testing the right behavior.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,364254447,2020-01-08T14:22:05Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -176,14 +166,25 @@ def cancelled(self) -> bool:         if not self._interceptors_task.done():             return False -        call = self._interceptors_task.result()-        return call.cancelled()+        try:+            call = self._interceptors_task.result()","> On the other hand, if the Task is not finished yet, this call will throw an InvalidStateErrorThis is way first what is being done is checking if the task was finished or not by doing this```python if not self._interceptors_task.done():    return False```Only if the task as been finished the `result()` method is called. In case of no exceptions the returned value should be a `UnaryUnaryCall` and this is the reason of having the ```pythontry:    call = self._interceptors_task.result()else:     return call.cancelled()```If there was an error, two exceptions are expected the `AioRpcError` or the `asyncio.CancelledError` and for each exception use case we implement the right logic, for example in the use case of the `canceled()` method```pythonexcept AioRpcError:    return Falseexcept asyncio.CancelledError:    return True```For methods that are coroutines, like the `code` one it's a bit easier since we do not need to check if the task is done and we have to basically _await_ till the task has finished, no matter if with an exception or with an error, for example:```pythonasync def initial_metadata(self) -> Optional[MetadataType]:    try:        call = await self._interceptors_task    except AioRpcError as err:        return err.initial_metadata()    except asyncio.CancelledError:        return None    else:        return await call.initial_metadata()```Theoretically, all use cases should be in the test suite .... so If I'm not missing something the pattern used should work for all of the methods and scenarios.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21455,364255362,2020-01-08T14:24:00Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -348,6 +294,106 @@ class Interceptor(aio.UnaryUnaryClientInterceptor):             self.assertEqual(await call.initial_metadata(), ())             self.assertEqual(await call.trailing_metadata(), ()) +    async def test_call_ok_awaited(self):++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                await call+                return call++        server_target, _ = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(server_target,+                                        interceptors=[Interceptor()+                                                     ]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())+            response = await call++            self.assertTrue(call.done())+            self.assertFalse(call.cancelled())+            self.assertEqual(type(response), messages_pb2.SimpleResponse)+            self.assertEqual(await call.code(), grpc.StatusCode.OK)+            self.assertEqual(await call.details(), '')+            self.assertEqual(await call.initial_metadata(), ())+            self.assertEqual(await call.trailing_metadata(), ())++    async def test_call_rpcerror(self):++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                call = await continuation(client_call_details, request)+                return call++        server_target, server = await start_test_server()  # pylint: disable=unused-variable++        async with aio.insecure_channel(server_target,+                                        interceptors=[Interceptor()+                                                     ]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)++            await server.stop(None)++            call = multicallable(messages_pb2.SimpleRequest(), timeout=0.1)","before the request is made the server is stopped, see one line above. So theoretically the timeout is reached because the server is no longer available.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21582,364272274,2020-01-08T14:56:20Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -45,10 +49,54 @@ cdef class RPCState:             grpc_call_unref(self.call)  +# TODO(lidiz) inherit this from Python level `AioRpcStatus`, we need to improve+# current code structure to make it happen.+class AbortError(Exception): pass+++def _raise_if_aborted(RPCState rpc_state):+    """"""Raise AbortError if RPC is aborted.++    Server method handlers may suppress the abort exception. We need to halt+    the RPC execution in that case. This function needs to be called after+    running application code.+    """"""+    if rpc_state.abort_exception is not None:+        raise rpc_state.abort_exception+++async def _perform_abort(RPCState rpc_state,+                         grpc_status_code code,+                         str details, +                         tuple trailing_metadata,+                         object loop):+    """"""Perform the abort logic.++    Sends final status to the client, and then set the RPC into corresponding+    state.+    """"""+    if rpc_state.abort_exception is not None:+        raise RuntimeError('Abort already called!')+    else:+        # Keeps track of the exception object. After abort happen, the RPC+        # should stop execution. However, if users decided to suppress it, it+        # could lead to undefined behavior.+        rpc_state.abort_exception = AbortError('Locally aborted.')",I'm wondering if `AbortError` could be always the same exception.... but not seeing much benefits on using a constant exception.,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21582,364289412,2020-01-08T15:28:10Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -137,6 +207,7 @@ async def _handle_unary_unary_rpc(object method_handler,         SendMessageOperation(response_raw, _EMPTY_FLAGS),     )     await execute_batch(rpc_state, send_ops, loop)+    rpc_state.status_sent = True",Should we move this line before the `execute_batch`? So following the same pattern that you already used here [1]?.This is IMO the most effective way - that does not require a lock - for not having data races where one abort/read/write operation and a finalization status operation are tried to send at the same time.[1] https://github.com/grpc/grpc/pull/21582/files#diff-b53c077f7911f53dc8ce7656e16d35cdR86,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21517,364314672,2020-01-08T16:14:06Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -384,49 +376,265 @@ def __del__(self) -> None:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()-            return True+        if raw_response is None:+            return None         else:-            return False+            return _common.deserialize(raw_response,+                                       self._response_deserializer) -    def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)++        response_message = await self._read()++        if response_message is None:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+            # If no exception raised, there is something wrong internally.+            raise RuntimeError('Read operation failed with StatusCode.OK')+        else:+            return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())+        self._cancellation_list.append(self._call)++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._cancellation_list.append(+                self._loop.create_task(+                    self._consume_request_iterator(request_async_iterator)))++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise _create_rpc_error(_EMPTY_METADATA, self._status.result())+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())+        self._cancellation_list.append(self._setup_task)++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._cancellation_list.append(","Seems that by the unique need of the `StreamStreamCall` class for providing a way for consuming messages, we have needed to extend the cancellation implementation for supporting a list of tasks. I'm wondering if it would be possible to cancel this extra task by overriding the `cancel()` method, for example:```pythondef cancel():    ret = super().cancel()    if ret:       self._consume_request_iterator_task.cancel()```",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21517,364315517,2020-01-08T16:15:45Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -384,49 +376,265 @@ def __del__(self) -> None:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()-            return True+        if raw_response is None:+            return None         else:-            return False+            return _common.deserialize(raw_response,+                                       self._response_deserializer) -    def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)++        response_message = await self._read()++        if response_message is None:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+            # If no exception raised, there is something wrong internally.+            raise RuntimeError('Read operation failed with StatusCode.OK')+        else:+            return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())+        self._cancellation_list.append(self._call)++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._cancellation_list.append(+                self._loop.create_task(+                    self._consume_request_iterator(request_async_iterator)))++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise _create_rpc_error(_EMPTY_METADATA, self._status.result())+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())+        self._cancellation_list.append(self._setup_task)++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._cancellation_list.append(+                self._loop.create_task(+                    self._consume_request_iterator(request_async_iterator)))+        self._message_aiter = self._fetch_stream_responses()++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _setup(self):+        try:+            await self._cython_call.stream_stream(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            # No need to raise RpcError here, because no one will `await` this task.++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()",should we protect the whole logic of `asyncio.CancelledError` exceptions for leaving the trace in a clean way? so having something like this```pythontry:    await self._metadata_sent.wait()    async for request in request_async_iterator:        await self.write(request)    await self.done_writing()except asyncio.CancelledError:    pass```,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21517,364330726,2020-01-08T16:43:07Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -398,6 +399,197 @@ class TestUnaryStreamCall(AioTestBase):                 await task  +class TestStreamUnaryCall(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_cancel_stream_unary(self):+        async with aio.insecure_channel(self._server_target) as channel:+            stub = test_pb2_grpc.TestServiceStub(channel)++            call = stub.StreamingInputCall()++            # Prepares the request+            payload = messages_pb2.Payload(body=b'\0' * _REQUEST_PAYLOAD_SIZE)+            request = messages_pb2.StreamingInputCallRequest(payload=payload)++            # Sends out requests+            for _ in range(_NUM_STREAM_RESPONSES):+                await call.write(request)++            # Cancels the RPC+            self.assertFalse(call.done())+            self.assertFalse(call.cancelled())+            call.cancel()+            self.assertTrue(call.cancelled())++            await call.done_writing()++            with self.assertRaises(grpc.RpcError) as exception_context:","I'm a bit lost here, should not this throw an `asyncio.CancelledError` exception since this is a local cancellation?",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21517,364331956,2020-01-08T16:45:31Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -398,6 +399,197 @@ class TestUnaryStreamCall(AioTestBase):                 await task  +class TestStreamUnaryCall(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_cancel_stream_unary(self):+        async with aio.insecure_channel(self._server_target) as channel:+            stub = test_pb2_grpc.TestServiceStub(channel)++            call = stub.StreamingInputCall()++            # Prepares the request+            payload = messages_pb2.Payload(body=b'\0' * _REQUEST_PAYLOAD_SIZE)+            request = messages_pb2.StreamingInputCallRequest(payload=payload)++            # Sends out requests+            for _ in range(_NUM_STREAM_RESPONSES):+                await call.write(request)++            # Cancels the RPC+            self.assertFalse(call.done())+            self.assertFalse(call.cancelled())+            call.cancel()+            self.assertTrue(call.cancelled())++            await call.done_writing()++            with self.assertRaises(grpc.RpcError) as exception_context:+                await call++            rpc_error = exception_context.exception+            self.assertEqual(grpc.StatusCode.CANCELLED, rpc_error.code())+            self.assertEqual(grpc.StatusCode.CANCELLED, await call.code())++    async def test_early_cancel_stream_unary(self):+        async with aio.insecure_channel(self._server_target) as channel:+            stub = test_pb2_grpc.TestServiceStub(channel)+            call = stub.StreamingInputCall()++            # Cancels the RPC+            self.assertFalse(call.done())+            self.assertFalse(call.cancelled())+            self.assertTrue(call.cancel())+            self.assertTrue(call.cancelled())++            with self.assertRaises(asyncio.InvalidStateError):+                await call.write(messages_pb2.StreamingInputCallRequest())++            # Should be no-op+            await call.done_writing()++            with self.assertRaises(grpc.RpcError) as exception_context:","same as before, should this throw an `asyncio.CancelledError` exception?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21582,364368638,2020-01-08T18:07:40Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -45,10 +49,54 @@ cdef class RPCState:             grpc_call_unref(self.call)  +# TODO(lidiz) inherit this from Python level `AioRpcStatus`, we need to improve+# current code structure to make it happen.+class AbortError(Exception): pass+++def _raise_if_aborted(RPCState rpc_state):+    """"""Raise AbortError if RPC is aborted.++    Server method handlers may suppress the abort exception. We need to halt+    the RPC execution in that case. This function needs to be called after+    running application code.+    """"""+    if rpc_state.abort_exception is not None:+        raise rpc_state.abort_exception+++async def _perform_abort(RPCState rpc_state,+                         grpc_status_code code,+                         str details, +                         tuple trailing_metadata,+                         object loop):+    """"""Perform the abort logic.++    Sends final status to the client, and then set the RPC into corresponding+    state.+    """"""+    if rpc_state.abort_exception is not None:+        raise RuntimeError('Abort already called!')+    else:+        # Keeps track of the exception object. After abort happen, the RPC+        # should stop execution. However, if users decided to suppress it, it+        # could lead to undefined behavior.+        rpc_state.abort_exception = AbortError('Locally aborted.')","The issue I'm trying to solve is that if users suppressed the exception, and continue whatever logic. We need a way to explicitly log an error saying, hey, you should not suppress the exception, and here is the location that you called abort (see `traceback`).",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364395391,2020-01-08T19:09:34Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -384,49 +376,265 @@ def __del__(self) -> None:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()-            return True+        if raw_response is None:+            return None         else:-            return False+            return _common.deserialize(raw_response,+                                       self._response_deserializer) -    def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)++        response_message = await self._read()++        if response_message is None:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+            # If no exception raised, there is something wrong internally.+            raise RuntimeError('Read operation failed with StatusCode.OK')+        else:+            return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())+        self._cancellation_list.append(self._call)++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._cancellation_list.append(+                self._loop.create_task(+                    self._consume_request_iterator(request_async_iterator)))++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise _create_rpc_error(_EMPTY_METADATA, self._status.result())+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())+        self._cancellation_list.append(self._setup_task)++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._cancellation_list.append(","Updated to this pattern for all `Call` objects.I'm neutral about different alternatives here. Inheritance can be tricky, but communicate through a member variable is also not ideal. After the change, the logic does look more cohesive 👍 ",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21351,364431993,2020-01-08T20:37:48Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/channel.pyx.pxi,"@@ -28,11 +38,12 @@ cdef class AioChannel:      def call(self,              bytes method,-             object deadline):+             object deadline,+             CallCredentials credentials):         """"""Assembles a Cython Call object.          Returns:           The _AioCall object.         """"""-        cdef _AioCall call = _AioCall(self, deadline, method)+        cdef _AioCall call = _AioCall(self, deadline, method, credentials)",`CallCredentials` are only valid for secure channels. We should raise an exception if users are trying to pass in a `CallCredentials` to an insecure channel.,X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21517,364442304,2020-01-08T21:04:30Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/common.pyx.pxi,"@@ -33,3 +33,24 @@ cdef bytes serialize(object serializer, object message):         return serializer(message)     else:         return message+++class _EOF(object):",is the `object` inheritance needed?,X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21517,364450085,2020-01-08T21:24:16Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -384,49 +376,265 @@ def __del__(self) -> None:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()-            return True+        if raw_response is None:+            return None         else:-            return False+            return _common.deserialize(raw_response,+                                       self._response_deserializer) -    def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)++        response_message = await self._read()++        if response_message is None:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+            # If no exception raised, there is something wrong internally.+            raise RuntimeError('Read operation failed with StatusCode.OK')+        else:+            return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())+        self._cancellation_list.append(self._call)++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._cancellation_list.append(+                self._loop.create_task(+                    self._consume_request_iterator(request_async_iterator)))++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise _create_rpc_error(_EMPTY_METADATA, self._status.result())+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())+        self._cancellation_list.append(self._setup_task)++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._cancellation_list.append(+                self._loop.create_task(+                    self._consume_request_iterator(request_async_iterator)))+        self._message_aiter = self._fetch_stream_responses()++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _setup(self):+        try:+            await self._cython_call.stream_stream(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            # No need to raise RpcError here, because no one will `await` this task.++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()","It seems that if a task/future exception is not consumed but it is an `Asyncio.CanclledError` nothing will be logged at destruction time [1] [2]Forget about the comment regarding suppressing the `asyncio.CancelledError` exception.But I would say that the following one still applies:> or, how can we notify the upstream layer - the one that is producing messages - that no more messages will be consumed?So if the iterator is no longer consumed, how the producer would know that needs to stop? should we do our best and do something like this.```pythontry:    await self._metadata_sent.wait()    async for request in request_async_iterator:        await self.write(request)    await self.done_writing()except asyncio.CancelledError:    async for request in request_async_iterator:        logger.warning(""RPC cancelled, consuming pending messages but not sending them"")     raise```[1] https://github.com/python/cpython/blob/master/Lib/asyncio/futures.py#L91[2] https://github.com/python/cpython/blob/master/Lib/asyncio/futures.py#L133",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21517,364453175,2020-01-08T21:31:45Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -384,49 +376,265 @@ def __del__(self) -> None:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()-            return True+        if raw_response is None:+            return None         else:-            return False+            return _common.deserialize(raw_response,+                                       self._response_deserializer) -    def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)++        response_message = await self._read()++        if response_message is None:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+            # If no exception raised, there is something wrong internally.+            raise RuntimeError('Read operation failed with StatusCode.OK')+        else:+            return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())+        self._cancellation_list.append(self._call)++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._cancellation_list.append(+                self._loop.create_task(+                    self._consume_request_iterator(request_async_iterator)))++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise _create_rpc_error(_EMPTY_METADATA, self._status.result())+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())+        self._cancellation_list.append(self._setup_task)++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._cancellation_list.append(+                self._loop.create_task(+                    self._consume_request_iterator(request_async_iterator)))+        self._message_aiter = self._fetch_stream_responses()","Hard to say, I do not have any example where mixing could give a benefit. Maybe mixing both patterns is the same as having two tasks/coros reading at the same time, if the second one works right now I would not prohibit mixing neither because it should be doable.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21517,364454966,2020-01-08T21:36:27Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -445,6 +653,6 @@ def __aiter__(self) -> AsyncIterable[ResponseType]:             # If the read operation failed, Core should explain why.             await self._raise_for_status()             # If no exception raised, there is something wrong internally.-            assert False, 'Read operation failed with StatusCode.OK'+            raise RuntimeError('Read operation failed with StatusCode.OK')","For me `None` should be also a valid terminal value, but also I see the value of EOF which is more explicit. I'm fine with either of them.My main worry was more about the exception raised yes or yes when a None value was returned when an explicit `read()` was done,  which you have addressed.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,364475586,2020-01-08T22:30:10Z,src/python/grpcio/grpc/experimental/aio/_base_call.py,"@@ -157,3 +157,76 @@ def __aiter__(self) -> AsyncIterable[ResponseType]:         Returns:           A response message of the RPC.         """"""+++class StreamUnaryCall(Generic[RequestType, ResponseType],+                      Call,+                      metaclass=ABCMeta):++    @abstractmethod+    async def write(self, request: RequestType) -> None:+        """"""Writes one message to the RPC.++        Raises:+          An RpcError exception if the write failed.+        """"""++    @abstractmethod+    async def done_writing(self) -> None:+        """"""Notifies server that the client is done sending messages.++        After done_writing is called, any additional invocation to the write+        function will fail.+        """"""++    @abstractmethod+    def __await__(self) -> Awaitable[ResponseType]:+        """"""Await the response message to be ready.++        Returns:+          The response message of the RPC.+        """"""+++class StreamStreamCall(Generic[RequestType, ResponseType],+                       Call,+                       metaclass=ABCMeta):++    @abstractmethod+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        """"""Returns the async iterable representation that yields messages.++        Under the hood, it is calling the ""read"" method.++        Returns:+          An async iterable object that yields messages.+        """"""++    @abstractmethod+    async def read(self) -> ResponseType:+        """"""Reads one message from the RPC.++        For each streaming RPC, concurrent reads in multiple coroutines are not+        allowed. If you want to perform read in multiple coroutines, you needs+        synchronization. So, you can start another read after current read is+        finished.++        Returns:+          A response message of the RPC.","We should document the behavior when the stream ends. I.e., something along the lines of ""Returns `grpc.EOF` to indicate the end of the stream.""",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,364482639,2020-01-08T22:50:08Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -146,31 +147,49 @@ def _create_rpc_error(initial_metadata: Optional[MetadataType],   class Call(_base_call.Call):+    """"""Base implementation of client RPC Call object.++    Implements logic around final status, metadata and cancellation.+    """"""     _loop: asyncio.AbstractEventLoop     _code: grpc.StatusCode     _status: Awaitable[cygrpc.AioRpcStatus]     _initial_metadata: Awaitable[MetadataType]     _locally_cancelled: bool+    _cython_call: cygrpc._AioCall -    def __init__(self) -> None:+    def __init__(self, deadline: Optional[float], channel: cygrpc.AioChannel,",I don't think users are able to instantiate this class themselves. Could we just take a `cygrpc._AioCall` in the constructor instead of the three things needed to create one?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364496217,2020-01-08T23:33:32Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -384,49 +376,265 @@ def __del__(self) -> None:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()-            return True+        if raw_response is None:+            return None         else:-            return False+            return _common.deserialize(raw_response,+                                       self._response_deserializer) -    def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)++        response_message = await self._read()++        if response_message is None:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+            # If no exception raised, there is something wrong internally.+            raise RuntimeError('Read operation failed with StatusCode.OK')+        else:+            return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())+        self._cancellation_list.append(self._call)++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._cancellation_list.append(+                self._loop.create_task(+                    self._consume_request_iterator(request_async_iterator)))++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise _create_rpc_error(_EMPTY_METADATA, self._status.result())+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())+        self._cancellation_list.append(self._setup_task)++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._cancellation_list.append(+                self._loop.create_task(+                    self._consume_request_iterator(request_async_iterator)))+        self._message_aiter = self._fetch_stream_responses()++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _setup(self):+        try:+            await self._cython_call.stream_stream(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            # No need to raise RpcError here, because no one will `await` this task.++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()","Since we already cancelling the consumer task in the override `cancel()`, we are injecting `CancelledError` into the `async for`. So users will observe the exception. On the other hand, I didn't know this behavior before, added two test case naming ""test_error_in_async_generator"".",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,364496480,2020-01-08T23:34:36Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF++        response_message = await self._read()++        if response_message is cygrpc.EOF:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+        return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()+        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())++        # If user passes in an async iterator, create a consumer coroutine.",Nit: `s/coroutine/task/``_consume_request_iterator` is the coroutine. `self._consumer` is a `Task` instantiated using the coroutine.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,364498800,2020-01-08T23:43:47Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF++        response_message = await self._read()++        if response_message is cygrpc.EOF:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+        return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task","Naming here is a bit tricky. From the perspective of the application, we are a consumer. From the perspective of core, we are a producer. We probably want to write things from the perspective of this class. In that vein, maybe we can rename `_consumer` to `_input` and `_call` to `_output`? ",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,364500188,2020-01-08T23:49:03Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF++        response_message = await self._read()++        if response_message is cygrpc.EOF:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+        return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()+        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None++    def cancel(self) -> bool:+        if super().cancel():+            self._call.cancel()+            if self._consumer is not None:+                self._consumer.cancel()             return True         else:             return False +    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:","Hm... Invoke doesn't necessarily seem like the right name for this arity. ""Invoke"" would imply that we're supplying parameters to the call, but that's not what's happening here. All we're really doing is collecting the response. Maybe `await_response` would be a better name?",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,364502082,2020-01-08T23:56:40Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF++        response_message = await self._read()++        if response_message is cygrpc.EOF:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+        return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()+        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None++    def cancel(self) -> bool:+        if super().cancel():+            self._call.cancel()+            if self._consumer is not None:+                self._consumer.cancel()             return True         else:             return False +    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None+        self._message_aiter = None+     def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+        if super().cancel():+            self._setup_task.cancel()+            if self._consumer is not None:+                self._consumer.cancel()+            return True+        else:+            return False++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _setup(self):",Looks like this is the equivalent of `_invoke` for stream-unary. Let's settle on some common naming for the two Task objects and be consistent across the arities.,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,364503823,2020-01-09T00:03:46Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF++        response_message = await self._read()++        if response_message is cygrpc.EOF:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+        return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()+        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None++    def cancel(self) -> bool:+        if super().cancel():+            self._call.cancel()+            if self._consumer is not None:+                self._consumer.cancel()             return True         else:             return False +    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None+        self._message_aiter = None+     def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+        if super().cancel():+            self._setup_task.cancel()+            if self._consumer is not None:+                self._consumer.cancel()+            return True+        else:+            return False++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _setup(self):+        try:+            await self._cython_call.stream_stream(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            # No need to raise RpcError here, because no one will `await` this task.++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    async def write(self, request: RequestType) -> None:",Some of this seems very similar to stream-unary. Opportunity for deduplication?,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,364507950,2020-01-09T00:20:12Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF++        response_message = await self._read()++        if response_message is cygrpc.EOF:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+        return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()+        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None++    def cancel(self) -> bool:+        if super().cancel():+            self._call.cancel()+            if self._consumer is not None:+                self._consumer.cancel()             return True         else:             return False +    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None+        self._message_aiter = None+     def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+        if super().cancel():+            self._setup_task.cancel()+            if self._consumer is not None:+                self._consumer.cancel()+            return True+        else:+            return False++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _setup(self):+        try:+            await self._cython_call.stream_stream(","Hm... I think we might want to firm up the naming in the cython layer. In a vacuum, I would think that `await self._cython_call.stream_stream` would put me in a state where a stream-stream RPC has completely finished. But a look at the implementation seems to imply that this only *initiates* the RPC. That is, when this invocation completes, the only guarantee we have about the state of the RPC is that initial metadata has been state. Maybe the method name should change from `stream_stream` to `initiate_stream_stream`?In general, I think it's a good idea to document awaitables public to a class with the state that class will be left in when the awaitable is finished.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,364508684,2020-01-09T00:23:37Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF++        response_message = await self._read()++        if response_message is cygrpc.EOF:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+        return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()+        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None++    def cancel(self) -> bool:+        if super().cancel():+            self._call.cancel()+            if self._consumer is not None:+                self._consumer.cancel()             return True         else:             return False +    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None+        self._message_aiter = None+     def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+        if super().cancel():+            self._setup_task.cancel()+            if self._consumer is not None:+                self._consumer.cancel()+            return True+        else:+            return False++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _setup(self):+        try:+            await self._cython_call.stream_stream(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            # No need to raise RpcError here, because no one will `await` this task.++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    async def write(self, request: RequestType) -> None:",I think there's a race between the completion of `_setup_task` and the application's first invocation of `write`. You explicitly handle this in the case that the user supplies an iterator by calling `await self._metadata_sent.wait()`. How can we avoid that race here?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21607,364525477,2020-01-09T01:40:08Z,src/python/grpcio_tests/tests_aio/unit/channel_argument_test.py,"@@ -0,0 +1,168 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests behavior around the Core channel arguments.""""""++import asyncio+import logging+import unittest+import socket++import grpc+import random++from grpc.experimental import aio+from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc+from tests.unit.framework.common import test_constants+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+# 100 servers in sequence++_RANDOM_SEED = 42++_ENABLE_REUSE_PORT = 'SO_REUSEPORT enabled'+_DISABLE_REUSE_PORT = 'SO_REUSEPORT disabled'+_SOCKET_OPT_SO_REUSEPORT = 'grpc.so_reuseport'+_OPTIONS = (+    (_ENABLE_REUSE_PORT, ((_SOCKET_OPT_SO_REUSEPORT, 1),)),+    (_DISABLE_REUSE_PORT, ((_SOCKET_OPT_SO_REUSEPORT, 0),)),+)++_NUM_SERVER_CREATED = 100++_GRPC_ARG_MAX_RECEIVE_MESSAGE_LENGTH = 'grpc.max_receive_message_length'+_MAX_MESSAGE_LENGTH = 1024+++class _TestPointerWrapper(object):++    def __int__(self):+        return 123456+++_TEST_CHANNEL_ARGS = (+    ('arg1', b'bytes_val'),+    ('arg2', 'str_val'),+    ('arg3', 1),+    (b'arg4', 'str_val'),+    ('arg6', _TestPointerWrapper()),+)++_INVALID_TEST_CHANNEL_ARGS = [+    {+        'foo': 'bar'+    },+    (('key',),),+    'str',+]+++async def test_if_reuse_port_enabled(server: aio.Server):+    port = server.add_insecure_port('127.0.0.1:0')+    await server.start()++    try:+        another_socket = socket.socket(family=socket.AF_INET)+        another_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)+        another_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)+        another_socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, True)+        another_socket.bind(('127.0.0.1', port))+    except OSError as e:+        assert 'Address already in use' in str(e)+        return False+    else:+        return True+    finally:+        another_socket.close()+++class TestChannelArgument(AioTestBase):++    async def setUp(self):+        random.seed(_RANDOM_SEED)++    @unittest.skip('https://github.com/grpc/grpc/issues/20667')+    async def test_server_so_reuse_port_is_set_properly(self):++        async def test_body():+            fact, options = random.choice(_OPTIONS)",Very nice! I love this style of test! :heart:The level of coverage is so much higher. The only question is how long this test takes to complete.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21607,364525703,2020-01-09T01:41:16Z,src/python/grpcio_tests/tests_aio/unit/channel_argument_test.py,"@@ -0,0 +1,168 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests behavior around the Core channel arguments.""""""++import asyncio+import logging+import unittest+import socket++import grpc+import random++from grpc.experimental import aio+from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc+from tests.unit.framework.common import test_constants+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+# 100 servers in sequence++_RANDOM_SEED = 42++_ENABLE_REUSE_PORT = 'SO_REUSEPORT enabled'+_DISABLE_REUSE_PORT = 'SO_REUSEPORT disabled'+_SOCKET_OPT_SO_REUSEPORT = 'grpc.so_reuseport'+_OPTIONS = (+    (_ENABLE_REUSE_PORT, ((_SOCKET_OPT_SO_REUSEPORT, 1),)),+    (_DISABLE_REUSE_PORT, ((_SOCKET_OPT_SO_REUSEPORT, 0),)),+)++_NUM_SERVER_CREATED = 100++_GRPC_ARG_MAX_RECEIVE_MESSAGE_LENGTH = 'grpc.max_receive_message_length'+_MAX_MESSAGE_LENGTH = 1024+++class _TestPointerWrapper(object):++    def __int__(self):+        return 123456+++_TEST_CHANNEL_ARGS = (+    ('arg1', b'bytes_val'),+    ('arg2', 'str_val'),+    ('arg3', 1),+    (b'arg4', 'str_val'),+    ('arg6', _TestPointerWrapper()),+)++_INVALID_TEST_CHANNEL_ARGS = [+    {+        'foo': 'bar'+    },+    (('key',),),+    'str',+]+++async def test_if_reuse_port_enabled(server: aio.Server):+    port = server.add_insecure_port('127.0.0.1:0')",Beware using specific address families. Kokoro only supports IPV4 and Borg only supports IPV6. Be agnostic and use `localhost` or prepare for a bad import cycle.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21607,364888303,2020-01-09T18:16:08Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -468,5 +468,11 @@ cdef class AioServer:         If the Cython representation is deallocated without underlying objects         freed, raise an RuntimeError.         """"""+        # TODO(lidiz) if users create server, and then dealloc it immediately.+        # There is a potential memory leak of created Core server.         if self._status != AIO_SERVER_STATUS_STOPPED:-            raise RuntimeError('__dealloc__ called on running server: %d', self._status)+            _LOGGER.warn(","Fixed the `warning` typo.That is what I'am expecting that `__del__` is called before `__dealloc__`, however, this line is reached during the development of this PR. The repro condition is simply letting server go out-of-scope without invoking `stop`.We already invoked shutdown in the Python level `__del__`. Calling shutdown again in `__dealloc__` may resurrect the object, or lead to undefined behavior.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364905080,2020-01-09T18:52:32Z,src/python/grpcio/grpc/experimental/aio/_base_call.py,"@@ -157,3 +157,76 @@ def __aiter__(self) -> AsyncIterable[ResponseType]:         Returns:           A response message of the RPC.         """"""+++class StreamUnaryCall(Generic[RequestType, ResponseType],+                      Call,+                      metaclass=ABCMeta):++    @abstractmethod+    async def write(self, request: RequestType) -> None:+        """"""Writes one message to the RPC.++        Raises:+          An RpcError exception if the write failed.+        """"""++    @abstractmethod+    async def done_writing(self) -> None:","This name is picked during discussion with Jan.For Csharp, their name is `CompleteAsync`: https://github.com/grpc/grpc/blob/master/examples/csharp/RouteGuide/RouteGuideClient/Program.cs#L183For Golang, their pick is `CloseSend`: https://github.com/grpc/grpc-go/blob/master/examples/route_guide/client/client.go#L143To me, the name should imply that it only closes the writing side of the stream, not the reading side. So, `close_stream` might not be accurate enough.",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/20568,364922457,2020-01-09T19:33:46Z,test/cpp/end2end/end2end_test.cc,"@@ -405,6 +410,7 @@ class End2endTest : public ::testing::TestWithParam<TestScenario> {      stub_ = grpc::testing::EchoTestService::NewStub(channel_);     DummyInterceptor::Reset();+    WaitOnServerAuthorizationToComplete(GetCredentialsProvider());","I added a `credentials_type` parameter to `WaitOnSpawnedThreads`. If `credentials_type` is anything other than `kSpiffeCredentialsType`, then it immediately returns.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364933618,2020-01-09T19:59:29Z,src/python/grpcio/grpc/experimental/aio/_base_call.py,"@@ -157,3 +157,76 @@ def __aiter__(self) -> AsyncIterable[ResponseType]:         Returns:           A response message of the RPC.         """"""+++class StreamUnaryCall(Generic[RequestType, ResponseType],+                      Call,+                      metaclass=ABCMeta):++    @abstractmethod+    async def write(self, request: RequestType) -> None:+        """"""Writes one message to the RPC.++        Raises:+          An RpcError exception if the write failed.+        """"""++    @abstractmethod+    async def done_writing(self) -> None:+        """"""Notifies server that the client is done sending messages.++        After done_writing is called, any additional invocation to the write+        function will fail.+        """"""++    @abstractmethod+    def __await__(self) -> Awaitable[ResponseType]:+        """"""Await the response message to be ready.++        Returns:+          The response message of the RPC.+        """"""+++class StreamStreamCall(Generic[RequestType, ResponseType],+                       Call,+                       metaclass=ABCMeta):++    @abstractmethod+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        """"""Returns the async iterable representation that yields messages.++        Under the hood, it is calling the ""read"" method.++        Returns:+          An async iterable object that yields messages.+        """"""++    @abstractmethod+    async def read(self) -> ResponseType:+        """"""Reads one message from the RPC.++        For each streaming RPC, concurrent reads in multiple coroutines are not+        allowed. If you want to perform read in multiple coroutines, you needs+        synchronization. So, you can start another read after current read is+        finished.++        Returns:+          A response message of the RPC.+        """"""++    @abstractmethod+    async def write(self, request: RequestType) -> None:+        """"""Writes one message to the RPC.++        Raises:+          An RpcError exception if the write failed.+        """"""++    @abstractmethod+    async def done_writing(self) -> None:","That is actually a good idea! I like the new idea. It shrinks the exposed interface, and has good semantical meaning.Let's get one more vote from @pfreixes.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364934573,2020-01-09T20:01:48Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -146,31 +147,49 @@ def _create_rpc_error(initial_metadata: Optional[MetadataType],   class Call(_base_call.Call):+    """"""Base implementation of client RPC Call object.++    Implements logic around final status, metadata and cancellation.+    """"""     _loop: asyncio.AbstractEventLoop     _code: grpc.StatusCode     _status: Awaitable[cygrpc.AioRpcStatus]     _initial_metadata: Awaitable[MetadataType]     _locally_cancelled: bool+    _cython_call: cygrpc._AioCall -    def __init__(self) -> None:+    def __init__(self, deadline: Optional[float], channel: cygrpc.AioChannel,","We are exposing the `_base_call.Call`, so technically the concrete implementations are private to us. What's your rationale for accepting the `cygrpc._AioCall` object instead of creating in initialization function?",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364940549,2020-01-09T20:16:44Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF","`cygrpc.EOF` is exposed as `grpc.aio.EOF` (`grpc.experimental.aio.EOF` for now). It should be fine to return the same underlying object.I would also like to create this object in Python instead of Cython. However, our current implementation will generate a cyclic dependency issue if we import `grpc` in Cython code. For the long term solution, we could design a Python module for interfaces and shared libraries between Cython and Python.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364941930,2020-01-09T20:20:19Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF",Yes. Same pattern could apply to your `asyncio.Queue` design.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364945515,2020-01-09T20:29:20Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF++        response_message = await self._read()++        if response_message is cygrpc.EOF:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+        return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task","The input / output still seems a bit confusing. I renamed the `_consumer` to `_async_request_poller`, and the `_call` to `_call_finisher`, so the naming better reflects their responsibility.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364946683,2020-01-09T20:32:26Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF++        response_message = await self._read()++        if response_message is cygrpc.EOF:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+        return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()+        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None++    def cancel(self) -> bool:+        if super().cancel():+            self._call.cancel()+            if self._consumer is not None:+                self._consumer.cancel()             return True         else:             return False +    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:","Changed to `_conduct_rpc`. This function oversees the life cycle of the entire RPC, and I think the naming should suggest it is doing work that is more than the last step of receiving response.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364952283,2020-01-09T20:47:07Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF++        response_message = await self._read()++        if response_message is cygrpc.EOF:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+        return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()+        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None++    def cancel(self) -> bool:+        if super().cancel():+            self._call.cancel()+            if self._consumer is not None:+                self._consumer.cancel()             return True         else:             return False +    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None+        self._message_aiter = None+     def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+        if super().cancel():+            self._setup_task.cancel()+            if self._consumer is not None:+                self._consumer.cancel()+            return True+        else:+            return False++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _setup(self):+        try:+            await self._cython_call.stream_stream(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            # No need to raise RpcError here, because no one will `await` this task.++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    async def write(self, request: RequestType) -> None:","As discussed in the description, yes it is very similar. Comparing options for deduplication:1. Mixing classes, which means multiple inheritance (reduce readability and error-prone);2. Common methods that accepts many arguments (reduce readability because the logic the separated to somewhere else in the hundreds lines of code) (which is like existing implementation).Which way do you think we should go?",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364953644,2020-01-09T20:50:50Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF++        response_message = await self._read()++        if response_message is cygrpc.EOF:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+        return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()+        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None++    def cancel(self) -> bool:+        if super().cancel():+            self._call.cancel()+            if self._consumer is not None:+                self._consumer.cancel()             return True         else:             return False +    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None+        self._message_aiter = None+     def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+        if super().cancel():+            self._setup_task.cancel()+            if self._consumer is not None:+                self._consumer.cancel()+            return True+        else:+            return False++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _setup(self):+        try:+            await self._cython_call.stream_stream(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            # No need to raise RpcError here, because no one will `await` this task.++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    async def write(self, request: RequestType) -> None:",Good catch! I missed this path. Waiting for `_metadata_set` is moved into `write`.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364957770,2020-01-09T21:01:23Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -170,6 +170,144 @@ def __call__(self,         )  +class StreamUnaryMultiCallable:+    """"""Afford invoking a stream-unary RPC from client-side in an asynchronous way.""""""+","Good idea! Type annotations added. Also, I found many of the logic is simple boilerplates. We could merge them together using a base class `_BaseMultiCallable`.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21607,364961846,2020-01-09T21:12:28Z,src/python/grpcio_tests/tests_aio/unit/channel_argument_test.py,"@@ -0,0 +1,168 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests behavior around the Core channel arguments.""""""++import asyncio+import logging+import unittest+import socket++import grpc+import random++from grpc.experimental import aio+from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc+from tests.unit.framework.common import test_constants+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+# 100 servers in sequence++_RANDOM_SEED = 42++_ENABLE_REUSE_PORT = 'SO_REUSEPORT enabled'+_DISABLE_REUSE_PORT = 'SO_REUSEPORT disabled'+_SOCKET_OPT_SO_REUSEPORT = 'grpc.so_reuseport'+_OPTIONS = (+    (_ENABLE_REUSE_PORT, ((_SOCKET_OPT_SO_REUSEPORT, 1),)),+    (_DISABLE_REUSE_PORT, ((_SOCKET_OPT_SO_REUSEPORT, 0),)),+)++_NUM_SERVER_CREATED = 100++_GRPC_ARG_MAX_RECEIVE_MESSAGE_LENGTH = 'grpc.max_receive_message_length'+_MAX_MESSAGE_LENGTH = 1024+++class _TestPointerWrapper(object):++    def __int__(self):+        return 123456+++_TEST_CHANNEL_ARGS = (+    ('arg1', b'bytes_val'),+    ('arg2', 'str_val'),+    ('arg3', 1),+    (b'arg4', 'str_val'),+    ('arg6', _TestPointerWrapper()),+)++_INVALID_TEST_CHANNEL_ARGS = [+    {+        'foo': 'bar'+    },+    (('key',),),+    'str',+]+++async def test_if_reuse_port_enabled(server: aio.Server):+    port = server.add_insecure_port('127.0.0.1:0')+    await server.start()++    try:+        another_socket = socket.socket(family=socket.AF_INET)","Well, the implementation of this `bool` doesn't match its description: https://github.com/python/cpython/blob/39afa2d3147e4b05a1161cc90dbf09b95072c2bb/configure.ac#L3240Basically, this `bool` is set if CPython compiled with `--enable_ipv6`.Since we prefer IPv6, I suggest we wait until this test is enabled to further debug this behavior.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,364964117,2020-01-09T21:18:13Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -217,26 +351,47 @@ async def _schedule_rpc_coro(object rpc_coro,   async def _handle_rpc(list generic_handlers, RPCState rpc_state, object loop):","Yes. We could move it to Python layer, the `RPCState` Cython class should be automatically recognized when crossing the border of Cython / Python, so we are not losing access to C/C++ objects.EDIT: What's your rationale for moving the implementation up? Keeping them in Cython has at least 10% performance benefit.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,364977755,2020-01-09T21:52:30Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -98,89 +174,167 @@ def service(self, handler_details): class TestServer(AioTestBase):      async def setUp(self):-        self._server_target, self._server, self._generic_handler = await _start_test_server(-        )+        addr, self._server, self._generic_handler = await _start_test_server()+        self._channel = aio.insecure_channel(addr)      async def tearDown(self):+        await self._channel.close()         await self._server.stop(None)      async def test_unary_unary(self):-        async with aio.insecure_channel(self._server_target) as channel:-            unary_unary_call = channel.unary_unary(_SIMPLE_UNARY_UNARY)-            response = await unary_unary_call(_REQUEST)-            self.assertEqual(response, _RESPONSE)+        unary_unary_call = self._channel.unary_unary(_SIMPLE_UNARY_UNARY)+        response = await unary_unary_call(_REQUEST)+        self.assertEqual(response, _RESPONSE)      async def test_unary_stream_async_generator(self):-        async with aio.insecure_channel(self._server_target) as channel:-            unary_stream_call = channel.unary_stream(_UNARY_STREAM_ASYNC_GEN)-            call = unary_stream_call(_REQUEST)+        unary_stream_call = self._channel.unary_stream(_UNARY_STREAM_ASYNC_GEN)+        call = unary_stream_call(_REQUEST) -            # Expecting the request message to reach server before retriving-            # any responses.-            await asyncio.wait_for(self._generic_handler.wait_for_call(),-                                   test_constants.SHORT_TIMEOUT)+        # Expecting the request message to reach server before retriving+        # any responses.+        await asyncio.wait_for(self._generic_handler.wait_for_call(),","Interesting. I like that we have a first-class API for this now. In the past, I would have managed my own `Event` in the server handler to ensure this sort of serialization.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/21620,364987558,2020-01-09T22:18:02Z,CMakeLists.txt,"@@ -10125,6 +10125,258 @@ if(gRPC_BUILD_TESTS)  add_executable(alts_grpc_record_protocol_test   test/core/tsi/alts/zero_copy_frame_protector/alts_grpc_record_protocol_test.cc+  src/core/ext/filters/client_channel/resolver/fake/fake_resolver.cc","Yes, this change showed up because it needs to have new dependency against `grpc_test_util_base` to use `grpc::testing::TestEnvironment`",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21517,365004302,2020-01-09T23:08:33Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -13,25 +13,33 @@ # limitations under the License.  import asyncio+import gc import logging-import unittest import time-import gc+import unittest  import grpc from grpc.experimental import aio-from tests_aio.unit._test_base import AioTestBase+ from tests.unit.framework.common import test_constants+from tests_aio.unit._test_base import AioTestBase  _SIMPLE_UNARY_UNARY = '/test/SimpleUnaryUnary' _BLOCK_FOREVER = '/test/BlockForever' _BLOCK_BRIEFLY = '/test/BlockBriefly' _UNARY_STREAM_ASYNC_GEN = '/test/UnaryStreamAsyncGen' _UNARY_STREAM_READER_WRITER = '/test/UnaryStreamReaderWriter' _UNARY_STREAM_EVILLY_MIXED = '/test/UnaryStreamEvillyMixed'+_STREAM_UNARY_ASYNC_GEN = '/test/StreamUnaryAsyncGen'+_STREAM_UNARY_READER_WRITER = '/test/StreamUnaryReaderWriter'+_STREAM_UNARY_EVILLY_MIXED = '/test/StreamUnaryEvillyMixed'","Mixing two styles might have unexpected behavior due to race between them.For iterators, users are trusting our library to consume them. But for reader / writer API, the manner of message consumption is controlled by users.E.g. if a user supplied a request iterator, and then invoking `write` at another coroutine. If the request iterator depleted, it will trigger `done_writing`, then the other coroutine will fail seems randomly.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,365004673,2020-01-09T23:09:53Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -217,26 +351,47 @@ async def _schedule_rpc_coro(object rpc_coro,   async def _handle_rpc(list generic_handlers, RPCState rpc_state, object loop):","My rationale *was* that we shouldn't lose any performance. I'm wondering where the 10% performance benefit comes from if all of these are Python types. What's happening in Cython should happen basically the same way it would run as Python bytecode: - The GIL will remain held - The various `PyObject`s will be interacted with the same wayI suppose the difference is that instead of translating Python bytecode to interactions with the `PyObject`s, Cython functions directly make these calls, with no conditionals.Thanks for looking into the performance delta!",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,365007690,2020-01-09T23:20:33Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -146,31 +147,49 @@ def _create_rpc_error(initial_metadata: Optional[MetadataType],   class Call(_base_call.Call):+    """"""Base implementation of client RPC Call object.++    Implements logic around final status, metadata and cancellation.+    """"""     _loop: asyncio.AbstractEventLoop     _code: grpc.StatusCode     _status: Awaitable[cygrpc.AioRpcStatus]     _initial_metadata: Awaitable[MetadataType]     _locally_cancelled: bool+    _cython_call: cygrpc._AioCall -    def __init__(self) -> None:+    def __init__(self, deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes) -> None:         self._loop = asyncio.get_event_loop()         self._code = None         self._status = self._loop.create_future()         self._initial_metadata = self._loop.create_future()         self._locally_cancelled = False+        self._cython_call = channel.call(method, deadline) -    def cancel(self) -> bool:-        """"""Placeholder cancellation method.--        The implementation of this method needs to pass the cancellation reason-        into self._cancellation, using `set_result` instead of-        `set_exception`.-        """"""-        raise NotImplementedError()+    def __del__(self) -> None:+        if not self._status.done():+            self._cancel(+                cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,+                                    _GC_CANCELLATION_DETAILS, None, None))      def cancelled(self) -> bool:         return self._code == grpc.StatusCode.CANCELLED +    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:+        """"""Forwards the application cancellation reasoning.""""""+        if not self._status.done():+            self._set_status(status)+            self._cython_call.cancel(status)+            return True+        else:+            return False++    def cancel(self) -> bool:","You're right. I did miss that. The docstring says ""A bool indicates if the cancellation is performed or not."" That's not quite what I'm seeing in the code here though. As currently written, I would assume that `False` means cancellation failed. What I'm seeing here is a `True` value means that this particular invocation resulted in active cancellation of the RPC, whereas a value of `False` implies that no work was performed to cancel this RPC as it was already cancelled.I'm now wondering what the use case for the return value of this function is. Perhaps we should just document that it's idempotent and return no value at all. In case there is an actual problem cancelling the RPC, we can raise an exception.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,365010811,2020-01-09T23:31:51Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF",Didn't notice. As long as we don't support applications that textually use the string `cygrpc.EOF`.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,365014576,2020-01-09T23:46:42Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -373,78 +371,310 @@ def __del__(self) -> None:                                                  self._set_initial_metadata,                                                  self._set_status)         except asyncio.CancelledError:-            if self._code != grpc.StatusCode.CANCELLED:+            if not self.cancelled():                 self.cancel()             raise      async def _fetch_stream_responses(self) -> ResponseType:-        await self._send_unary_request_task         message = await self._read()         while message:             yield message             message = await self._read() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:-        """"""Forwards the application cancellation reasoning.+    def __aiter__(self) -> AsyncIterable[ResponseType]:+        if self._message_aiter is None:+            self._message_aiter = self._fetch_stream_responses()+        return self._message_aiter -        Async generator will receive an exception. The cancellation will go-        deep down into Core, and then propagates backup as the-        `cygrpc.AioRpcStatus` exception.+    async def _read(self) -> ResponseType:+        # Wait for the request being sent+        await self._send_unary_request_task -        So, under race condition, e.g. the server sent out final state headers-        and the client calling ""cancel"" at the same time, this method respects-        the winner in Core.-        """"""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        # Reads response message from Core+        try:+            raw_response = await self._cython_call.receive_serialized_message()+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++        if raw_response is cygrpc.EOF:+            return cygrpc.EOF+        else:+            return _common.deserialize(raw_response,+                                       self._response_deserializer)++    async def read(self) -> ResponseType:+        if self._status.done():+            await self._raise_for_status()+            return cygrpc.EOF++        response_message = await self._read()++        if response_message is cygrpc.EOF:+            # If the read operation failed, Core should explain why.+            await self._raise_for_status()+        return response_message+++# pylint: disable=abstract-method+class StreamUnaryCall(Call, _base_call.StreamUnaryCall):+    """"""Object for managing stream-unary RPC calls.++    Returned when an instance of `StreamUnaryMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer -            if not self._send_unary_request_task.done():-                # Injects CancelledError to the Task. The exception will-                # propagate to _fetch_stream_responses as well, if the sending-                # is not done.-                self._send_unary_request_task.cancel()+        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._call = self._loop.create_task(self._invoke())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None++    def cancel(self) -> bool:+        if super().cancel():+            self._call.cancel()+            if self._consumer is not None:+                self._consumer.cancel()             return True         else:             return False +    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _invoke(self) -> ResponseType:+        try:+            serialized_response = await self._cython_call.stream_unary(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()++        # Raises RpcError if the RPC failed or cancelled+        await self._raise_for_status()++        return _common.deserialize(serialized_response,+                                   self._response_deserializer)++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    def __await__(self) -> ResponseType:+        """"""Wait till the ongoing RPC request finishes.""""""+        try:+            response = yield from self._call+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            raise+        return response++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()+++# pylint: disable=abstract-method+class StreamStreamCall(Call, _base_call.StreamStreamCall):+    """"""Object for managing stream-stream RPC calls.++    Returned when an instance of `StreamStreamMultiCallable` object is called.+    """"""+    _metadata: MetadataType+    _request_serializer: SerializingFunction+    _response_deserializer: DeserializingFunction++    _metadata_sent: asyncio.Event+    _done_writing: bool+    _call: asyncio.Task+    _consumer: asyncio.Task+    _message_aiter: AsyncIterable[ResponseType]++    def __init__(self,+                 request_async_iterator: Optional[AsyncIterable[RequestType]],+                 deadline: Optional[float], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction) -> None:+        super().__init__(deadline, channel, method)+        self._metadata = _EMPTY_METADATA+        self._request_serializer = request_serializer+        self._response_deserializer = response_deserializer++        self._metadata_sent = asyncio.Event(loop=self._loop)+        self._done_writing = False++        self._setup_task = self._loop.create_task(self._setup())++        # If user passes in an async iterator, create a consumer coroutine.+        if request_async_iterator is not None:+            self._consumer = self._loop.create_task(+                self._consume_request_iterator(request_async_iterator))+        else:+            self._consumer = None+        self._message_aiter = None+     def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+        if super().cancel():+            self._setup_task.cancel()+            if self._consumer is not None:+                self._consumer.cancel()+            return True+        else:+            return False++    def _metadata_sent_observer(self):+        self._metadata_sent.set()++    async def _setup(self):+        try:+            await self._cython_call.stream_stream(+                self._metadata,+                self._metadata_sent_observer,+                self._set_initial_metadata,+                self._set_status,+            )+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            # No need to raise RpcError here, because no one will `await` this task.++    async def _consume_request_iterator(+            self, request_async_iterator: Optional[AsyncIterable[RequestType]]+    ) -> None:+        await self._metadata_sent.wait()+        async for request in request_async_iterator:+            await self.write(request)+        await self.done_writing()++    async def write(self, request: RequestType) -> None:+        if self._status.done():+            raise asyncio.InvalidStateError(_RPC_ALREADY_FINISHED_DETAILS)+        if self._done_writing:+            raise asyncio.InvalidStateError(_RPC_HALF_CLOSED_DETAILS)++        serialized_request = _common.serialize(request,+                                               self._request_serializer)++        try:+            await self._cython_call.send_serialized_message(serialized_request)+        except asyncio.CancelledError:+            if not self.cancelled():+                self.cancel()+            await self._raise_for_status()++    async def done_writing(self) -> None:+        """"""Implementation of done_writing is idempotent.""""""+        if self._status.done():+            # If the RPC is finished, do nothing.+            return+        if not self._done_writing:+            # If the done writing is not sent before, try to send it.+            self._done_writing = True+            try:+                await self._cython_call.send_receive_close()+            except asyncio.CancelledError:+                if not self.cancelled():+                    self.cancel()+                await self._raise_for_status()++    async def _fetch_stream_responses(self) -> ResponseType:+        """"""The async generator that yields responses from peer.""""""+        message = await self._read()",If only we were an application and not a library. :(,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21517,365017955,2020-01-10T00:00:30Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -98,89 +174,167 @@ def service(self, handler_details): class TestServer(AioTestBase):      async def setUp(self):-        self._server_target, self._server, self._generic_handler = await _start_test_server(-        )+        addr, self._server, self._generic_handler = await _start_test_server()+        self._channel = aio.insecure_channel(addr)      async def tearDown(self):+        await self._channel.close()         await self._server.stop(None)      async def test_unary_unary(self):-        async with aio.insecure_channel(self._server_target) as channel:-            unary_unary_call = channel.unary_unary(_SIMPLE_UNARY_UNARY)-            response = await unary_unary_call(_REQUEST)-            self.assertEqual(response, _RESPONSE)+        unary_unary_call = self._channel.unary_unary(_SIMPLE_UNARY_UNARY)+        response = await unary_unary_call(_REQUEST)+        self.assertEqual(response, _RESPONSE)      async def test_unary_stream_async_generator(self):-        async with aio.insecure_channel(self._server_target) as channel:-            unary_stream_call = channel.unary_stream(_UNARY_STREAM_ASYNC_GEN)-            call = unary_stream_call(_REQUEST)+        unary_stream_call = self._channel.unary_stream(_UNARY_STREAM_ASYNC_GEN)+        call = unary_stream_call(_REQUEST) -            # Expecting the request message to reach server before retriving-            # any responses.-            await asyncio.wait_for(self._generic_handler.wait_for_call(),-                                   test_constants.SHORT_TIMEOUT)+        # Expecting the request message to reach server before retriving+        # any responses.+        await asyncio.wait_for(self._generic_handler.wait_for_call(),+                               test_constants.SHORT_TIMEOUT) -            response_cnt = 0-            async for response in call:-                response_cnt += 1-                self.assertEqual(_RESPONSE, response)+        response_cnt = 0+        async for response in call:+            response_cnt += 1+            self.assertEqual(_RESPONSE, response)","I think you can simply do```pythonresponses = [resp async for resp in call]self.assertSequenceEqual([_RESPONSE] * _NUM_STREAM_RESPONSES, responses)```But again, this is purely stylistic and therefore optional.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21607,365040089,2020-01-10T01:40:38Z,src/python/grpcio_tests/tests_aio/unit/channel_argument_test.py,"@@ -0,0 +1,168 @@+# Copyright 2019 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests behavior around the Core channel arguments.""""""++import asyncio+import logging+import unittest+import socket++import grpc+import random++from grpc.experimental import aio+from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc+from tests.unit.framework.common import test_constants+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+# 100 servers in sequence++_RANDOM_SEED = 42++_ENABLE_REUSE_PORT = 'SO_REUSEPORT enabled'+_DISABLE_REUSE_PORT = 'SO_REUSEPORT disabled'+_SOCKET_OPT_SO_REUSEPORT = 'grpc.so_reuseport'+_OPTIONS = (+    (_ENABLE_REUSE_PORT, ((_SOCKET_OPT_SO_REUSEPORT, 1),)),+    (_DISABLE_REUSE_PORT, ((_SOCKET_OPT_SO_REUSEPORT, 0),)),+)++_NUM_SERVER_CREATED = 100++_GRPC_ARG_MAX_RECEIVE_MESSAGE_LENGTH = 'grpc.max_receive_message_length'+_MAX_MESSAGE_LENGTH = 1024+++class _TestPointerWrapper(object):++    def __int__(self):+        return 123456+++_TEST_CHANNEL_ARGS = (+    ('arg1', b'bytes_val'),+    ('arg2', 'str_val'),+    ('arg3', 1),+    (b'arg4', 'str_val'),+    ('arg6', _TestPointerWrapper()),+)++_INVALID_TEST_CHANNEL_ARGS = [+    {+        'foo': 'bar'+    },+    (('key',),),+    'str',+]+++async def test_if_reuse_port_enabled(server: aio.Server):+    port = server.add_insecure_port('127.0.0.1:0')+    await server.start()++    try:+        another_socket = socket.socket(family=socket.AF_INET)","Actually, why don't you just give [this function](https://github.com/grpc/grpc/blob/master/src/python/grpcio_tests/tests/unit/framework/common/__init__.py#L22) a shot? I wrote it after being burnt by this incompatibility multiple times upon import.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21621,365387120,2020-01-10T19:10:04Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -201,6 +202,51 @@ def __init__(self, target: Text,          self._channel = cygrpc.AioChannel(_common.encode(target)) +    def check_connectivity_state(self, try_to_connect: bool = False",It's unfortunate that [there isn't more documentation on the `try_to_connect` parameter in core](https://github.com/grpc/grpc/blob/7388c47c912a82b76beb72b2e4989c575976750f/include/grpc/grpc.h#L189). Do you have any insight into the use case for it? Shouldn't a constructed channel already be trying to connect?,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/20316,365433415,2020-01-10T21:16:18Z,src/core/tsi/ssl_transport_security.cc,"@@ -568,21 +572,84 @@ static tsi_result ssl_ctx_use_private_key(SSL_CTX* context, const char* pem_key,   EVP_PKEY* private_key = nullptr;   BIO* pem;   GPR_ASSERT(pem_key_size <= INT_MAX);-  pem = BIO_new_mem_buf((void*)pem_key, static_cast<int>(pem_key_size));-  if (pem == nullptr) return TSI_OUT_OF_RESOURCES;-  do {-    private_key = PEM_read_bio_PrivateKey(pem, nullptr, nullptr, (void*)"""");-    if (private_key == nullptr) {-      result = TSI_INVALID_ARGUMENT;-      break;-    }-    if (!SSL_CTX_use_PrivateKey(context, private_key)) {-      result = TSI_INVALID_ARGUMENT;-      break;-    }-  } while (0);-  if (private_key != nullptr) EVP_PKEY_free(private_key);-  BIO_free(pem);++// BoringSSL does not have ENGINE_load_dynamic, ENGINE_by_id+// support.+#ifndef OPENSSL_IS_BORINGSSL+  if (strncmp(pem_key, kSslEnginePrefix, strlen(kSslEnginePrefix)) == 0) {+    ENGINE* engine;",Can you split this function into engine and non-engine part since it's getting bigger?,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/20316,365436786,2020-01-10T21:27:00Z,test/core/end2end/h2_ssl_cert_test.cc,"@@ -370,6 +383,12 @@ int main(int argc, char** argv) {   grpc_init();   ::testing::InitGoogleTest(&argc, argv);   int ret = RUN_ALL_TESTS();+#ifndef OPENSSL_IS_BORINGSSL",Can you make a separate test instead of rerunning `RUN_ALL_TESTS` with the different configuration? `RUN_ALL_TESTS` is expected to be called at once. ([ref](https://github.com/google/googletest/blob/master/googletest/docs/primer.md)),
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21621,365462485,2020-01-10T22:54:36Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/channel.pyx.pxi,"@@ -12,17 +12,55 @@ # See the License for the specific language governing permissions and # limitations under the License. ++class _WatchConnectivityFailed(Exception): pass",Done. Adding a docstring describing the failing conditions.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21621,365483058,2020-01-11T00:46:22Z,src/python/grpcio_tests/tests_aio/unit/connectivity_test.py,"@@ -38,59 +46,65 @@ class TestConnectivityState(AioTestBase):         await self._server.stop(None)      async def test_unavailable_backend(self):-        channel = aio.insecure_channel(_INVALID_BACKEND_ADDRESS)--        self.assertEqual(grpc.ChannelConnectivity.IDLE,-                         channel.check_connectivity_state(False))-        self.assertEqual(grpc.ChannelConnectivity.IDLE,-                         channel.check_connectivity_state(True))-        self.assertEqual(-            grpc.ChannelConnectivity.CONNECTING, await-            channel.watch_connectivity_state(grpc.ChannelConnectivity.IDLE))-        self.assertEqual(-            grpc.ChannelConnectivity.TRANSIENT_FAILURE, await-            channel.watch_connectivity_state(grpc.ChannelConnectivity.CONNECTING-                                            ))--        await channel.close()+        async with aio.insecure_channel(_INVALID_BACKEND_ADDRESS) as channel:+            self.assertEqual(grpc.ChannelConnectivity.IDLE,+                             channel.get_state(False))+            self.assertEqual(grpc.ChannelConnectivity.IDLE,+                             channel.get_state(True))++            async def waiting_transient_failure():+                state = channel.get_state()+                while state != grpc.ChannelConnectivity.TRANSIENT_FAILURE:+                    channel.wait_for_state_change(state)++            # Should not time out+            await asyncio.wait_for(+                _block_until_certain_state(+                    channel, grpc.ChannelConnectivity.TRANSIENT_FAILURE),+                test_constants.SHORT_TIMEOUT)      async def test_normal_backend(self):-        channel = aio.insecure_channel(self._server_address)--        current_state = channel.check_connectivity_state(True)-        self.assertEqual(grpc.ChannelConnectivity.IDLE, current_state)--        deadline = time.time() + test_constants.SHORT_TIMEOUT+        async with aio.insecure_channel(self._server_address) as channel:+            current_state = channel.get_state(True)+            self.assertEqual(grpc.ChannelConnectivity.IDLE, current_state) -        while current_state != grpc.ChannelConnectivity.READY:-            current_state = await channel.watch_connectivity_state(-                current_state, deadline - time.time())-            self.assertIsNotNone(current_state)--        await channel.close()+            # Should not time out+            await asyncio.wait_for(+                _block_until_certain_state(channel,+                                           grpc.ChannelConnectivity.READY),+                test_constants.SHORT_TIMEOUT)      async def test_timeout(self):-        channel = aio.insecure_channel(self._server_address)--        self.assertEqual(grpc.ChannelConnectivity.IDLE,-                         channel.check_connectivity_state(False))+        async with aio.insecure_channel(self._server_address) as channel:+            self.assertEqual(grpc.ChannelConnectivity.IDLE,+                             channel.get_state(False)) -        # If timed out, the function should return None.-        self.assertIsNone(await channel.watch_connectivity_state(-            grpc.ChannelConnectivity.IDLE, test_constants.SHORT_TIMEOUT))--        await channel.close()+            # If timed out, the function should return None.+            with self.assertRaises(asyncio.TimeoutError):+                await asyncio.wait_for(+                    _block_until_certain_state(channel,+                                               grpc.ChannelConnectivity.READY),+                    test_constants.SHORT_TIMEOUT)      async def test_shutdown(self):         channel = aio.insecure_channel(self._server_address)          self.assertEqual(grpc.ChannelConnectivity.IDLE,-                         channel.check_connectivity_state(False))+                         channel.get_state(False))          await channel.close()          self.assertEqual(grpc.ChannelConnectivity.SHUTDOWN,-                         channel.check_connectivity_state(False))+                         channel.get_state(True))++        self.assertEqual(grpc.ChannelConnectivity.SHUTDOWN,+                         channel.get_state(False))++        # It can raise Exception since it is an usage error, but it should not+        # segfault or abort.+        with self.assertRaises(Exception):","Narrowed it to `RuntimeError`.We might change the type of exceptions soon. So, I didn't want to commit to any type of exceptions just yet.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21582,365488224,2020-01-11T01:33:34Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi,"@@ -173,3 +173,23 @@ async def _receive_initial_metadata(GrpcCallWrapper grpc_call_wrapper,     cdef tuple ops = (op,)     await execute_batch(grpc_call_wrapper, ops, loop)     return op.initial_metadata()++async def _send_error_status_from_server(GrpcCallWrapper grpc_call_wrapper,+                                         grpc_status_code code,+                                         str details,+                                         tuple trailing_metadata,+                                         bint metadata_sent,+                                         object loop):+    assert code != StatusCode.ok, 'Expecting non-ok status code.'+    cdef SendStatusFromServerOperation op = SendStatusFromServerOperation(+        trailing_metadata,+        code,+        details,+        _EMPTY_FLAGS,+    )+    cdef tuple ops+    if metadata_sent:",I've seen this pattern in several places in the `asyncio` codebase. Time to factor it out to a common function?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21621,365526643,2020-01-11T15:17:16Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/channel.pyx.pxi,"@@ -12,19 +12,80 @@ # See the License for the specific language governing permissions and # limitations under the License. ++class _WatchConnectivityFailed(Exception):+    """"""Dedicated exception class for watch connectivity failed.++    It might be failed due to deadline exceeded, or the channel is closing.+    """"""+cdef CallbackFailureHandler _WATCH_CONNECTIVITY_FAILURE_HANDLER = CallbackFailureHandler(+    'watch_connectivity_state',+    'Timed out or channel closed.',+    _WatchConnectivityFailed)++ cdef class AioChannel:     def __cinit__(self, bytes target):         self.channel = grpc_insecure_channel_create(<char *>target, NULL, NULL)         self.cq = CallbackCompletionQueue()         self._target = target+        self._loop = asyncio.get_event_loop()+        self._status = AIO_CHANNEL_STATUS_READY      def __repr__(self):         class_name = self.__class__.__name__         id_ = id(self)         return f""<{class_name} {id_}>"" +    def check_connectivity_state(self, bint try_to_connect):+        """"""A Cython wrapper for Core's check connectivity state API.""""""+        return grpc_channel_check_connectivity_state(+            self.channel,+            try_to_connect,+        )++    async def watch_connectivity_state(self,+                                       grpc_connectivity_state last_observed_state,+                                       object deadline):+        """"""Watch for one connectivity state change.++        Keeps mirroring the behavior from Core, so we can easily switch to+        other design of API if necessary.+        """"""+        if self._status == AIO_CHANNEL_STATUS_DESTROYED:+            # TODO(lidiz) switch to UsageError+            raise RuntimeError('Channel is closed.')+        cdef gpr_timespec c_deadline = _timespec_from_time(deadline)++        cdef object future = self._loop.create_future()+        cdef CallbackWrapper wrapper = CallbackWrapper(+            future,+            _WATCH_CONNECTIVITY_FAILURE_HANDLER)+        cpython.Py_INCREF(wrapper)+        grpc_channel_watch_connectivity_state(+            self.channel,+            last_observed_state,+            c_deadline,+            self.cq.c_ptr(),+            wrapper.c_functor())++        # NOTE(lidiz) The callback will be invoked after the channel is closed+        # with a failure state. We need to keep wrapper alive until then, or we+        # will observe a segfault.","So you mean that in case of closing the `AioChannel` which would result in destructing the object, if later arrives the callback from the core for notifying a change in the satus - or a timeout - the cb would find a deallocated wrapper?How is this aligned with this [1]? Are both trying to solve the same issue? Just saying because the description of the problem is different but they are doing basically the same, with just one little difference, for [1] if there is an exception in the `future` the `cpython.Py_DECREF(wrapper)` will never be executed which could lead to some leaks.[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi#L115",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/20316,365874072,2020-01-13T15:44:21Z,build.yaml,"@@ -1699,6 +1699,17 @@ libs:   filegroups:   - grpc_test_util_base   secure: true+- name: e_passthrough","I don't have much context, but the `e_passthrough` name is not very descriptive. Is this a test or a library?Should this be `engine_passthrough_test`?",
28269509,akshayku,https://api.github.com/repos/grpc/grpc/pulls/20316,365919892,2020-01-13T17:06:44Z,build.yaml,"@@ -1699,6 +1699,17 @@ libs:   filegroups:   - grpc_test_util_base   secure: true+- name: e_passthrough",Going to change to `engine_passthrough` as that is also the filename and other options are becoming too long.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21621,365967818,2020-01-13T18:50:45Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/channel.pyx.pxi,"@@ -12,19 +12,80 @@ # See the License for the specific language governing permissions and # limitations under the License. ++class _WatchConnectivityFailed(Exception):+    """"""Dedicated exception class for watch connectivity failed.++    It might be failed due to deadline exceeded, or the channel is closing.+    """"""+cdef CallbackFailureHandler _WATCH_CONNECTIVITY_FAILURE_HANDLER = CallbackFailureHandler(+    'watch_connectivity_state',+    'Timed out or channel closed.',+    _WatchConnectivityFailed)++ cdef class AioChannel:     def __cinit__(self, bytes target):         self.channel = grpc_insecure_channel_create(<char *>target, NULL, NULL)         self.cq = CallbackCompletionQueue()         self._target = target+        self._loop = asyncio.get_event_loop()+        self._status = AIO_CHANNEL_STATUS_READY      def __repr__(self):         class_name = self.__class__.__name__         id_ = id(self)         return f""<{class_name} {id_}>"" +    def check_connectivity_state(self, bint try_to_connect):+        """"""A Cython wrapper for Core's check connectivity state API.""""""+        return grpc_channel_check_connectivity_state(+            self.channel,+            try_to_connect,+        )++    async def watch_connectivity_state(self,+                                       grpc_connectivity_state last_observed_state,+                                       object deadline):+        """"""Watch for one connectivity state change.++        Keeps mirroring the behavior from Core, so we can easily switch to+        other design of API if necessary.+        """"""+        if self._status == AIO_CHANNEL_STATUS_DESTROYED:+            # TODO(lidiz) switch to UsageError+            raise RuntimeError('Channel is closed.')+        cdef gpr_timespec c_deadline = _timespec_from_time(deadline)++        cdef object future = self._loop.create_future()+        cdef CallbackWrapper wrapper = CallbackWrapper(+            future,+            _WATCH_CONNECTIVITY_FAILURE_HANDLER)+        cpython.Py_INCREF(wrapper)+        grpc_channel_watch_connectivity_state(+            self.channel,+            last_observed_state,+            c_deadline,+            self.cq.c_ptr(),+            wrapper.c_functor())++        # NOTE(lidiz) The callback will be invoked after the channel is closed+        # with a failure state. We need to keep wrapper alive until then, or we+        # will observe a segfault.","Yes, they are caused by the same issue that the C callback function being prematurely deallocated. I updated the one in `callback_common.pyx.pxi` to the same pattern here. It is indeed a potential risk I overlooked. Good catch!",X
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/20568,366074850,2020-01-13T23:00:53Z,include/grpcpp/security/tls_credentials_options.h,"@@ -56,7 +56,7 @@ class TlsKeyMaterialsConfig {   int version() const { return version_; }    /** Setter for key materials that will be called by the user. The setter-   * transfers ownership of the arguments to the config. **/+   *  takes ownership of the arguments to the config. **/","Per our offline discussion, this comment is no longer needed.",
2030323,JunTaoLuo,https://api.github.com/repos/grpc/grpc/pulls/21653,366184611,2020-01-14T07:30:28Z,src/csharp/Grpc.Core.Api/BufferMarshaller.cs,"@@ -0,0 +1,68 @@+#region Copyright notice and license++// Copyright 2015 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#endregion++using System;+using System.Buffers;+using Grpc.Core.Utils;++namespace Grpc.Core+{+    /// <summary>+    /// Encapsulates the logic for serializing and deserializing messages.+    /// </summary>+    public class BufferMarshaller<T> : Marshaller<T>",Still considering alternative to creating a subclass.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21351,366496508,2020-01-14T18:18:53Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -71,6 +74,11 @@ cdef class _AioCall:             c_deadline,             NULL         )+        if credentials:+            set_credentials_error = grpc_call_set_credentials(self._grpc_call_wrapper.call, credentials.c())+            if set_credentials_error != GRPC_CALL_OK:+                raise Exception(""Credentials couldn't have been set"")","Ping. If possible, please include the error code in the exception message. It could make our life easier in future.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21351,366497820,2020-01-14T18:21:44Z,src/python/grpcio/grpc/experimental/aio/__init__.py,"@@ -64,4 +87,4 @@ def insecure_channel(            'UnaryStreamCall', 'init_grpc_aio', 'Channel',            'UnaryUnaryMultiCallable', 'ClientCallDetails',            'UnaryUnaryClientInterceptor', 'InterceptedUnaryUnaryCall',-           'insecure_channel', 'server', 'Server')+           'insecure_channel', 'secure_channel', 'server')",Don't forget the `Server` class.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21643,366516324,2020-01-14T19:00:03Z,src/core/lib/iomgr/tcp_custom.h,"@@ -24,6 +24,8 @@ #include ""src/core/lib/iomgr/endpoint.h"" #include ""src/core/lib/iomgr/sockaddr.h"" +#define GRPC_CUSTOM_SOCKET_OPT_SO_REUSEPORT (0x00000010u)","How did you come up with this magic number? Since this value is being passed to `grpc_custom_socket_vtable->bind()`, I assume this value must be defined elsewhere, either in Core, or in the kernel. In either case, there should be some way to use *that* constant instead of adding a duplicate.",X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/21643,366516418,2020-01-14T19:00:14Z,src/core/lib/iomgr/tcp_custom.h,"@@ -24,6 +24,8 @@ #include ""src/core/lib/iomgr/endpoint.h"" #include ""src/core/lib/iomgr/sockaddr.h"" +#define GRPC_CUSTOM_SOCKET_OPT_SO_REUSEPORT (0x00000010u)",isn't there existing macro in any of the library headers you can use instead of defining a new one?,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21643,366526402,2020-01-14T19:21:41Z,src/core/lib/iomgr/tcp_custom.h,"@@ -24,6 +24,8 @@ #include ""src/core/lib/iomgr/endpoint.h"" #include ""src/core/lib/iomgr/sockaddr.h"" +#define GRPC_CUSTOM_SOCKET_OPT_SO_REUSEPORT (0x00000010u)",Changed to use the kernel defined value: `0x0200`https://github.com/torvalds/linux/blob/a2d79c7174aeb43b13020dd53d85a7aefdd9f3e5/arch/parisc/include/uapi/asm/socket.h#L18,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21643,366532300,2020-01-14T19:33:51Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/iomgr.pyx.pxi,"@@ -122,11 +124,12 @@ cdef grpc_error* asyncio_socket_listen(grpc_custom_socket* grpc_socket) with gil     return grpc_error_none()  -def _asyncio_apply_socket_options(object socket):-    # TODO(https://github.com/grpc/grpc/issues/20667)-    # Connects the so_reuse_port option to channel arguments-    socket.setsockopt(native_socket.SOL_SOCKET, native_socket.SO_REUSEADDR, 1)-    socket.setsockopt(native_socket.IPPROTO_TCP, native_socket.TCP_NODELAY, True)+def _asyncio_apply_socket_options(object s, int flags):+    s.setsockopt(native_socket.SOL_SOCKET, native_socket.SO_REUSEADDR, 1)+    if platform.system() != 'Windows':",My mistake. I see we're unconditionally setting `SO_REUSEADDR`. Is that desirable? How does C++ handle the setting of `SO_REUSEADDR`? Is there any scenario where a user might like to disable this option on Windows?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21643,366541320,2020-01-14T19:53:40Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/iomgr.pyx.pxi,"@@ -122,11 +124,12 @@ cdef grpc_error* asyncio_socket_listen(grpc_custom_socket* grpc_socket) with gil     return grpc_error_none()  -def _asyncio_apply_socket_options(object socket):-    # TODO(https://github.com/grpc/grpc/issues/20667)-    # Connects the so_reuse_port option to channel arguments-    socket.setsockopt(native_socket.SOL_SOCKET, native_socket.SO_REUSEADDR, 1)-    socket.setsockopt(native_socket.IPPROTO_TCP, native_socket.TCP_NODELAY, True)+def _asyncio_apply_socket_options(object s, int flags):+    s.setsockopt(native_socket.SOL_SOCKET, native_socket.SO_REUSEADDR, 1)+    if platform.system() != 'Windows':","In Core, the `SO_REUSEADDR` is enabled by default for Posix TCP connections, but not other protocols. For Windows, I didn't find a line that manipulates this socket option (maybe it is named differently?).https://github.com/grpc/grpc/blob/617c43013a68bcdb491dfddeb0eb3e4d3323c53f/src/core/lib/iomgr/tcp_server_utils_posix_common.cc#L175From what I'm reading, Windows do support `SO_REUSEADDR`: https://docs.microsoft.com/en-us/windows-hardware/drivers/network/so-reuseaddr. It should be fine to supply this flag.Currently, we only implemented TCP sockets, so we can turn it on by default. If we are going to support UDS, then we will need to change many other components. I'm adding a comments stating this.",
28269509,akshayku,https://api.github.com/repos/grpc/grpc/pulls/20316,366547991,2020-01-14T20:08:27Z,build.yaml,"@@ -1699,6 +1699,17 @@ libs:   filegroups:   - grpc_test_util_base   secure: true+- name: engine_passthrough+  build: test+  language: c+  src:+  - test/core/end2end/engine_passthrough.cc+  dll: only+  platforms:","No we can't remove dll: only as without that it produces .a file which is a static library and cannot be used as openssl engine. It has to be dynamic. Hence, we require dll: only.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21653,366807573,2020-01-15T10:43:52Z,src/csharp/Grpc.Core.Api/BufferMarshaller.cs,"@@ -0,0 +1,68 @@+#region Copyright notice and license++// Copyright 2015 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#endregion++using System;+using System.Buffers;+using Grpc.Core.Utils;++namespace Grpc.Core+{+    /// <summary>+    /// Encapsulates the logic for serializing and deserializing messages.+    /// </summary>+    public class BufferMarshaller<T> : Marshaller<T>","I don't what what value does it have to have a subclass like this, doesn't seem that the generated code gets any simpler?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21653,366811292,2020-01-15T10:51:14Z,src/csharp/Grpc.IntegrationTesting/TestGrpc.cs,"@@ -34,7 +34,15 @@ public static partial class TestService   {     static readonly string __ServiceName = ""grpc.testing.TestService""; -    static readonly grpc::Marshaller<global::Grpc.Testing.Empty> __Marshaller_grpc_testing_Empty = grpc::Marshallers.Create((arg) => global::Google.Protobuf.MessageExtensions.ToByteArray(arg), global::Grpc.Testing.Empty.Parser.ParseFrom);+    static readonly grpc::Marshaller<global::Grpc.Testing.Empty> __Marshaller_grpc_testing_Empty = grpc::Marshallers.Create(+      (arg) => global::Google.Protobuf.MessageExtensions.ToByteArray(arg),+      global::Grpc.Testing.Empty.Parser.ParseFrom+      (output, bufferWriter) =>+      {+          var writer = new CodedOutputWriter(bufferWriter);+          output.WriteTo(ref writer);+      },+      global::Grpc.Testing.Empty.Parser.ParseFrom);","IMHO the simplest possible approach is to generate code like this:```grpc::Marshallers.Create( (msg, ctx) =>{  // serialization  ctx.SetPayloadLength(msg.CalculateSize());  msg.writeTo(ctx.GetBufferWriter());  ctx.Complete();},(ctx) =>{  return global::Math.DivArgs.Parser.ParseFrom(ctx.PayloadAsReadOnlySequence());});```That doesn't require the ""BufferMarshaller"" class at all.At the same time, I was wondering if we can come up with a more ""intelligent"" solution that would choose the right serialization approach depending on whether the message type supports IBufferMessage etc.(so that we opt for faster code path automatically if it's available).",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21647,366883080,2020-01-15T13:47:50Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -148,11 +150,11 @@ def __del__(self):             else:                 return UnaryUnaryCall(                     request, _timeout_to_deadline(client_call_details.timeout),-                    client_call_details.credentials, self._channel,+                    metadata, client_call_details.credentials, self._channel,",I would say better `client_call_details.metadata`,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21647,367038945,2020-01-15T18:34:02Z,src/python/grpcio_tests/tests_aio/unit/metadata_test.py,"@@ -0,0 +1,188 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests behavior around the metadata mechanism.""""""++import asyncio+import logging+import platform+import random+import unittest++import grpc+from grpc.experimental import aio++from tests_aio.unit._test_base import AioTestBase++_TEST_CLIENT_TO_SERVER = '/test/TestClientToServer'+_TEST_SERVER_TO_CLIENT = '/test/TestServerToClient'+_TEST_TRAILING_METADATA = '/test/TestTrailingMetadata'+_TEST_ECHO_INITIAL_METADATA = '/test/TestEchoInitialMetadata'+_TEST_GENERIC_HANDLER = '/test/TestGenericHandler'++_REQUEST = b'\x00\x00\x00'+_RESPONSE = b'\x01\x01\x01'++_INITIAL_METADATA_FROM_CLIENT_TO_SERVER = (+    ('client-to-server', 'question'),+    ('client-to-server-bin', b'\x07\x07\x07'),+)+_INITIAL_METADATA_FROM_SERVER_TO_CLIENT = (+    ('server-to-client', 'answer'),+    ('server-to-client-bin', b'\x06\x06\x06'),+)+_TRAILING_METADATA = (('a-trailing-metadata', 'stack-trace'),+                      ('a-trailing-metadata-bin', b'\x05\x05\x05'))+_INITIAL_METADATA_FOR_GENERIC_HANDLER = (('a-must-have-key', 'secret'),)++_INVALID_METADATA_TEST_CASES = (+    (+        TypeError,+        ((42, 42),),+    ),+    (+        TypeError,+        (({}, {}),),+    ),+    (+        TypeError,+        (('normal', object()),),+    ),+    (+        TypeError,+        object(),+    ),+    (+        TypeError,+        (object(),),+    ),+)+++def _seen_metadata(expected, actual):+    metadata_dict = dict(actual)+    for metadatum in expected:+        if metadata_dict.get(metadatum[0]) != metadatum[1]:+            return False+    return True+++class _TestGenericHandlerForMethods(grpc.GenericRpcHandler):++    @staticmethod+    async def _test_client_to_server(request, context):+        assert _REQUEST == request+        assert _seen_metadata(_INITIAL_METADATA_FROM_CLIENT_TO_SERVER,+                              context.invocation_metadata())+        return _RESPONSE++    @staticmethod+    async def _test_server_to_client(request, context):+        assert _REQUEST == request+        await context.send_initial_metadata(+            _INITIAL_METADATA_FROM_SERVER_TO_CLIENT)+        return _RESPONSE++    @staticmethod+    async def _test_trailing_metadata(request, context):+        assert _REQUEST == request+        context.set_trailing_metadata(_TRAILING_METADATA)+        return _RESPONSE++    def service(self, handler_details):+        if handler_details.method == _TEST_CLIENT_TO_SERVER:+            return grpc.unary_unary_rpc_method_handler(+                self._test_client_to_server)+        if handler_details.method == _TEST_SERVER_TO_CLIENT:+            return grpc.unary_unary_rpc_method_handler(+                self._test_server_to_client)+        if handler_details.method == _TEST_TRAILING_METADATA:+            return grpc.unary_unary_rpc_method_handler(+                self._test_trailing_metadata)+        return None+++class _TestGenericHandlerItself(grpc.GenericRpcHandler):++    @staticmethod+    async def _method(request, unused_context):+        assert _REQUEST == request+        return _RESPONSE++    def service(self, handler_details):+        assert _seen_metadata(_INITIAL_METADATA_FOR_GENERIC_HANDLER,+                              handler_details.invocation_metadata)+        return grpc.unary_unary_rpc_method_handler(self._method)+++async def _start_test_server():+    server = aio.server()+    port = server.add_insecure_port('[::]:0')+    server.add_generic_rpc_handlers((+        _TestGenericHandlerForMethods(),+        _TestGenericHandlerItself(),+    ))+    await server.start()+    return 'localhost:%d' % port, server+++class TestMetadata(AioTestBase):++    async def setUp(self):+        address, self._server = await _start_test_server()+        self._client = aio.insecure_channel(address)++    async def tearDown(self):+        await self._client.close()+        await self._server.stop(None)++    async def test_from_client_to_server(self):+        multicallable = self._client.unary_unary(_TEST_CLIENT_TO_SERVER)+        call = multicallable(_REQUEST,+                             metadata=_INITIAL_METADATA_FROM_CLIENT_TO_SERVER)+        self.assertEqual(_RESPONSE, await call)+        self.assertEqual(grpc.StatusCode.OK, await call.code())++    async def test_from_server_to_client(self):+        multicallable = self._client.unary_unary(_TEST_SERVER_TO_CLIENT)+        call = multicallable(_REQUEST)+        self.assertEqual(_INITIAL_METADATA_FROM_SERVER_TO_CLIENT, await+                         call.initial_metadata())+        self.assertEqual(_RESPONSE, await call)+        self.assertEqual(grpc.StatusCode.OK, await call.code())++    async def test_trailing_metadata(self):+        multicallable = self._client.unary_unary(_TEST_TRAILING_METADATA)+        call = multicallable(_REQUEST)+        self.assertEqual(_TRAILING_METADATA, await call.trailing_metadata())+        self.assertEqual(_RESPONSE, await call)+        self.assertEqual(grpc.StatusCode.OK, await call.code())++    async def test_invalid_metadata(self):+        multicallable = self._client.unary_unary(_TEST_CLIENT_TO_SERVER)+        for exception_type, metadata in _INVALID_METADATA_TEST_CASES:",Nice coverage! Consider using https://docs.python.org/3/library/unittest.html#distinguishing-test-iterations-using-subtests,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21647,367042473,2020-01-15T18:41:47Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -148,11 +150,11 @@ def __del__(self):             else:                 return UnaryUnaryCall(                     request, _timeout_to_deadline(client_call_details.timeout),-                    client_call_details.credentials, self._channel,+                    metadata, client_call_details.credentials, self._channel,",Good catch! I totally missed this one.(And I found the trailing metadata cannot be modified easily.),
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21647,367047368,2020-01-15T18:52:46Z,src/python/grpcio_tests/tests_aio/unit/_common.py,"@@ -0,0 +1,24 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+++def seen_metadata(expected, actual):","Nit: Type annotations for this? It looks like `expected` is `Union[Tuple[Text, AnyStr], MetadataType]`? Might make sense to split out the two cases into `metadata_seen` and `metadatum_seen`. Then `metadata_seen` can just be `return bool(set(expected) - set(actual))`",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21647,367048443,2020-01-15T18:55:03Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -532,6 +541,42 @@ class Interceptor(aio.UnaryUnaryClientInterceptor):             self.assertEqual(await call.initial_metadata(), tuple())             self.assertEqual(await call.trailing_metadata(), None) +    async def test_initial_metadata_modification(self):++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                if client_call_details.metadata is not None:",Should we allow metadata to be `None` here? An empty tuple should be equivalent and would cut down on the conditionals that our users will have to write.,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21647,367051114,2020-01-15T19:00:35Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -178,16 +183,28 @@ async def _handle_unary_unary_rpc(object method_handler,     )      # Sends response message-    cdef tuple send_ops = (-        SendStatusFromServerOperation(-            tuple(),-            StatusCode.ok,-            b'',-            _EMPTY_FLAGS,-        ),-        SendInitialMetadataOperation(None, _EMPTY_FLAGS),-        SendMessageOperation(response_raw, _EMPTY_FLAGS),-    )+    cdef tuple send_ops+    if rpc_state.metadata_sent:",There's a lot of similarity between this sequence of operations and the one in the next block. You can construct the second by just prepending a `SendInitialMetadata` operation to the first.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21647,367071978,2020-01-15T19:45:49Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -178,16 +183,28 @@ async def _handle_unary_unary_rpc(object method_handler,     )      # Sends response message-    cdef tuple send_ops = (-        SendStatusFromServerOperation(-            tuple(),-            StatusCode.ok,-            b'',-            _EMPTY_FLAGS,-        ),-        SendInitialMetadataOperation(None, _EMPTY_FLAGS),-        SendMessageOperation(response_raw, _EMPTY_FLAGS),-    )+    cdef tuple send_ops+    if rpc_state.metadata_sent:","After the streaming API PR merged, I unified all similar logic into `prepend_send_initial_metadata_op`. PTALA.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21647,367082736,2020-01-15T20:09:48Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -532,6 +541,42 @@ class Interceptor(aio.UnaryUnaryClientInterceptor):             self.assertEqual(await call.initial_metadata(), tuple())             self.assertEqual(await call.trailing_metadata(), None) +    async def test_initial_metadata_modification(self):++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                if client_call_details.metadata is not None:","Changed the default value to `tuple()`.I'm conflicted about whether we should assign a default value or we could keep the metadata `None` for the sake of cleanness (`tuple()` is not `None`, so we can't use `Optional` annotation)",
303201,JamesNK,https://api.github.com/repos/grpc/grpc/pulls/21667,367084164,2020-01-15T20:13:07Z,src/csharp/Grpc.Core/Channel.cs,"@@ -221,7 +221,7 @@ public async Task ConnectAsync(DateTime? deadline = null)         /// before shutting down the channel to ensure channel shutdown won't impact         /// the outcome of those remote calls.         /// </remarks>-        public async Task ShutdownAsync()+        public new async Task ShutdownAsync()",https://docs.microsoft.com/en-us/dotnet/core/compatibility/> ✔️ Moving a member into a class higher in the hierarchy than the type from which it was removed,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21647,367102410,2020-01-15T20:55:57Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -532,6 +541,42 @@ class Interceptor(aio.UnaryUnaryClientInterceptor):             self.assertEqual(await call.initial_metadata(), tuple())             self.assertEqual(await call.trailing_metadata(), None) +    async def test_initial_metadata_modification(self):++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):+                if client_call_details.metadata is not None:","I guess that here what we have is the metadata parameter that has given by the user which as not been yet normalized to an empty tuple, indeed this last step only happens IIRC in last part of the Cython code.No strong opinion, the good part of changing the type from `Optional[Metadata]` to `Metadata` is that would avoid as you said the usage of some conditionals that are always making the code uglier :)",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21667,367793501,2020-01-17T07:02:40Z,src/csharp/Grpc.Core.Api/ChannelBase.cs,"@@ -48,5 +49,34 @@ public string Target         /// </summary>         /// <returns>A new <see cref=""CallInvoker""/>.</returns>         public abstract CallInvoker CreateCallInvoker();++        /// <summary>+        /// Shuts down the channel cleanly. It is strongly recommended to shutdown+        /// the channel once you stopped using it.+        /// </summary>+        /// <remarks>+        /// Guidance for implementors:+        /// This method doesn't wait for all calls on this channel to finish (nor does+        /// it have to explicitly cancel all outstanding calls). It is user's responsibility to make sure+        /// all the calls on this channel have finished (successfully or with an error)+        /// before shutting down the channel to ensure channel shutdown won't impact+        /// the outcome of those remote calls.+        /// </remarks>+        public Task ShutdownAsync()+        {+            return ShutdownAsyncCore();+        }++        /// <summary>Provides implementation of a non-virtual public member.</summary>+        #pragma warning disable 1998+        protected virtual async Task ShutdownAsyncCore()","Normally, I'd do that but Task.CompletedTask is not available on net45, so I'd need to do this:https://github.com/grpc/grpc/blob/8eea254922fb86ab4172e99d851175bc21a2887a/src/csharp/Grpc.Core/Utils/TaskUtils.cs#L36 and it felt wrong just copying the code.The empty state machine is just a tiny overhead compared to creation and shutting down of a channel (which doesn't happen that often). Also all existing subclasses will soon override this method (GrpcChannel from Grpc.Net.Client will call Dispose() ) and the overhead won't matter anyway.",
2030323,JunTaoLuo,https://api.github.com/repos/grpc/grpc/pulls/21653,367802944,2020-01-17T07:42:05Z,src/csharp/Grpc.Core.Api/BufferMarshaller.cs,"@@ -0,0 +1,68 @@+#region Copyright notice and license++// Copyright 2015 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#endregion++using System;+using System.Buffers;+using Grpc.Core.Utils;++namespace Grpc.Core+{+    /// <summary>+    /// Encapsulates the logic for serializing and deserializing messages.+    /// </summary>+    public class BufferMarshaller<T> : Marshaller<T>","Right, I couldn't isolate a helper method by itself and needed something in the generated code. Originally I wasn't sure we'd want to use the contextual serializer/deserializers because it's marked as experimental API: https://github.com/grpc/grpc/blob/d18b52f5db44b1bfae42a08b3622a0d9b3688fa2/src/csharp/Grpc.Core.Api/Marshaller.cs#L119. But now I think about it more, it's probably why the API was added in the first place.",
2030323,JunTaoLuo,https://api.github.com/repos/grpc/grpc/pulls/21653,367803101,2020-01-17T07:42:45Z,src/csharp/Grpc.IntegrationTesting/TestGrpc.cs,"@@ -34,7 +34,15 @@ public static partial class TestService   {     static readonly string __ServiceName = ""grpc.testing.TestService""; -    static readonly grpc::Marshaller<global::Grpc.Testing.Empty> __Marshaller_grpc_testing_Empty = grpc::Marshallers.Create((arg) => global::Google.Protobuf.MessageExtensions.ToByteArray(arg), global::Grpc.Testing.Empty.Parser.ParseFrom);+    static readonly grpc::Marshaller<global::Grpc.Testing.Empty> __Marshaller_grpc_testing_Empty = grpc::Marshallers.Create(","Ah yes, bad copy paste.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21688,367900724,2020-01-17T11:55:04Z,test/distrib/cpp/run_distrib_test_cmake_pkgconfig_simple.sh,"@@ -0,0 +1,42 @@+#!/bin/bash+# Copyright 2017 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++cd ""$(dirname ""$0"")/../../..""++echo ""deb http://archive.debian.org/debian jessie-backports main"" | tee /etc/apt/sources.list.d/jessie-backports.list+echo 'Acquire::Check-Valid-Until ""false"";' > /etc/apt/apt.conf+sed -i '/deb http:\/\/deb.debian.org\/debian jessie-updates main/d' /etc/apt/sources.list+apt-get update+apt-get install -t jessie-backports -y libssl-dev pkg-config++# Install gRPC with all dependencies+mkdir -p ""cmake/build""+pushd ""cmake/build""+cmake ../..","looks like you're basically try to install all grpc's dependencies directly from submodules (same as https://github.com/grpc/grpc/blob/master/test/distrib/cpp/run_distrib_test_cmake_module_install.sh) this won't work without further improvements.- installing from submodules actually requires newer cmake version (I think 3.13+), otherwise you'd get an error about exporting targets from subprojects- I suspect it won't work without setting `-DgRPC_SSL_PROVIDER=package` because boringssl doesn't have any installation rules.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21696,368082414,2020-01-17T18:47:22Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -327,7 +402,31 @@ cdef class _AioCall(GrpcCallWrapper):         metadata_sent_observer()          # Receives initial metadata.-        initial_metadata_observer(-            await _receive_initial_metadata(self,-                                            self._loop),+        self._set_initial_metadata(+            await _receive_initial_metadata(self, self._loop)         )+++cdef _AioCall new_AioCall(AioChannel channel, object deadline,","1. Can we use snake case for the function name?2. Can we make it a `@staticmethod` function for `_AioCall` class? The official guide uses similar pattern, and it makes the code more cohesive.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21696,368107403,2020-01-17T19:47:38Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -327,7 +402,31 @@ cdef class _AioCall(GrpcCallWrapper):         metadata_sent_observer()          # Receives initial metadata.-        initial_metadata_observer(-            await _receive_initial_metadata(self,-                                            self._loop),+        self._set_initial_metadata(+            await _receive_initial_metadata(self, self._loop)         )+++cdef _AioCall new_AioCall(AioChannel channel, object deadline,+                          bytes method, CallCredentials call_credentials):+    """"""Instantiate a new _AioCall object.++    Internally uses a raw initialization without using the traditional","For future reference, here is the generated C++ code for the `__cinit__` currently on master:<details><summary>__cinit__ Source</summary>```C++static int __pyx_pw_6cygrpc_8_AioCall_1__cinit__(PyObject *__pyx_v_self, PyObject *__pyx_args, PyObject *__pyx_kwds) {  struct __pyx_obj_6cygrpc_AioChannel *__pyx_v_channel = 0;  PyObject *__pyx_v_deadline = 0;  PyObject *__pyx_v_method = 0;  struct __pyx_obj_6cygrpc_CallCredentials *__pyx_v_call_credentials = 0;  int __pyx_r;  __Pyx_RefNannyDeclarations  __Pyx_RefNannySetupContext(""__cinit__ (wrapper)"", 0);  {    static PyObject **__pyx_pyargnames[] = {&__pyx_n_s_channel,&__pyx_n_s_deadline,&__pyx_n_s_method,&__pyx_n_s_call_credentials,0};    PyObject* values[4] = {0,0,0,0};    if (unlikely(__pyx_kwds)) {      Py_ssize_t kw_args;      const Py_ssize_t pos_args = PyTuple_GET_SIZE(__pyx_args);      switch (pos_args) {        case  4: values[3] = PyTuple_GET_ITEM(__pyx_args, 3);        CYTHON_FALLTHROUGH;        case  3: values[2] = PyTuple_GET_ITEM(__pyx_args, 2);        CYTHON_FALLTHROUGH;        case  2: values[1] = PyTuple_GET_ITEM(__pyx_args, 1);        CYTHON_FALLTHROUGH;        case  1: values[0] = PyTuple_GET_ITEM(__pyx_args, 0);        CYTHON_FALLTHROUGH;        case  0: break;         default: goto __pyx_L5_argtuple_error;      }            kw_args = PyDict_Size(__pyx_kwds);      switch (pos_args) {        case  0:        if (likely((values[0] = __Pyx_PyDict_GetItemStr(__pyx_kwds, __pyx_n_s_channel)) != 0)) kw_args--;        else goto __pyx_L5_argtuple_error;        CYTHON_FALLTHROUGH;        case  1:        if (likely((values[1] = __Pyx_PyDict_GetItemStr(__pyx_kwds, __pyx_n_s_deadline)) != 0)) kw_args--;        else {           __Pyx_RaiseArgtupleInvalid(""__cinit__"", 1, 4, 4, 1); __PYX_ERR(33, 28, __pyx_L3_error)        }              CYTHON_FALLTHROUGH;        case  2:        if (likely((values[2] = __Pyx_PyDict_GetItemStr(__pyx_kwds, __pyx_n_s_method)) != 0)) kw_args--;        else {           __Pyx_RaiseArgtupleInvalid(""__cinit__"", 1, 4, 4, 2); __PYX_ERR(33, 28, __pyx_L3_error)        }              CYTHON_FALLTHROUGH;        case  3:        if (likely((values[3] = __Pyx_PyDict_GetItemStr(__pyx_kwds, __pyx_n_s_call_credentials)) != 0)) kw_args--;        else {           __Pyx_RaiseArgtupleInvalid(""__cinit__"", 1, 4, 4, 3); __PYX_ERR(33, 28, __pyx_L3_error)        }            }            if (unlikely(kw_args > 0)) {        if (unlikely(__Pyx_ParseOptionalKeywords(__pyx_kwds, __pyx_pyargnames, 0, values, pos_args, ""__cinit__"") < 0)) __PYX_ERR(33, 28, __pyx_L3_error)      }          } else if (PyTuple_GET_SIZE(__pyx_args) != 4) {      goto __pyx_L5_argtuple_error;    } else {       values[0] = PyTuple_GET_ITEM(__pyx_args, 0);      values[1] = PyTuple_GET_ITEM(__pyx_args, 1);      values[2] = PyTuple_GET_ITEM(__pyx_args, 2);      values[3] = PyTuple_GET_ITEM(__pyx_args, 3);    }    __pyx_v_channel = ((struct __pyx_obj_6cygrpc_AioChannel *)values[0]);    __pyx_v_deadline = values[1];    __pyx_v_method = ((PyObject*)values[2]);    __pyx_v_call_credentials = ((struct __pyx_obj_6cygrpc_CallCredentials *)values[3]);  }  goto __pyx_L4_argument_unpacking_done;  __pyx_L5_argtuple_error:;  __Pyx_RaiseArgtupleInvalid(""__cinit__"", 1, 4, 4, PyTuple_GET_SIZE(__pyx_args)); __PYX_ERR(33, 28, __pyx_L3_error)  __pyx_L3_error:;  __Pyx_AddTraceback(""cygrpc._AioCall.__cinit__"", __pyx_clineno, __pyx_lineno, __pyx_filename);  __Pyx_RefNannyFinishContext();  return -1;  __pyx_L4_argument_unpacking_done:;  if (unlikely(!__Pyx_ArgTypeTest(((PyObject *)__pyx_v_channel), __pyx_ptype_6cygrpc_AioChannel, 1, ""channel"", 0))) __PYX_ERR(33, 29, __pyx_L1_error)  if (unlikely(!__Pyx_ArgTypeTest(((PyObject *)__pyx_v_method), (&PyBytes_Type), 1, ""method"", 1))) __PYX_ERR(33, 31, __pyx_L1_error)  if (unlikely(!__Pyx_ArgTypeTest(((PyObject *)__pyx_v_call_credentials), __pyx_ptype_6cygrpc_CallCredentials, 1, ""call_credentials"", 0))) __PYX_ERR(33, 32, __pyx_L1_error)  __pyx_r = __pyx_pf_6cygrpc_8_AioCall___cinit__(((struct __pyx_obj_6cygrpc__AioCall *)__pyx_v_self), __pyx_v_channel, __pyx_v_deadline, __pyx_v_method, __pyx_v_call_credentials);  /* function exit code */  goto __pyx_L0;  __pyx_L1_error:;  __pyx_r = -1;  __pyx_L0:;  __Pyx_RefNannyFinishContext();  return __pyx_r;}static int __pyx_pf_6cygrpc_8_AioCall___cinit__(struct __pyx_obj_6cygrpc__AioCall *__pyx_v_self, struct __pyx_obj_6cygrpc_AioChannel *__pyx_v_channel, PyObject *__pyx_v_deadline, PyObject *__pyx_v_method, struct __pyx_obj_6cygrpc_CallCredentials *__pyx_v_call_credentials) {  int __pyx_r;  __Pyx_RefNannyDeclarations  PyObject *__pyx_t_1 = NULL;  PyObject *__pyx_t_2 = NULL;  PyObject *__pyx_t_3 = NULL;                           int __pyx_t_4;  __Pyx_RefNannySetupContext(""__cinit__"", 0);    /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":33 *                   bytes method, *                   CallCredentials call_credentials, /): *         self.call = NULL             # <<<<<<<<<<<<<< *         self._channel = channel                *         self._references = [] */  __pyx_v_self->__pyx_base.call = NULL;                                                                                                        /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":34 *                   CallCredentials call_credentials, /): *         self.call = NULL *         self._channel = channel             # <<<<<<<<<<<<<< *         self._references = [] *         self._loop = asyncio.get_event_loop() */  __Pyx_INCREF(((PyObject *)__pyx_v_channel));  __Pyx_GIVEREF(((PyObject *)__pyx_v_channel));  __Pyx_GOTREF(__pyx_v_self->_channel);  __Pyx_DECREF(((PyObject *)__pyx_v_self->_channel));                                                                                          __pyx_v_self->_channel = __pyx_v_channel;    /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":35 *         self.call = NULL *         self._channel = channel *         self._references = []             # <<<<<<<<<<<<<< *         self._loop = asyncio.get_event_loop() *         self._create_grpc_call(deadline, method, call_credentials)                                                                         */  __pyx_t_1 = PyList_New(0); if (unlikely(!__pyx_t_1)) __PYX_ERR(33, 35, __pyx_L1_error)  __Pyx_GOTREF(__pyx_t_1);  __Pyx_GIVEREF(__pyx_t_1);  __Pyx_GOTREF(__pyx_v_self->_references);  __Pyx_DECREF(__pyx_v_self->_references);  __pyx_v_self->_references = ((PyObject*)__pyx_t_1);  __pyx_t_1 = 0;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":36 *         self._channel = channel *         self._references = [] *         self._loop = asyncio.get_event_loop()             # <<<<<<<<<<<<<< *         self._create_grpc_call(deadline, method, call_credentials) *         self._is_locally_cancelled = False */  __Pyx_GetModuleGlobalName(__pyx_t_2, __pyx_n_s_asyncio); if (unlikely(!__pyx_t_2)) __PYX_ERR(33, 36, __pyx_L1_error)  __Pyx_GOTREF(__pyx_t_2);  __pyx_t_3 = __Pyx_PyObject_GetAttrStr(__pyx_t_2, __pyx_n_s_get_event_loop); if (unlikely(!__pyx_t_3)) __PYX_ERR(33, 36, __pyx_L1_error)  __Pyx_GOTREF(__pyx_t_3);  __Pyx_DECREF(__pyx_t_2); __pyx_t_2 = 0;  __pyx_t_2 = NULL;  __pyx_t_4 = 0;  if (CYTHON_UNPACK_METHODS && unlikely(PyMethod_Check(__pyx_t_3))) {    __pyx_t_2 = PyMethod_GET_SELF(__pyx_t_3);    if (likely(__pyx_t_2)) {      PyObject* function = PyMethod_GET_FUNCTION(__pyx_t_3);      __Pyx_INCREF(__pyx_t_2);      __Pyx_INCREF(function);      __Pyx_DECREF_SET(__pyx_t_3, function);      __pyx_t_4 = 1;    }  }  {    PyObject *__pyx_callargs[1] = {__pyx_t_2, };    __pyx_t_1 = __Pyx_PyObject_FastCall(__pyx_t_3, __pyx_callargs+1-__pyx_t_4, 0+__pyx_t_4);    PyObject *__pyx_callargs[1] = {__pyx_t_2, };    __pyx_t_1 = __Pyx_PyObject_FastCall(__pyx_t_3, __pyx_callargs+1-__pyx_t_4, 0+__pyx_t_4);    __Pyx_XDECREF(__pyx_t_2); __pyx_t_2 = 0;    if (unlikely(!__pyx_t_1)) __PYX_ERR(33, 36, __pyx_L1_error)    __Pyx_GOTREF(__pyx_t_1);    __Pyx_DECREF(__pyx_t_3); __pyx_t_3 = 0;  }  __Pyx_GIVEREF(__pyx_t_1);  __Pyx_GOTREF(__pyx_v_self->_loop);  __Pyx_DECREF(__pyx_v_self->_loop);  __pyx_v_self->_loop = __pyx_t_1;  __pyx_t_1 = 0;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":37 *         self._references = [] *         self._loop = asyncio.get_event_loop() *         self._create_grpc_call(deadline, method, call_credentials)             # <<<<<<<<<<<<<< *         self._is_locally_cancelled = False *  */  ((struct __pyx_vtabstruct_6cygrpc__AioCall *)__pyx_v_self->__pyx_vtab)->_create_grpc_call(__pyx_v_self, __pyx_v_deadline, __pyx_v_method, __pyx_v_call_credentials); if (unlikely(PyErr_Occurred())) __PYX_ERR(33, 37, __pyx_L1_error)  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":38 *         self._loop = asyncio.get_event_loop() *         self._create_grpc_call(deadline, method, call_credentials) *         self._is_locally_cancelled = False             # <<<<<<<<<<<<<< *  *     def __dealloc__(self): */  __pyx_v_self->_is_locally_cancelled = 0;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":28 * cdef class _AioCall(GrpcCallWrapper): *  *     def __cinit__(self,             # <<<<<<<<<<<<<< *                   AioChannel channel, *                   object deadline, */  /* function exit code */  __pyx_r = 0;  goto __pyx_L0;  __pyx_L1_error:;  __Pyx_XDECREF(__pyx_t_1);  __Pyx_XDECREF(__pyx_t_2);  __Pyx_XDECREF(__pyx_t_3);  __Pyx_AddTraceback(""cygrpc._AioCall.__cinit__"", __pyx_clineno, __pyx_lineno, __pyx_filename);  __pyx_r = -1;  __pyx_L0:;  __Pyx_RefNannyFinishContext();  return __pyx_r;}```</details>Meanwhile, here's the generated code for this new `new_AioCall` function.<details><summary>new_AioCall Source</summary>```C++static struct __pyx_obj_6cygrpc__AioCall *__pyx_f_6cygrpc_new_AioCall(struct __pyx_obj_6cygrpc_AioChannel *__pyx_v_channel, PyObject *__pyx_v_deadline, PyObject *__pyx_v_method, struct __pyx_obj_6cygrpc_CallCredentials *__pyx_v_call_credentials) {  struct __pyx_obj_6cygrpc__AioCall *__pyx_v_aio_call = 0;  struct __pyx_obj_6cygrpc__AioCall *__pyx_r = NULL;  __Pyx_RefNannyDeclarations  PyObject *__pyx_t_1 = NULL;  __Pyx_RefNannySetupContext(""new_AioCall"", 0);  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":421 *     """""" *     cdef _AioCall aio_call *     aio_call = _AioCall.__new__(_AioCall)             # <<<<<<<<<<<<<< *     aio_call.call = NULL *     aio_call._channel = channel */  __pyx_t_1 = ((PyObject *)__pyx_tp_new_6cygrpc__AioCall(((PyTypeObject *)__pyx_ptype_6cygrpc__AioCall), __pyx_empty_tuple, NULL)); if (unlikely(!__pyx_t_1)) __PYX_ERR(33, 421, __pyx_L1_error)  __Pyx_GOTREF(((PyObject *)__pyx_t_1));  __pyx_v_aio_call = ((struct __pyx_obj_6cygrpc__AioCall *)__pyx_t_1);  __pyx_t_1 = 0;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":422 *     cdef _AioCall aio_call *     aio_call = _AioCall.__new__(_AioCall) *     aio_call.call = NULL             # <<<<<<<<<<<<<< *     aio_call._channel = channel *     aio_call._loop = channel.loop */  __pyx_v_aio_call->__pyx_base.call = NULL;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":423 *     aio_call = _AioCall.__new__(_AioCall) *     aio_call.call = NULL *     aio_call._channel = channel             # <<<<<<<<<<<<<< *     aio_call._loop = channel.loop *     aio_call._references = [] */  __Pyx_INCREF(((PyObject *)__pyx_v_channel));  __Pyx_GIVEREF(((PyObject *)__pyx_v_channel));  __Pyx_GOTREF(__pyx_v_aio_call->_channel);  __Pyx_DECREF(((PyObject *)__pyx_v_aio_call->_channel));  __pyx_v_aio_call->_channel = __pyx_v_channel;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":424 *     aio_call.call = NULL *     aio_call._channel = channel *     aio_call._loop = channel.loop             # <<<<<<<<<<<<<< *     aio_call._references = [] *     aio_call._status = None */  __pyx_t_1 = __pyx_v_channel->loop;  __Pyx_INCREF(__pyx_t_1);  __Pyx_GIVEREF(__pyx_t_1);  __Pyx_GOTREF(__pyx_v_aio_call->_loop);  __Pyx_DECREF(__pyx_v_aio_call->_loop);  __pyx_v_aio_call->_loop = __pyx_t_1;  __pyx_t_1 = 0;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":425 *     aio_call._channel = channel *     aio_call._loop = channel.loop *     aio_call._references = []             # <<<<<<<<<<<<<< *     aio_call._status = None *     aio_call._initial_metadata = None */  __pyx_t_1 = PyList_New(0); if (unlikely(!__pyx_t_1)) __PYX_ERR(33, 425, __pyx_L1_error)  __Pyx_GOTREF(__pyx_t_1);  __Pyx_GIVEREF(__pyx_t_1);  __Pyx_GOTREF(__pyx_v_aio_call->_references);  __Pyx_DECREF(__pyx_v_aio_call->_references);  __pyx_v_aio_call->_references = ((PyObject*)__pyx_t_1);  __pyx_t_1 = 0;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":426 *     aio_call._loop = channel.loop *     aio_call._references = [] *     aio_call._status = None             # <<<<<<<<<<<<<< *     aio_call._initial_metadata = None *     aio_call._waiters_status = [] */  __Pyx_INCREF(Py_None);  __Pyx_GIVEREF(Py_None);  __Pyx_GOTREF(__pyx_v_aio_call->_status);  __Pyx_DECREF(__pyx_v_aio_call->_status);  __pyx_v_aio_call->_status = Py_None;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":427 *     aio_call._references = [] *     aio_call._status = None *     aio_call._initial_metadata = None             # <<<<<<<<<<<<<< *     aio_call._waiters_status = [] *     aio_call._waiters_initial_metadata = [] */  __Pyx_INCREF(Py_None);  __Pyx_GIVEREF(Py_None);  __Pyx_GOTREF(__pyx_v_aio_call->_initial_metadata);  __Pyx_DECREF(__pyx_v_aio_call->_initial_metadata);  __pyx_v_aio_call->_initial_metadata = Py_None;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":428 *     aio_call._status = None *     aio_call._initial_metadata = None *     aio_call._waiters_status = []             # <<<<<<<<<<<<<< *     aio_call._waiters_initial_metadata = [] *     aio_call._create_grpc_call(deadline, method, call_credentials) */  __pyx_t_1 = PyList_New(0); if (unlikely(!__pyx_t_1)) __PYX_ERR(33, 428, __pyx_L1_error)  __Pyx_GOTREF(__pyx_t_1);  __Pyx_GIVEREF(__pyx_t_1);  __Pyx_GOTREF(__pyx_v_aio_call->_waiters_status);  __Pyx_DECREF(__pyx_v_aio_call->_waiters_status);  __pyx_v_aio_call->_waiters_status = ((PyObject*)__pyx_t_1);  __pyx_t_1 = 0;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":429 *     aio_call._initial_metadata = None *     aio_call._waiters_status = [] *     aio_call._waiters_initial_metadata = []             # <<<<<<<<<<<<<< *     aio_call._create_grpc_call(deadline, method, call_credentials) *     aio_call._is_locally_cancelled = False */  __pyx_t_1 = PyList_New(0); if (unlikely(!__pyx_t_1)) __PYX_ERR(33, 429, __pyx_L1_error)  __Pyx_GOTREF(__pyx_t_1);  __Pyx_GIVEREF(__pyx_t_1);  __Pyx_GOTREF(__pyx_v_aio_call->_waiters_initial_metadata);  __Pyx_DECREF(__pyx_v_aio_call->_waiters_initial_metadata);  __pyx_v_aio_call->_waiters_initial_metadata = ((PyObject*)__pyx_t_1);  __pyx_t_1 = 0;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":430 *     aio_call._waiters_status = [] *     aio_call._waiters_initial_metadata = [] *     aio_call._create_grpc_call(deadline, method, call_credentials)             # <<<<<<<<<<<<<< *     aio_call._is_locally_cancelled = False *     return aio_call */  ((struct __pyx_vtabstruct_6cygrpc__AioCall *)__pyx_v_aio_call->__pyx_vtab)->_create_grpc_call(__pyx_v_aio_call, __pyx_v_deadline, __pyx_v_method, __pyx_v_call_credentials); if (unlikely(PyErr_Occurred())) __PYX_ERR(33, 430, __pyx_L1_error)  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":431 *     aio_call._waiters_initial_metadata = [] *     aio_call._create_grpc_call(deadline, method, call_credentials) *     aio_call._is_locally_cancelled = False             # <<<<<<<<<<<<<< *     return aio_call*/  __pyx_v_aio_call->_is_locally_cancelled = 0;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":432 *     aio_call._create_grpc_call(deadline, method, call_credentials) *     aio_call._is_locally_cancelled = False *     return aio_call             # <<<<<<<<<<<<<< */  __Pyx_XDECREF(((PyObject *)__pyx_r));  __Pyx_INCREF(((PyObject *)__pyx_v_aio_call));  __pyx_r = __pyx_v_aio_call;  goto __pyx_L0;  /* ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"":410 *  *  * cdef _AioCall new_AioCall(AioChannel channel, object deadline,             # <<<<<<<<<<<<<< *                           bytes method, CallCredentials call_credentials): *     """"""Instantiate a new _AioCall object. */  /* function exit code */  __pyx_L1_error:;  __Pyx_XDECREF(__pyx_t_1);  __Pyx_AddTraceback(""cygrpc.new_AioCall"", __pyx_clineno, __pyx_lineno, __pyx_filename);  __pyx_r = 0;  __pyx_L0:;  __Pyx_XDECREF((PyObject *)__pyx_v_aio_call);  __Pyx_XGIVEREF((PyObject *)__pyx_r);  __Pyx_RefNannyFinishContext();  return __pyx_r;}```</details>Some takeaways from my comparison of the two: - The `__cinit__`-based version has a lot of code for pathways that we will never take, like parsing keyword arguments. We could remove those branches by taking advantage of [keyword-only arguments on a newer version of Cython](https://github.com/cython/cython/pull/2949). But those paths are guarded with `unlikely` macros, meaning they're probably not super impactful. - `self._loop = asyncio.get_event_loop()` was changed to `aio_call._loop = channel.loop`. This eliminated the only actual Python function call (as far as I can tell). This seems to be the only change on the codepath that we actually go down.@pfreixes Would you mind running another benchmark with `channel.loop` changed to `asyncio.get_event_loop`? I'm wondering if the change from `__cinit__` to C-style constructor was actually what led to the performance improvement or if it was mostly the removal of the Python call. If it's not the cause of the performance regression, I'd like to keep the encapsulation provided by `__cinit__`.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21696,368110039,2020-01-17T19:54:10Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -327,7 +402,31 @@ cdef class _AioCall(GrpcCallWrapper):         metadata_sent_observer()          # Receives initial metadata.-        initial_metadata_observer(-            await _receive_initial_metadata(self,-                                            self._loop),+        self._set_initial_metadata(+            await _receive_initial_metadata(self, self._loop)         )+++cdef _AioCall new_AioCall(AioChannel channel, object deadline,","@lidizheng If we follow CPython C API naming conventions, this would be named `PyAioCall_New`.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21696,368113109,2020-01-17T20:01:46Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -327,7 +402,31 @@ cdef class _AioCall(GrpcCallWrapper):         metadata_sent_observer()          # Receives initial metadata.-        initial_metadata_observer(-            await _receive_initial_metadata(self,-                                            self._loop),+        self._set_initial_metadata(+            await _receive_initial_metadata(self, self._loop)         )+++cdef _AioCall new_AioCall(AioChannel channel, object deadline,","If instantiation of these `_AioCall` objects is one of the bottlenecks, perhaps we would benefit from a [freelist](https://cython.readthedocs.io/en/latest/src/userguide/extension_types.html#fast-instantiation)? It might be more performant than the default allocation strategy provided by individual calls to `PyType_GenericAlloc`.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21696,368141253,2020-01-17T21:21:20Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -130,6 +130,81 @@ class TestUnaryUnaryCall(AioTestBase):             call = hi(messages_pb2.SimpleRequest())             self.assertEqual((), await call.trailing_metadata()) +    async def test_call_initial_metadata_cancelable(self):+        async with aio.insecure_channel(self._server_target) as channel:+            hi = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)","Just wanted to give the same shape as other tests of the same test class. Im planning on make a PR with adopting your pattern [1] for having all of the unit tests using the same pattern.I avoided doing so in this PR for not polluting the PR with changes that were not strictly related to the PR, also considering that changing most of the tests would make more difficult the review.[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio_tests/tests_aio/unit/metadata_test.py#L183",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21696,368142918,2020-01-17T21:26:13Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pxd.pxi,"@@ -28,4 +28,15 @@ cdef class _AioCall(GrpcCallWrapper):         # because Core is holding a pointer for the callback handler.         bint _is_locally_cancelled +        # Following attributes are used for storing the status of the call and+        # the initial metadata. Waiters are used for pausing the execution of+        # tasks that are asking for one of the field when they are not yet+        # available.+        object _status+        object _initial_metadata+        list _waiters_status+        list _waiters_initial_metadata","Yeps the reason is one of the comments that made in the PR> This PR also addresses an issue with the previous code where isolated tasks that were asking for the initial metadata or the RPC status might leave polluted the path with an asyncio.CasncelledError exception. With the changes proposed multiple calls can be done to the initial_metadata and status coroutines provided by AioCall without polluting the other waiters.There is a new test implemented here [1] which if you try to run it in master will fail.There is a recurrent thought in my head. if we all agree that the usage of multiple waiters is the way to go for isolating concurrent access to the coros that are informing about the status or other attributes of an ongoing RPC, should be the same rationale appliable to the `await call`?[1] https://github.com/grpc/grpc/pull/21696/files#diff-ebe6c75bf1347f8032019b99fbf4a8bdR133",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21696,368149072,2020-01-17T21:44:40Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -327,7 +402,31 @@ cdef class _AioCall(GrpcCallWrapper):         metadata_sent_observer()          # Receives initial metadata.-        initial_metadata_observer(-            await _receive_initial_metadata(self,-                                            self._loop),+        self._set_initial_metadata(+            await _receive_initial_metadata(self, self._loop)         )+++cdef _AioCall new_AioCall(AioChannel channel, object deadline,+                          bytes method, CallCredentials call_credentials):+    """"""Instantiate a new _AioCall object.++    Internally uses a raw initialization without using the traditional","Maybe I was a bit too eager on that, yes the optimization that you can get by not having the default constructor could be negligible or unnoticeable.  That the only stuff that will be circumvented would be the arguments unpacking:```cpp      values[0] = PyTuple_GET_ITEM(__pyx_args, 0);      values[1] = PyTuple_GET_ITEM(__pyx_args, 1);      values[2] = PyTuple_GET_ITEM(__pyx_args, 2);      values[3] = PyTuple_GET_ITEM(__pyx_args, 3);```And later on this other part:```cpp__pyx_L4_argument_unpacking_done:;  if (unlikely(!__Pyx_ArgTypeTest(((PyObject *)__pyx_v_channel), __pyx_ptype_6cygrpc_AioChannel, 1, ""channel"", 0))) __PYX_ERR(33, 29, __pyx_L1_error)  if (unlikely(!__Pyx_ArgTypeTest(((PyObject *)__pyx_v_method), (&PyBytes_Type), 1, ""method"", 1))) __PYX_ERR(33, 31, __pyx_L1_error)  if (unlikely(!__Pyx_ArgTypeTest(((PyObject *)__pyx_v_call_credentials), __pyx_ptype_6cygrpc_CallCredentials, 1, ""call_credentials"", 0))) __PYX_ERR(33, 32, __pyx_L1_error)  __pyx_r = __pyx_pf_6cygrpc_8_AioCall___cinit__(((struct __pyx_obj_6cygrpc__AioCall *)__pyx_v_self), __pyx_v_channel, __pyx_v_deadline, __pyx_v_method, __pyx_v_call_credentials);```Almost for sure that most of the optimization in the construction time would come from the fact that we had removed the call to the `get_event_loop`, but since we were not explicitly allowing the _AioCall instantiation at Python level I had the feeling that this could become a ""quick-win"".I'll be back with some numbers comparing current code with an without the calling to the `get_event_loop`.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21696,368151068,2020-01-17T21:50:54Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -327,7 +402,31 @@ cdef class _AioCall(GrpcCallWrapper):         metadata_sent_observer()          # Receives initial metadata.-        initial_metadata_observer(-            await _receive_initial_metadata(self,-                                            self._loop),+        self._set_initial_metadata(+            await _receive_initial_metadata(self, self._loop)         )+++cdef _AioCall new_AioCall(AioChannel channel, object deadline,","I didn't know about the freelist, indeed I was thinking that how we could do preallocation. Thanks for sharing it.But in any case, I ended up by making some tests and it turned out that with the constructor proposed in this PR, 90% of the time is spent in the `_create_grpc_call`, presumably in the `grpc_channel_create_call` which for me was a big surprise. I can share later more numbers about this. ",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21696,368152811,2020-01-17T21:56:29Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -327,7 +402,31 @@ cdef class _AioCall(GrpcCallWrapper):         metadata_sent_observer()          # Receives initial metadata.-        initial_metadata_observer(-            await _receive_initial_metadata(self,-                                            self._loop),+        self._set_initial_metadata(+            await _receive_initial_metadata(self, self._loop)         )+++cdef _AioCall new_AioCall(AioChannel channel, object deadline,","Happy to change the factory to something that we all agree. In principle, I would prefer to have the static method rather than have a function with a lot of similarities with the ones coming from the CPython API, basically for avoiding confusion when the CPP file is read.But before moving forward with this discussion lets try to close this one first [1], where @gnossen challenges - If I'm not wrong - the idea of not using the traditional `__ciinit__`.[1] https://github.com/grpc/grpc/pull/21696#discussion_r368107403",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21696,368153614,2020-01-17T21:58:55Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -327,7 +402,31 @@ cdef class _AioCall(GrpcCallWrapper):         metadata_sent_observer()          # Receives initial metadata.-        initial_metadata_observer(-            await _receive_initial_metadata(self,-                                            self._loop),+        self._set_initial_metadata(+            await _receive_initial_metadata(self, self._loop)         )+++cdef _AioCall new_AioCall(AioChannel channel, object deadline,","Core uses arena allocation behind the scenes. As you have more queries in flight, it will expand the size of the arena (I believe it uses doubling). So your first queries will take the hit of that allocation. Perhaps we could do some tuning in core to start the arena out with a bigger initial size. That should be equivalent to adding a free list to the memory backing `grpc_channel_create_call`.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/21688,368165045,2020-01-17T22:38:16Z,test/distrib/cpp/run_distrib_test_cmake_pkgconfig_simple.sh,"@@ -0,0 +1,42 @@+#!/bin/bash+# Copyright 2017 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++cd ""$(dirname ""$0"")/../../..""++echo ""deb http://archive.debian.org/debian jessie-backports main"" | tee /etc/apt/sources.list.d/jessie-backports.list+echo 'Acquire::Check-Valid-Until ""false"";' > /etc/apt/apt.conf+sed -i '/deb http:\/\/deb.debian.org\/debian jessie-updates main/d' /etc/apt/sources.list+apt-get update+apt-get install -t jessie-backports -y libssl-dev pkg-config++# Install gRPC with all dependencies+mkdir -p ""cmake/build""+pushd ""cmake/build""+cmake ../..","Installing cmake 3.16 seems necessary, thanks. When I tried this script with docker image with recent gcc and cmake, it was working without ssl parameter. Let's try it again.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21698,368172721,2020-01-17T23:07:53Z,BUILD,"@@ -77,11 +77,11 @@ config_setting( python_config_settings()  # This should be updated along with build.yaml-g_stands_for = ""guantao""+g_stands_for = ""gallactic""",Two L's? Does this refer to [the blockchain project](https://github.com/gallactic/gallactic)?**Edit:** Hey! [Looks like they use gRPC!](https://github.com/gallactic/gallactic/tree/master/rpc/grpc),
303201,JamesNK,https://api.github.com/repos/grpc/grpc/pulls/21653,368341773,2020-01-20T01:24:31Z,src/csharp/Grpc.Examples/MathGrpc.cs,"@@ -27,10 +27,73 @@ public static partial class Math   {     static readonly string __ServiceName = ""math.Math""; +    #if !GOOGLE_PROTOBUF_DISABLE_BUFFER_SERIALIZATION+    static readonly grpc::Marshaller<global::Math.DivArgs> __Marshaller_math_DivArgs = ","The buffer marshaller methods are quite long. @jtattermusch what do you think of including a private method here that the marshallers call so their delegates are only 1 line? Generated code will be slightly longer if there is only one message type, but as soon as there are multiple then the reusable method will greatly reduce amount of code gen.```csprivate static void WriteBufferMessage<T>(T message, SerializationContext context) where T : IBufferMessage{    var writer = new global::Google.Protobuf.CodedOutputWriter(context.GetBufferWriter());    arg.WriteTo(ref writer);    writer.Flush();    context.Complete();}```Don't need one for reading. Code generation can use parser's `IMessageParser<T>.ParseFrom(ReadOnlySequence<byte>)`:```cs(context) => global::Math.DivArgs.Parser.ParseFrom(context.PayloadAsReadOnlySequence())```Before:```csharp    static readonly grpc::Marshaller<global::Math.DivReply> __Marshaller_math_DivReply =       grpc::Marshallers.Create(        (arg, context) =>        {          var writer = new global::Google.Protobuf.CodedOutputWriter(context.GetBufferWriter());          arg.WriteTo(ref writer);          writer.Flush();          context.Complete();        },        context =>        {          var result = new global::Math.DivReply();          var reader = new global::Google.Protobuf.CodedInputReader(context.PayloadAsReadOnlySequence());          result.MergeFrom(ref reader);          return result;        });```After:```csharp    static readonly grpc::Marshaller<global::Math.DivReply> __Marshaller_math_DivReply = grpc::Marshallers.Create(WriteBufferMessage, (context) => global::Math.DivReply.Parser.ParseFrom(context.PayloadAsReadOnlySequence());```",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21688,368392548,2020-01-20T07:00:09Z,test/distrib/cpp/run_distrib_test_cmake_module_install_pkgconfig.sh,"@@ -0,0 +1,47 @@+#!/bin/bash+# Copyright 2017 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++cd ""$(dirname ""$0"")/../../..""++echo ""deb http://archive.debian.org/debian jessie-backports main"" | tee /etc/apt/sources.list.d/jessie-backports.list+echo 'Acquire::Check-Valid-Until ""false"";' > /etc/apt/apt.conf+sed -i '/deb http:\/\/deb.debian.org\/debian jessie-updates main/d' /etc/apt/sources.list+apt-get update+apt-get install -t jessie-backports -y libssl-dev pkg-config wget++# Install CMake 3.16+wget -q -O cmake-linux.sh https://github.com/Kitware/CMake/releases/download/v3.16.1/cmake-3.16.1-Linux-x86_64.sh+sh cmake-linux.sh -- --skip-license --prefix=/usr+rm cmake-linux.sh++# Install gRPC with all dependencies+mkdir -p ""cmake/build""+pushd ""cmake/build""+cmake ../..",See the failure https://source.cloud.google.com/results/invocations/0b684d72-e21f-46f0-8a27-56140c7812da/targets/github%2Fgrpc/testsI think you should add all the cmake defines from here (for the tests to pass and also for consistency) https://github.com/grpc/grpc/blob/eba60d8dbe4099c34b8097b2c89998d4484740ac/test/distrib/cpp/run_distrib_test_cmake_module_install.sh#L34,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21696,368580452,2020-01-20T14:38:37Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pxd.pxi,"@@ -28,4 +28,15 @@ cdef class _AioCall(GrpcCallWrapper):         # because Core is holding a pointer for the callback handler.         bint _is_locally_cancelled +        # Following attributes are used for storing the status of the call and+        # the initial metadata. Waiters are used for pausing the execution of+        # tasks that are asking for one of the field when they are not yet+        # available.+        object _status+        object _initial_metadata+        list _waiters_status+        list _waiters_initial_metadata","Having the feeling that any change regarding the `await __call__` would need to be handled in another PR - if everybody agrees that the current one already provides some value and the `await __call__` is a different problem (that could be solved with the same pattern). Also if we want to tackle again the `await __call__` most likely is gonna impact again - sorry for that - with how cancellation is being managed. So, I would prefer to have all of the arguments and a good story compressed in a new tiquet. And having all of us aligned with the goal of this.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21696,368626165,2020-01-20T16:09:03Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -327,7 +402,31 @@ cdef class _AioCall(GrpcCallWrapper):         metadata_sent_observer()          # Receives initial metadata.-        initial_metadata_observer(-            await _receive_initial_metadata(self,-                                            self._loop),+        self._set_initial_metadata(+            await _receive_initial_metadata(self, self._loop)         )+++cdef _AioCall new_AioCall(AioChannel channel, object deadline,+                          bytes method, CallCredentials call_credentials):+    """"""Instantiate a new _AioCall object.++    Internally uses a raw initialization without using the traditional","Here we go, I've executed three tests running the benchmark for 100 seconds - originally the benchmarks that I ran the benchmarks took only 10 seconds, I've wanted to run them again for a longer time..The results are narrowed to the `_AioChannel.call` [1] function since it is the last Python function that can be profiled with a python profiler, this function encapsulates basically the process of creating a new `AioCall`.Avg time of `_AioChannel.call` with **master**```python>>> (5.116 / 1555389) * 1e63.2892093231982478```_(5.116 stands for total time accumulated for this funciton, 1555389 stands for number of requests made, 1e6 converting to microseconds)_ Avg of time of `AioChannel.call` with current PR```python>>> (4.406 / 1669214) * 1e62.639565687802762```Avg of time of `AioChannel.call` with current PR but replacing `loop = channel.loop` by `loop = asyncio.get_event_loop()````python>>> (5.653 / 1694820) * 1e63.335457452708842```So you were right, the main benefit comes basically from removing the call to the `get_event_loop`.Considering that the improvements - if there are - of moving from `__cinit__` to an ad-hoc method is imperceptible I would revert the usage of an ad-hoc method and use basically the traditionally `__cinit__`.[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/channel.pyx.pxi#L96",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21653,368954053,2020-01-21T11:43:37Z,src/csharp/Grpc.Examples/MathGrpc.cs,"@@ -27,10 +27,73 @@ public static partial class Math   {     static readonly string __ServiceName = ""math.Math""; +    #if !GOOGLE_PROTOBUF_DISABLE_BUFFER_SERIALIZATION+    static readonly grpc::Marshaller<global::Math.DivArgs> __Marshaller_math_DivArgs = ","For parsing, you can just use `global::Math.DivReply.Parser.ParseFrom(ctx.PayloadAsReadOnlySequence());` (very similar pattern as before, using the Parser static property of the message).   The `IMessageParser<T>.ParseFrom(ReadOnlySequence<byte>)` proposed by @JamesNK might work as well, but using the Parser property seems cleaner.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21696,369266037,2020-01-21T21:59:51Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -153,116 +152,67 @@ class Call(_base_call.Call):     """"""     _loop: asyncio.AbstractEventLoop     _code: grpc.StatusCode-    _status: Awaitable[cygrpc.AioRpcStatus]-    _initial_metadata: Awaitable[MetadataType]-    _locally_cancelled: bool     _cython_call: cygrpc._AioCall-    _done_callbacks: List[DoneCallbackType]--    def __init__(self, cython_call: cygrpc._AioCall) -> None:-        self._loop = asyncio.get_event_loop()-        self._code = None-        self._status = self._loop.create_future()-        self._initial_metadata = self._loop.create_future()-        self._locally_cancelled = False++    def __init__(self, cython_call: cygrpc._AioCall,+                 loop: asyncio.AbstractEventLoop) -> None:+        self._loop = loop         self._cython_call = cython_call-        self._done_callbacks = []      def __del__(self) -> None:-        if not self._status.done():-            self._cancel(-                cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                    _GC_CANCELLATION_DETAILS, None, None))+        if not self._cython_call.done():+            self._cancel(_GC_CANCELLATION_DETAILS)      def cancelled(self) -> bool:-        return self._code == grpc.StatusCode.CANCELLED+        return self._cython_call.cancelled() -    def _cancel(self, status: cygrpc.AioRpcStatus) -> bool:+    def _cancel(self, details: str) -> bool:         """"""Forwards the application cancellation reasoning.""""""-        if not self._status.done():-            self._set_status(status)-            self._cython_call.cancel(status)+        if not self._cython_call.done():+            self._cython_call.cancel(details)             return True         else:             return False      def cancel(self) -> bool:-        return self._cancel(-            cygrpc.AioRpcStatus(cygrpc.StatusCode.cancelled,-                                _LOCAL_CANCELLATION_DETAILS, None, None))+        return self._cancel(_LOCAL_CANCELLATION_DETAILS)      def done(self) -> bool:-        return self._status.done()+        return self._cython_call.done()      def add_done_callback(self, callback: DoneCallbackType) -> None:-        if self.done():-            callback(self)-        else:-            self._done_callbacks.append(callback)+        cb = partial(callback, self)+        self._cython_call.add_done_callback(cb)      def time_remaining(self) -> Optional[float]:         return self._cython_call.time_remaining()      async def initial_metadata(self) -> MetadataType:-        return await self._initial_metadata+        return await self._cython_call.initial_metadata()      async def trailing_metadata(self) -> MetadataType:-        return (await self._status).trailing_metadata()+        return (await self._cython_call.status()).trailing_metadata()      async def code(self) -> grpc.StatusCode:-        await self._status-        return self._code+        cygrpc_code = (await self._cython_call.status()).code()+        return _common.CYGRPC_STATUS_CODE_TO_STATUS_CODE[cygrpc_code]","I understand your point, and the value of neat surface interfaces. If we code the callback in snippets, the latter version looks easier to use to me. To date, the additional callback mechanism is not too complex to manage. @gnossen for additional vote.```pythondef your_callback(call, code_future): passtask = asyncio.ensure_future(call.code())task.add_done_callback(functools.partial(your_callback, call))``````pythondef your_callback(call): passcall.add_done_callback(your_callback)```",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21708,369269324,2020-01-21T22:07:45Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -48,6 +52,23 @@ cdef class RPCState:     cdef tuple invocation_metadata(self):         return _metadata(&self.request_metadata) +    cdef void raise_for_termination(self) except *:+        """"""Raise exceptions if RPC is not running.++        Server method handlers may suppress the abort exception. We need to halt+        the RPC execution in that case. This function needs to be called after+        running application code.++        Also, the server may stop unexpected. We need to check before calling+        into Core functions, otherwise, segfault.+        """"""+        if self.abort_exception is not None:+            raise self.abort_exception+        if self.status_sent:+            raise RuntimeError(_RPC_FINISHED_DETAILS)","Optional: It might be nice for us to have a common base class for our exceptions. I can imagine a user of the library wanting to do the following:```pythontry:  do_grpc_stuff()except grpc.BaseException as e:  logging.warning(""Uh-oh. gRPC failed but not my application: %s"", e)  degrade_gracefully()```",
2030323,JunTaoLuo,https://api.github.com/repos/grpc/grpc/pulls/21653,369724784,2020-01-22T18:21:14Z,src/compiler/csharp_generator.cc,"@@ -323,17 +323,50 @@ std::vector<const Descriptor*> GetUsedMessages(   return result; } -void GenerateMarshallerFields(Printer* out, const ServiceDescriptor* service) {+void GenerateMarshallerFields(Printer* out, const ServiceDescriptor* service,+                              bool use_buffer_serialization) {   std::vector<const Descriptor*> used_messages = GetUsedMessages(service);-  for (size_t i = 0; i < used_messages.size(); i++) {-    const Descriptor* message = used_messages[i];+  if (use_buffer_serialization) {+    // Generate buffer serialization marshallers","Other than I think you reversed the condition in #if, yes it works.",
961599,murgatroid99,https://api.github.com/repos/grpc/grpc/pulls/21761,369739120,2020-01-22T18:50:07Z,BUILD.gn,"@@ -909,7 +833,12 @@ config(""grpc_config"") {         ""//third_party/boringssl"",         ""//third_party/zlib"",         "":gpr"",+        "":alts_upb"",         "":upb"",+        "":grpc_health_upb"",","I don't understand how that file is generated or what it's supposed to contain. But if this change breaks it, that implies that it is determining the libraries it needs separately from the dependency tree defined in `build.yaml`, which is an unsustainably fragile code generation strategy.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21770,369881297,2020-01-23T00:40:25Z,src/python/grpcio_tests/tests_aio/unit/timeout_test.py,"@@ -0,0 +1,178 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests behavior of the timeout mechanism on client side.""""""++import asyncio+import logging+import platform+import random+import unittest+import datetime++import grpc+from grpc.experimental import aio++from tests_aio.unit._test_base import AioTestBase+from tests_aio.unit import _common++_SLEEP_TIME_UNIT_S = datetime.timedelta(seconds=1).total_seconds()++_TEST_SLEEPY_UNARY_UNARY = '/test/Test/SleepyUnaryUnary'+_TEST_SLEEPY_UNARY_STREAM = '/test/Test/SleepyUnaryStream'+_TEST_SLEEPY_STREAM_UNARY = '/test/Test/SleepyStreamUnary'+_TEST_SLEEPY_STREAM_STREAM = '/test/Test/SleepyStreamStream'++_REQUEST = b'\x00\x00\x00'+_RESPONSE = b'\x01\x01\x01'++# logging.getLogger('asyncio').setLevel(logging.WARNING)++class _GenericHandler(grpc.GenericRpcHandler):++    def __init__(self):+        self._routing_table = {",Nit: Any reason for this to be an instance member rather than a class member?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21770,369883115,2020-01-23T00:47:25Z,src/python/grpcio_tests/tests_aio/unit/timeout_test.py,"@@ -0,0 +1,178 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests behavior of the timeout mechanism on client side.""""""++import asyncio+import logging+import platform+import random+import unittest+import datetime++import grpc+from grpc.experimental import aio++from tests_aio.unit._test_base import AioTestBase+from tests_aio.unit import _common++_SLEEP_TIME_UNIT_S = datetime.timedelta(seconds=1).total_seconds()++_TEST_SLEEPY_UNARY_UNARY = '/test/Test/SleepyUnaryUnary'+_TEST_SLEEPY_UNARY_STREAM = '/test/Test/SleepyUnaryStream'+_TEST_SLEEPY_STREAM_UNARY = '/test/Test/SleepyStreamUnary'+_TEST_SLEEPY_STREAM_STREAM = '/test/Test/SleepyStreamStream'++_REQUEST = b'\x00\x00\x00'+_RESPONSE = b'\x01\x01\x01'++# logging.getLogger('asyncio').setLevel(logging.WARNING)++class _GenericHandler(grpc.GenericRpcHandler):++    def __init__(self):+        self._routing_table = {","It can be both, but I didn't find a nicer way to initiate those values. Comparing:1. Using `__init__`, like this;2. Using `__new__`, feels a bit too powerful;3. Using class constant leads to ""NameError: name '_GenericHandler' is not defined"".Suggestions welcomed.",
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/21747,369895334,2020-01-23T01:38:00Z,test/cpp/end2end/test_service_impl.cc,"@@ -579,35 +591,43 @@ experimental::ServerUnaryReactor* CallbackTestServiceImpl::Echo(       Finish(Status::OK);     }     void LoopUntilCancelled(int loop_delay_us) {-      if (!ctx_->IsCancelled()) {-        alarm_.experimental().Set(-            gpr_time_add(gpr_now(GPR_CLOCK_REALTIME),-                         gpr_time_from_micros(loop_delay_us, GPR_TIMESPAN)),-            [this, loop_delay_us](bool ok) {-              if (!ok) {-                EXPECT_TRUE(ctx_->IsCancelled());-              }-              LoopUntilCancelled(loop_delay_us);-            });-      } else {-        Finish(Status::CANCELLED);+      {+        std::lock_guard<std::mutex> l(alarm_mu_);+        if (!ctx_->IsCancelled()) {",Remind me why we have to set alarms and loop instead of using a condition variable,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21770,369895924,2020-01-23T01:40:57Z,src/python/grpcio_tests/tests_aio/unit/timeout_test.py,"@@ -0,0 +1,178 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests behavior of the timeout mechanism on client side.""""""++import asyncio+import logging+import platform+import random+import unittest+import datetime++import grpc+from grpc.experimental import aio++from tests_aio.unit._test_base import AioTestBase+from tests_aio.unit import _common++_SLEEP_TIME_UNIT_S = datetime.timedelta(seconds=1).total_seconds()++_TEST_SLEEPY_UNARY_UNARY = '/test/Test/SleepyUnaryUnary'+_TEST_SLEEPY_UNARY_STREAM = '/test/Test/SleepyUnaryStream'+_TEST_SLEEPY_STREAM_UNARY = '/test/Test/SleepyStreamUnary'+_TEST_SLEEPY_STREAM_STREAM = '/test/Test/SleepyStreamStream'++_REQUEST = b'\x00\x00\x00'+_RESPONSE = b'\x01\x01\x01'++# logging.getLogger('asyncio').setLevel(logging.WARNING)++class _GenericHandler(grpc.GenericRpcHandler):++    def __init__(self):+        self._routing_table = {","In snippet below, if the class constants is referencing the methods, it will generate an `NameError`. ```pythonclass A:    val = A.func_1(1)     @staticmethod    def func_1(x):        return x*2```After review this trunk of code, you are right, we could simplify it. The routing table is now module constants, since those server method handlers are not using `self`.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21771,369905002,2020-01-23T02:18:39Z,src/python/grpcio/grpc/BUILD.bazel,"@@ -15,16 +15,25 @@ py_library(         "":interceptor"",         "":server"",         "":compression"",+        "":importer"",         ""//src/python/grpcio/grpc/_cython:cygrpc"",         ""//src/python/grpcio/grpc/experimental"",         ""//src/python/grpcio/grpc/framework"",-        ""@six_archive//:six"",+        ""@six//:six"",     ] + select({         ""//conditions:default"": [""@enum34//:enum34""],         ""//:python3"": [],     }), ) +py_library(+    name = ""importer"",+    srcs = [+      ""_importer.py"",+      ""_service_reflection.py"",+    ],+)",Should we use two `py_library` rules instead of one?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21771,369905452,2020-01-23T02:20:57Z,src/python/grpcio/grpc/_importer.py,"@@ -0,0 +1,190 @@+# Copyright 2020, the gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import sys+++def _uninstalled_protos(*args, **kwargs):+    raise NotImplementedError(+        ""Install the protobuf package to use the protos function."")+++def _uninstalled_services(*args, **kwargs):+    raise NotImplementedError(+        ""Install the protobuf package to use the services function."")+++def _uninstalled_protos_and_services(*args, **kwargs):+    raise NotImplementedError(+        ""Install the protobuf package to use the protos_and_services function."")+++def _interpreter_version_protos(*args, **kwargs):+    raise NotImplementedError(+        ""The protos function is only on available on Python 3.X interpreters."")+++def _interpreter_version_services(*args, **kwargs):+    raise NotImplementedError(+        ""The services function is only on available on Python 3.X interpreters.""+    )+++def _interpreter_version_protos_and_services(*args, **kwargs):+    raise NotImplementedError(+        ""The protos_and_services function is only on available on Python 3.X interpreters.""+    )+++if sys.version_info[0] < 3:+    protos = _interpreter_version_protos+    services = _interpreter_version_services+    protos_and_services = _interpreter_version_protos_and_services+else:+    try:+        from google import protobuf+    except (ModuleNotFoundError, ImportError) as e:+        # NOTE: It's possible that we're encountering a transitive ImportError, so+        # we check for that and re-raise if so.+        if ""google"" not in e.args[0]:","Should we use a more solid check here? Otherwise, we might accidentally swallow some exceptions.",
303201,JamesNK,https://api.github.com/repos/grpc/grpc/pulls/21653,370023152,2020-01-23T09:56:25Z,src/csharp/Grpc.Examples/MathGrpc.cs,"@@ -27,10 +27,35 @@ public static partial class Math   {     static readonly string __ServiceName = ""math.Math""; -    static readonly grpc::Marshaller<global::Math.DivArgs> __Marshaller_math_DivArgs = grpc::Marshallers.Create((arg) => global::Google.Protobuf.MessageExtensions.ToByteArray(arg), global::Math.DivArgs.Parser.ParseFrom);-    static readonly grpc::Marshaller<global::Math.DivReply> __Marshaller_math_DivReply = grpc::Marshallers.Create((arg) => global::Google.Protobuf.MessageExtensions.ToByteArray(arg), global::Math.DivReply.Parser.ParseFrom);-    static readonly grpc::Marshaller<global::Math.FibArgs> __Marshaller_math_FibArgs = grpc::Marshallers.Create((arg) => global::Google.Protobuf.MessageExtensions.ToByteArray(arg), global::Math.FibArgs.Parser.ParseFrom);-    static readonly grpc::Marshaller<global::Math.Num> __Marshaller_math_Num = grpc::Marshallers.Create((arg) => global::Google.Protobuf.MessageExtensions.ToByteArray(arg), global::Math.Num.Parser.ParseFrom);+    static void __Helper_WriteBufferMessage(global::Google.Protobuf.IMessage message, global::Grpc.Core.SerializationContext context)",Good point. The methods are no longer specific to buffer messages.,
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/21747,370283337,2020-01-23T18:29:01Z,test/cpp/end2end/test_service_impl.cc,"@@ -579,35 +591,43 @@ experimental::ServerUnaryReactor* CallbackTestServiceImpl::Echo(       Finish(Status::OK);     }     void LoopUntilCancelled(int loop_delay_us) {-      if (!ctx_->IsCancelled()) {-        alarm_.experimental().Set(-            gpr_time_add(gpr_now(GPR_CLOCK_REALTIME),-                         gpr_time_from_micros(loop_delay_us, GPR_TIMESPAN)),-            [this, loop_delay_us](bool ok) {-              if (!ok) {-                EXPECT_TRUE(ctx_->IsCancelled());-              }-              LoopUntilCancelled(loop_delay_us);-            });-      } else {-        Finish(Status::CANCELLED);+      {+        std::lock_guard<std::mutex> l(alarm_mu_);+        if (!ctx_->IsCancelled()) {","We can't do a CV wait inside a reaction because that's a blocking operation. The other approach instead of looping would be to store a continuation function in the class object and then invoke that continuation function from OnCancel, or something like that.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21771,370283823,2020-01-23T18:30:04Z,src/python/grpcio/grpc/_importer.py,"@@ -0,0 +1,190 @@+# Copyright 2020, the gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import sys+++def _uninstalled_protos(*args, **kwargs):+    raise NotImplementedError(+        ""Install the protobuf package to use the protos function."")+++def _uninstalled_services(*args, **kwargs):+    raise NotImplementedError(+        ""Install the protobuf package to use the services function."")+++def _uninstalled_protos_and_services(*args, **kwargs):+    raise NotImplementedError(+        ""Install the protobuf package to use the protos_and_services function."")+++def _interpreter_version_protos(*args, **kwargs):+    raise NotImplementedError(+        ""The protos function is only on available on Python 3.X interpreters."")+++def _interpreter_version_services(*args, **kwargs):+    raise NotImplementedError(+        ""The services function is only on available on Python 3.X interpreters.""+    )+++def _interpreter_version_protos_and_services(*args, **kwargs):+    raise NotImplementedError(+        ""The protos_and_services function is only on available on Python 3.X interpreters.""+    )+++if sys.version_info[0] < 3:+    protos = _interpreter_version_protos+    services = _interpreter_version_services+    protos_and_services = _interpreter_version_protos_and_services+else:+    try:+        from google import protobuf+    except (ModuleNotFoundError, ImportError) as e:+        # NOTE: It's possible that we're encountering a transitive ImportError, so+        # we check for that and re-raise if so.+        if ""google"" not in e.args[0]:+            raise e+        protos = _uninstalled_protos+        services = _uninstalled_services+        protos_and_services = _uninstalled_protos_and_services+    else:+        from google.protobuf import protos++        import contextlib+        import importlib+        import importlib.machinery+        import os++        from grpc import _service_reflection++        _PROTO_MODULE_SUFFIX = ""_pb2_grpc""++        def _module_name_to_proto_file(module_name):+            components = module_name.split(""."")+            proto_name = components[-1][:-1 * len(_PROTO_MODULE_SUFFIX)]+            return os.path.sep.join(components[:-1] + [proto_name + "".proto""])++        def _proto_file_to_module_name(proto_file):+            components = proto_file.split(os.path.sep)+            proto_base_name = os.path.splitext(components[-1])[0]+            return ""."".join(+                components[:-1] + [proto_base_name + _PROTO_MODULE_SUFFIX])++        @contextlib.contextmanager+        def _augmented_syspath(new_paths):+            original_sys_path = sys.path+            if new_paths is not None:+                sys.path = sys.path + new_paths+            try:+                yield+            finally:+                sys.path = original_sys_path++        # NOTE(rbellevi): module_repr is an abstract method in Python 3.3 only,b+        #   but is still picked up by the linter.+        class ProtoLoader(importlib.abc.Loader):  # pylint: disable=abstract-method++            def __init__(self, module_name, protobuf_path):+                self._module_name = module_name+                self._protobuf_path = protobuf_path++            def create_module(self, spec):+                return None++            def exec_module(self, module):+                """"""Instantiate a module identical to the generated version.+                """"""+                # NOTE(rbellevi): include_paths are propagated via sys.path.+                proto_module = protos(self._protobuf_path)+                file_descriptor = getattr(proto_module,+                                          _service_reflection.DESCRIPTOR_KEY)+                for service_descriptor in file_descriptor.services_by_name.values(+                ):+                    _service_reflection.add_service_to_module(+                        module, service_descriptor)++        class ProtoFinder(importlib.abc.MetaPathFinder):++            def find_spec(self, fullname, path, target=None):  # pylint: disable=no-self-use+                del path+                del target+                filepath = _module_name_to_proto_file(fullname)+                for search_path in sys.path:+                    try:+                        prospective_path = os.path.join(search_path, filepath)+                        os.stat(prospective_path)+                    except (FileNotFoundError, NotADirectoryError):+                        continue+                    else:+                        return importlib.machinery.ModuleSpec(+                            fullname, ProtoLoader(fullname, filepath))++        def services(protobuf_path, *, include_paths=None):+            """"""Loads gRPC service classes and functions from a .proto file.++            THIS IS AN EXPERIMENTAL API.++            Returns a module object equivalent to a ""_pb2_grpc.py"" file.++            Together with the google.protobuf.protos function, this enables+            deployment of services without a code generation step beforehand.++            Args:+              protobuf_path: A string representing the path to the desired+                "".proto"" file.+              include_paths: A sequence of strings which should be searched for+                the "".proto"" file. By default, the entries on sys.path are+                searched.++            Returns:+              A module object containing servicers, stubs, and associated+              functions for the requested "".proto"" file.+            """"""+            with _augmented_syspath(include_paths):+                module_name = _proto_file_to_module_name(protobuf_path)+                module = importlib.import_module(module_name)+                return module++        def protos_and_services(protobuf_path, *, include_paths=None):+            """"""Loads gRPC services and Protobuf messages from a .proto file++            THIS IS AN EXPERIMENTAL API.++            Equivalent to+            ```+            return google.protobuf.protos(protobuf_path, include_paths=include_paths),+                    grpc.services(protobuf_path, include_paths=include_paths)+            ```++            Args:+              protobuf_path: A string representing the path to the desired+                "".proto"" file.+              include_paths: A sequence of strings which should be searched for+                the "".proto"" file. By default, the entries on sys.path are+                searched.++            Returns:+              A 2-tuple of module objects. The first contains protocol buffer+              message classes, while the other contains servicer classes.+            """"""+            protos_ = protobuf.protos(+                protobuf_path, include_paths=include_paths)+            services_ = services(protobuf_path, include_paths=include_paths)+            return protos_, services_++        sys.meta_path.extend([ProtoFinder()])","It's not possible. The only way to implement a module loader is for the entire process. This *was* possible using Python 2's module loading infrastructure, but it's no longer possible in Python 3. If we're worried about people who don't want this Finder in their process, we can add an environment variable to turn the feature off.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21771,370289623,2020-01-23T18:42:30Z,src/python/grpcio_tests/tests/unit/_dynamic_stubs_test.py,"@@ -0,0 +1,155 @@+# Copyright 2019 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Test of dynamic stub import API.""""""++import inspect+import unittest+import logging+import contextlib+import sys+import multiprocessing+import functools+++@contextlib.contextmanager+def _protobuf_unimportable():+    original_sys_path = sys.path+    sys.path = [path for path in sys.path if ""protobuf"" not in path]+    try:+        yield+    finally:+        sys.path = original_sys_path+++def _wrap_in_subprocess(error_queue, fn):++    @functools.wraps(fn)+    def _wrapped():+        try:+            fn()+        except Exception as e:+            error_queue.put(e)+            raise++    return _wrapped+++def _run_in_subprocess(test_case):+    error_queue = multiprocessing.Queue()+    wrapped_case = _wrap_in_subprocess(error_queue, test_case)+    proc = multiprocessing.Process(target=wrapped_case)+    proc.start()+    proc.join()+    if not error_queue.empty():+        raise error_queue.get()+    assert proc.exitcode == 0, ""Process exited with code {}"".format(+        proc.exitcode)+++def _assert_unimplemented(msg_substr):+    import grpc",Yes. the intent here is only to do imports in subprocesses. Otherwise the test cases aren't actually independent.,
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/21747,370302709,2020-01-23T19:10:24Z,test/cpp/end2end/test_service_impl.cc,"@@ -579,35 +591,43 @@ experimental::ServerUnaryReactor* CallbackTestServiceImpl::Echo(       Finish(Status::OK);     }     void LoopUntilCancelled(int loop_delay_us) {-      if (!ctx_->IsCancelled()) {-        alarm_.experimental().Set(-            gpr_time_add(gpr_now(GPR_CLOCK_REALTIME),-                         gpr_time_from_micros(loop_delay_us, GPR_TIMESPAN)),-            [this, loop_delay_us](bool ok) {-              if (!ok) {-                EXPECT_TRUE(ctx_->IsCancelled());-              }-              LoopUntilCancelled(loop_delay_us);-            });-      } else {-        Finish(Status::CANCELLED);+      {+        std::lock_guard<std::mutex> l(alarm_mu_);+        if (!ctx_->IsCancelled()) {","Recap from our offline discussion:The test code would be a lot simpler if we:1) Use a condition variable that's set to true in OnCancel and delete the alarm use in this function2) Rename LoopUntilCancelled to FinishWhenCancelled3) Have FinishWhenCancelled run from another thread, and join that thread in OnDoneWe could probably get rid of alarms throughout this file, but that's out of scope for this PR.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21772,370354492,2020-01-23T21:07:20Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -221,63 +229,24 @@ def __str__(self) -> str:         return self._repr()  -class UnaryUnaryCall(Call, _base_call.UnaryUnaryCall):-    """"""Object for managing unary-unary RPC calls.+class _UnaryResponseMixin(Call):+    _call_finisher: asyncio.Task",Semantically I would prefer something like `_call_response` or `_response_call` or something like that.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/21775,370716538,2020-01-24T16:14:16Z,src/core/ext/filters/client_channel/xds/xds_client.cc,"@@ -125,31 +125,122 @@ class XdsClient::ChannelState::AdsCallState   XdsClient* xds_client() const { return chand()->xds_client(); }   bool seen_response() const { return seen_response_; } -  // If \a type_url is an unsupported type, \a nonce_for_unsupported_type and-  // \a error_for_unsupported_type will be used in the request; otherwise, the-  // nonce and error stored in each ADS call state will be used. Takes ownership-  // of \a error_for_unsupported_type.-  void SendMessageLocked(const std::string& type_url,-                         const std::string& nonce_for_unsupported_type,-                         grpc_error* error_for_unsupported_type,-                         bool is_first_message);+  void Subscribe(const std::string& type_url, const std::string& name);+  void Unsubscribe(const std::string& type_url, const std::string& name);++  bool HasSubscribedResources() const;   private:-  struct BufferedRequest {-    std::string nonce;-    grpc_error* error;+  class ResourceState : public InternallyRefCounted<ResourceState> {+   public:+    ResourceState(const std::string& type_url, const std::string& name)+        : type_url_(type_url), name_(name) {+      GRPC_CLOSURE_INIT(&timer_callback_, OnTimer, this,+                        grpc_schedule_on_exec_ctx);+    }++    void Orphan() override {+      Finish();+      Unref();+    }++    void Start(RefCountedPtr<AdsCallState> ads_calld) {+      if (sent_) return;+      sent_ = true;+      ads_calld_ = std::move(ads_calld);+      Ref().release();+      timer_pending_ = true;+      grpc_timer_init(+          &timer_,+          ExecCtx::Get()->Now() + ads_calld_->xds_client()->request_timeout_,+          &timer_callback_);+    }++    void Finish() {+      if (timer_pending_) {+        grpc_timer_cancel(&timer_);+        timer_pending_ = false;+      }+    }++   private:+    static void OnTimer(void* arg, grpc_error* error) {+      ResourceState* self = static_cast<ResourceState*>(arg);+      self->ads_calld_->xds_client()->combiner_->Run(+          GRPC_CLOSURE_INIT(&self->timer_callback_, OnTimerLocked, self,+                            nullptr),+          GRPC_ERROR_REF(error));+    }++    static void OnTimerLocked(void* arg, grpc_error* error) {+      ResourceState* self = static_cast<ResourceState*>(arg);+      if (error == GRPC_ERROR_NONE && self->timer_pending_) {+        self->timer_pending_ = false;+        char* msg;+        gpr_asprintf(+            &msg,+            ""timeout obtaining resource {type=%s name=%s} from xds server"",+            self->type_url_.c_str(), self->name_.c_str());+        grpc_error* error = GRPC_ERROR_CREATE_FROM_COPIED_STRING(msg);+        gpr_free(msg);+        if (GRPC_TRACE_FLAG_ENABLED(grpc_xds_client_trace)) {+          gpr_log(GPR_INFO, ""[xds_client %p] %s"",+                  self->ads_calld_->xds_client(), grpc_error_string(error));+        }+        if (self->type_url_ == kLdsTypeUrl || self->type_url_ == kRdsTypeUrl) {+          self->ads_calld_->xds_client()->service_config_watcher_->OnError(","No.  The rules for grpc_error ownership are documented here:https://github.com/grpc/grpc/blob/master/doc/core/grpc-error.md#ownership-rulesThis case is rule 3.  The error object started out with exactly one ref when it was created on line 184 above, and here we are passing ownership of that ref to the `OnError()` method, which is why we don't need to separately unref the error here.The grpc_error API (and the corresponding ownership rules) were established before we started converting C-core from C to C++, which is why ownership is handled this way instead of using smart pointers.  If we wind up keeping grpc_error (we may wind up replacing it with something similar to grpc::Status), then I would eventually like to see it converted to a C++ API that uses a smart pointer to track ownership.  That way, the compiler can enforce ownership, and human engineers can stop having to think about these ownership rules (and can stop spending time debugging problems that arise when we inevitably get the ownership wrong).",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/21775,370721508,2020-01-24T16:24:10Z,src/core/ext/filters/client_channel/xds/xds_client.cc,"@@ -756,8 +837,30 @@ void XdsClient::ChannelState::AdsCallState::SendMessageLocked(   } } +void XdsClient::ChannelState::AdsCallState::Subscribe(+    const std::string& type_url, const std::string& name) {+  auto& state = state_map_[type_url].subscribed_resources[name];+  if (state == nullptr) {+    state = MakeOrphanable<ResourceState>(type_url, name);+    SendMessageLocked(type_url);+  }+}++void XdsClient::ChannelState::AdsCallState::Unsubscribe(+    const std::string& type_url, const std::string& name) {+  state_map_[type_url].subscribed_resources.erase(name);+  SendMessageLocked(type_url);","I'm not sure which caller you're referring to, but I think we need to remove the resource from the set of subscribed resources before we send a message, since sending the message will look at that set to determine the list of resources to ask the server for.  In other words, doing it in this order is what causes us to unsubscribe from the resource when talking to the server; if we did it in the opposite order, we would wind up telling the server that we are still interested in this resource.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21819,372114038,2020-01-28T23:21:35Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -322,6 +366,53 @@ def __init__(self, target: Text, options: Optional[ChannelArgumentType],         self._loop = asyncio.get_event_loop()         self._channel = cygrpc.AioChannel(_common.encode(target), options,                                           credentials, self._loop)+        self._ongoing_calls = _OngoingCalls()++    async def __aenter__(self):+        """"""Starts an asynchronous context manager.++        Returns:+          Channel the channel that was instantiated.+        """"""+        return self++    async def __aexit__(self, exc_type, exc_val, exc_tb):+        """"""Finishes the asynchronous context manager by closing gracefully the channel.""""""+        await self._close()++    async def _wait_for_close_ongoing_calls(self):+        sleep_iterations_sec = 0.001++        while self._ongoing_calls.size() > 0:+            await asyncio.sleep(sleep_iterations_sec)++    async def _close(self):+        # No new calls will be accepted by the Cython channel.+        self._channel.closing()++        calls = self._ongoing_calls.calls+        for call in calls:+            call.cancel()++        try:+            await asyncio.wait_for(self._wait_for_close_ongoing_calls(),+                                   _TIMEOUT_WAIT_FOR_CLOSE_ONGOING_CALLS_SEC,","Is this a temporary step or a long term solution? If it is later, we might need a way to specify this number.We could make the close of client-side channel the same way as server's graceful shutdown.",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/21747,372248265,2020-01-29T08:36:43Z,test/cpp/end2end/test_service_impl.cc,"@@ -579,35 +591,43 @@ experimental::ServerUnaryReactor* CallbackTestServiceImpl::Echo(       Finish(Status::OK);     }     void LoopUntilCancelled(int loop_delay_us) {-      if (!ctx_->IsCancelled()) {-        alarm_.experimental().Set(-            gpr_time_add(gpr_now(GPR_CLOCK_REALTIME),-                         gpr_time_from_micros(loop_delay_us, GPR_TIMESPAN)),-            [this, loop_delay_us](bool ok) {-              if (!ok) {-                EXPECT_TRUE(ctx_->IsCancelled());-              }-              LoopUntilCancelled(loop_delay_us);-            });-      } else {-        Finish(Status::CANCELLED);+      {+        std::lock_guard<std::mutex> l(alarm_mu_);+        if (!ctx_->IsCancelled()) {+          // dont_alarm_anymore_ wouldn't be set either since that is only set+          // in OnCancel+          EXPECT_FALSE(dont_alarm_anymore_);+          alarm_.experimental().Set(+              gpr_time_add(gpr_now(GPR_CLOCK_REALTIME),+                           gpr_time_from_micros(loop_delay_us, GPR_TIMESPAN)),+              [this, loop_delay_us](bool ok) {+                if (!ok) {+                  EXPECT_TRUE(ctx_->IsCancelled());+                }+                LoopUntilCancelled(loop_delay_us);+              });+          return;+        }       }+      Finish(Status::CANCELLED);     }      CallbackTestServiceImpl* const service_;     experimental::CallbackServerContext* const ctx_;     const EchoRequest* const req_;     EchoResponse* const resp_;-    Alarm alarm_;-    bool initial_metadata_sent_{false};-    bool started_{false};-    bool on_cancel_invoked_{false};+    std::mutex alarm_mu_;+    bool dont_alarm_anymore_ /* GUARDED_BY(alarm_mu_) */ = false;","I just decided to use ctx_->IsCancelled() as the source of truth rather than on_cancel_invoked_ . And now, consistent with the discussion above, OnCancel no longer cancels alarms so alarm_mu_ is not needed anymore. There is still a cancel_mu_ to control the new cancellation CV though.'",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21714,372549305,2020-01-29T18:17:42Z,src/python/grpcio_tests/tests_aio/unit/_test_server.py,"@@ -42,9 +41,14 @@  class _TestServiceServicer(test_pb2_grpc.TestServiceServicer): -    async def UnaryCall(self, unused_request, context):+    async def UnaryCall(self, request, context):","The interop server has kind of strict spec that will fail the interop test if we don't follow. Also, the spec has been implemented in many more languages. Personally, I believe reuse a stable common test server won't hurt, if the test case doesn't require any additional modification for the test server. E.g. like the metadata test.For tests more complex than that, I agree we should use ad-hoc servers.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21714,372550859,2020-01-29T18:20:46Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -33,8 +33,8 @@ _STREAMING_OUTPUT_CALL_METHOD = '/grpc.testing.TestService/StreamingOutputCall'  _INVOCATION_METADATA = (-    ('initial-md-key', 'initial-md-value'),-    ('trailing-md-key-bin', b'\x00\x02'),+    ('x-grpc-test-echo-initial', 'initial-md-value'),+    ('x-grpc-test-echo-trailing-bin', b'\x00\x02'),",I overlooked this metadata key in the first place. There shouldn't be more upcoming conflicts between interop test and unit tests.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21714,372553021,2020-01-29T18:24:46Z,tools/interop_matrix/client_matrix.py,"@@ -56,7 +56,7 @@ def should_build_docker_interop_image_from_release_tag(lang):     'cxx': ['cxx'],  # This is actually debian8.     'go': ['go1.8', 'go1.11'],     'java': ['java'],-    'python': ['python'],+    'python': ['python', 'pythonasyncio'],","Yes. Not only between languages, but also between historical versions. https://github.com/grpc/grpc/blob/master/tools/interop_matrix/client_matrix.py#L77Yes, interop tests are executed for every push. It's named `Interop Cloud-to-Cloud Tests`.",
8952658,hcaseyal,https://api.github.com/repos/grpc/grpc/pulls/21747,372562196,2020-01-29T18:43:04Z,test/cpp/end2end/test_service_impl.cc,"@@ -476,6 +479,9 @@ experimental::ServerUnaryReactor* CallbackTestServiceImpl::Echo(       if (rpc_wait_thread_.joinable()) {         rpc_wait_thread_.join();       }+      if (finish_when_cancelled_.joinable()) {","Do you think this version of your change makes async_cancel_check_ redundant? Your change makes it so that we call ctx_->IsCancelled() asynchronously (but only in cancellation tests), so maybe that's sufficient for checking  if there are any data races associated with calling IsCancelled asynchronously.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21819,372621600,2020-01-29T20:48:15Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -322,6 +366,53 @@ def __init__(self, target: Text, options: Optional[ChannelArgumentType],         self._loop = asyncio.get_event_loop()         self._channel = cygrpc.AioChannel(_common.encode(target), options,                                           credentials, self._loop)+        self._ongoing_calls = _OngoingCalls()++    async def __aenter__(self):+        """"""Starts an asynchronous context manager.++        Returns:+          Channel the channel that was instantiated.+        """"""+        return self++    async def __aexit__(self, exc_type, exc_val, exc_tb):+        """"""Finishes the asynchronous context manager by closing gracefully the channel.""""""+        await self._close()++    async def _wait_for_close_ongoing_calls(self):+        sleep_iterations_sec = 0.001++        while self._ongoing_calls.size() > 0:+            await asyncio.sleep(sleep_iterations_sec)++    async def _close(self):+        # No new calls will be accepted by the Cython channel.+        self._channel.closing()++        calls = self._ongoing_calls.calls+        for call in calls:+            call.cancel()++        try:+            await asyncio.wait_for(self._wait_for_close_ongoing_calls(),+                                   _TIMEOUT_WAIT_FOR_CLOSE_ONGOING_CALLS_SEC,",Ok I will try to implement the same logic that is under the `server.stop` method [1][1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/experimental/aio/_server.py#L87,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21819,372621942,2020-01-29T20:49:00Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -42,6 +44,43 @@ _RESPONSE_PAYLOAD_SIZE = 42  +class Test_OngoingCalls(unittest.TestCase):","technically the class name is `_OngoingCalls`, just wanted to name the test with the classic naming pattern. Any inconvenience?",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21819,372622554,2020-01-29T20:50:30Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -42,6 +44,43 @@ _RESPONSE_PAYLOAD_SIZE = 42  +class Test_OngoingCalls(unittest.TestCase):","technically the `_OngoingCalls` is implemented as within the `channel_test.py`, having the feeling that we are not really following the convention of one python file one test file. If you want I can move the whole testing to a new file called `chanel_close_test.py`",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21819,372625731,2020-01-29T20:57:52Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -322,6 +366,53 @@ def __init__(self, target: Text, options: Optional[ChannelArgumentType],         self._loop = asyncio.get_event_loop()         self._channel = cygrpc.AioChannel(_common.encode(target), options,                                           credentials, self._loop)+        self._ongoing_calls = _OngoingCalls()++    async def __aenter__(self):+        """"""Starts an asynchronous context manager.++        Returns:+          Channel the channel that was instantiated.+        """"""+        return self++    async def __aexit__(self, exc_type, exc_val, exc_tb):+        """"""Finishes the asynchronous context manager by closing gracefully the channel.""""""+        await self._close()++    async def _wait_for_close_ongoing_calls(self):+        sleep_iterations_sec = 0.001++        while self._ongoing_calls.size() > 0:+            await asyncio.sleep(sleep_iterations_sec)","You made the point, technically an `asyncio.gather(*calls, timeout)` should work (the equivalent of Golang's wait-group). Thanks, I will try it.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21819,372646836,2020-01-29T21:43:30Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -42,6 +44,43 @@ _RESPONSE_PAYLOAD_SIZE = 42  +class Test_OngoingCalls(unittest.TestCase):",I'm in favor of splitting a new file. The `call_test.py` we had is too long for new comers to understand.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21819,372647730,2020-01-29T21:45:26Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -42,6 +44,43 @@ _RESPONSE_PAYLOAD_SIZE = 42  +class Test_OngoingCalls(unittest.TestCase):","It looks like mixing snake case and camel case. As you suggested in another comment, maybe we can rename it by the testee function instead of the class.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21803,372677474,2020-01-29T22:57:02Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -168,8 +171,9 @@ def __init__(self, cython_call: cygrpc._AioCall, metadata: MetadataType,         self._response_deserializer = response_deserializer      def __del__(self) -> None:-        if not self._cython_call.done():-            self._cancel(_GC_CANCELLATION_DETAILS)+        if hasattr(self, '_cython_call'):","Under unclear condition either the `_cython_call` is deallocated prior to `Call` or the initialization failed, I observed an exception during development that said attribute `_cython_call` not found while invoking `__del__`. It is safer to add this guarding statement.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21819,372681257,2020-01-29T23:08:21Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -322,6 +366,53 @@ def __init__(self, target: Text, options: Optional[ChannelArgumentType],         self._loop = asyncio.get_event_loop()         self._channel = cygrpc.AioChannel(_common.encode(target), options,                                           credentials, self._loop)+        self._ongoing_calls = _OngoingCalls()++    async def __aenter__(self):+        """"""Starts an asynchronous context manager.++        Returns:+          Channel the channel that was instantiated.+        """"""+        return self++    async def __aexit__(self, exc_type, exc_val, exc_tb):+        """"""Finishes the asynchronous context manager by closing gracefully the channel.""""""+        await self._close()++    async def _wait_for_close_ongoing_calls(self):+        sleep_iterations_sec = 0.001++        while self._ongoing_calls.size() > 0:+            await asyncio.sleep(sleep_iterations_sec)++    async def _close(self):+        # No new calls will be accepted by the Cython channel.+        self._channel.closing()++        calls = self._ongoing_calls.calls+        for call in calls:+            call.cancel()++        try:+            await asyncio.wait_for(self._wait_for_close_ongoing_calls(),+                                   _TIMEOUT_WAIT_FOR_CLOSE_ONGOING_CALLS_SEC,","So you are proposing on not waiting until all calls claim that have terminated and return immediately?My point for making this `wait_for` was not having a data race with the channel destruction event sent by the channel to all of the still ongoing calls with the event that would cancel explicitly the call, By having this small timeout we can almost guarantee that cancellations will happen and any RPC will receive the channel destruction event. WDYT?",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21803,372682682,2020-01-29T23:12:47Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -384,16 +397,21 @@ cdef class _AioCall(GrpcCallWrapper):             send_close_op,         ) -        # Sends out the request message.-        await execute_batch(self,-                            outbound_ops,-                            self._loop)--        # Receives initial metadata.-        self._set_initial_metadata(-            await _receive_initial_metadata(self,-                                            self._loop),-        )+        try:+            # Sends out the request message.+            await execute_batch(self,+                                outbound_ops,+                                self._loop)++            # Receives initial metadata.+            self._set_initial_metadata(+                await _receive_initial_metadata(self,+                                                self._loop),+            )+        except ExecuteBatchError as batch_error:+            # Core should explain why this batch failed+            await status_task+            assert self._status.code() != StatusCode.ok","Having the feeling that the main purpose of this exception is not progressing the exception to the up layers, I guess that is what the following sentence tries to explain```# Core should explain why this batch failed```Regarding guards, how much value is adding to us? does it mean that guards should be/can be explicitly used during the code? if not, why in that use case is so important?",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/21747,372810556,2020-01-30T08:22:07Z,test/cpp/end2end/test_service_impl.cc,"@@ -476,6 +479,9 @@ experimental::ServerUnaryReactor* CallbackTestServiceImpl::Echo(       if (rpc_wait_thread_.joinable()) {         rpc_wait_thread_.join();       }+      if (finish_when_cancelled_.joinable()) {","I'd like to keep that. As you pointed out, we're only checking ctx->IsCancelled in the Rpc function for actual cancellation cases whereas the AsyncCancelCheck thread checks it for both cancellation and non-cancellation cases. I can understand the merit of killing the async cancel check since technically its join is a potentially blocking operation (unlike the joins of finish when canceled and rpc wait thread) but I think it still provides test signal.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21714,372883767,2020-01-30T10:58:53Z,tools/dockerfile/interoptest/grpc_interop_pythonasyncio/Dockerfile,"@@ -0,0 +1,68 @@+# Copyright 2019 The gRPC Authors","this Dockerfile seems just copy-pasted.Instead, you should introduce a new template (similar to this one https://github.com/grpc/grpc/blob/master/templates/tools/dockerfile/interoptest/grpc_interop_python/Dockerfile.template) and use it to generate this Dockerfile (otherwise we'll end up with maintenance hell).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21714,372885888,2020-01-30T11:03:27Z,tools/dockerfile/interoptest/grpc_interop_pythonasyncio/build_interop.sh,"@@ -0,0 +1,32 @@+#!/bin/bash",Seems identical to https://github.com/grpc/grpc/blob/master/tools/dockerfile/interoptest/grpc_interop_python/build_interop.sh Consider introducing build_interop.sh.template and generate both files to avoid the need to maintain these two files separately?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21819,373063952,2020-01-30T16:44:19Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -322,6 +366,53 @@ def __init__(self, target: Text, options: Optional[ChannelArgumentType],         self._loop = asyncio.get_event_loop()         self._channel = cygrpc.AioChannel(_common.encode(target), options,                                           credentials, self._loop)+        self._ongoing_calls = _OngoingCalls()++    async def __aenter__(self):+        """"""Starts an asynchronous context manager.++        Returns:+          Channel the channel that was instantiated.+        """"""+        return self++    async def __aexit__(self, exc_type, exc_val, exc_tb):+        """"""Finishes the asynchronous context manager by closing gracefully the channel.""""""+        await self._close()++    async def _wait_for_close_ongoing_calls(self):+        sleep_iterations_sec = 0.001++        while self._ongoing_calls.size() > 0:+            await asyncio.sleep(sleep_iterations_sec)++    async def _close(self):+        # No new calls will be accepted by the Cython channel.+        self._channel.closing()++        calls = self._ongoing_calls.calls+        for call in calls:+            call.cancel()++        try:+            await asyncio.wait_for(self._wait_for_close_ongoing_calls(),+                                   _TIMEOUT_WAIT_FOR_CLOSE_ONGOING_CALLS_SEC,","currently, if I'm not missing something, cancellation implies calling the cancel for the Cython call and canceling the Asyncio task that is holding the execution.  Which would imply having to deal with asynchronicity.By having this asynchronicity, a data race between the event sent by the Core for telling that the call has been ""aborted"" because of the shutdown and the event for cancelling the Asyncio task could happen.What event would arrive first? I guess that seeing the current implementation of Asyncio the cancel is the first thing that will arrive [1], so the future would be canceled immediately and a later event from the core would be basically ""discarded"".I guess that my concern was on not having to be coupled to an implementation detail, but if you agree that we can take for guaranteed that cancellation will happen first let's go for it.As a side conversation, but related. What do we need to cancel the RPC? should the RPCs that are ongoing be aborted and not canceled? should we simply destroy the channel and wait till all of the calls that were ongoing are reporting that are in a final status?Indeed if we go for cancellation? should we just wait till all of the calls that were ongoing are claiming that have been canceled?I guess that the `_TIMEOUT_WAIT_FOR_CLOSE_ONGOING_CALLS_SEC` is not really necessary, so we can just do a `asyncio.wait` without a timeout, all calls would need to be eventually aborted or canceled.So the question would be, let's keep using the cancellation or let's use the abort event?[1] https://github.com/python/cpython/blob/master/Lib/asyncio/tasks.py#L257",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21714,373104973,2020-01-30T18:01:58Z,tools/run_tests/run_interop_tests.py,"@@ -668,6 +668,60 @@ def __str__(self):         return 'python'  +class PythonAsyncIOClient:",Changed.My bad. I saw HTTP clients are named after `*Client`.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21714,373107543,2020-01-30T18:07:31Z,tools/dockerfile/interoptest/grpc_interop_pythonasyncio/Dockerfile,"@@ -0,0 +1,68 @@+# Copyright 2019 The gRPC Authors",My bad. Created a template for pythonasyncio Dockerfile.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21714,373116524,2020-01-30T18:26:13Z,src/python/grpcio_tests/commands.py,"@@ -235,12 +235,15 @@ class RunInterop(test.test):     description = 'run interop test client/server'     user_options = [('args=', 'a', 'pass-thru arguments for the client/server'),                     ('client', 'c', 'flag indicating to run the client'),-                    ('server', 's', 'flag indicating to run the server')]+                    ('server', 's', 'flag indicating to run the server'),+                    ('asyncio', 'i', 'flag indicating to run the asyncio stack')",What I meant is that it's better to have no short-form option than having a non-intutive short-form option. So I'd prefer to only have the long-form option if possible. Leaving up to you.,
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/21841,373200386,2020-01-30T21:22:31Z,test/cpp/qps/server_async.cc,"@@ -162,7 +162,10 @@ class AsyncQpsServerTest final : public grpc::testing::Server {       std::lock_guard<std::mutex> lock((*ss)->mutex);       (*ss)->shutdown = true;     }-    std::thread shutdown_thread(&AsyncQpsServerTest::ShutdownThreadFunc, this);+    // TODO(vjpai): Remove the following deadline and allow full proper+    // shutdown.","Ideally we should be able to remove the deadline on it. The deadline prevents it from taking too long but, on the other hand, taking too long might be a signal that something bad is happening in the stack. Note that this is a pre-existing TODO; it just moved from the ShutdownThreadFunc where it was an async TODO to the destructor where it is a synchronously-executed TODO.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21819,373265372,2020-01-31T00:31:22Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -322,6 +366,53 @@ def __init__(self, target: Text, options: Optional[ChannelArgumentType],         self._loop = asyncio.get_event_loop()         self._channel = cygrpc.AioChannel(_common.encode(target), options,                                           credentials, self._loop)+        self._ongoing_calls = _OngoingCalls()++    async def __aenter__(self):+        """"""Starts an asynchronous context manager.++        Returns:+          Channel the channel that was instantiated.+        """"""+        return self++    async def __aexit__(self, exc_type, exc_val, exc_tb):+        """"""Finishes the asynchronous context manager by closing gracefully the channel.""""""+        await self._close()++    async def _wait_for_close_ongoing_calls(self):+        sleep_iterations_sec = 0.001++        while self._ongoing_calls.size() > 0:+            await asyncio.sleep(sleep_iterations_sec)++    async def _close(self):+        # No new calls will be accepted by the Cython channel.+        self._channel.closing()++        calls = self._ongoing_calls.calls+        for call in calls:+            call.cancel()++        try:+            await asyncio.wait_for(self._wait_for_close_ongoing_calls(),+                                   _TIMEOUT_WAIT_FOR_CLOSE_ONGOING_CALLS_SEC,","I think we are considering different layers here. I mostly focus on will the cancellation proceed in __Core__, if there isn't an explicit sleep. And based on your comment, it is also true that even if we invoked `cancel()`, those __tasks__ are not actually being injected with `CancelledError` yet.In `OngoingCalls`, the list contains `Call` objects which is managed by us. And we implemented their `cancel()` method. So, I wonder if we controls the order of cancellation in Core and in AsyncIO?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21819,373273637,2020-01-31T01:03:59Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -28,8 +29,37 @@                       SerializingFunction) from ._utils import _timeout_to_deadline +_TIMEOUT_WAIT_FOR_CLOSE_ONGOING_CALLS_SEC = 0.1 _IMMUTABLE_EMPTY_TUPLE = tuple() +_LOGGER = logging.getLogger(__name__)+++class _OngoingCalls:+    """"""Internal class used for have visibility of the ongoing calls.""""""++    _calls: Sequence[_base_call.RpcContext]","Since this as an internal member of an internal class, we should probably be more specific and use `List` rather than `Sequence`.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21819,373275856,2020-01-31T01:13:58Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -28,8 +29,37 @@                       SerializingFunction) from ._utils import _timeout_to_deadline +_TIMEOUT_WAIT_FOR_CLOSE_ONGOING_CALLS_SEC = 0.1 _IMMUTABLE_EMPTY_TUPLE = tuple() +_LOGGER = logging.getLogger(__name__)+++class _OngoingCalls:+    """"""Internal class used for have visibility of the ongoing calls.""""""++    _calls: Sequence[_base_call.RpcContext]++    def __init__(self):+        self._calls = []++    def _remove_call(self, call: _base_call.RpcContext):+        self._calls.remove(call)",`list` is implemented as a dynamic array. [`remove` is going to be `O(n)`](https://github.com/python/cpython/blob/master/Objects/listobject.c#L2619).  Seems like `set` would be more optimal here. [It's implemented as a dynamically resized hashmap](https://github.com/python/cpython/blob/master/Objects/setobject.c). Insert should still be amortized `O(1)` and remove will be `O(1)`.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21834,373564065,2020-01-31T16:20:30Z,templates/src/objective-c/BoringSSL-GRPC.podspec.template,"@@ -141,8 +139,8 @@                         'src/crypto/**/*.{h,c,cc}',                         # We have to include fiat because spake25519 depends on it                         'src/third_party/fiat/*.{h,c,cc}',-                        # Include the err_data.c generated in prepare_command below-                        'src/err_data.c'+                        # Include the err_data.c pre-generated in boringssl's master-with-bazel branch+                        'err_data.c'","Yes, it's kind of required, because this podspec has been adjusted to expect the directory layout of `master-with-bazel` branch.Also, the sanity tests enforce that third_party/boringssl-with-bazel submodule is at the same commit as this podspec.Btw, the `third_party/boringssl` submodule no longer exists - it's been removed in my unification PR.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21803,373629206,2020-01-31T18:47:32Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/socket.pyx.pxi,"@@ -167,6 +167,11 @@ cdef class _AsyncioSocket:             self._py_socket.close()      def _new_connection_callback(self, object reader, object writer):+        # Close the connection if server is not started yet.","The add port methods on servers will bind the underlying TCP port. Before this change, the server will start to handle new connections (invoking this callback) even before `await server.start()`, but none of the resources are ready. This behavior discrepancy is due to the semantic difference between Core's `listen` & `accept` and AsyncIO's `asyncio.start_server`.",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/21393,373648080,2020-01-31T19:31:20Z,src/core/ext/filters/client_channel/resolver/dns/c_ares/dns_resolver_ares.cc,"@@ -223,100 +223,88 @@ void AresDnsResolver::OnNextResolutionLocked(void* arg, grpc_error* error) {   r->Unref(DEBUG_LOCATION, ""next_resolution_timer""); } -bool ValueInJsonArray(grpc_json* array, const char* value) {-  for (grpc_json* entry = array->child; entry != nullptr; entry = entry->next) {-    if (entry->type == GRPC_JSON_STRING && strcmp(entry->value, value) == 0) {+bool ValueInJsonArray(const Json::Array& array, const char* value) {+  for (const Json& entry : array) {+    if (entry.type() == Json::Type::STRING && entry.string_value() == value) {       return true;     }   }   return false; } -char* ChooseServiceConfig(char* service_config_choice_json,-                          grpc_error** error) {-  grpc_json* choices_json = grpc_json_parse_string(service_config_choice_json);-  if (choices_json == nullptr) {-    *error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(-        ""Service Config JSON Parsing, error: could not parse"");-    return nullptr;-  }-  if (choices_json->type != GRPC_JSON_ARRAY) {+std::string ChooseServiceConfig(char* service_config_choice_json,+                                grpc_error** error) {+  Json json = Json::Parse(service_config_choice_json, error);+  if (*error != GRPC_ERROR_NONE) return nullptr;+  if (json.type() != Json::Type::ARRAY) {     *error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(         ""Service Config Choices, error: should be of type array"");     return nullptr;   }-  char* service_config = nullptr;+  std::string service_config = nullptr;   InlinedVector<grpc_error*, 4> error_list;-  bool found_choice = false;  // have we found a choice?","looks like there is a semantic change here. Earlier, we were validating all the choices, but now we choose the first choice and exit. The proposal suggests that we should be using the first choice that fits, so the new code seems right.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21819,374128505,2020-02-03T14:22:36Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -28,8 +29,37 @@                       SerializingFunction) from ._utils import _timeout_to_deadline +_TIMEOUT_WAIT_FOR_CLOSE_ONGOING_CALLS_SEC = 0.1 _IMMUTABLE_EMPTY_TUPLE = tuple() +_LOGGER = logging.getLogger(__name__)+++class _OngoingCalls:+    """"""Internal class used for have visibility of the ongoing calls.""""""++    _calls: Sequence[_base_call.RpcContext]++    def __init__(self):+        self._calls = []++    def _remove_call(self, call: _base_call.RpcContext):+        self._calls.remove(call)","Yeps, I thought the same as you  ... the only reason because I've made this using a list was for maintaining the order that the calls were created, so if the user made to calls with the list we will guarantee that the cancellation will happen in some kind of order. But seeing that1) Current implementation iterates from left to right, so a weird preserving order will be meet2) When the grace period is finished, cancellation happens within the same loop iteration so calls won't have any chance to react on that.Moving to use a set.Thanks for the feedback.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21819,374135916,2020-02-03T14:35:37Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -322,6 +366,53 @@ def __init__(self, target: Text, options: Optional[ChannelArgumentType],         self._loop = asyncio.get_event_loop()         self._channel = cygrpc.AioChannel(_common.encode(target), options,                                           credentials, self._loop)+        self._ongoing_calls = _OngoingCalls()++    async def __aenter__(self):+        """"""Starts an asynchronous context manager.++        Returns:+          Channel the channel that was instantiated.+        """"""+        return self++    async def __aexit__(self, exc_type, exc_val, exc_tb):+        """"""Finishes the asynchronous context manager by closing gracefully the channel.""""""+        await self._close()++    async def _wait_for_close_ongoing_calls(self):+        sleep_iterations_sec = 0.001++        while self._ongoing_calls.size() > 0:+            await asyncio.sleep(sleep_iterations_sec)++    async def _close(self):+        # No new calls will be accepted by the Cython channel.+        self._channel.closing()++        calls = self._ongoing_calls.calls+        for call in calls:+            call.cancel()++        try:+            await asyncio.wait_for(self._wait_for_close_ongoing_calls(),+                                   _TIMEOUT_WAIT_FOR_CLOSE_ONGOING_CALLS_SEC,","Finally removed the last `wait` since the `cancel` will automatically call behind the scenes the `_set_status` which will automatically remove that call from the `ongoing_calls` list.So does not make sense on having to make a wait, indeed a `wait` raises an error saying that the input of _awaitables_ was an empty list, doing the `if self._ongoing_call.calls` could be an option but since won't be possible to test that - all calls would be deallocated as I said - I preferred to not leave a code that nowadays will be never executed. ",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21819,374271872,2020-02-03T18:41:36Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -577,6 +577,106 @@ class Interceptor(aio.UnaryUnaryClientInterceptor):              self.assertEqual(await call.code(), grpc.StatusCode.OK) +    async def test_add_done_callback_before_finishes(self):+        called = asyncio.Event()+        interceptor_can_continue = asyncio.Event()++        def callback(call):+            called.set()++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):++                await interceptor_can_continue.wait()+                call = await continuation(client_call_details, request)+                return call++        async with aio.insecure_channel(self._server_target,+                                        interceptors=[Interceptor()+                                                     ]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())+            call.add_done_callback(callback)+            interceptor_can_continue.set()+            await call++            try:+                await asyncio.wait_for(called.wait(), timeout=0.1)+            except:+                self.fail(""Callback was not called"")++    async def test_add_done_callback_after_finishes(self):+        called = asyncio.Event()++        def callback(call):+            called.set()++        class Interceptor(aio.UnaryUnaryClientInterceptor):++            async def intercept_unary_unary(self, continuation,+                                            client_call_details, request):++                call = await continuation(client_call_details, request)+                return call++        async with aio.insecure_channel(self._server_target,+                                        interceptors=[Interceptor()+                                                     ]) as channel:++            multicallable = channel.unary_unary(+                '/grpc.testing.TestService/UnaryCall',+                request_serializer=messages_pb2.SimpleRequest.SerializeToString,+                response_deserializer=messages_pb2.SimpleResponse.FromString)+            call = multicallable(messages_pb2.SimpleRequest())++            await call++            call.add_done_callback(callback)++            try:+                await asyncio.wait_for(called.wait(), timeout=0.1)","Let's use a constant and make the timeout longer.Yes., if the test case work, this operation takes microseconds. But it might introduce flake in slower machines (CI workers).",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21819,374272249,2020-02-03T18:42:25Z,src/python/grpcio_tests/tests_aio/unit/channel_test.py,"@@ -15,11 +15,11 @@  import logging import os-import threading import unittest  import grpc from grpc.experimental import aio+from grpc.experimental.aio import _base_call",nit: unused import,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21819,374340406,2020-02-03T21:05:18Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -307,6 +351,62 @@ def __init__(self, target: Text, options: Optional[ChannelArgumentType],         self._loop = asyncio.get_event_loop()         self._channel = cygrpc.AioChannel(_common.encode(target), options,                                           credentials, self._loop)+        self._ongoing_calls = _OngoingCalls()++    async def __aenter__(self):+        """"""Starts an asynchronous context manager.++        Returns:+          Channel the channel that was instantiated.+        """"""+        return self++    async def __aexit__(self, exc_type, exc_val, exc_tb):+        """"""Finishes the asynchronous context manager by closing the channel.++        Still active RPCs will be cancelled.+        """"""+        await self._close(None)++    async def _close(self, grace):+        if self._channel.closed():+            return++        # No new calls will be accepted by the Cython channel.+        self._channel.closing()++        if grace:+            # pylint: disable=unused-variable+            _, pending = await asyncio.wait(self._ongoing_calls.calls,+                                            timeout=grace,+                                            loop=self._loop)++            if not pending:+                return++        # A new set is created acting as a shallow copy because+        # when cancellation happens the calls are automatically+        # removed from the originally set.+        calls = WeakSet(data=self._ongoing_calls.calls)+        for call in calls:+            call.cancel()","I haven't experiment much about `WeakRef` with GC behaviors. Will the `close` code path be ran as a part of `__del__`? If so, there might not be a reliable guarantee that all `Call` objects are still intact. My question is about if the referenced `Call` object in the `WeakSet` got deallocated, maybe it will generate a weird exception? Should we add a if condition to check if `call` is not `None`?",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21819,374344794,2020-02-03T21:15:27Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -307,6 +351,62 @@ def __init__(self, target: Text, options: Optional[ChannelArgumentType],         self._loop = asyncio.get_event_loop()         self._channel = cygrpc.AioChannel(_common.encode(target), options,                                           credentials, self._loop)+        self._ongoing_calls = _OngoingCalls()++    async def __aenter__(self):+        """"""Starts an asynchronous context manager.++        Returns:+          Channel the channel that was instantiated.+        """"""+        return self++    async def __aexit__(self, exc_type, exc_val, exc_tb):+        """"""Finishes the asynchronous context manager by closing the channel.++        Still active RPCs will be cancelled.+        """"""+        await self._close(None)++    async def _close(self, grace):+        if self._channel.closed():+            return++        # No new calls will be accepted by the Cython channel.+        self._channel.closing()++        if grace:+            # pylint: disable=unused-variable+            _, pending = await asyncio.wait(self._ongoing_calls.calls,+                                            timeout=grace,+                                            loop=self._loop)++            if not pending:+                return++        # A new set is created acting as a shallow copy because+        # when cancellation happens the calls are automatically+        # removed from the originally set.+        calls = WeakSet(data=self._ongoing_calls.calls)+        for call in calls:+            call.cancel()","> My question is about if the referenced Call object in the WeakSet got deallocated, maybe it will generate a weird exception? Should we add a if condition to check if call is not None?In case of being deallocated won't be returned in the iteration, `None` objects are filtered automatically by `WeaKSet` [1].[1] https://github.com/python/cpython/blob/master/Lib/_weakrefset.py#L62",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21855,374361209,2020-02-03T21:50:48Z,src/python/grpcio_tests/tests_aio/unit/abort_test.py,"@@ -136,6 +136,7 @@ class TestAbort(AioTestBase):          with self.assertRaises(aio.AioRpcError) as exception_context:             await call.read()+            await call.read()","There is a race condition.If the status arrived before calling the first `call.read()`, even the response message is buffered in Core, it will still raise an `AioRpcError` exception. The result of the data race does change due to implementation details (before this PR, the status always arrive first; after this PR, the message always arrive first).I hope this test case can be neutral to that data race, and save us some head scratching time debugging it.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21809,374392347,2020-02-03T23:05:46Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -174,6 +208,15 @@ cdef class _ServicerContext:     def set_details(self, str details):         self._rpc_state.status_details = details +    def set_compression(self, object compression):+        if self._rpc_state.metadata_sent:+            raise RuntimeError('Compression setting must be specified before sending initial metadata')+        else:+            self._rpc_state.compression_algorithm = compression++    def disable_next_message_compression(self):+        self._rpc_state.disable_next_compression = True","This function is useful against attacks to the compression traffic: https://github.com/grpc/grpc/blob/master/doc/compression.md#specific-disabling-of-compressionThis function marks that the next message should be sent without compression, and it should work for unary response and stream responses.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21809,374393149,2020-02-03T23:07:53Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -541,7 +584,7 @@ cdef CallbackFailureHandler SERVER_SHUTDOWN_FAILURE_HANDLER = CallbackFailureHan cdef class AioServer:      def __init__(self, loop, thread_pool, generic_handlers, interceptors,-                 options, maximum_concurrent_rpcs, compression):+                 options, maximum_concurrent_rpcs):","It is still supported. I found the compression can be specified in options (aka. channel argument), so I merged the compression channel argument in `Server.__init__` from `_server.py`. A new test case added `test_server_default_compression_algorithm`.Also, there are three tuning options for compression: 1. set the default algorithm; 2. set the desired compression level (see [doc](https://github.com/grpc/grpc/blob/master/doc/compression_cookbook.md)); 3. supported algorithms. I hope we can promote them to first class API in future. Do you think they are helpful?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21809,374404189,2020-02-03T23:41:45Z,src/python/grpcio_tests/tests_aio/unit/compression_test.py,"@@ -0,0 +1,174 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests behavior around the compression mechanism.""""""++import asyncio+import logging+import platform+import random+import unittest++import grpc+from grpc.experimental import aio++from tests_aio.unit._test_base import AioTestBase+from tests_aio.unit import _common++_GZIP_CHANNEL_ARGUMENT = ('grpc.default_compression_algorithm', 2)+_GZIP_DISABLED_CHANNEL_ARGUMENT = ('grpc.compression_enabled_algorithms_bitset',+                                   3)+_DEFLATE_DISABLED_CHANNEL_ARGUMENT = (+    'grpc.compression_enabled_algorithms_bitset', 5)++_TEST_UNARY_UNARY = '/test/TestUnaryUnary'+_TEST_SET_COMPRESSION = '/test/TestSetCompression'+_TEST_DISABLE_COMPRESSION_UNARY = '/test/TestDisableCompressionUnary'+_TEST_DISABLE_COMPRESSION_STREAM = '/test/TestDisableCompressionStream'++_REQUEST = b'\x01' * 100+_RESPONSE = b'\x02' * 100+++async def _test_unary_unary(unused_request, unused_context):+    return _RESPONSE+++async def _test_set_compression(unused_request_iterator, context):+    assert _REQUEST == await context.read()+    context.set_compression(grpc.Compression.Deflate)+    await context.write(_RESPONSE)+    try:+        context.set_compression(grpc.Compression.Deflate)+    except RuntimeError:+        pass","Comment added:```        # NOTE(lidiz) Testing if the servicer context raises exception when        # the set_compression method is called after initial_metadata sent.        # After the initial_metadata sent, the server-side has no control over        # which compression algorithm it should use.```",
35056280,srini100,https://api.github.com/repos/grpc/grpc/pulls/21869,374423630,2020-02-04T00:51:43Z,tools/run_tests/run_xds_tests.py,"@@ -0,0 +1,571 @@+#!/usr/bin/env python+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Run xDS integration tests on GCP using Traffic Director.""""""++import argparse+import googleapiclient.discovery+import grpc+import logging+import os+import shlex+import subprocess+import sys+import time++from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc++logger = logging.getLogger(__name__)+console_handler = logging.StreamHandler()+logger.addHandler(console_handler)++argp = argparse.ArgumentParser(description='Run xDS interop tests on GCP')+argp.add_argument('--project_id', help='GCP project id')+argp.add_argument(+    '--gcp_suffix',+    default='',+    help='Optional suffix for all generated GCP resource names. Useful to ensure '+    'distinct names across test runs.')+argp.add_argument('--test_case',+                  default=None,+                  choices=['all', 'ping_pong', 'round_robin'])+argp.add_argument(+    '--client_cmd',+    default=None,+    help='Command to launch xDS test client. This script will fill in '+    '{service_host}, {service_port},{stats_port} and {qps} parameters using '+    'str.format()')+argp.add_argument('--bootstrap_file',+                  default=None,+                  help='Path to xDS bootstrap file.')+argp.add_argument('--zone', default='us-central1-a')+argp.add_argument('--qps', default=10, help='Client QPS')+argp.add_argument(+    '--wait_for_backend_sec',+    default=900,+    help='Time limit for waiting for created backend services to report healthy '+    'when launching test suite')+argp.add_argument(+    '--keep_gcp_resources',+    default=False,+    action='store_true',+    help=+    'Leave GCP VMs and configuration running after test. Default behavior is '+    'to delete when tests complete.')+argp.add_argument(+    '--tolerate_gcp_errors',+    default=False,+    action='store_true',+    help=+    'Continue with test even when an error occurs during setup. Intended for '+    'manual testing, where attempts to recreate any GCP resources already '+    'existing will result in an error')+argp.add_argument('--verbose',+                  help='verbose log output',+                  default=False,+                  action=""store_true"")+args = argp.parse_args()++if args.verbose:+    logger.setLevel(logging.DEBUG)++PROJECT_ID = args.project_id+ZONE = args.zone+QPS = args.qps+TEST_CASE = args.test_case+BOOTSTRAP_FILE = args.bootstrap_file+CLIENT_CMD = args.client_cmd+WAIT_FOR_BACKEND_SEC = args.wait_for_backend_sec+TEMPLATE_NAME = 'test-template' + args.gcp_suffix+INSTANCE_GROUP_NAME = 'test-ig' + args.gcp_suffix+HEALTH_CHECK_NAME = 'test-hc' + args.gcp_suffix+FIREWALL_RULE_NAME = 'test-fw-rule' + args.gcp_suffix+BACKEND_SERVICE_NAME = 'test-backend-service' + args.gcp_suffix+URL_MAP_NAME = 'test-map' + args.gcp_suffix+SERVICE_HOST = 'grpc-test' + args.gcp_suffix+TARGET_PROXY_NAME = 'test-target-proxy' + args.gcp_suffix+FORWARDING_RULE_NAME = 'test-forwarding-rule' + args.gcp_suffix+KEEP_GCP_RESOURCES = args.keep_gcp_resources+TOLERATE_GCP_ERRORS = args.tolerate_gcp_errors+SERVICE_PORT = 55551+STATS_PORT = 55552+INSTANCE_GROUP_SIZE = 2+WAIT_FOR_OPERATION_SEC = 60+NUM_TEST_RPCS = 10 * QPS+WAIT_FOR_STATS_SEC = 30+++def get_client_stats(num_rpcs, timeout_sec):+    with grpc.insecure_channel('localhost:%d' % STATS_PORT) as channel:+        stub = test_pb2_grpc.LoadBalancerStatsServiceStub(channel)+        request = messages_pb2.LoadBalancerStatsRequest()+        request.num_rpcs = num_rpcs+        request.timeout_sec = timeout_sec+        try:+            response = stub.GetClientStats(request, wait_for_ready=True)+            logger.debug('Invoked GetClientStats RPC: %s', response)+            return response+        except grpc.RpcError as rpc_error:+            raise Exception('GetClientStats RPC failed')+++def wait_until_only_given_backends_receive_load(backends, timeout_sec):+    start_time = time.time()+    error_msg = None+    while time.time() - start_time <= timeout_sec:+        error_msg = None+        stats = get_client_stats(max(len(backends), 1), timeout_sec)+        rpcs_by_peer = stats.rpcs_by_peer+        for backend in backends:+            if backend not in rpcs_by_peer:+                error_msg = 'Backend %s did not receive load' % backend+                break+        if not error_msg and len(rpcs_by_peer) > len(backends):+            error_msg = 'Unexpected backend received load: %s' % rpcs_by_peer+        if not error_msg:+            return+    raise Exception(error_msg)+++def test_ping_pong(backends, num_rpcs, stats_timeout_sec):+    start_time = time.time()+    error_msg = None+    while time.time() - start_time <= stats_timeout_sec:+        error_msg = None+        stats = get_client_stats(num_rpcs, stats_timeout_sec)+        rpcs_by_peer = stats.rpcs_by_peer+        for backend in backends:+            if backend not in rpcs_by_peer:+                error_msg = 'Backend %s did not receive load' % backend+                break+        if not error_msg and len(rpcs_by_peer) > len(backends):+            error_msg = 'Unexpected backend received load: %s' % rpcs_by_peer+        if not error_msg:+            return+    raise Exception(error_msg)+++def test_round_robin(backends, num_rpcs, stats_timeout_sec):+    threshold = 1+    wait_until_only_given_backends_receive_load(backends, stats_timeout_sec)+    stats = get_client_stats(num_rpcs, stats_timeout_sec)+    requests_received = [stats.rpcs_by_peer[x] for x in stats.rpcs_by_peer]+    total_requests_received = sum(+        [stats.rpcs_by_peer[x] for x in stats.rpcs_by_peer])+    if total_requests_received != num_rpcs:+        raise Exception('Unexpected RPC failures', stats)+    expected_requests = total_requests_received / len(backends)+    for backend in backends:+        if abs(stats.rpcs_by_peer[backend] - expected_requests) > threshold:+            raise Exception(+                'RPC peer distribution differs from expected by more than %d for backend %s (%s)',+                threshold, backend, stats)+++def create_instance_template(compute, name, grpc_port, project):+    config = {+        'name': name,+        'properties': {+            'tags': {+                'items': ['grpc-td-tag']+            },+            'machineType': 'n1-standard-1',","Recently saw this ""expect E2 to deliver similar performance to N1, at a significantly lower cost.""Change it to e2-standard-1?",
35056280,srini100,https://api.github.com/repos/grpc/grpc/pulls/21869,374425478,2020-02-04T00:58:59Z,tools/run_tests/run_xds_tests.py,"@@ -0,0 +1,571 @@+#!/usr/bin/env python+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Run xDS integration tests on GCP using Traffic Director.""""""++import argparse+import googleapiclient.discovery+import grpc+import logging+import os+import shlex+import subprocess+import sys+import time++from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc++logger = logging.getLogger(__name__)+console_handler = logging.StreamHandler()+logger.addHandler(console_handler)++argp = argparse.ArgumentParser(description='Run xDS interop tests on GCP')+argp.add_argument('--project_id', help='GCP project id')+argp.add_argument(+    '--gcp_suffix',+    default='',+    help='Optional suffix for all generated GCP resource names. Useful to ensure '+    'distinct names across test runs.')+argp.add_argument('--test_case',+                  default=None,+                  choices=['all', 'ping_pong', 'round_robin'])+argp.add_argument(+    '--client_cmd',+    default=None,+    help='Command to launch xDS test client. This script will fill in '+    '{service_host}, {service_port},{stats_port} and {qps} parameters using '+    'str.format()')+argp.add_argument('--bootstrap_file',+                  default=None,+                  help='Path to xDS bootstrap file.')+argp.add_argument('--zone', default='us-central1-a')+argp.add_argument('--qps', default=10, help='Client QPS')+argp.add_argument(+    '--wait_for_backend_sec',+    default=900,+    help='Time limit for waiting for created backend services to report healthy '+    'when launching test suite')+argp.add_argument(+    '--keep_gcp_resources',+    default=False,+    action='store_true',+    help=+    'Leave GCP VMs and configuration running after test. Default behavior is '+    'to delete when tests complete.')+argp.add_argument(+    '--tolerate_gcp_errors',+    default=False,+    action='store_true',+    help=+    'Continue with test even when an error occurs during setup. Intended for '+    'manual testing, where attempts to recreate any GCP resources already '+    'existing will result in an error')+argp.add_argument('--verbose',+                  help='verbose log output',+                  default=False,+                  action=""store_true"")+args = argp.parse_args()++if args.verbose:+    logger.setLevel(logging.DEBUG)++PROJECT_ID = args.project_id+ZONE = args.zone+QPS = args.qps+TEST_CASE = args.test_case+BOOTSTRAP_FILE = args.bootstrap_file+CLIENT_CMD = args.client_cmd+WAIT_FOR_BACKEND_SEC = args.wait_for_backend_sec+TEMPLATE_NAME = 'test-template' + args.gcp_suffix+INSTANCE_GROUP_NAME = 'test-ig' + args.gcp_suffix+HEALTH_CHECK_NAME = 'test-hc' + args.gcp_suffix+FIREWALL_RULE_NAME = 'test-fw-rule' + args.gcp_suffix+BACKEND_SERVICE_NAME = 'test-backend-service' + args.gcp_suffix+URL_MAP_NAME = 'test-map' + args.gcp_suffix+SERVICE_HOST = 'grpc-test' + args.gcp_suffix+TARGET_PROXY_NAME = 'test-target-proxy' + args.gcp_suffix+FORWARDING_RULE_NAME = 'test-forwarding-rule' + args.gcp_suffix+KEEP_GCP_RESOURCES = args.keep_gcp_resources+TOLERATE_GCP_ERRORS = args.tolerate_gcp_errors+SERVICE_PORT = 55551+STATS_PORT = 55552+INSTANCE_GROUP_SIZE = 2+WAIT_FOR_OPERATION_SEC = 60+NUM_TEST_RPCS = 10 * QPS+WAIT_FOR_STATS_SEC = 30+++def get_client_stats(num_rpcs, timeout_sec):+    with grpc.insecure_channel('localhost:%d' % STATS_PORT) as channel:+        stub = test_pb2_grpc.LoadBalancerStatsServiceStub(channel)+        request = messages_pb2.LoadBalancerStatsRequest()+        request.num_rpcs = num_rpcs+        request.timeout_sec = timeout_sec+        try:+            response = stub.GetClientStats(request, wait_for_ready=True)+            logger.debug('Invoked GetClientStats RPC: %s', response)+            return response+        except grpc.RpcError as rpc_error:+            raise Exception('GetClientStats RPC failed')+++def wait_until_only_given_backends_receive_load(backends, timeout_sec):+    start_time = time.time()+    error_msg = None+    while time.time() - start_time <= timeout_sec:+        error_msg = None+        stats = get_client_stats(max(len(backends), 1), timeout_sec)+        rpcs_by_peer = stats.rpcs_by_peer+        for backend in backends:+            if backend not in rpcs_by_peer:+                error_msg = 'Backend %s did not receive load' % backend+                break+        if not error_msg and len(rpcs_by_peer) > len(backends):+            error_msg = 'Unexpected backend received load: %s' % rpcs_by_peer+        if not error_msg:+            return+    raise Exception(error_msg)+++def test_ping_pong(backends, num_rpcs, stats_timeout_sec):+    start_time = time.time()+    error_msg = None+    while time.time() - start_time <= stats_timeout_sec:+        error_msg = None+        stats = get_client_stats(num_rpcs, stats_timeout_sec)+        rpcs_by_peer = stats.rpcs_by_peer+        for backend in backends:+            if backend not in rpcs_by_peer:+                error_msg = 'Backend %s did not receive load' % backend+                break+        if not error_msg and len(rpcs_by_peer) > len(backends):+            error_msg = 'Unexpected backend received load: %s' % rpcs_by_peer+        if not error_msg:+            return+    raise Exception(error_msg)+++def test_round_robin(backends, num_rpcs, stats_timeout_sec):+    threshold = 1+    wait_until_only_given_backends_receive_load(backends, stats_timeout_sec)+    stats = get_client_stats(num_rpcs, stats_timeout_sec)+    requests_received = [stats.rpcs_by_peer[x] for x in stats.rpcs_by_peer]+    total_requests_received = sum(+        [stats.rpcs_by_peer[x] for x in stats.rpcs_by_peer])+    if total_requests_received != num_rpcs:+        raise Exception('Unexpected RPC failures', stats)+    expected_requests = total_requests_received / len(backends)+    for backend in backends:+        if abs(stats.rpcs_by_peer[backend] - expected_requests) > threshold:+            raise Exception(+                'RPC peer distribution differs from expected by more than %d for backend %s (%s)',+                threshold, backend, stats)+++def create_instance_template(compute, name, grpc_port, project):+    config = {+        'name': name,+        'properties': {+            'tags': {+                'items': ['grpc-td-tag']+            },+            'machineType': 'n1-standard-1',+            'serviceAccounts': [{+                'email': 'default',+                'scopes': ['https://www.googleapis.com/auth/cloud-platform',]+            }],+            'networkInterfaces': [{+                'accessConfigs': [{+                    'type': 'ONE_TO_ONE_NAT'+                }],+                'network': 'global/networks/default'+            }],+            'disks': [{+                'boot': True,+                'initializeParams': {+                    'sourceImage':+                        'projects/debian-cloud/global/images/family/debian-9'+                }+            }],+            'metadata': {+                'items': [{+                    'key':+                        'startup-script',+                    'value':+                        """"""#!/bin/bash++sudo apt update+sudo apt install -y git default-jdk+mkdir java_server+pushd java_server+git clone https://github.com/grpc/grpc-java.git+pushd grpc-java+pushd interop-testing+../gradlew installDist -x test -PskipCodegen=true -PskipAndroid=true+ +nohup build/install/grpc-interop-testing/bin/xds-test-server --port=%d 1>/dev/null &""""""+                        % grpc_port+                }]+            }+        }+    }++    result = compute.instanceTemplates().insert(project=project,+                                                body=config).execute()+    wait_for_global_operation(compute, project, result['name'])+    return result['targetLink']+++def create_instance_group(compute, name, size, grpc_port, template_url, project,+                          zone):+    config = {+        'name': name,+        'instanceTemplate': template_url,+        'targetSize': size,+        'namedPorts': [{+            'name': 'grpc',+            'port': grpc_port+        }]+    }++    result = compute.instanceGroupManagers().insert(project=project,+                                                    zone=zone,+                                                    body=config).execute()+    wait_for_zone_operation(compute, project, zone, result['name'])+    result = compute.instanceGroupManagers().get(+        project=PROJECT_ID, zone=ZONE, instanceGroupManager=name).execute()+    return result['instanceGroup']+++def create_health_check(compute, name, project):+    config = {+        'name': name,+        'type': 'TCP',+        'tcpHealthCheck': {+            'portName': 'grpc'+        }+    }+    result = compute.healthChecks().insert(project=project,+                                           body=config).execute()+    wait_for_global_operation(compute, project, result['name'])+    return result['targetLink']+++def create_health_check_firewall_rule(compute, name, project):",Are you sure these fw rules won't be deleted by gce_enforcer for google projects before the tests complete? Get the project whitelisted if this becomes problem.,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21809,374576778,2020-02-04T10:04:12Z,src/python/grpcio/grpc/experimental/aio/_server.py,"@@ -13,34 +13,47 @@ # limitations under the License. """"""Server-side implementation of gRPC Asyncio Python."""""" -from typing import Text, Optional import asyncio+from concurrent.futures import Executor+from typing import Any, Optional, Sequence, Text+ import grpc-from grpc import _common+from grpc import _common, _compression from grpc._cython import cygrpc +from ._typing import ChannelArgumentType+++def _augment_channel_arguments(base_options: ChannelArgumentType,+                               compression: Optional[grpc.Compression]):+    compression_option = _compression.create_channel_option(compression)+    return tuple(base_options) + compression_option+  class Server:     """"""Serves RPCs."""""" -    def __init__(self, thread_pool, generic_handlers, interceptors, options,-                 maximum_concurrent_rpcs, compression):+    def __init__(self, thread_pool: Optional[Executor],+                 generic_handlers: Optional[Sequence[grpc.GenericRpcHandler]],+                 interceptors: Optional[Sequence[Any]],+                 options: ChannelArgumentType,+                 maximum_concurrent_rpcs: Optional[int],+                 compression: Optional[grpc.Compression]):         self._loop = asyncio.get_event_loop()-        self._server = cygrpc.AioServer(self._loop, thread_pool,-                                        generic_handlers, interceptors, options,-                                        maximum_concurrent_rpcs, compression)+        self._server = cygrpc.AioServer(+            self._loop, thread_pool, generic_handlers, interceptors,+            _augment_channel_arguments(options, compression),","So in case of having it enabled by default, we do not need to explicitly send [1] any flag or so for saying that compression is enabled? The Core will take care of that, by for example expanding the metadata operations and adding the compression ""headers""?[1] https://github.com/grpc/grpc/pull/21809/files#diff-b53c077f7911f53dc8ce7656e16d35cdR24 ",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21809,374585705,2020-02-04T10:21:13Z,src/python/grpcio/grpc/experimental/aio/_server.py,"@@ -13,34 +13,47 @@ # limitations under the License. """"""Server-side implementation of gRPC Asyncio Python."""""" -from typing import Text, Optional import asyncio+from concurrent.futures import Executor+from typing import Any, Optional, Sequence, Text+ import grpc-from grpc import _common+from grpc import _common, _compression from grpc._cython import cygrpc +from ._typing import ChannelArgumentType+++def _augment_channel_arguments(base_options: ChannelArgumentType,+                               compression: Optional[grpc.Compression]):+    compression_option = _compression.create_channel_option(compression)+    return tuple(base_options) + compression_option+  class Server:     """"""Serves RPCs."""""" -    def __init__(self, thread_pool, generic_handlers, interceptors, options,-                 maximum_concurrent_rpcs, compression):+    def __init__(self, thread_pool: Optional[Executor],+                 generic_handlers: Optional[Sequence[grpc.GenericRpcHandler]],+                 interceptors: Optional[Sequence[Any]],+                 options: ChannelArgumentType,+                 maximum_concurrent_rpcs: Optional[int],+                 compression: Optional[grpc.Compression]):","Maybe I'm a bit late on that discussion or it's worth it to preserve the current API, but I'm wondering how this `compression` parameter fits on the idea of having support for levels of compression eventually (are they currently supported in Python?, any plan?)How good or bad will be naming the `compression` parameter as `default_compression_algorithm`??",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21809,374587361,2020-02-04T10:24:40Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -541,7 +584,7 @@ cdef CallbackFailureHandler SERVER_SHUTDOWN_FAILURE_HANDLER = CallbackFailureHan cdef class AioServer:      def __init__(self, loop, thread_pool, generic_handlers, interceptors,-                 options, maximum_concurrent_rpcs, compression):+                 options, maximum_concurrent_rpcs):","Thanks.By chance the server that we are targeting for test the `aio` module has the compression enabled :), but they are basically using the `compression` parameter in the server side, beyond that I dio not have more experience about how useful is having all of these methods for enabling, tunning the compression. So I will give you feedback once I have it :)Something that has been asked is when the `experimental` flag will be removed for this feature. Im wondering too what would need to happen for not having it under the experimental flag. Any idea?",
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/21809,374658325,2020-02-04T13:06:28Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -188,12 +204,13 @@ def __call__(self,         Returns:           A Call object instance which is an awaitable object.         """"""+        if metadata is None:+            metadata = _IMMUTABLE_EMPTY_TUPLE","(nit) alternatively, we can make ``_IMMUTABLE_EMPTY_TUPLE`` the default value for the ``metadata`` argument, WDYT?",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/21869,374675474,2020-02-04T13:41:56Z,tools/run_tests/run_xds_tests.py,"@@ -0,0 +1,571 @@+#!/usr/bin/env python+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Run xDS integration tests on GCP using Traffic Director.""""""++import argparse+import googleapiclient.discovery+import grpc+import logging+import os+import shlex+import subprocess+import sys+import time++from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import test_pb2_grpc++logger = logging.getLogger(__name__)+console_handler = logging.StreamHandler()+logger.addHandler(console_handler)++argp = argparse.ArgumentParser(description='Run xDS interop tests on GCP')+argp.add_argument('--project_id', help='GCP project id')+argp.add_argument(+    '--gcp_suffix',+    default='',+    help='Optional suffix for all generated GCP resource names. Useful to ensure '+    'distinct names across test runs.')+argp.add_argument('--test_case',+                  default=None,+                  choices=['all', 'ping_pong', 'round_robin'])+argp.add_argument(+    '--client_cmd',+    default=None,+    help='Command to launch xDS test client. This script will fill in '+    '{service_host}, {service_port},{stats_port} and {qps} parameters using '+    'str.format()')+argp.add_argument('--bootstrap_file',+                  default=None,+                  help='Path to xDS bootstrap file.')+argp.add_argument('--zone', default='us-central1-a')+argp.add_argument('--qps', default=10, help='Client QPS')+argp.add_argument(+    '--wait_for_backend_sec',+    default=900,+    help='Time limit for waiting for created backend services to report healthy '+    'when launching test suite')+argp.add_argument(+    '--keep_gcp_resources',+    default=False,+    action='store_true',+    help=+    'Leave GCP VMs and configuration running after test. Default behavior is '+    'to delete when tests complete.')+argp.add_argument(+    '--tolerate_gcp_errors',+    default=False,+    action='store_true',+    help=+    'Continue with test even when an error occurs during setup. Intended for '+    'manual testing, where attempts to recreate any GCP resources already '+    'existing will result in an error')+argp.add_argument('--verbose',+                  help='verbose log output',+                  default=False,+                  action=""store_true"")+args = argp.parse_args()++if args.verbose:+    logger.setLevel(logging.DEBUG)++PROJECT_ID = args.project_id+ZONE = args.zone+QPS = args.qps+TEST_CASE = args.test_case+BOOTSTRAP_FILE = args.bootstrap_file+CLIENT_CMD = args.client_cmd+WAIT_FOR_BACKEND_SEC = args.wait_for_backend_sec+TEMPLATE_NAME = 'test-template' + args.gcp_suffix+INSTANCE_GROUP_NAME = 'test-ig' + args.gcp_suffix+HEALTH_CHECK_NAME = 'test-hc' + args.gcp_suffix+FIREWALL_RULE_NAME = 'test-fw-rule' + args.gcp_suffix+BACKEND_SERVICE_NAME = 'test-backend-service' + args.gcp_suffix+URL_MAP_NAME = 'test-map' + args.gcp_suffix+SERVICE_HOST = 'grpc-test' + args.gcp_suffix+TARGET_PROXY_NAME = 'test-target-proxy' + args.gcp_suffix+FORWARDING_RULE_NAME = 'test-forwarding-rule' + args.gcp_suffix+KEEP_GCP_RESOURCES = args.keep_gcp_resources+TOLERATE_GCP_ERRORS = args.tolerate_gcp_errors+SERVICE_PORT = 55551+STATS_PORT = 55552+INSTANCE_GROUP_SIZE = 2+WAIT_FOR_OPERATION_SEC = 60+NUM_TEST_RPCS = 10 * QPS+WAIT_FOR_STATS_SEC = 30+++def get_client_stats(num_rpcs, timeout_sec):+    with grpc.insecure_channel('localhost:%d' % STATS_PORT) as channel:+        stub = test_pb2_grpc.LoadBalancerStatsServiceStub(channel)+        request = messages_pb2.LoadBalancerStatsRequest()+        request.num_rpcs = num_rpcs+        request.timeout_sec = timeout_sec+        try:+            response = stub.GetClientStats(request, wait_for_ready=True)+            logger.debug('Invoked GetClientStats RPC: %s', response)+            return response+        except grpc.RpcError as rpc_error:+            raise Exception('GetClientStats RPC failed')+++def wait_until_only_given_backends_receive_load(backends, timeout_sec):+    start_time = time.time()+    error_msg = None+    while time.time() - start_time <= timeout_sec:+        error_msg = None+        stats = get_client_stats(max(len(backends), 1), timeout_sec)+        rpcs_by_peer = stats.rpcs_by_peer+        for backend in backends:+            if backend not in rpcs_by_peer:+                error_msg = 'Backend %s did not receive load' % backend+                break+        if not error_msg and len(rpcs_by_peer) > len(backends):+            error_msg = 'Unexpected backend received load: %s' % rpcs_by_peer+        if not error_msg:+            return+    raise Exception(error_msg)+++def test_ping_pong(backends, num_rpcs, stats_timeout_sec):+    start_time = time.time()+    error_msg = None+    while time.time() - start_time <= stats_timeout_sec:+        error_msg = None+        stats = get_client_stats(num_rpcs, stats_timeout_sec)+        rpcs_by_peer = stats.rpcs_by_peer+        for backend in backends:+            if backend not in rpcs_by_peer:+                error_msg = 'Backend %s did not receive load' % backend+                break+        if not error_msg and len(rpcs_by_peer) > len(backends):+            error_msg = 'Unexpected backend received load: %s' % rpcs_by_peer+        if not error_msg:+            return+    raise Exception(error_msg)+++def test_round_robin(backends, num_rpcs, stats_timeout_sec):+    threshold = 1+    wait_until_only_given_backends_receive_load(backends, stats_timeout_sec)+    stats = get_client_stats(num_rpcs, stats_timeout_sec)+    requests_received = [stats.rpcs_by_peer[x] for x in stats.rpcs_by_peer]+    total_requests_received = sum(+        [stats.rpcs_by_peer[x] for x in stats.rpcs_by_peer])+    if total_requests_received != num_rpcs:+        raise Exception('Unexpected RPC failures', stats)+    expected_requests = total_requests_received / len(backends)+    for backend in backends:+        if abs(stats.rpcs_by_peer[backend] - expected_requests) > threshold:+            raise Exception(+                'RPC peer distribution differs from expected by more than %d for backend %s (%s)',+                threshold, backend, stats)+++def create_instance_template(compute, name, grpc_port, project):+    config = {+        'name': name,+        'properties': {+            'tags': {+                'items': ['grpc-td-tag']+            },+            'machineType': 'n1-standard-1',+            'serviceAccounts': [{+                'email': 'default',+                'scopes': ['https://www.googleapis.com/auth/cloud-platform',]+            }],+            'networkInterfaces': [{+                'accessConfigs': [{+                    'type': 'ONE_TO_ONE_NAT'+                }],+                'network': 'global/networks/default'+            }],+            'disks': [{+                'boot': True,+                'initializeParams': {+                    'sourceImage':+                        'projects/debian-cloud/global/images/family/debian-9'+                }+            }],+            'metadata': {+                'items': [{+                    'key':+                        'startup-script',+                    'value':+                        """"""#!/bin/bash++sudo apt update+sudo apt install -y git default-jdk+mkdir java_server+pushd java_server+git clone https://github.com/grpc/grpc-java.git+pushd grpc-java+pushd interop-testing+../gradlew installDist -x test -PskipCodegen=true -PskipAndroid=true+ +nohup build/install/grpc-interop-testing/bin/xds-test-server --port=%d 1>/dev/null &""""""+                        % grpc_port+                }]+            }+        }+    }++    result = compute.instanceTemplates().insert(project=project,+                                                body=config).execute()+    wait_for_global_operation(compute, project, result['name'])+    return result['targetLink']+++def create_instance_group(compute, name, size, grpc_port, template_url, project,+                          zone):+    config = {+        'name': name,+        'instanceTemplate': template_url,+        'targetSize': size,+        'namedPorts': [{+            'name': 'grpc',+            'port': grpc_port+        }]+    }++    result = compute.instanceGroupManagers().insert(project=project,+                                                    zone=zone,+                                                    body=config).execute()+    wait_for_zone_operation(compute, project, zone, result['name'])+    result = compute.instanceGroupManagers().get(+        project=PROJECT_ID, zone=ZONE, instanceGroupManager=name).execute()+    return result['instanceGroup']+++def create_health_check(compute, name, project):+    config = {+        'name': name,+        'type': 'TCP',+        'tcpHealthCheck': {+            'portName': 'grpc'+        }+    }+    result = compute.healthChecks().insert(project=project,+                                           body=config).execute()+    wait_for_global_operation(compute, project, result['name'])+    return result['targetLink']+++def create_health_check_firewall_rule(compute, name, project):","They [can](https://sponge.corp.google.com/target?id=c09399d3-e8dd-40f5-806d-547ad1ae5ddd&target=grpc%2Fjava%2Fmaster%2Fpresubmit%2Fandroid&searchFor=) be deleted mid-run but ""usually"" won't. I'll get the project whitelisted before running this automatically.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21885,374840000,2020-02-04T18:18:37Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -512,3 +512,15 @@ def stream_stream(                                          request_serializer,                                          response_deserializer, None,                                          self._loop)+++async def channel_ready(channel: Channel) -> None:","Promoted to a method of `aio.Channel`.When I first design this API, I felt a sense of duplication, since this API can be implemented by `get_state` and `wait_for_state_change`. I'm okay with either option.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21885,374841491,2020-02-04T18:21:29Z,src/python/grpcio_tests/tests_aio/unit/channel_ready_test.py,"@@ -0,0 +1,68 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Testing the channel_ready function.""""""++import asyncio+import gc+import logging+import time+import unittest++import grpc+from grpc.experimental import aio++from tests.unit.framework.common import get_socket, test_constants+from tests_aio.unit import _common+from tests_aio.unit._test_base import AioTestBase+from tests_aio.unit._test_server import start_test_server+++class TestChannelReady(AioTestBase):++    async def setUp(self):+        address, self._port, self._socket = get_socket(listen=False)+        self._channel = aio.insecure_channel(f""{address}:{self._port}"")+        self._socket.close()++    async def tearDown(self):+        await self._channel.close()++    async def test_channel_ready_success(self):+        # Start `channel_ready` as another Task+        channel_ready_task = self.loop.create_task(+            aio.channel_ready(self._channel))++        # Wait for TRANSIENT_FAILURE+        await _common.block_until_certain_state(+            self._channel, grpc.ChannelConnectivity.TRANSIENT_FAILURE)++        try:+            # Start the server+            _, server = await start_test_server(port=self._port)++            # The RPC should recover itself+            await channel_ready_task+        finally:+            if server is not None:",Removed the if condition.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21885,374847742,2020-02-04T18:33:09Z,src/python/grpcio_tests/tests_aio/unit/channel_ready_test.py,"@@ -0,0 +1,68 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Testing the channel_ready function.""""""++import asyncio+import gc+import logging+import time+import unittest++import grpc+from grpc.experimental import aio++from tests.unit.framework.common import get_socket, test_constants+from tests_aio.unit import _common+from tests_aio.unit._test_base import AioTestBase+from tests_aio.unit._test_server import start_test_server+++class TestChannelReady(AioTestBase):++    async def setUp(self):+        address, self._port, self._socket = get_socket(listen=False)+        self._channel = aio.insecure_channel(f""{address}:{self._port}"")","Hm... This seems like a bug in the `LocalCredentials` implementation. I can see the argument to be made against whitelisting `localhost` wholesale, since you can configure that to be anything in `/etc/hosts`, but I think local TCP should work with names that resolve to local addresses.Let's gloss over this particular usage of `insecure_channel`, but we should try to address this problem in general.@yihuazhang Thoughts?",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/21832,374938166,2020-02-04T21:40:20Z,src/core/lib/security/credentials/credentials.cc,"@@ -45,6 +45,14 @@ void grpc_channel_credentials_release(grpc_channel_credentials* creds) {   if (creds) creds->Unref(); } +grpc_channel_credentials* grpc_channel_credentials_copy(",@HannahShiSFB Can you please comment on why we need to introduce this new function in c-core? Please also comment / give example to why we find it consistent to call the function `_copy` with other similar existing functions. Thanks.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21739,375352929,2020-02-05T16:10:01Z,test/cpp/microbenchmarks/BUILD,"@@ -198,47 +222,50 @@ grpc_cc_binary(     deps = ["":fullstack_unary_ping_pong_h""], ) -grpc_cc_binary(+grpc_cc_test(     name = ""bm_metadata"",-    testonly = 1,     srcs = [""bm_metadata.cc""],     tags = [""no_windows""],+    uses_polling = False,     deps = ["":helpers""], ) -grpc_cc_binary(+grpc_cc_test(     name = ""bm_chttp2_hpack"",-    testonly = 1,     srcs = [""bm_chttp2_hpack.cc""],     tags = [""no_windows""],+    uses_polling = False,     deps = ["":helpers""], ) -grpc_cc_binary(+grpc_cc_test(     name = ""bm_opencensus_plugin"",-    testonly = 1,     srcs = [""bm_opencensus_plugin.cc""],     language = ""C++"",     deps = [-        "":helpers"",+        "":helpers_secure"",         ""//:grpc_opencensus_plugin"",         ""//src/proto/grpc/testing:echo_proto"",     ], ) -grpc_cc_binary(+grpc_cc_test(     name = ""bm_timer"",-    testonly = 1,     srcs = [""bm_timer.cc""],     tags = [""no_windows""],+    uses_polling = False,     deps = ["":helpers""], ) -grpc_cc_binary(+grpc_cc_test(     name = ""bm_threadpool"",-    testonly = 1,+    size = ""enormous"",","how long does this take so you need ""enormous""?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21739,375355048,2020-02-05T16:13:23Z,test/cpp/microbenchmarks/BUILD,"@@ -198,47 +222,50 @@ grpc_cc_binary(     deps = ["":fullstack_unary_ping_pong_h""], ) -grpc_cc_binary(+grpc_cc_test(     name = ""bm_metadata"",-    testonly = 1,     srcs = [""bm_metadata.cc""],     tags = [""no_windows""],+    uses_polling = False,     deps = ["":helpers""], ) -grpc_cc_binary(+grpc_cc_test(     name = ""bm_chttp2_hpack"",-    testonly = 1,     srcs = [""bm_chttp2_hpack.cc""],     tags = [""no_windows""],+    uses_polling = False,     deps = ["":helpers""], ) -grpc_cc_binary(+grpc_cc_test(     name = ""bm_opencensus_plugin"",-    testonly = 1,     srcs = [""bm_opencensus_plugin.cc""],     language = ""C++"",     deps = [-        "":helpers"",+        "":helpers_secure"",         ""//:grpc_opencensus_plugin"",         ""//src/proto/grpc/testing:echo_proto"",     ], ) -grpc_cc_binary(+grpc_cc_test(     name = ""bm_timer"",-    testonly = 1,     srcs = [""bm_timer.cc""],     tags = [""no_windows""],+    uses_polling = False,     deps = ["":helpers""], ) -grpc_cc_binary(+grpc_cc_test(     name = ""bm_threadpool"",-    testonly = 1,+    size = ""enormous"",","note that foundry currently has trouble with test that take very long time: b/143515898also, tests that take very long time are not very useful in general.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21919,375358561,2020-02-05T16:19:03Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -270,8 +275,12 @@ def __await__(self) -> ResponseType:             if self._cython_call.is_locally_cancelled():                 raise asyncio.CancelledError()             else:+                call_status = self._cython_call._status+                debug_error_string = None+                if call_status is not None:+                    debug_error_string = call_status._debug_error_string","I would say that we have public methods for accessing to the attributes that you are using:* `status` can be retrieved by [1], its a coroutine so if you want to make sure that the RPC is in a finished state you can always use [2]* `debug_error_string` can be retrieved through [3][1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi#L232[2] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi#L210[3] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/rpc_status.pxd.pxi#L28",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21919,375360579,2020-02-05T16:22:07Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -270,8 +275,12 @@ def __await__(self) -> ResponseType:             if self._cython_call.is_locally_cancelled():                 raise asyncio.CancelledError()             else:+                call_status = self._cython_call._status",Out of curiosity is `cygrpc.EOF is None`? Otherwise I do not understand how it is working considering this [1].Would it make sense for making this piece of code change [1] for returning a EOF?/cc @lidizheng [1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi#L330,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21920,375450949,2020-02-05T19:08:58Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -47,6 +49,33 @@ class _MulticallableTestMixin():         await self._server.stop(None)  ++class _SecureCallMixin:+    """"""A Mixin to run the call tests over a secure channel.""""""++    async def setUp(self):+        server_credentials = grpc.ssl_server_credentials([+            (resources.private_key(), resources.certificate_chain())+        ])+        channel_credentials = grpc.ssl_channel_credentials(+            resources.test_root_certificates())++        self._server_address, self._server = await start_test_server(+            secure=True, server_credentials=server_credentials)+        channel_options = (+            (+                'grpc.ssl_target_name_override',+                _SERVER_HOST_OVERRIDE,+            ),+        )+        self._channel = aio.secure_channel(self._server_address, channel_credentials, channel_options)+        self._stub = test_pb2_grpc.TestServiceStub(self._channel)++    async def tearDown(self):+        await self._channel.close()+        await self._server.stop(None)",Can we create a new test file? Let's try to manage the complex of each single test file.,
10470658,donnadionne,https://api.github.com/repos/grpc/grpc/pulls/21888,375462249,2020-02-05T19:30:02Z,test/cpp/interop/xds_interop_client.cc,"@@ -0,0 +1,242 @@+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <chrono>+#include <condition_variable>+#include <map>+#include <mutex>+#include <set>+#include <sstream>+#include <string>+#include <thread>+#include <vector>++#include <gflags/gflags.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include ""src/proto/grpc/testing/empty.pb.h""+#include ""src/proto/grpc/testing/messages.pb.h""+#include ""src/proto/grpc/testing/test.grpc.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/test_config.h""++DEFINE_int32(num_channels, 1, ""Number of channels."");+DEFINE_bool(print_response, false, ""Write RPC response to stdout."");+DEFINE_int32(qps, 1, ""Qps per channel."");+DEFINE_int32(rpc_timeout_sec, 10, ""Per RPC timeout seconds."");+DEFINE_string(server, ""localhost:50051"", ""Address of server."");+DEFINE_int32(stats_port, 50052,+             ""Port to expose peer distribution stats service."");++using grpc::Channel;+using grpc::ClientContext;+using grpc::Server;+using grpc::ServerBuilder;+using grpc::ServerContext;+using grpc::ServerCredentials;+using grpc::ServerReader;+using grpc::ServerReaderWriter;+using grpc::ServerWriter;+using grpc::Status;+using grpc::testing::LoadBalancerStatsRequest;+using grpc::testing::LoadBalancerStatsResponse;+using grpc::testing::LoadBalancerStatsService;+using grpc::testing::SimpleRequest;+using grpc::testing::SimpleResponse;+using grpc::testing::TestService;++class XdsStatsWatcher;++// Unique ID for each outgoing RPC+int global_request_id;+// Stores a set of watchers that should be notified upon outgoing RPC completion+std::set<XdsStatsWatcher*> watchers;+// Mutex for global_request_id and watchers+std::mutex mu;++/** Records the remote peer distribution for a given range of RPCs. */+class XdsStatsWatcher {+ public:+  XdsStatsWatcher(int start_id, int end_id)+      : start_id_(start_id), end_id_(end_id), rpcs_needed_(end_id - start_id) {}++  void RpcCompleted(int request_id, std::string peer) {+    if (start_id_ <= request_id && request_id < end_id_) {+      {+        std::lock_guard<std::mutex> lk(m_);+        if (peer.empty()) {+          no_remote_peer_++;+        } else {+          rpcs_by_peer_[peer]++;+        }+        rpcs_needed_--;+      }+      cv_.notify_one();+    }+  }++  void WaitForRpcStatsResponse(LoadBalancerStatsResponse* response,+                               int timeout_sec) {+    {+      std::unique_lock<std::mutex> lk(m_);+      cv_.wait_for(lk, std::chrono::seconds(timeout_sec),+                   [this] { return rpcs_needed_ == 0; });+      response->mutable_rpcs_by_peer()->insert(rpcs_by_peer_.begin(),+                                               rpcs_by_peer_.end());+      response->set_num_failures(no_remote_peer_ + rpcs_needed_);+    }+  }++ private:+  int start_id_;+  int end_id_;+  int rpcs_needed_;+  std::map<std::string, int> rpcs_by_peer_;+  int no_remote_peer_;+  std::mutex m_;+  std::condition_variable cv_;+};++class TestClient {+ public:+  TestClient(std::shared_ptr<Channel> channel)+      : stub_(TestService::NewStub(channel)) {}++  void UnaryCall() {+    SimpleResponse response;+    ClientContext context;++    int saved_request_id;+    {+      std::lock_guard<std::mutex> lk(mu);+      saved_request_id = ++global_request_id;+    }+    std::chrono::system_clock::time_point deadline =+        std::chrono::system_clock::now() ++        std::chrono::seconds(FLAGS_rpc_timeout_sec);+    context.set_deadline(deadline);+    Status status = stub_->UnaryCall(+        &context, SimpleRequest::default_instance(), &response);++    {+      std::lock_guard<std::mutex> lk(mu);+      for (auto watcher : watchers) {+        watcher->RpcCompleted(saved_request_id, response.hostname());+      }+    }++    if (FLAGS_print_response) {+      if (status.ok()) {+        std::cout << ""Greeting: Hello world, this is "" << response.hostname()+                  << "", from "" << context.peer() << std::endl;+      } else {+        std::cout << ""RPC failed: "" << status.error_code() << "": ""+                  << status.error_message() << std::endl;+      }+    }+  }++ private:+  std::unique_ptr<TestService::Stub> stub_;+};++class LoadBalancerStatsServiceImpl : public LoadBalancerStatsService::Service {+ public:+  Status GetClientStats(ServerContext* context,+                        const LoadBalancerStatsRequest* request,+                        LoadBalancerStatsResponse* response) {+    int start_id;","more of a question, how is this server context triggered?  i don't see where LoadBalancerStatsRequest is built and GetClientStats is calledI understand the watcher will wait on the conditional var and then respond.  i am a bit confused how we even get into this server context",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/21739,375464036,2020-02-05T19:33:27Z,test/cpp/microbenchmarks/BUILD,"@@ -48,75 +48,98 @@ grpc_cc_library(     ], ) -grpc_cc_binary(-    name = ""bm_closure"",+# Need a secure version of helpers to benchmark opencensus+grpc_cc_library(+    name = ""helpers_secure"",     testonly = 1,+    srcs = [""helpers.cc""],+    hdrs = [+        ""fullstack_context_mutators.h"",+        ""fullstack_fixtures.h"",+        ""helpers.h"",+    ],+    external_deps = [+        ""benchmark"",+    ],+    tags = [""no_windows""],+    deps = [+        ""//:grpc++"",+        ""//src/proto/grpc/testing:echo_proto"",+        ""//test/core/util:grpc_test_util"",+        ""//test/cpp/util:test_config"",+    ],+)++grpc_cc_test(+    name = ""bm_closure"",     srcs = [""bm_closure.cc""],     tags = [""no_windows""],     deps = ["":helpers""], ) -grpc_cc_binary(+grpc_cc_test(     name = ""bm_alarm"",-    testonly = 1,     srcs = [""bm_alarm.cc""],     tags = [""no_windows""],     deps = ["":helpers""], ) -grpc_cc_binary(+grpc_cc_test(     name = ""bm_arena"",-    testonly = 1,+    size = ""enormous"",",I can take the enormous tests back down to large or less. I think that was when I was just experimenting. I can also disable all tests on mac.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21809,375482989,2020-02-05T20:12:11Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -69,6 +81,24 @@ cdef class RPCState:         if self.server._status == AIO_SERVER_STATUS_STOPPED:             raise _ServerStoppedError(_SERVER_STOPPED_DETAILS) +    cdef int get_write_flag(self):+        if self.disable_next_compression:+            self.disable_next_compression = False+            return WriteFlag.no_compress+        else:+            return _EMPTY_FLAG++    cdef Operation create_send_initial_metadata_op_if_not_sent(self):+        if self.metadata_sent:+            return None++        cdef SendInitialMetadataOperation op = SendInitialMetadataOperation(+            _augment_metadata(_IMMUTABLE_EMPTY_METADATA, self.compression_algorithm),+            _EMPTY_FLAG+        )+        self.metadata_sent = True","This is a bit odd. The naming implies that this is a pure function, but it's actually side-effecting. And this is actually a bit disingenuous; the metadata hasn't actually been sent yet.",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21920,375517509,2020-02-05T21:28:02Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -202,6 +231,23 @@ class TestUnaryUnaryCall(_MulticallableTestMixin, AioTestBase):         with self.assertRaises(asyncio.CancelledError):             await task +    async def test_passing_credentials_fails_over_insecure_channel(self):+        call_credentials = grpc.composite_call_credentials(+            grpc.access_token_call_credentials(""abc""),+            grpc.access_token_call_credentials(""def""),+        )+        with self.assertRaisesRegex(RuntimeError, ""Call credentials are only valid on secure channels""):+            self._stub.UnaryCall(messages_pb2.SimpleRequest(), credentials=call_credentials)+++class TestUnaryUnarySecureCall(_SecureCallMixin, AioTestBase):+    """"""Calls made over a secure channel.""""""+    async def test_call_ok_with_credentials(self):+        call = self._stub.UnaryCall(messages_pb2.SimpleRequest())",is it missing the credentials parameter?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21809,375553585,2020-02-05T22:52:51Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -69,6 +81,24 @@ cdef class RPCState:         if self.server._status == AIO_SERVER_STATUS_STOPPED:             raise _ServerStoppedError(_SERVER_STOPPED_DETAILS) +    cdef int get_write_flag(self):+        if self.disable_next_compression:+            self.disable_next_compression = False+            return WriteFlag.no_compress+        else:+            return _EMPTY_FLAG++    cdef Operation create_send_initial_metadata_op_if_not_sent(self):+        if self.metadata_sent:+            return None++        cdef SendInitialMetadataOperation op = SendInitialMetadataOperation(+            _augment_metadata(_IMMUTABLE_EMPTY_METADATA, self.compression_algorithm),+            _EMPTY_FLAG+        )+        self.metadata_sent = True","Good catch. I moved the flag flipping out of this function to places that the `SendInitialMetadataOperation` actually goes onto wire. In that way, this function will have cleaner semantics.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/21911,375559901,2020-02-05T23:09:30Z,examples/cpp/helloworld/greeter_client.cc,"@@ -73,11 +73,32 @@ class GreeterClient {  int main(int argc, char** argv) {   // Instantiate the client. It requires a channel, out of which the actual RPCs-  // are created. This channel models a connection to an endpoint (in this case,-  // localhost at port 50051). We indicate that the channel isn't authenticated-  // (use of InsecureChannelCredentials()).+  // are created. This channel models a connection to an endpoint specified by+  // the argument ""--target="" which is the only expected argument.+  // We indicate that the channel isn't authenticated (use of+  // InsecureChannelCredentials()).+  std::string target_str;+  std::string arg_str(""--target"");",How hard would it be to use the gflags library in this example?  That might be easier than writing this argument-parsing code by hand.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21910,376054300,2020-02-06T20:05:35Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/common.pyx.pxi,"@@ -67,3 +67,23 @@ class _EOF:   EOF = _EOF()+++class BaseError(Exception):+    """"""The base class for all exceptions generated by gRPC framework.""""""",Rephrased to `The base class for exceptions generated by gRPC AsyncIO stack.`,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21910,376056716,2020-02-06T20:11:13Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/common.pyx.pxi,"@@ -67,3 +67,23 @@ class _EOF:   EOF = _EOF()+++class BaseError(Exception):+    """"""The base class for all exceptions generated by gRPC framework.""""""+++class UsageError(BaseError):+    """"""Raised when the usage might lead to undefined behavior.""""""","Good suggestions. More details added:```pythonclass UsageError(BaseError):    """"""Raised when the usage of API by applications is inappropriate.    For example, trying to invoke RPC on a closed channel, mixing two styles    of streaming API on the client side. This exception should not be    suppressed.    """"""```",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/21916,376119648,2020-02-06T22:33:02Z,include/grpcpp/generic/generic_stub_impl.h,"@@ -38,44 +39,56 @@ typedef ::grpc_impl::ClientAsyncResponseReader<ByteBuffer> namespace grpc_impl { class CompletionQueue; -/// Generic stubs provide a type-unsafe interface to call gRPC methods+/// Generic stubs provide a type-unaware interface to call gRPC methods /// by name.-class GenericStub final {+template <class MessageType>","Practically, an asymmetric use case is not very useful. Most likely you'll have a common base class (like `proto2::MessageLite`) at the base of your serialization hierarchy and thus can use this for at least basic ser/deser. Maybe the case where that wouldn't work is if you're using a service to translate between data formats, but that doesn't seem immediately useful (and you can always use raw ByteBuffer in that case).Theoretically, there could be a use at some point, but I'm trying to keep this customer-driven (esp since this is just within the grpc_impl namespace, not true API) and we haven't had a request for anything other than symmetric use at the current time.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/21888,376185338,2020-02-07T02:22:54Z,test/cpp/interop/xds_interop_client.cc,"@@ -0,0 +1,242 @@+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <chrono>+#include <condition_variable>+#include <map>+#include <mutex>+#include <set>+#include <sstream>+#include <string>+#include <thread>+#include <vector>++#include <gflags/gflags.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include ""src/proto/grpc/testing/empty.pb.h""+#include ""src/proto/grpc/testing/messages.pb.h""+#include ""src/proto/grpc/testing/test.grpc.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/test_config.h""++DEFINE_int32(num_channels, 1, ""Number of channels."");+DEFINE_bool(print_response, false, ""Write RPC response to stdout."");+DEFINE_int32(qps, 1, ""Qps per channel."");+DEFINE_int32(rpc_timeout_sec, 10, ""Per RPC timeout seconds."");+DEFINE_string(server, ""localhost:50051"", ""Address of server."");+DEFINE_int32(stats_port, 50052,+             ""Port to expose peer distribution stats service."");++using grpc::Channel;+using grpc::ClientContext;+using grpc::Server;+using grpc::ServerBuilder;+using grpc::ServerContext;+using grpc::ServerCredentials;+using grpc::ServerReader;+using grpc::ServerReaderWriter;+using grpc::ServerWriter;+using grpc::Status;+using grpc::testing::LoadBalancerStatsRequest;+using grpc::testing::LoadBalancerStatsResponse;+using grpc::testing::LoadBalancerStatsService;+using grpc::testing::SimpleRequest;+using grpc::testing::SimpleResponse;+using grpc::testing::TestService;++class XdsStatsWatcher;++// Unique ID for each outgoing RPC+int global_request_id;+// Stores a set of watchers that should be notified upon outgoing RPC completion+std::set<XdsStatsWatcher*> watchers;+// Mutex for global_request_id and watchers+std::mutex mu;++/** Records the remote peer distribution for a given range of RPCs. */+class XdsStatsWatcher {+ public:+  XdsStatsWatcher(int start_id, int end_id)+      : start_id_(start_id), end_id_(end_id), rpcs_needed_(end_id - start_id) {}++  void RpcCompleted(int request_id, std::string peer) {+    if (start_id_ <= request_id && request_id < end_id_) {+      {+        std::lock_guard<std::mutex> lk(m_);+        if (peer.empty()) {+          no_remote_peer_++;+        } else {+          rpcs_by_peer_[peer]++;+        }+        rpcs_needed_--;+      }+      cv_.notify_one();+    }+  }++  void WaitForRpcStatsResponse(LoadBalancerStatsResponse* response,+                               int timeout_sec) {+    {+      std::unique_lock<std::mutex> lk(m_);+      cv_.wait_for(lk, std::chrono::seconds(timeout_sec),+                   [this] { return rpcs_needed_ == 0; });+      response->mutable_rpcs_by_peer()->insert(rpcs_by_peer_.begin(),+                                               rpcs_by_peer_.end());+      response->set_num_failures(no_remote_peer_ + rpcs_needed_);+    }+  }++ private:+  int start_id_;+  int end_id_;+  int rpcs_needed_;+  std::map<std::string, int> rpcs_by_peer_;+  int no_remote_peer_;+  std::mutex m_;+  std::condition_variable cv_;+};++class TestClient {+ public:+  TestClient(std::shared_ptr<Channel> channel)+      : stub_(TestService::NewStub(channel)) {}++  void UnaryCall() {+    SimpleResponse response;+    ClientContext context;++    int saved_request_id;+    {+      std::lock_guard<std::mutex> lk(mu);+      saved_request_id = ++global_request_id;+    }+    std::chrono::system_clock::time_point deadline =+        std::chrono::system_clock::now() ++        std::chrono::seconds(FLAGS_rpc_timeout_sec);+    context.set_deadline(deadline);+    Status status = stub_->UnaryCall(+        &context, SimpleRequest::default_instance(), &response);++    {+      std::lock_guard<std::mutex> lk(mu);+      for (auto watcher : watchers) {+        watcher->RpcCompleted(saved_request_id, response.hostname());+      }+    }++    if (FLAGS_print_response) {+      if (status.ok()) {+        std::cout << ""Greeting: Hello world, this is "" << response.hostname()+                  << "", from "" << context.peer() << std::endl;+      } else {+        std::cout << ""RPC failed: "" << status.error_code() << "": ""+                  << status.error_message() << std::endl;+      }+    }+  }++ private:+  std::unique_ptr<TestService::Stub> stub_;+};++class LoadBalancerStatsServiceImpl : public LoadBalancerStatsService::Service {+ public:+  Status GetClientStats(ServerContext* context,+                        const LoadBalancerStatsRequest* request,+                        LoadBalancerStatsResponse* response) {+    int start_id;","This is the mechanism for the client to expose LB-related stats during the test. It's a gRPC server exposed by the client process and queried by an external test driver: This is implemented in https://github.com/grpc/grpc/pull/21869 (`run_xds_tests.py`) in OSS and a similar Python-based test driver in our google3 grpclb tests. The driver spins up the server(s) and a client process, manipulates the LB (traffic director) configuration as appropriate, and sends a gRPC request to the `GetClientStats` method to verify client-side LB decisions.",
28025951,HannahShiSFB,https://api.github.com/repos/grpc/grpc/pulls/21832,376530519,2020-02-07T18:09:01Z,src/core/lib/security/credentials/credentials.cc,"@@ -45,6 +45,14 @@ void grpc_channel_credentials_release(grpc_channel_credentials* creds) {   if (creds) creds->Unref(); } +grpc_channel_credentials* grpc_channel_credentials_copy(","The goal is adding a copy(ref) of the grpc creds object.Since the grpc creds object is created by php within channel object, when php destroy its channel object, if we do nothing,  the grpc creds will be destroyed with it. To avoid the grpc creds be destroyed then, since we still need it later(when fork), we introduce a c-core API to add a copy(ref) of the grpc creds object. Adding a c-core API is the only way to call creds->Ref().release() in it to add a copy(ref).Q: why we name it _copy instead of _ref? A: several other similar existing functions with Ref().release() named this way. eg, credentials_pointer_arg_copy(), target_authority_table_copy(), channelz_node_copy(), grpc_channel_args_copy(), etc.Q: why we can only call Ref().release() to add a copy(ref)?A: adding a copy(ref) equals count++. cpp can automatically add 1 when we put creds obj in persistent list, but in c, it's just a pointer, we have to add 1 by ourselves. Since we cannot access the count directly, cannot simply call count++, so we use Ref().release() to get there.How? in ref_counted.h:L253 Ref() returns RefCountedPtr. release() is to make sure when RefCountedPtr distruct, count-- will not happen. When construct count will +1, when distruct, count remains its value, eventually we made count++. ",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/21856,376768612,2020-02-09T09:33:52Z,doc/xds-test-descriptions.md,"@@ -0,0 +1,307 @@+# xDS (Load-Balancing) Interop Test Case Descriptions++Client and server use [test.proto](../src/proto/grpc/testing/test.proto).++## Server++The code for the xDS test server can be found+[here](https://github.com/grpc/grpc-java/blob/master/interop-testing/src/main/java/io/grpc/testing/integration/XdsTestServer.java)+(Java; other language implementations are in progress). are in progress).++Server should accept these arguments:++*   --port=PORT+    *   The port the server will run on.+","This is actually not required, and convenient to not use, for OSS tests. The hostname on GCP corresponds nicely to the instance name, and so the instance template used for the server backends can just start a gRPC server, without needing an `id` parameter from the test script (otherwise `run_xds_tests.py` would have to start up the instances and separately pass a parameter to each server, likely at start up time - instance templates cannot have params). This wouldn't be too inconvenient given the final structure of `run_xds_tests.py`, but it's not necessary right now.Internally, we currently use `server_id` (and may have to continue to do so) to distinguish the backends for these tests, but I don't think it needs to be specced out here at the moment.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21954,377245340,2020-02-10T18:42:38Z,src/python/grpcio/grpc/BUILD.bazel,"@@ -85,3 +86,8 @@ py_library(         "":common"",     ], )++py_library(+    name = ""_simple_stubs"",+    srcs = [""_simple_stubs.py""],+)",How about move this library before it is referenced?,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21954,377279781,2020-02-10T19:49:34Z,src/python/grpcio/grpc/_simple_stubs.py,"@@ -0,0 +1,440 @@+# Copyright 2020 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Functions that obviate explicit stubs and explicit channels.""""""++import collections+import datetime+import os+import logging+import threading+from typing import Any, AnyStr, Callable, Iterator, Optional, Sequence, Tuple, TypeVar, Union++import grpc++_LOGGER = logging.getLogger(__name__)++_EVICTION_PERIOD_KEY = ""GRPC_PYTHON_MANAGED_CHANNEL_EVICTION_SECONDS""+if _EVICTION_PERIOD_KEY in os.environ:+    _EVICTION_PERIOD = datetime.timedelta(+        seconds=float(os.environ[_EVICTION_PERIOD_KEY]))+    _LOGGER.info(+        ""Setting managed channel eviction period to %s"", _EVICTION_PERIOD)+else:+    _EVICTION_PERIOD = datetime.timedelta(minutes=10)++_MAXIMUM_CHANNELS_KEY = ""GRPC_PYTHON_MANAGED_CHANNEL_MAXIMUM""+if _MAXIMUM_CHANNELS_KEY in os.environ:+    _MAXIMUM_CHANNELS = int(os.environ[_MAXIMUM_CHANNELS_KEY])+    _LOGGER.info(""Setting maximum managed channels to %d"", _MAXIMUM_CHANNELS)+else:+    _MAXIMUM_CHANNELS = 2**8+++def _create_channel(target: str, options: Sequence[Tuple[str, str]],+                    channel_credentials: Optional[grpc.ChannelCredentials],+                    compression: Optional[grpc.Compression]) -> grpc.Channel:+    channel_credentials = channel_credentials or grpc.local_channel_credentials(+    )+    if channel_credentials._credentials is grpc.experimental._insecure_channel_credentials:+        _LOGGER.info(f""Creating insecure channel with options '{options}' "" ++                     f""and compression '{compression}'"")+        return grpc.insecure_channel(target,+                                     options=options,+                                     compression=compression)+    else:+        _LOGGER.info(+            f""Creating secure channel with credentials '{channel_credentials}', ""+            + f""options '{options}' and compression '{compression}'"")+        return grpc.secure_channel(target,+                                   credentials=channel_credentials,+                                   options=options,+                                   compression=compression)+++class ChannelCache:+    _singleton = None+    _lock = threading.RLock()+    _condition = threading.Condition(lock=_lock)+    _eviction_ready = threading.Event()++    def __init__(self):+        self._mapping = collections.OrderedDict()",optional: we could use another list to enforce LRU eviction. So we can save the copy of `keys()` or `items()`.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21954,377309264,2020-02-10T20:51:54Z,src/python/grpcio/grpc/_simple_stubs.py,"@@ -0,0 +1,440 @@+# Copyright 2020 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Functions that obviate explicit stubs and explicit channels.""""""++import collections+import datetime+import os+import logging+import threading+from typing import Any, AnyStr, Callable, Iterator, Optional, Sequence, Tuple, TypeVar, Union++import grpc++_LOGGER = logging.getLogger(__name__)++_EVICTION_PERIOD_KEY = ""GRPC_PYTHON_MANAGED_CHANNEL_EVICTION_SECONDS""+if _EVICTION_PERIOD_KEY in os.environ:+    _EVICTION_PERIOD = datetime.timedelta(+        seconds=float(os.environ[_EVICTION_PERIOD_KEY]))+    _LOGGER.info(+        ""Setting managed channel eviction period to %s"", _EVICTION_PERIOD)+else:+    _EVICTION_PERIOD = datetime.timedelta(minutes=10)++_MAXIMUM_CHANNELS_KEY = ""GRPC_PYTHON_MANAGED_CHANNEL_MAXIMUM""+if _MAXIMUM_CHANNELS_KEY in os.environ:+    _MAXIMUM_CHANNELS = int(os.environ[_MAXIMUM_CHANNELS_KEY])+    _LOGGER.info(""Setting maximum managed channels to %d"", _MAXIMUM_CHANNELS)+else:+    _MAXIMUM_CHANNELS = 2**8+++def _create_channel(target: str, options: Sequence[Tuple[str, str]],+                    channel_credentials: Optional[grpc.ChannelCredentials],+                    compression: Optional[grpc.Compression]) -> grpc.Channel:+    channel_credentials = channel_credentials or grpc.local_channel_credentials(+    )+    if channel_credentials._credentials is grpc.experimental._insecure_channel_credentials:+        _LOGGER.info(f""Creating insecure channel with options '{options}' "" ++                     f""and compression '{compression}'"")+        return grpc.insecure_channel(target,+                                     options=options,+                                     compression=compression)+    else:+        _LOGGER.info(+            f""Creating secure channel with credentials '{channel_credentials}', ""+            + f""options '{options}' and compression '{compression}'"")+        return grpc.secure_channel(target,+                                   credentials=channel_credentials,+                                   options=options,+                                   compression=compression)+++class ChannelCache:+    _singleton = None+    _lock = threading.RLock()+    _condition = threading.Condition(lock=_lock)+    _eviction_ready = threading.Event()++    def __init__(self):+        self._mapping = collections.OrderedDict()","Discussed offline. It's a good point that we should be worried about any potential copies. But in Python 3, `items()` is [natively a lazily evaluated ""view"" object](https://github.com/python/cpython/blob/3c5dec65e99ab7b641f1663c88e3332fff944881/Objects/dictobject.c#L3456), so this should be good as is.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21954,377315410,2020-02-10T21:04:41Z,src/python/grpcio/grpc/_simple_stubs.py,"@@ -0,0 +1,440 @@+# Copyright 2020 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Functions that obviate explicit stubs and explicit channels.""""""++import collections+import datetime+import os+import logging+import threading+from typing import Any, AnyStr, Callable, Iterator, Optional, Sequence, Tuple, TypeVar, Union++import grpc++_LOGGER = logging.getLogger(__name__)++_EVICTION_PERIOD_KEY = ""GRPC_PYTHON_MANAGED_CHANNEL_EVICTION_SECONDS""+if _EVICTION_PERIOD_KEY in os.environ:+    _EVICTION_PERIOD = datetime.timedelta(+        seconds=float(os.environ[_EVICTION_PERIOD_KEY]))+    _LOGGER.info(+        ""Setting managed channel eviction period to %s"", _EVICTION_PERIOD)+else:+    _EVICTION_PERIOD = datetime.timedelta(minutes=10)++_MAXIMUM_CHANNELS_KEY = ""GRPC_PYTHON_MANAGED_CHANNEL_MAXIMUM""+if _MAXIMUM_CHANNELS_KEY in os.environ:+    _MAXIMUM_CHANNELS = int(os.environ[_MAXIMUM_CHANNELS_KEY])+    _LOGGER.info(""Setting maximum managed channels to %d"", _MAXIMUM_CHANNELS)+else:+    _MAXIMUM_CHANNELS = 2**8+++def _create_channel(target: str, options: Sequence[Tuple[str, str]],+                    channel_credentials: Optional[grpc.ChannelCredentials],+                    compression: Optional[grpc.Compression]) -> grpc.Channel:+    channel_credentials = channel_credentials or grpc.local_channel_credentials(+    )+    if channel_credentials._credentials is grpc.experimental._insecure_channel_credentials:+        _LOGGER.info(f""Creating insecure channel with options '{options}' "" ++                     f""and compression '{compression}'"")+        return grpc.insecure_channel(target,+                                     options=options,+                                     compression=compression)+    else:+        _LOGGER.info(+            f""Creating secure channel with credentials '{channel_credentials}', ""+            + f""options '{options}' and compression '{compression}'"")+        return grpc.secure_channel(target,+                                   credentials=channel_credentials,+                                   options=options,+                                   compression=compression)+++class ChannelCache:+    _singleton = None+    _lock = threading.RLock()+    _condition = threading.Condition(lock=_lock)+    _eviction_ready = threading.Event()++    def __init__(self):+        self._mapping = collections.OrderedDict()+        self._eviction_thread = threading.Thread(+            target=ChannelCache._perform_evictions, daemon=True)+        self._eviction_thread.start()++    @staticmethod+    def get():+        with ChannelCache._lock:+            if ChannelCache._singleton is None:+                ChannelCache._singleton = ChannelCache()+        ChannelCache._eviction_ready.wait()+        return ChannelCache._singleton++    # TODO: Type annotate key.+    def _evict_locked(self, key):+        channel, _ = self._mapping.pop(key)+        _LOGGER.info(""Evicting channel %s with configuration %s."", channel, key)+        channel.close()+        del channel++    # TODO: Refactor. Way too deeply nested.+    @staticmethod+    def _perform_evictions():+        while True:+            with ChannelCache._lock:+                ChannelCache._eviction_ready.set()+                if not ChannelCache._singleton._mapping:+                    ChannelCache._condition.wait()+                elif len(ChannelCache._singleton._mapping) > _MAXIMUM_CHANNELS:+                    key = next(iter(ChannelCache._singleton._mapping.keys()))+                    ChannelCache._singleton._evict_locked(key)+                    # And immediately reevaluate.+                else:+                    key, (_, eviction_time) = next(+                        iter(ChannelCache._singleton._mapping.items()))+                    now = datetime.datetime.now()+                    if eviction_time <= now:+                        ChannelCache._singleton._evict_locked(key)+                        continue+                    else:+                        time_to_eviction = (eviction_time - now).total_seconds()+                        ChannelCache._condition.wait(timeout=time_to_eviction)","Discussed offline.This is true. However, the thread calling `get_channel` notifies when it creates a channel and we are at or above `_MAXIMUM_CHANNELS`. So the eviction thread will soon wake up and rectify the situation.As for the test you suggest, I believe I already wrote one. It caught several race conditions.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21954,377316105,2020-02-10T21:06:14Z,src/python/grpcio/grpc/experimental/__init__.py,"@@ -30,3 +35,40 @@ class ChannelOptions(object):  class UsageError(Exception):     """"""Raised by the gRPC library to indicate usage not allowed by the API.""""""+++_insecure_channel_credentials = object()+++def insecure_channel_credentials():+    """"""Creates a ChannelCredentials for use with an insecure channel.++    THIS IS AN EXPERIMENTAL API.++    This is not for use with secure_channel function. Intead, this should be+    used with grpc.unary_unary, grpc.unary_stream, grpc.stream_unary, or+    grpc.stream_stream.+    """"""+    return grpc.ChannelCredentials(_insecure_channel_credentials)+++class ExperimentalApiWarning(Warning):+    """"""A warning that an API is experimental.""""""+++def warn_experimental(api_name):+    msg = (""{} is an experimental API. It is subject to change or "".format(+        api_name) + ""removal between minor releases. Proceed with caution."")+    warnings.warn(msg, ExperimentalApiWarning, stacklevel=2)","I wanted to be able to use this for arbitrary objects. Not just functions, but also classes. A decorator needs to know what sort of object it's decorating. Making it a parameter gives the author of the API the ability to determine the exact scope of what they are calling an experimental API. Thoughts?",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/21946,377329593,2020-02-10T21:34:21Z,src/core/ext/transport/inproc/inproc_transport.cc,"@@ -1103,10 +1087,6 @@ void perform_stream_op(grpc_transport* gt, grpc_stream* gs,                error);     grpc_core::ExecCtx::Run(DEBUG_LOCATION, on_complete, GRPC_ERROR_REF(error));   }-  if (needs_close) {","This turned out to be dead code. If you look at this particular function, needs_close starts as false and never gets set to true in any circumstance. I don't know why the compiler never identified it as unreachable code (could be because the code overall is quite complicated?)",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21954,377330465,2020-02-10T21:36:01Z,src/python/grpcio/grpc/_simple_stubs.py,"@@ -0,0 +1,440 @@+# Copyright 2020 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Functions that obviate explicit stubs and explicit channels.""""""++import collections+import datetime+import os+import logging+import threading+from typing import Any, AnyStr, Callable, Iterator, Optional, Sequence, Tuple, TypeVar, Union++import grpc++_LOGGER = logging.getLogger(__name__)++_EVICTION_PERIOD_KEY = ""GRPC_PYTHON_MANAGED_CHANNEL_EVICTION_SECONDS""+if _EVICTION_PERIOD_KEY in os.environ:+    _EVICTION_PERIOD = datetime.timedelta(+        seconds=float(os.environ[_EVICTION_PERIOD_KEY]))+    _LOGGER.info(+        ""Setting managed channel eviction period to %s"", _EVICTION_PERIOD)+else:+    _EVICTION_PERIOD = datetime.timedelta(minutes=10)++_MAXIMUM_CHANNELS_KEY = ""GRPC_PYTHON_MANAGED_CHANNEL_MAXIMUM""+if _MAXIMUM_CHANNELS_KEY in os.environ:+    _MAXIMUM_CHANNELS = int(os.environ[_MAXIMUM_CHANNELS_KEY])+    _LOGGER.info(""Setting maximum managed channels to %d"", _MAXIMUM_CHANNELS)+else:+    _MAXIMUM_CHANNELS = 2**8+++def _create_channel(target: str, options: Sequence[Tuple[str, str]],+                    channel_credentials: Optional[grpc.ChannelCredentials],+                    compression: Optional[grpc.Compression]) -> grpc.Channel:+    channel_credentials = channel_credentials or grpc.local_channel_credentials(+    )+    if channel_credentials._credentials is grpc.experimental._insecure_channel_credentials:+        _LOGGER.info(f""Creating insecure channel with options '{options}' "" ++                     f""and compression '{compression}'"")+        return grpc.insecure_channel(target,+                                     options=options,+                                     compression=compression)+    else:+        _LOGGER.info(+            f""Creating secure channel with credentials '{channel_credentials}', ""+            + f""options '{options}' and compression '{compression}'"")+        return grpc.secure_channel(target,+                                   credentials=channel_credentials,+                                   options=options,+                                   compression=compression)+++class ChannelCache:+    _singleton = None+    _lock = threading.RLock()+    _condition = threading.Condition(lock=_lock)+    _eviction_ready = threading.Event()","As for the naming, my intent was to refer to the behavior (eviction) rather than to the concurrency primitive which accomplishes it (a thread). But I'm deferring to the advice of the reviewer.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21904,377644681,2020-02-11T13:50:04Z,src/python/grpcio_tests/tests_aio/benchmark/worker_servicer.py,"@@ -0,0 +1,368 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import asyncio+import collections+import logging+import multiprocessing+import os+import sys+import time+from typing import Tuple++import grpc+from grpc.experimental import aio++from src.proto.grpc.testing import (benchmark_service_pb2_grpc, control_pb2,+                                    stats_pb2, worker_service_pb2_grpc)+from tests.qps import histogram+from tests.unit import resources+from tests.unit.framework.common import get_socket+from tests_aio.benchmark import benchmark_client, benchmark_servicer++_NUM_CORES = multiprocessing.cpu_count()+_WORKER_ENTRY_FILE = os.path.split(os.path.abspath(__file__))[0] + '/worker.py'++_LOGGER = logging.getLogger(__name__)+++class _SubWorker(+        collections.namedtuple('_SubWorker',+                               ['process', 'port', 'channel', 'stub'])):+    """"""A data class that holds information about a child qps worker.""""""++    def _repr(self):+        return f'<_SubWorker pid={self.process.pid} port={self.port}>'++    def __repr__(self):+        return self._repr()++    def __str__(self):+        return self._repr()+++def _get_server_status(start_time: float, end_time: float,+                       port: int) -> control_pb2.ServerStatus:+    """"""Creates ServerStatus proto message.""""""+    end_time = time.time()+    elapsed_time = end_time - start_time+    stats = stats_pb2.ServerStats(time_elapsed=elapsed_time,+                                  time_user=elapsed_time,+                                  time_system=elapsed_time)+    return control_pb2.ServerStatus(stats=stats, port=port, cores=_NUM_CORES)+++def _create_server(config: control_pb2.ServerConfig) -> Tuple[aio.Server, int]:+    """"""Creates a server object according to the ServerConfig.""""""+    channel_args = tuple(+        (arg.name,+         arg.str_value) if arg.HasField('str_value') else (arg.name,+                                                           int(arg.int_value))+        for arg in config.channel_args)++    server = aio.server(options=channel_args + (('grpc.so_reuseport', 1),))+    if config.server_type == control_pb2.ASYNC_SERVER:+        servicer = benchmark_servicer.BenchmarkServicer()+        benchmark_service_pb2_grpc.add_BenchmarkServiceServicer_to_server(+            servicer, server)+    elif config.server_type == control_pb2.ASYNC_GENERIC_SERVER:+        resp_size = config.payload_config.bytebuf_params.resp_size+        servicer = benchmark_servicer.GenericBenchmarkServicer(resp_size)+        method_implementations = {+            'StreamingCall':+                grpc.stream_stream_rpc_method_handler(servicer.StreamingCall),+            'UnaryCall':+                grpc.unary_unary_rpc_method_handler(servicer.UnaryCall),+        }+        handler = grpc.method_handlers_generic_handler(+            'grpc.testing.BenchmarkService', method_implementations)+        server.add_generic_rpc_handlers((handler,))+    else:+        raise NotImplementedError('Unsupported server type {}'.format(+            config.server_type))++    if config.HasField('security_params'):  # Use SSL+        server_creds = grpc.ssl_server_credentials(+            ((resources.private_key(), resources.certificate_chain()),))+        port = server.add_secure_port('[::]:{}'.format(config.port),+                                      server_creds)+    else:+        port = server.add_insecure_port('[::]:{}'.format(config.port))++    return server, port+++def _get_client_status(start_time: float, end_time: float,+                       qps_data: histogram.Histogram+                      ) -> control_pb2.ClientStatus:+    """"""Creates ClientStatus proto message.""""""+    latencies = qps_data.get_data()+    end_time = time.time()+    elapsed_time = end_time - start_time+    stats = stats_pb2.ClientStats(latencies=latencies,+                                  time_elapsed=elapsed_time,+                                  time_user=elapsed_time,+                                  time_system=elapsed_time)+    return control_pb2.ClientStatus(stats=stats)+++def _create_client(server: str, config: control_pb2.ClientConfig,+                   qps_data: histogram.Histogram+                  ) -> benchmark_client.BenchmarkClient:+    """"""Creates a client object according to the ClientConfig.""""""+    if config.load_params.WhichOneof('load') != 'closed_loop':+        raise NotImplementedError(+            f'Unsupported load parameter {config.load_params}')++    if config.client_type == control_pb2.ASYNC_CLIENT:+        if config.rpc_type == control_pb2.UNARY:+            client_type = benchmark_client.UnaryAsyncBenchmarkClient+        elif config.rpc_type == control_pb2.STREAMING:+            client_type = benchmark_client.StreamingAsyncBenchmarkClient+        else:+            raise NotImplementedError(+                f'Unsupported rpc_type [{config.rpc_type}]')+    else:+        raise NotImplementedError(+            f'Unsupported client type {config.client_type}')++    return client_type(server, config, qps_data)+++def _pick_an_unused_port() -> int:+    """"""Picks an unused TCP port.""""""+    _, port, sock = get_socket()+    sock.close()+    return port+++async def _create_sub_worker() -> _SubWorker:+    """"""Creates a child qps worker as a subprocess.""""""+    port = _pick_an_unused_port()++    _LOGGER.info('Creating sub worker at port [%d]...', port)+    process = await asyncio.create_subprocess_exec(sys.executable,+                                                   _WORKER_ENTRY_FILE,+                                                   '--driver_port', str(port))","Maybe I'm playing as the advocate of the devil, but IMO there is an outstanding question.How much could benefit the synchronous client by parallelizing also by using different processes? Having the feeling that the QPS of the Synchronous client once we start parallelizing by a process, besides by threads within each process, would bost up the QPS.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21904,377846667,2020-02-11T19:22:06Z,src/python/grpcio_tests/tests_aio/benchmark/benchmark_client.py,"@@ -0,0 +1,157 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""The Python AsyncIO Benchmark Clients.""""""++import abc+import asyncio+import time+import logging+import random++import grpc+from grpc.experimental import aio++from src.proto.grpc.testing import (benchmark_service_pb2_grpc, control_pb2,+                                    messages_pb2)+from tests.qps import histogram+from tests.unit import resources+++class GenericStub(object):++    def __init__(self, channel: aio.Channel):+        self.UnaryCall = channel.unary_unary(+            '/grpc.testing.BenchmarkService/UnaryCall')+        self.StreamingCall = channel.stream_stream(+            '/grpc.testing.BenchmarkService/StreamingCall')+++class BenchmarkClient(abc.ABC):+    """"""Benchmark client interface that exposes a non-blocking send_request().""""""++    def __init__(self, address: str, config: control_pb2.ClientConfig,+                 hist: histogram.Histogram):+        # Disables underlying reuse of subchannels+        unique_option = (('iv', random.random()),)++        # Parses the channel argument from config+        channel_args = tuple(+            (arg.name, arg.str_value) if arg.HasField('str_value') else (+                arg.name, int(arg.int_value)) for arg in config.channel_args)++        # Creates the channel+        if config.HasField('security_params'):+            channel_credentials = grpc.ssl_channel_credentials(+                resources.test_root_certificates(),)+            server_host_override_option = ((+                'grpc.ssl_target_name_override',+                config.security_params.server_host_override,+            ),)+            self._channel = aio.secure_channel(+                address, channel_credentials,+                unique_option + channel_args + server_host_override_option)+        else:+            self._channel = aio.insecure_channel(address,+                                                 options=unique_option ++                                                 channel_args)++        # Creates the stub+        if config.payload_config.WhichOneof('payload') == 'simple_params':+            self._generic = False+            self._stub = benchmark_service_pb2_grpc.BenchmarkServiceStub(+                self._channel)+            payload = messages_pb2.Payload(+                body=b'\0' * config.payload_config.simple_params.req_size)+            self._request = messages_pb2.SimpleRequest(+                payload=payload,+                response_size=config.payload_config.simple_params.resp_size)+        else:+            self._generic = True+            self._stub = GenericStub(self._channel)+            self._request = b'\0' * config.payload_config.bytebuf_params.req_size++        self._hist = hist+        self._response_callbacks = []+        self._concurrency = config.outstanding_rpcs_per_channel++    async def run(self) -> None:+        await self._channel.channel_ready()++    async def stop(self) -> None:+        await self._channel.close()++    def _record_query_time(self, query_time: float) -> None:+        self._hist.add(query_time * 1e9)+++class UnaryAsyncBenchmarkClient(BenchmarkClient):++    def __init__(self, address: str, config: control_pb2.ClientConfig,+                 hist: histogram.Histogram):+        super().__init__(address, config, hist)+        self._running = None+        self._stopped = asyncio.Event()++    async def _send_request(self):+        start_time = time.time()+        await self._stub.UnaryCall(self._request)+        self._record_query_time(time.time() - start_time)++    async def _infinite_sender(self) -> None:+        while self._running:+            await self._send_request()++    async def run(self) -> None:+        await super().run()+        self._running = True+        senders = (self._infinite_sender() for _ in range(self._concurrency))+        await asyncio.gather(*senders)+        self._stopped.set()++    async def stop(self) -> None:+        self._running = False+        await self._stopped.wait()+        await super().stop()+++class StreamingAsyncBenchmarkClient(BenchmarkClient):++    def __init__(self, address: str, config: control_pb2.ClientConfig,+                 hist: histogram.Histogram):+        super().__init__(address, config, hist)+        self._running = None+        self._stopped = asyncio.Event()++    async def _one_streamming_call(self):+        call = self._stub.StreamingCall()+        while self._running:+            start_time = time.time()+            await call.write(self._request)+            await call.read()+            self._record_query_time(time.time() - start_time)+        await call.done_writing()+        assert grpc.StatusCode.OK == await call.code()","Removed. This is not necessary. If anything goes wrong, the exception will halt the benchmark.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21954,377885919,2020-02-11T20:41:33Z,src/python/grpcio/grpc/experimental/__init__.py,"@@ -56,16 +59,30 @@ class ExperimentalApiWarning(Warning):     """"""A warning that an API is experimental.""""""  -def warn_experimental(api_name):-    msg = (""{} is an experimental API. It is subject to change or "".format(-        api_name) + ""removal between minor releases. Proceed with caution."")-    warnings.warn(msg, ExperimentalApiWarning, stacklevel=2)+def _warn_experimental(api_name, stack_offset):+    if api_name not in _EXPERIMENTAL_APIS_USED:+        _EXPERIMENTAL_APIS_USED.add(api_name)+        msg = (""'{}' is an experimental API. It is subject to change or "".+               format(api_name) ++               ""removal between minor releases. Proceed with caution."")+        warnings.warn(msg, ExperimentalApiWarning, stacklevel=2 + stack_offset)+++def experimental_api(f):++    @functools.wraps(f)+    def _wrapper(*args, **kwargs):+        _warn_experimental(f.__name__, 1)+        return f(*args, **kwargs)++    return _wrapper   __all__ = (     'ChannelOptions',     'ExperimentalApiWarning',     'UsageError',+    'experimental_api',","optional: If this API is only meant for our library's internal usage, maybe we should not put it in `__all__`.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21904,377893207,2020-02-11T20:56:26Z,src/python/grpcio_tests/tests_aio/benchmark/worker_servicer.py,"@@ -0,0 +1,368 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import asyncio+import collections+import logging+import multiprocessing+import os+import sys+import time+from typing import Tuple++import grpc+from grpc.experimental import aio++from src.proto.grpc.testing import (benchmark_service_pb2_grpc, control_pb2,+                                    stats_pb2, worker_service_pb2_grpc)+from tests.qps import histogram+from tests.unit import resources+from tests.unit.framework.common import get_socket+from tests_aio.benchmark import benchmark_client, benchmark_servicer++_NUM_CORES = multiprocessing.cpu_count()+_WORKER_ENTRY_FILE = os.path.split(os.path.abspath(__file__))[0] + '/worker.py'++_LOGGER = logging.getLogger(__name__)+++class _SubWorker(+        collections.namedtuple('_SubWorker',+                               ['process', 'port', 'channel', 'stub'])):+    """"""A data class that holds information about a child qps worker.""""""++    def _repr(self):+        return f'<_SubWorker pid={self.process.pid} port={self.port}>'++    def __repr__(self):+        return self._repr()++    def __str__(self):+        return self._repr()+++def _get_server_status(start_time: float, end_time: float,+                       port: int) -> control_pb2.ServerStatus:+    """"""Creates ServerStatus proto message.""""""+    end_time = time.time()+    elapsed_time = end_time - start_time+    stats = stats_pb2.ServerStats(time_elapsed=elapsed_time,+                                  time_user=elapsed_time,+                                  time_system=elapsed_time)+    return control_pb2.ServerStatus(stats=stats, port=port, cores=_NUM_CORES)+++def _create_server(config: control_pb2.ServerConfig) -> Tuple[aio.Server, int]:+    """"""Creates a server object according to the ServerConfig.""""""+    channel_args = tuple(+        (arg.name,+         arg.str_value) if arg.HasField('str_value') else (arg.name,+                                                           int(arg.int_value))+        for arg in config.channel_args)++    server = aio.server(options=channel_args + (('grpc.so_reuseport', 1),))+    if config.server_type == control_pb2.ASYNC_SERVER:+        servicer = benchmark_servicer.BenchmarkServicer()+        benchmark_service_pb2_grpc.add_BenchmarkServiceServicer_to_server(+            servicer, server)+    elif config.server_type == control_pb2.ASYNC_GENERIC_SERVER:+        resp_size = config.payload_config.bytebuf_params.resp_size+        servicer = benchmark_servicer.GenericBenchmarkServicer(resp_size)+        method_implementations = {+            'StreamingCall':+                grpc.stream_stream_rpc_method_handler(servicer.StreamingCall),+            'UnaryCall':+                grpc.unary_unary_rpc_method_handler(servicer.UnaryCall),+        }+        handler = grpc.method_handlers_generic_handler(+            'grpc.testing.BenchmarkService', method_implementations)+        server.add_generic_rpc_handlers((handler,))+    else:+        raise NotImplementedError('Unsupported server type {}'.format(+            config.server_type))++    if config.HasField('security_params'):  # Use SSL+        server_creds = grpc.ssl_server_credentials(+            ((resources.private_key(), resources.certificate_chain()),))+        port = server.add_secure_port('[::]:{}'.format(config.port),+                                      server_creds)+    else:+        port = server.add_insecure_port('[::]:{}'.format(config.port))++    return server, port+++def _get_client_status(start_time: float, end_time: float,+                       qps_data: histogram.Histogram+                      ) -> control_pb2.ClientStatus:+    """"""Creates ClientStatus proto message.""""""+    latencies = qps_data.get_data()+    end_time = time.time()+    elapsed_time = end_time - start_time+    stats = stats_pb2.ClientStats(latencies=latencies,+                                  time_elapsed=elapsed_time,+                                  time_user=elapsed_time,+                                  time_system=elapsed_time)+    return control_pb2.ClientStatus(stats=stats)+++def _create_client(server: str, config: control_pb2.ClientConfig,+                   qps_data: histogram.Histogram+                  ) -> benchmark_client.BenchmarkClient:+    """"""Creates a client object according to the ClientConfig.""""""+    if config.load_params.WhichOneof('load') != 'closed_loop':+        raise NotImplementedError(+            f'Unsupported load parameter {config.load_params}')++    if config.client_type == control_pb2.ASYNC_CLIENT:+        if config.rpc_type == control_pb2.UNARY:+            client_type = benchmark_client.UnaryAsyncBenchmarkClient+        elif config.rpc_type == control_pb2.STREAMING:+            client_type = benchmark_client.StreamingAsyncBenchmarkClient+        else:+            raise NotImplementedError(+                f'Unsupported rpc_type [{config.rpc_type}]')+    else:+        raise NotImplementedError(+            f'Unsupported client type {config.client_type}')++    return client_type(server, config, qps_data)+++def _pick_an_unused_port() -> int:+    """"""Picks an unused TCP port.""""""+    _, port, sock = get_socket()+    sock.close()+    return port+++async def _create_sub_worker() -> _SubWorker:+    """"""Creates a child qps worker as a subprocess.""""""+    port = _pick_an_unused_port()++    _LOGGER.info('Creating sub worker at port [%d]...', port)+    process = await asyncio.create_subprocess_exec(sys.executable,+                                                   _WORKER_ENTRY_FILE,+                                                   '--driver_port', str(port))","I will mark multi-processing results differently than single threaded results. The improvement of AsyncIO stack lands in both QPS and QPS per core, which are individually significant enough.As for improving sync stack benchmark, we can do it in another PR if we want to compare the parallel ability. Tuning the infrastructure is time-consuming. Existing benchmark results in database are around 2k-3k, which means there are certainly many space for improvement (for client-side).",
6262765,jboeuf,https://api.github.com/repos/grpc/grpc/pulls/21967,378483773,2020-02-12T20:04:25Z,test/core/security/oauth2_utils.cc,"@@ -110,6 +108,9 @@ char* grpc_test_fetch_oauth2_token_with_credentials(   gpr_mu_unlock(request.mu);    grpc_pollset_shutdown(grpc_polling_entity_pollset(&request.pops),-                        &destroy_after_shutdown_closure);+                        &do_nothing_closure);",Just followed some existing pattern but it looks like the nullptr approach also works (just tested it).  @vjpai can you please confirm that it's safe.,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21988,378499862,2020-02-12T20:39:50Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -383,22 +351,32 @@ def __init__(self, target: str, options: ChannelArgumentType,         # No new calls will be accepted by the Cython channel.         self._channel.closing() -        if grace:-            # pylint: disable=unused-variable-            _, pending = await asyncio.wait(self._ongoing_calls.calls,-                                            timeout=grace,-                                            loop=self._loop)--            if not pending:-                return--        # A new set is created acting as a shallow copy because-        # when cancellation happens the calls are automatically-        # removed from the originally set.-        calls = WeakSet(data=self._ongoing_calls.calls)+        # Iterate through running tasks+        tasks = _all_tasks()+        calls = []+        call_tasks = []+        for task in tasks:+            stack = task.get_stack(limit=1)+            if not stack:+                continue++            # Locate ones created by `aio.Call`.+            frame = stack[0]+            if 'self' in frame.f_locals:+                if isinstance(frame.f_locals['self'], _base_call.Call):+                    calls.append(frame.f_locals['self'])+                    call_tasks.append(task)++        # If needed, try to wait for them to finish.+        # Call objects are not always awaitables.+        if grace and call_tasks:+            await asyncio.wait(call_tasks, timeout=grace, loop=self._loop)",Race condition? Isn't it possible for `call_tasks` to change as we await here?,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21987,378512245,2020-02-12T21:06:31Z,doc/python/sphinx/grpc_asyncio.rst,"@@ -0,0 +1,120 @@+gRPC AsyncIO API+================++.. module:: grpc.experimental.aio++Overview+--------++gRPC AsyncIO API is the **new version** of gRPC Python whose architecture is+tailored to AsyncIO. Underlying, it is using C-Core's callback API, and+replaced all IO operations with methods provided by the AsyncIO library.++This stack currently is under active development. Feel free to offer+suggestions by opening issues on our GitHub repo `grpc/grpc <https://github.com/grpc/grpc>`_.++The design doc can be found here as `gRFC <https://github.com/grpc/proposal/pull/155>`_.+++Caveats+-------++gRPC Async API objects may only be used on the thread on which they were+created. AsyncIO doesn't provide thread safety for most of its APIs.+++Module Contents+---------------++Turn-On AsyncIO Mode+^^^^^^^^^^^^^^^^^^^^++.. function:: init_grpc_aio++    Turn-on AsyncIO mode for gRPC Python.++    This function is idempotent, and it should be invoked before creation of+    AsyncIO stack objects. Otherwise, the application might deadlock.++    This function enables AsyncIO IO manager and disables threading for entire+    process. After this point, there should not be blocking calls unless it is+    taken cared by AsyncIO.","Suggestion: ""After invoking this function, making blocking function calls will block the event loop, potentially starving all RPCs in the process. Refer to the Python language documentation on AsyncIO for more details (https://docs.python.org/3/library/asyncio-dev.html#running-blocking-code)"".",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/21987,378513600,2020-02-12T21:09:27Z,doc/python/sphinx/grpc_asyncio.rst,"@@ -0,0 +1,120 @@+gRPC AsyncIO API+================++.. module:: grpc.experimental.aio++Overview+--------++gRPC AsyncIO API is the **new version** of gRPC Python whose architecture is+tailored to AsyncIO. Underlying, it is using C-Core's callback API, and+replaced all IO operations with methods provided by the AsyncIO library.++This stack currently is under active development. Feel free to offer+suggestions by opening issues on our GitHub repo `grpc/grpc <https://github.com/grpc/grpc>`_.++The design doc can be found here as `gRFC <https://github.com/grpc/proposal/pull/155>`_.+++Caveats+-------++gRPC Async API objects may only be used on the thread on which they were+created. AsyncIO doesn't provide thread safety for most of its APIs.+++Module Contents+---------------++Turn-On AsyncIO Mode+^^^^^^^^^^^^^^^^^^^^++.. function:: init_grpc_aio++    Turn-on AsyncIO mode for gRPC Python.++    This function is idempotent, and it should be invoked before creation of+    AsyncIO stack objects. Otherwise, the application might deadlock.++    This function enables AsyncIO IO manager and disables threading for entire","disables threading? the user might get it in the wrong way, the user is free to create new threads after the initialization of the Asyncio IO manager.You meant that gRPC library would be executed using only one thread? if so I would rephrase a bit the content for making it more clear.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21988,378549214,2020-02-12T22:26:37Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -383,22 +351,32 @@ def __init__(self, target: str, options: ChannelArgumentType,         # No new calls will be accepted by the Cython channel.         self._channel.closing() -        if grace:-            # pylint: disable=unused-variable-            _, pending = await asyncio.wait(self._ongoing_calls.calls,-                                            timeout=grace,-                                            loop=self._loop)--            if not pending:-                return--        # A new set is created acting as a shallow copy because-        # when cancellation happens the calls are automatically-        # removed from the originally set.-        calls = WeakSet(data=self._ongoing_calls.calls)+        # Iterate through running tasks+        tasks = _all_tasks()+        calls = []+        call_tasks = []+        for task in tasks:+            stack = task.get_stack(limit=1)+            if not stack:","The exception case is that if the Task is created in an environment that doesn't have normal Python stack, like C-extensions. Here is an example:```<Task pending coro=<AioServer._server_main_loop()> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x10dbb9dc8>()] created at /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/asyncio/base_events.py:396> cb=[AioServer._serving_task_crash_handler()] created at /private/var/tmp/_bazel_lidiz/ee2c6ba1011b403418e3cd5ac063655b/execroot/com_github_grpc_grpc/bazel-out/darwin-fastbuild/bin/src/python/grpcio_tests/tests_aio/unit/close_channel_test.runfiles/com_github_grpc_grpc/src/python/grpcio/grpc/experimental/aio/_server.py:98>```Comment added:```Python# If the Task is created by a C-extension, the stack will be empty.```",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21988,378551128,2020-02-12T22:31:07Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -383,22 +351,32 @@ def __init__(self, target: str, options: ChannelArgumentType,         # No new calls will be accepted by the Cython channel.         self._channel.closing() -        if grace:-            # pylint: disable=unused-variable-            _, pending = await asyncio.wait(self._ongoing_calls.calls,-                                            timeout=grace,-                                            loop=self._loop)--            if not pending:-                return--        # A new set is created acting as a shallow copy because-        # when cancellation happens the calls are automatically-        # removed from the originally set.-        calls = WeakSet(data=self._ongoing_calls.calls)+        # Iterate through running tasks+        tasks = _all_tasks()+        calls = []+        call_tasks = []+        for task in tasks:+            stack = task.get_stack(limit=1)+            if not stack:+                continue++            # Locate ones created by `aio.Call`.+            frame = stack[0]+            if 'self' in frame.f_locals:+                if isinstance(frame.f_locals['self'], _base_call.Call):+                    calls.append(frame.f_locals['self'])+                    call_tasks.append(task)++        # If needed, try to wait for them to finish.+        # Call objects are not always awaitables.+        if grace and call_tasks:+            await asyncio.wait(call_tasks, timeout=grace, loop=self._loop)","The `call_tasks` can only be referenced within this function, and I think they should be free of race condition.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21987,378554663,2020-02-12T22:39:57Z,doc/python/sphinx/grpc_asyncio.rst,"@@ -0,0 +1,120 @@+gRPC AsyncIO API+================++.. module:: grpc.experimental.aio++Overview+--------++gRPC AsyncIO API is the **new version** of gRPC Python whose architecture is+tailored to AsyncIO. Underlying, it is using C-Core's callback API, and","I tried to explain what is Core, since we want to differentiate our effort with pure Python implementation. Here is the updated version:```gRPC AsyncIO API is the **new version** of gRPC Python whose architecture istailored to AsyncIO. Underlying, it utilizes the same C-extension, gRPC C-Core,as existing stack, and it replaces all gRPC IO operations with methods providedby the AsyncIO library.```",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21987,378562411,2020-02-12T22:59:30Z,doc/python/sphinx/grpc_asyncio.rst,"@@ -0,0 +1,120 @@+gRPC AsyncIO API+================++.. module:: grpc.experimental.aio++Overview+--------++gRPC AsyncIO API is the **new version** of gRPC Python whose architecture is+tailored to AsyncIO. Underlying, it is using C-Core's callback API, and+replaced all IO operations with methods provided by the AsyncIO library.++This stack currently is under active development. Feel free to offer+suggestions by opening issues on our GitHub repo `grpc/grpc <https://github.com/grpc/grpc>`_.++The design doc can be found here as `gRFC <https://github.com/grpc/proposal/pull/155>`_.+++Caveats+-------++gRPC Async API objects may only be used on the thread on which they were+created. AsyncIO doesn't provide thread safety for most of its APIs.+++Module Contents+---------------++Turn-On AsyncIO Mode+^^^^^^^^^^^^^^^^^^^^++.. function:: init_grpc_aio++    Turn-on AsyncIO mode for gRPC Python.++    This function is idempotent, and it should be invoked before creation of+    AsyncIO stack objects. Otherwise, the application might deadlock.++    This function enables AsyncIO IO manager and disables threading for entire+    process. After this point, there should not be blocking calls unless it is+    taken cared by AsyncIO.","Updated to your version, and loosen the restriction of making blocking calls. Making blocking calls in a separated thread should be permitted, e.g. writing to files.```    After invoking this function, making blocking function calls in coroutines    or in the thread running event loop will block the event loop, potentially    starving all RPCs in the process. Refer to the Python language    documentation on AsyncIO for more `details <https://docs.python.org/3/library/asyncio-dev.html#running-blocking-code>`_.```",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21987,378572443,2020-02-12T23:29:03Z,src/python/grpcio/grpc/experimental/aio/_base_channel.py,"@@ -0,0 +1,338 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Abstract base classes for Channel objects and Multicallable objects.""""""++import abc+from typing import Any, AsyncIterable, Optional++import grpc++from . import _base_call+from ._typing import DeserializingFunction, MetadataType, SerializingFunction++_IMMUTABLE_EMPTY_TUPLE = tuple()+++class UnaryUnaryMultiCallable(abc.ABC):+    """"""Factory an asynchronous unary-unary RPC stub call from client-side.""""""++    @abc.abstractmethod+    def __call__(self,+                 request: Any,+                 *,+                 timeout: Optional[float] = None,+                 metadata: Optional[MetadataType] = _IMMUTABLE_EMPTY_TUPLE,+                 credentials: Optional[grpc.CallCredentials] = None,+                 wait_for_ready: Optional[bool] = None,+                 compression: Optional[grpc.Compression] = None+                ) -> _base_call.UnaryUnaryCall:+        """"""Asynchronously invokes the underlying RPC.++        Args:+          request: The request value for the RPC.+          timeout: An optional duration of time in seconds to allow+            for the RPC.+          metadata: Optional :term:`metadata` to be transmitted to the+            service-side of the RPC.+          credentials: An optional CallCredentials for the RPC. Only valid for+            secure Channel.+          wait_for_ready: This is an EXPERIMENTAL argument. An optional+            flag to enable wait for ready mechanism+          compression: An element of grpc.compression, e.g.+            grpc.compression.Gzip. This is an EXPERIMENTAL option.++        Returns:+          A Call object instance which is an awaitable object.++        Raises:+          RpcError: Indicating that the RPC terminated with non-OK status. The+            raised RpcError will also be a Call for the RPC affording the RPC's+            metadata, status code, and details.+        """"""+++class UnaryStreamMultiCallable(abc.ABC):+    """"""Affords invoking a unary-stream RPC from client-side in an asynchronous way.""""""++    @abc.abstractmethod+    def __call__(self,+                 request: Any,+                 *,+                 timeout: Optional[float] = None,+                 metadata: Optional[MetadataType] = _IMMUTABLE_EMPTY_TUPLE,+                 credentials: Optional[grpc.CallCredentials] = None,+                 wait_for_ready: Optional[bool] = None,+                 compression: Optional[grpc.Compression] = None+                ) -> _base_call.UnaryStreamCall:+        """"""Asynchronously invokes the underlying RPC.++        Args:+          request: The request value for the RPC.+          timeout: An optional duration of time in seconds to allow+            for the RPC.+          metadata: Optional :term:`metadata` to be transmitted to the+            service-side of the RPC.+          credentials: An optional CallCredentials for the RPC. Only valid for+            secure Channel.+          wait_for_ready: This is an EXPERIMENTAL argument. An optional+            flag to enable wait for ready mechanism+          compression: An element of grpc.compression, e.g.+            grpc.compression.Gzip. This is an EXPERIMENTAL option.++        Returns:+          A Call object instance which is an awaitable object.+        """"""+++class StreamUnaryMultiCallable(abc.ABC):+    """"""Affords invoking a stream-unary RPC from client-side in an asynchronous way.""""""++    @abc.abstractmethod+    def __call__(self,+                 request_async_iterator: Optional[AsyncIterable[Any]] = None,+                 timeout: Optional[float] = None,+                 metadata: Optional[MetadataType] = _IMMUTABLE_EMPTY_TUPLE,+                 credentials: Optional[grpc.CallCredentials] = None,+                 wait_for_ready: Optional[bool] = None,+                 compression: Optional[grpc.Compression] = None+                ) -> _base_call.StreamUnaryCall:+        """"""Asynchronously invokes the underlying RPC.++        Args:+          request: The request value for the RPC.+          timeout: An optional duration of time in seconds to allow+            for the RPC.+          metadata: Optional :term:`metadata` to be transmitted to the+            service-side of the RPC.+          credentials: An optional CallCredentials for the RPC. Only valid for+            secure Channel.+          wait_for_ready: This is an EXPERIMENTAL argument. An optional+            flag to enable wait for ready mechanism+          compression: An element of grpc.compression, e.g.+            grpc.compression.Gzip. This is an EXPERIMENTAL option.++        Returns:+          A Call object instance which is an awaitable object.++        Raises:+          RpcError: Indicating that the RPC terminated with non-OK status. The+            raised RpcError will also be a Call for the RPC affording the RPC's+            metadata, status code, and details.+        """"""+++class StreamStreamMultiCallable(abc.ABC):+    """"""Affords invoking a stream-stream RPC from client-side in an asynchronous way.""""""++    @abc.abstractmethod+    def __call__(self,+                 request_async_iterator: Optional[AsyncIterable[Any]] = None,+                 timeout: Optional[float] = None,+                 metadata: Optional[MetadataType] = _IMMUTABLE_EMPTY_TUPLE,+                 credentials: Optional[grpc.CallCredentials] = None,+                 wait_for_ready: Optional[bool] = None,+                 compression: Optional[grpc.Compression] = None+                ) -> _base_call.StreamStreamCall:+        """"""Asynchronously invokes the underlying RPC.++        Args:+          request: The request value for the RPC.+          timeout: An optional duration of time in seconds to allow+            for the RPC.+          metadata: Optional :term:`metadata` to be transmitted to the+            service-side of the RPC.+          credentials: An optional CallCredentials for the RPC. Only valid for+            secure Channel.+          wait_for_ready: This is an EXPERIMENTAL argument. An optional+            flag to enable wait for ready mechanism+          compression: An element of grpc.compression, e.g.+            grpc.compression.Gzip. This is an EXPERIMENTAL option.++        Returns:+          A Call object instance which is an awaitable object.++        Raises:+          RpcError: Indicating that the RPC terminated with non-OK status. The+            raised RpcError will also be a Call for the RPC affording the RPC's+            metadata, status code, and details.+        """"""+++class Channel(abc.ABC):+    """"""Asynchronous Channel implementation.++    A cygrpc.AioChannel-backed implementation.","Good catch, I missed this docstring. I removed Cython specific description, and adding details about `async with`:```Pythonclass Channel(abc.ABC):    """"""Enables asynchronous RPC invocation as a client.    Channel objects implement the Asynchronous Context Manager (aka. async    with) type, although they are not supportted to be entered and exited    multiple times.    """"""```",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21987,378573150,2020-02-12T23:31:22Z,src/python/grpcio/grpc/experimental/aio/_base_channel.py,"@@ -0,0 +1,338 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Abstract base classes for Channel objects and Multicallable objects.""""""++import abc+from typing import Any, AsyncIterable, Optional++import grpc++from . import _base_call+from ._typing import DeserializingFunction, MetadataType, SerializingFunction++_IMMUTABLE_EMPTY_TUPLE = tuple()+++class UnaryUnaryMultiCallable(abc.ABC):+    """"""Factory an asynchronous unary-unary RPC stub call from client-side.""""""++    @abc.abstractmethod+    def __call__(self,+                 request: Any,+                 *,+                 timeout: Optional[float] = None,+                 metadata: Optional[MetadataType] = _IMMUTABLE_EMPTY_TUPLE,+                 credentials: Optional[grpc.CallCredentials] = None,+                 wait_for_ready: Optional[bool] = None,+                 compression: Optional[grpc.Compression] = None+                ) -> _base_call.UnaryUnaryCall:+        """"""Asynchronously invokes the underlying RPC.++        Args:+          request: The request value for the RPC.+          timeout: An optional duration of time in seconds to allow+            for the RPC.+          metadata: Optional :term:`metadata` to be transmitted to the+            service-side of the RPC.+          credentials: An optional CallCredentials for the RPC. Only valid for+            secure Channel.+          wait_for_ready: This is an EXPERIMENTAL argument. An optional+            flag to enable wait for ready mechanism+          compression: An element of grpc.compression, e.g.+            grpc.compression.Gzip. This is an EXPERIMENTAL option.++        Returns:+          A Call object instance which is an awaitable object.++        Raises:+          RpcError: Indicating that the RPC terminated with non-OK status. The+            raised RpcError will also be a Call for the RPC affording the RPC's+            metadata, status code, and details.+        """"""+++class UnaryStreamMultiCallable(abc.ABC):+    """"""Affords invoking a unary-stream RPC from client-side in an asynchronous way.""""""++    @abc.abstractmethod+    def __call__(self,+                 request: Any,+                 *,+                 timeout: Optional[float] = None,+                 metadata: Optional[MetadataType] = _IMMUTABLE_EMPTY_TUPLE,+                 credentials: Optional[grpc.CallCredentials] = None,+                 wait_for_ready: Optional[bool] = None,+                 compression: Optional[grpc.Compression] = None+                ) -> _base_call.UnaryStreamCall:+        """"""Asynchronously invokes the underlying RPC.++        Args:+          request: The request value for the RPC.+          timeout: An optional duration of time in seconds to allow+            for the RPC.+          metadata: Optional :term:`metadata` to be transmitted to the+            service-side of the RPC.+          credentials: An optional CallCredentials for the RPC. Only valid for+            secure Channel.+          wait_for_ready: This is an EXPERIMENTAL argument. An optional+            flag to enable wait for ready mechanism+          compression: An element of grpc.compression, e.g.+            grpc.compression.Gzip. This is an EXPERIMENTAL option.++        Returns:+          A Call object instance which is an awaitable object.+        """"""+++class StreamUnaryMultiCallable(abc.ABC):+    """"""Affords invoking a stream-unary RPC from client-side in an asynchronous way.""""""++    @abc.abstractmethod+    def __call__(self,+                 request_async_iterator: Optional[AsyncIterable[Any]] = None,+                 timeout: Optional[float] = None,+                 metadata: Optional[MetadataType] = _IMMUTABLE_EMPTY_TUPLE,+                 credentials: Optional[grpc.CallCredentials] = None,+                 wait_for_ready: Optional[bool] = None,+                 compression: Optional[grpc.Compression] = None+                ) -> _base_call.StreamUnaryCall:+        """"""Asynchronously invokes the underlying RPC.++        Args:+          request: The request value for the RPC.+          timeout: An optional duration of time in seconds to allow+            for the RPC.+          metadata: Optional :term:`metadata` to be transmitted to the+            service-side of the RPC.+          credentials: An optional CallCredentials for the RPC. Only valid for+            secure Channel.+          wait_for_ready: This is an EXPERIMENTAL argument. An optional+            flag to enable wait for ready mechanism+          compression: An element of grpc.compression, e.g.+            grpc.compression.Gzip. This is an EXPERIMENTAL option.++        Returns:+          A Call object instance which is an awaitable object.++        Raises:+          RpcError: Indicating that the RPC terminated with non-OK status. The+            raised RpcError will also be a Call for the RPC affording the RPC's+            metadata, status code, and details.+        """"""+++class StreamStreamMultiCallable(abc.ABC):+    """"""Affords invoking a stream-stream RPC from client-side in an asynchronous way.""""""++    @abc.abstractmethod+    def __call__(self,+                 request_async_iterator: Optional[AsyncIterable[Any]] = None,+                 timeout: Optional[float] = None,+                 metadata: Optional[MetadataType] = _IMMUTABLE_EMPTY_TUPLE,+                 credentials: Optional[grpc.CallCredentials] = None,+                 wait_for_ready: Optional[bool] = None,+                 compression: Optional[grpc.Compression] = None+                ) -> _base_call.StreamStreamCall:+        """"""Asynchronously invokes the underlying RPC.++        Args:+          request: The request value for the RPC.+          timeout: An optional duration of time in seconds to allow+            for the RPC.+          metadata: Optional :term:`metadata` to be transmitted to the+            service-side of the RPC.+          credentials: An optional CallCredentials for the RPC. Only valid for+            secure Channel.+          wait_for_ready: This is an EXPERIMENTAL argument. An optional+            flag to enable wait for ready mechanism+          compression: An element of grpc.compression, e.g.+            grpc.compression.Gzip. This is an EXPERIMENTAL option.++        Returns:+          A Call object instance which is an awaitable object.++        Raises:+          RpcError: Indicating that the RPC terminated with non-OK status. The+            raised RpcError will also be a Call for the RPC affording the RPC's+            metadata, status code, and details.+        """"""+++class Channel(abc.ABC):+    """"""Asynchronous Channel implementation.++    A cygrpc.AioChannel-backed implementation.+    """"""++    @abc.abstractmethod+    async def __aenter__(self):+        """"""Starts an asynchronous context manager.++        Returns:+          Channel the channel that was instantiated.+        """"""++    @abc.abstractmethod+    async def __aexit__(self, exc_type, exc_val, exc_tb):+        """"""Finishes the asynchronous context manager by closing the channel.++        Still active RPCs will be cancelled.+        """"""++    @abc.abstractmethod+    async def close(self, grace: Optional[float] = None):+        """"""Closes this Channel and releases all resources held by it.++        This method immediately stops the channel from executing new RPCs in+        all cases.++        If a grace period is specified, this method wait until all active+        RPCs are finshed, once the grace period is reached the ones that haven't+        been terminated are cancelled. If a grace period is not specified+        (by passing None for grace), all existing RPCs are cancelled immediately.++        This method is idempotent.+        """"""++    @abc.abstractmethod+    def get_state(self,+                  try_to_connect: bool = False) -> grpc.ChannelConnectivity:+        """"""Check the connectivity state of a channel.++        This is an EXPERIMENTAL API.++        If the channel reaches a stable connectivity state, it is guaranteed+        that the return value of this function will eventually converge to that+        state.++        Args:+          try_to_connect: a bool indicate whether the Channel should try to+            connect to peer or not.++        Returns: A ChannelConnectivity object.+        """"""++    @abc.abstractmethod+    async def wait_for_state_change(+            self,+            last_observed_state: grpc.ChannelConnectivity,+    ) -> None:+        """"""Wait for a change in connectivity state.++        This is an EXPERIMENTAL API.++        The function blocks until there is a change in the channel connectivity+        state from the ""last_observed_state"". If the state is already+        different, this function will return immediately.++        There is an inherent race between the invocation of+        ""Channel.wait_for_state_change"" and ""Channel.get_state"". The state can+        change arbitrary times during the race, so there is no way to observe","Updated, but why there is a difference here? I thought arbitrary times means number of occurrence is equal or larger than zero.  ",
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/21920,379309871,2020-02-14T08:48:12Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi,"@@ -125,7 +125,7 @@ cdef class _AioCall(GrpcCallWrapper):         if credentials is not None:             set_credentials_error = grpc_call_set_credentials(self.call, credentials.c())             if set_credentials_error != GRPC_CALL_OK:-                raise Exception(""Credentials couldn't have been set"")+                raise RuntimeError(f""Credentials couldn't have been set: {set_credentials_error}"")","No worries, I can change it back, but1. We are targetting ``Python3.6+``2. Python 2 is no longer supported, I don't think that should be a constraint",
5279114,ZHmao,https://api.github.com/repos/grpc/grpc/pulls/22032,380643876,2020-02-18T12:34:40Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -15,6 +15,7 @@  import inspect import traceback+import functools",I inject some parameters at here: https://github.com/grpc/grpc/pull/22032/files/6fef56573e9a0347c33c5aea4bc57ab625b0c6ea#diff-b53c077f7911f53dc8ce7656e16d35cdR222,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22028,380703045,2020-02-18T14:24:55Z,src/python/grpcio_health_checking/grpc_health/v1/_async.py,"@@ -0,0 +1,103 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Reference implementation for health checking in gRPC Python.""""""++import asyncio+import collections++import grpc++from grpc_health.v1 import health_pb2 as _health_pb2+from grpc_health.v1 import health_pb2_grpc as _health_pb2_grpc+++class AsyncHealthServicer(_health_pb2_grpc.HealthServicer):+    """"""An AsyncIO implementation of health checking servicer.""""""++    def __init__(self):+        self._server_status = dict()+        self._server_watchers = collections.defaultdict(asyncio.Condition)+        self._gracefully_shutting_down = False++    async def Check(self, request: _health_pb2.HealthCheckRequest, context):+        status = self._server_status.get(request.service)++        if status is None:+            await context.abort(grpc.StatusCode.NOT_FOUND)+        else:+            return _health_pb2.HealthCheckResponse(status=status)++    async def Watch(self, request: _health_pb2.HealthCheckRequest, context):+        status = self._server_status.get(request.service)++        if status is None:+            status = _health_pb2.HealthCheckResponse.SERVICE_UNKNOWN++        try:+            condition = self._server_watchers[request.service]+            async with condition:+                # Responds with current health state+                await context.write(+                    _health_pb2.HealthCheckResponse(status=status))++                # Polling on health state changes+                while True:+                    await condition.wait()++                    status = self._server_status.get(request.service)+                    await context.write(+                        _health_pb2.HealthCheckResponse(status=status))","Could we do the same but a bit more compressed, like:```pythonwhile True:    status = self._server_status.get(request.service)    if status:        await context.write(            _health_pb2.HealthCheckResponse(status=status))    else:         .... send _health_pb2.HealthCheckResponse.SERVICE_UNKNOWN    await condition.wait()```",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22029,380735784,2020-02-18T15:14:53Z,test/core/util/fuzzer_corpus_test.cc,"@@ -48,15 +48,21 @@ DEFINE_string(directory, """", ""Use this directory as test data""); class FuzzerCorpusTest : public ::testing::TestWithParam<std::string> {};  TEST_P(FuzzerCorpusTest, RunOneExample) {+  grpc_init();   gpr_log(GPR_DEBUG, ""Example file: %s"", GetParam().c_str());   grpc_slice buffer;   squelch = false;   leak_check = false;   GPR_ASSERT(GRPC_LOG_IF_ERROR(""load_file"",                                grpc_load_file(GetParam().c_str(), 0, &buffer)));-  LLVMFuzzerTestOneInput(GRPC_SLICE_START_PTR(buffer),-                         GRPC_SLICE_LENGTH(buffer));+  uint8_t* data;",Why do we need to copy the data out of the slice here?  Why not simply wait until after `LLVMFuzzerTestOneInput()` returns before calling `grpc_shutdown_blocking()`?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22032,380769302,2020-02-18T16:02:51Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -685,6 +685,110 @@ class Interceptor(aio.UnaryUnaryClientInterceptor):                 self.fail(""Callback was not called"")  +class _LoggingServerInterceptor(aio.ServerInterceptor):++    def __init__(self, tag, record):+        self.tag = tag+        self.record = record++    async def intercept_service(self, continuation, handler_call_details):+        self.record.append(self.tag + ':intercept_service')+        return await continuation(handler_call_details)+++class _GenericServerInterceptor(aio.ServerInterceptor):++    def __init__(self, fn):+        self._fn = fn++    async def intercept_service(self, continuation, handler_call_details):+        return await self._fn(continuation, handler_call_details)+++def _filter_server_interceptor(condition, interceptor):+    async def intercept_service(continuation, handler_call_details):+        if condition(handler_call_details):+            return await interceptor.intercept_service(continuation,+                                                       handler_call_details)+        return await continuation(handler_call_details)++    return _GenericServerInterceptor(intercept_service)+++class TestServerInterceptor(AioTestBase):+    async def setUp(self) -> None:+        self._record = []+        conditional_interceptor = _filter_server_interceptor(+            lambda x: ('secret', '42') in x.invocation_metadata,+            _LoggingServerInterceptor('log3', self._record))+        self._interceptors = (+            _LoggingServerInterceptor('log1', self._record),+            conditional_interceptor,+            _LoggingServerInterceptor('log2', self._record),+        )+        self._server_target, self._server = await start_test_server(+            interceptors=self._interceptors)++    async def tearDown(self) -> None:+        self._server.stop(None)++    async def test_invalid_interceptor(self):+        class InvalidInterceptor:+            """"""Just an invalid Interceptor""""""++        with self.assertRaises(aio.AioRpcError):+            server_target, _ = await start_test_server(+                interceptors=(InvalidInterceptor(),))","a `Channel` with an invalid interceptor is not allowed to be instantiated [1 ], having the feeling that we would need to do the same with the server.[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/experimental/aio/_channel.py#L317",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22032,380773789,2020-02-18T16:09:21Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -685,6 +685,110 @@ class Interceptor(aio.UnaryUnaryClientInterceptor):                 self.fail(""Callback was not called"")  +class _LoggingServerInterceptor(aio.ServerInterceptor):++    def __init__(self, tag, record):+        self.tag = tag+        self.record = record++    async def intercept_service(self, continuation, handler_call_details):+        self.record.append(self.tag + ':intercept_service')+        return await continuation(handler_call_details)+++class _GenericServerInterceptor(aio.ServerInterceptor):++    def __init__(self, fn):+        self._fn = fn++    async def intercept_service(self, continuation, handler_call_details):+        return await self._fn(continuation, handler_call_details)+++def _filter_server_interceptor(condition, interceptor):+    async def intercept_service(continuation, handler_call_details):+        if condition(handler_call_details):+            return await interceptor.intercept_service(continuation,+                                                       handler_call_details)+        return await continuation(handler_call_details)++    return _GenericServerInterceptor(intercept_service)+++class TestServerInterceptor(AioTestBase):+    async def setUp(self) -> None:+        self._record = []+        conditional_interceptor = _filter_server_interceptor(+            lambda x: ('secret', '42') in x.invocation_metadata,+            _LoggingServerInterceptor('log3', self._record))+        self._interceptors = (+            _LoggingServerInterceptor('log1', self._record),+            conditional_interceptor,+            _LoggingServerInterceptor('log2', self._record),+        )","Having all of the interceptors added into the setup which they would be used later for all of the tests makes the readability a bit harder for me, TBH I would prefer to have each test case with its own server with its own interceptors for testing the different functionalities.The same as you already did for the `test_invalid_interceptor` test case.Having the feeling that having this kind of sharing, the same happens with the `_record` class attribute, we have a lot of chances of creating down-side effects between tests.",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22032,380783018,2020-02-18T16:23:02Z,src/python/grpcio_tests/tests_aio/unit/interceptor_test.py,"@@ -685,6 +685,110 @@ class Interceptor(aio.UnaryUnaryClientInterceptor):                 self.fail(""Callback was not called"")  +class _LoggingServerInterceptor(aio.ServerInterceptor):++    def __init__(self, tag, record):+        self.tag = tag+        self.record = record++    async def intercept_service(self, continuation, handler_call_details):+        self.record.append(self.tag + ':intercept_service')+        return await continuation(handler_call_details)+++class _GenericServerInterceptor(aio.ServerInterceptor):++    def __init__(self, fn):+        self._fn = fn++    async def intercept_service(self, continuation, handler_call_details):+        return await self._fn(continuation, handler_call_details)+++def _filter_server_interceptor(condition, interceptor):+    async def intercept_service(continuation, handler_call_details):+        if condition(handler_call_details):+            return await interceptor.intercept_service(continuation,+                                                       handler_call_details)+        return await continuation(handler_call_details)++    return _GenericServerInterceptor(intercept_service)+++class TestServerInterceptor(AioTestBase):","I would like to have a test for checking observability of the code responses sent by the handler, in the same way as we have for the channel [1][1] https://github.com/grpc/grpc/blob/master/src/python/grpcio_tests/tests_aio/unit/interceptor_test.py#L93",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22028,380850503,2020-02-18T18:19:46Z,src/python/grpcio_tests/tests_aio/health_check/health_servicer_test.py,"@@ -0,0 +1,230 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests AsyncIO version of grpcio-health-checking.""""""++import asyncio+import logging+import time+import unittest++import grpc++from grpc_health.v1 import health+from grpc_health.v1 import health_pb2+from grpc_health.v1 import health_pb2_grpc+from grpc.experimental import aio++from tests.unit.framework.common import test_constants++from tests_aio.unit._test_base import AioTestBase++_SERVING_SERVICE = 'grpc.test.TestServiceServing'+_UNKNOWN_SERVICE = 'grpc.test.TestServiceUnknown'+_NOT_SERVING_SERVICE = 'grpc.test.TestServiceNotServing'+_WATCH_SERVICE = 'grpc.test.WatchService'+++async def _pipe_to_queue(call, queue):+    async for response in call:+        await queue.put(response)+++class HealthServicerTest(AioTestBase):++    async def setUp(self):+        self._servicer = health.AsyncHealthServicer()+        await self._servicer.set('', health_pb2.HealthCheckResponse.SERVING)",Empty means the status of the entire server... See https://github.com/grpc/proposal/blob/master/A17-client-side-health-checking.md#proposal,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22028,380858149,2020-02-18T18:34:37Z,src/python/grpcio_health_checking/grpc_health/v1/_async.py,"@@ -0,0 +1,103 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Reference implementation for health checking in gRPC Python.""""""++import asyncio+import collections++import grpc++from grpc_health.v1 import health_pb2 as _health_pb2+from grpc_health.v1 import health_pb2_grpc as _health_pb2_grpc+++class AsyncHealthServicer(_health_pb2_grpc.HealthServicer):+    """"""An AsyncIO implementation of health checking servicer.""""""++    def __init__(self):+        self._server_status = dict()+        self._server_watchers = collections.defaultdict(asyncio.Condition)+        self._gracefully_shutting_down = False++    async def Check(self, request: _health_pb2.HealthCheckRequest, context):+        status = self._server_status.get(request.service)++        if status is None:+            await context.abort(grpc.StatusCode.NOT_FOUND)+        else:+            return _health_pb2.HealthCheckResponse(status=status)++    async def Watch(self, request: _health_pb2.HealthCheckRequest, context):+        status = self._server_status.get(request.service)++        if status is None:+            status = _health_pb2.HealthCheckResponse.SERVICE_UNKNOWN++        try:+            condition = self._server_watchers[request.service]+            async with condition:+                # Responds with current health state+                await context.write(+                    _health_pb2.HealthCheckResponse(status=status))++                # Polling on health state changes+                while True:+                    await condition.wait()","The behavior of this implementation differs slightly from the semantics prescribed in the proto:```// ... It will then subsequently send a new message whenever// the service's serving status changes.```My reading of that comment is that the client will receive a message each time the health status changes and *only* when the status changes. But depending on scheduling in the event loop, it's possible for the application to very quickly go through the following state changes:```SERVING -> NOT_SERVING -> SERVING```and the caller of the watch method to observe ```... -> SERVING -> SERVING -> ...```This is a fundamental shortcoming of concurrent scheduling. Very similar to the quirks of the channel state API we discussed not too long ago. I think it's okay to swallow states that only last for negligible periods of time. So we should filter out duplicate states to account for that.Not sure how the synchronous health servicer works in this respect since `threading.Condition` should be susceptible to the same race, but it might be worth looking into.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22028,380863338,2020-02-18T18:44:45Z,src/python/grpcio_health_checking/grpc_health/v1/_async.py,"@@ -0,0 +1,98 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Reference implementation for health checking in gRPC Python.""""""++import asyncio+import collections++import grpc++from grpc_health.v1 import health_pb2 as _health_pb2+from grpc_health.v1 import health_pb2_grpc as _health_pb2_grpc+++class AsyncHealthServicer(_health_pb2_grpc.HealthServicer):+    """"""An AsyncIO implementation of health checking servicer.""""""++    def __init__(self):+        self._server_status = dict()+        self._server_watchers = collections.defaultdict(asyncio.Condition)+        self._gracefully_shutting_down = False++    async def Check(self, request: _health_pb2.HealthCheckRequest, context):+        status = self._server_status.get(request.service)++        if status is None:+            await context.abort(grpc.StatusCode.NOT_FOUND)+        else:+            return _health_pb2.HealthCheckResponse(status=status)++    async def Watch(self, request: _health_pb2.HealthCheckRequest, context):+        condition = self._server_watchers[request.service]+        try:+            async with condition:+                while True:+                    status = self._server_status.get(+                        request.service,+                        _health_pb2.HealthCheckResponse.SERVICE_UNKNOWN)++                    # Responds with current health state+                    await context.write(+                        _health_pb2.HealthCheckResponse(status=status))++                    # Polling on health state changes+                    await condition.wait()","The behavior of this implementation differs slightly from the semantics prescribed in the proto:```// ... It will then subsequently send a new message whenever// the service's serving status changes.```My reading of that comment is that the client will receive a message each time the health status changes and *only* when the status changes. But depending on scheduling in the event loop, it's possible for the application to very quickly go through the following state changes:```SERVING -> NOT_SERVING -> SERVING```and the caller of the watch method to observe ```... -> SERVING -> SERVING -> ...```This is a fundamental shortcoming of concurrent scheduling. Very similar to the quirks of the channel state API we discussed not too long ago. I think it's okay to swallow states that only last for negligible periods of time. So we should filter out duplicate states to account for that.Not sure how the synchronous health servicer works in this respect since `threading.Condition` should be susceptible to the same race, but it might be worth looking into.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22028,380888003,2020-02-18T19:31:26Z,src/python/grpcio_health_checking/grpc_health/v1/_async.py,"@@ -0,0 +1,98 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Reference implementation for health checking in gRPC Python.""""""++import asyncio+import collections++import grpc++from grpc_health.v1 import health_pb2 as _health_pb2+from grpc_health.v1 import health_pb2_grpc as _health_pb2_grpc+++class AsyncHealthServicer(_health_pb2_grpc.HealthServicer):+    """"""An AsyncIO implementation of health checking servicer.""""""++    def __init__(self):+        self._server_status = dict()+        self._server_watchers = collections.defaultdict(asyncio.Condition)+        self._gracefully_shutting_down = False++    async def Check(self, request: _health_pb2.HealthCheckRequest, context):+        status = self._server_status.get(request.service)++        if status is None:+            await context.abort(grpc.StatusCode.NOT_FOUND)+        else:+            return _health_pb2.HealthCheckResponse(status=status)++    async def Watch(self, request: _health_pb2.HealthCheckRequest, context):+        condition = self._server_watchers[request.service]+        try:+            async with condition:+                while True:+                    status = self._server_status.get(+                        request.service,+                        _health_pb2.HealthCheckResponse.SERVICE_UNKNOWN)++                    # Responds with current health state+                    await context.write(+                        _health_pb2.HealthCheckResponse(status=status))++                    # Polling on health state changes+                    await condition.wait()","I changed the logic to use `asyncio.Queue`, which buffers all state changes. It costs slightly more memory, but deterministic.As you mentioned, if we use `condition`, there could be an inherit data race to skip certain state. The sync version has the same behavior. According to our last discussion, it is unclear that sending all the details is beneficial or not, since the upstream health checker might only be interested in the ""eventual health state"".",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22028,380888538,2020-02-18T19:32:30Z,src/python/grpcio_health_checking/grpc_health/v1/health.py,"@@ -15,12 +15,16 @@  import collections import threading-+import sys import grpc  from grpc_health.v1 import health_pb2 as _health_pb2 from grpc_health.v1 import health_pb2_grpc as _health_pb2_grpc +if sys.version_info[0] >= 3 and sys.version_info[1] >= 6:+    # Exposes AsyncHealthServicer as public API.+    from ._async import AsyncHealthServicer  # pylint: disable=unused-import","Yes. From the relative import point, it works. The if condition skips the import unless interpreter version is greater than 3.6.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22028,380893716,2020-02-18T19:42:33Z,src/python/grpcio_tests/tests_aio/health_check/health_servicer_test.py,"@@ -0,0 +1,232 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests AsyncIO version of grpcio-health-checking.""""""++import asyncio+import logging+import time+import unittest++import grpc++from grpc_health.v1 import health+from grpc_health.v1 import health_pb2+from grpc_health.v1 import health_pb2_grpc+from grpc.experimental import aio++from tests.unit.framework.common import test_constants++from tests_aio.unit._test_base import AioTestBase++_SERVING_SERVICE = 'grpc.test.TestServiceServing'+_UNKNOWN_SERVICE = 'grpc.test.TestServiceUnknown'+_NOT_SERVING_SERVICE = 'grpc.test.TestServiceNotServing'+_WATCH_SERVICE = 'grpc.test.WatchService'+++async def _pipe_to_queue(call, queue):+    async for response in call:+        await queue.put(response)+++class HealthServicerTest(AioTestBase):++    async def setUp(self):+        self._servicer = health.AsyncHealthServicer()","Existing import is `grpc_health.v1.health.HealthServicer`. At one point, the previous team tried to fake namespace packaging when `grpcio` initiated, but end up with many corner cases. So, I think this is the first case of a new pattern.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22028,380896273,2020-02-18T19:47:22Z,src/python/grpcio_health_checking/grpc_health/v1/_async.py,"@@ -0,0 +1,98 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Reference implementation for health checking in gRPC Python.""""""++import asyncio+import collections++import grpc++from grpc_health.v1 import health_pb2 as _health_pb2+from grpc_health.v1 import health_pb2_grpc as _health_pb2_grpc+++class AsyncHealthServicer(_health_pb2_grpc.HealthServicer):+    """"""An AsyncIO implementation of health checking servicer.""""""++    def __init__(self):+        self._server_status = dict()+        self._server_watchers = collections.defaultdict(asyncio.Condition)+        self._gracefully_shutting_down = False++    async def Check(self, request: _health_pb2.HealthCheckRequest, context):+        status = self._server_status.get(request.service)++        if status is None:+            await context.abort(grpc.StatusCode.NOT_FOUND)+        else:+            return _health_pb2.HealthCheckResponse(status=status)++    async def Watch(self, request: _health_pb2.HealthCheckRequest, context):+        condition = self._server_watchers[request.service]+        try:+            async with condition:+                while True:+                    status = self._server_status.get(+                        request.service,+                        _health_pb2.HealthCheckResponse.SERVICE_UNKNOWN)++                    # Responds with current health state+                    await context.write(+                        _health_pb2.HealthCheckResponse(status=status))++                    # Polling on health state changes+                    await condition.wait()",Changed back to `asyncio.Condition` and filtered duplicated status. Added one more test case.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22028,380963897,2020-02-18T22:06:06Z,src/python/grpcio_tests/tests_aio/health_check/health_servicer_test.py,"@@ -0,0 +1,232 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests AsyncIO version of grpcio-health-checking.""""""++import asyncio+import logging+import time+import unittest++import grpc++from grpc_health.v1 import health+from grpc_health.v1 import health_pb2+from grpc_health.v1 import health_pb2_grpc+from grpc.experimental import aio++from tests.unit.framework.common import test_constants++from tests_aio.unit._test_base import AioTestBase++_SERVING_SERVICE = 'grpc.test.TestServiceServing'+_UNKNOWN_SERVICE = 'grpc.test.TestServiceUnknown'+_NOT_SERVING_SERVICE = 'grpc.test.TestServiceNotServing'+_WATCH_SERVICE = 'grpc.test.WatchService'+++async def _pipe_to_queue(call, queue):+    async for response in call:+        await queue.put(response)+++class HealthServicerTest(AioTestBase):++    async def setUp(self):+        self._servicer = health.AsyncHealthServicer()",Both option looks similar to me with minor differences. Changed to `health.aio.HealthServicer`.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22028,380971570,2020-02-18T22:23:16Z,src/python/grpcio_tests/tests_aio/health_check/health_servicer_test.py,"@@ -0,0 +1,230 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests AsyncIO version of grpcio-health-checking.""""""++import asyncio+import logging+import time+import unittest++import grpc++from grpc_health.v1 import health+from grpc_health.v1 import health_pb2+from grpc_health.v1 import health_pb2_grpc+from grpc.experimental import aio++from tests.unit.framework.common import test_constants++from tests_aio.unit._test_base import AioTestBase++_SERVING_SERVICE = 'grpc.test.TestServiceServing'+_UNKNOWN_SERVICE = 'grpc.test.TestServiceUnknown'+_NOT_SERVING_SERVICE = 'grpc.test.TestServiceNotServing'+_WATCH_SERVICE = 'grpc.test.WatchService'+++async def _pipe_to_queue(call, queue):+    async for response in call:+        await queue.put(response)+++class HealthServicerTest(AioTestBase):++    async def setUp(self):+        self._servicer = health.AsyncHealthServicer()+        await self._servicer.set('', health_pb2.HealthCheckResponse.SERVING)","`OVERALL_HEALTH` constant added.> What's the rationale of having the capacity of announcing different healthiness for different service names by the same server?Health status of a servicer is depended on downstream servers. For servers carrying multiple servicers, users might appreciate the ability to set health status with finer granularity. For example, the write-to-database servicer is in `NOT_SERVING` status due to overload, but read-from-memcached servicer is still `SERVING`. So, the control plane can steer traffic accordingly.> So, when the overall one is not used how the server leverages on that?Which server are you referring to? If you mean the server serving health status, it treats `''` just like other normal services. If you mean the upstream server, it is recommend to check `''` by default if no other config is given.> Is intended for having multiple services running on the same application?Yes. It is intended for multiple servicers in one application.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21920,381003758,2020-02-18T23:51:05Z,src/python/grpcio_tests/tests_aio/unit/secure_call_test.py,"@@ -0,0 +1,114 @@+import unittest+import logging++import grpc+from grpc.experimental import aio+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc+from tests_aio.unit._test_base import AioTestBase+from tests_aio.unit._test_server import start_test_server+from tests.unit import resources++_SERVER_HOST_OVERRIDE = 'foo.test.google.fr'+_NUM_STREAM_RESPONSES = 5+_RESPONSE_PAYLOAD_SIZE = 42+++class _SecureCallMixin:+    """"""A Mixin to run the call tests over a secure channel.""""""++    async def setUp(self):+        server_credentials = grpc.ssl_server_credentials([+            (resources.private_key(), resources.certificate_chain())+        ])+        channel_credentials = grpc.ssl_channel_credentials(+            resources.test_root_certificates())++        self._server_address, self._server = await start_test_server(+            secure=True, server_credentials=server_credentials)+        channel_options = ((+            'grpc.ssl_target_name_override',+            _SERVER_HOST_OVERRIDE,+        ),)+        self._channel = aio.secure_channel(self._server_address,+                                           channel_credentials, channel_options)+        self._stub = test_pb2_grpc.TestServiceStub(self._channel)++    async def tearDown(self):+        await self._channel.close()+        await self._server.stop(None)+++class TestUnaryUnarySecureCall(_SecureCallMixin, AioTestBase):+    """"""unary_unary Calls made over a secure channel.""""""++    async def test_call_ok_over_secure_channel(self):+        call = self._stub.UnaryCall(messages_pb2.SimpleRequest())+        response = await call+        self.assertIsInstance(response, messages_pb2.SimpleResponse)+        self.assertEqual(await call.code(), grpc.StatusCode.OK)++    async def test_call_with_credentials(self):+        call_credentials = grpc.composite_call_credentials(+            grpc.access_token_call_credentials(""abc""),+            grpc.access_token_call_credentials(""def""),+        )+        call = self._stub.UnaryCall(messages_pb2.SimpleRequest(),+                                    credentials=call_credentials)+        response = await call++        self.assertIsInstance(response, messages_pb2.SimpleResponse)+++class TestUnaryStreamSecureCall(_SecureCallMixin, AioTestBase):+    """"""unary_stream calls over a secure channel""""""++    async def test_unary_stream_async_generator_secure(self):+        request = messages_pb2.StreamingOutputCallRequest()+        request.response_parameters.extend(+            messages_pb2.ResponseParameters(size=_RESPONSE_PAYLOAD_SIZE,)+            for _ in range(_NUM_STREAM_RESPONSES))+        call_credentials = grpc.composite_call_credentials(+            grpc.access_token_call_credentials(""abc""),+            grpc.access_token_call_credentials(""def""),+        )+        call = self._stub.StreamingOutputCall(request,+                                              credentials=call_credentials)++        async for response in call:+            self.assertIsInstance(response,+                                  messages_pb2.StreamingOutputCallResponse)+            self.assertEqual(len(response.payload.body), _RESPONSE_PAYLOAD_SIZE)++        self.assertEqual(await call.code(), grpc.StatusCode.OK)+++# Prepares the request that stream in a ping-pong manner.+_STREAM_OUTPUT_REQUEST_ONE_RESPONSE = messages_pb2.StreamingOutputCallRequest()+_STREAM_OUTPUT_REQUEST_ONE_RESPONSE.response_parameters.append(+    messages_pb2.ResponseParameters(size=_RESPONSE_PAYLOAD_SIZE))+++class TestStreamStreamSecureCall(_SecureCallMixin, AioTestBase):++    async def test_async_generator_secure_channel(self):++        async def request_generator():+            for _ in range(2):+                yield _STREAM_OUTPUT_REQUEST_ONE_RESPONSE++        call_credentials = grpc.composite_call_credentials(+            grpc.access_token_call_credentials(""abc""),+            grpc.access_token_call_credentials(""def""),+        )","optional: if possible, can you check on the server-side if the metadata is modified by the `access_token_call_credentials`? It's fine that this test case not included by this PR.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22032,381010544,2020-02-19T00:14:28Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -214,15 +215,34 @@ cdef class _ServicerContext:         self._rpc_state.disable_next_compression = True  -cdef _find_method_handler(str method, tuple metadata, list generic_handlers):+async def _run_interceptor(object interceptors, object query_handler,+                      object handler_call_details):+    interceptor = next(interceptors, None)+    if interceptor:+        continuation = functools.partial(_run_interceptor, interceptors,+                                         query_handler)+        return await interceptor.intercept_service(continuation, handler_call_details)+    else:+        return query_handler(handler_call_details)+++async def _find_method_handler(str method, tuple metadata, list generic_handlers,+                          tuple interceptors):+    def query_handlers(handler_call_details):+        for generic_handler in generic_handlers:+            method_handler = generic_handler.service(handler_call_details)+            if method_handler is not None:+                return method_handler+        return None+     cdef _HandlerCallDetails handler_call_details = _HandlerCallDetails(method,                                                                         metadata)--    for generic_handler in generic_handlers:-        method_handler = generic_handler.service(handler_call_details)-        if method_handler is not None:-            return method_handler-    return None+    # interceptor+    if interceptors:+        return await _run_interceptor(iter(interceptors), query_handlers,+                                      handler_call_details)+    else:+        return query_handlers(handler_call_details)","Both `_run_interceptor` and `_find_method_handler` functions are on the data path, is it possible to simplify them? Or run the benchmark to check the impact on performance?",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21929,381386873,2020-02-19T16:24:50Z,templates/gRPC-C++.podspec.template,"@@ -60,42 +60,43 @@         files.update(lib.get(field, []))     return list(sorted(files)) -  def list_filegroup_files(expect_filegroups, groups):-    out = []-    for filegroup_name in expect_filegroups:-      filegroup = filegroup_maps[filegroup_name]-      for group in groups:-        out += filegroup.get(group, [])-    return list(sorted(set(out)))-   def filter_grpcpp(files):     return sorted([file for file in files if not file.startswith(""include/grpc++"")]) -  grpc_private_files = list_lib_files(""grpc"", (""headers"", ""src""))-  grpc_public_headers = list_lib_files(""grpc"", (""public_headers"",))-  grpc_private_headers = list_lib_files(""grpc"", (""headers"",))+  # TODO(jtattermusch): get rid of this hack+  address_sorting_unwanted_files = list_lib_files(""address_sorting"", (""public_headers"", ""headers"", ""src""))++  grpc_private_files = list(sorted(set(list_lib_files(""grpc"", (""headers"", ""src""))) - set(address_sorting_unwanted_files)))+  grpc_public_headers = list(sorted(set(list_lib_files(""grpc"", (""public_headers"",))) - set(address_sorting_unwanted_files)))+  grpc_private_headers = list(sorted(set(list_lib_files(""grpc"", (""headers"",))) - set(address_sorting_unwanted_files)))++  # TODO(jtattermusch): build.yaml no longer has filegroups, so the files here are just hand-listed","I feel this makes it more than a bit fragile. The podspec templates were written so that build.yaml is the source of truth for the file list, and this change seems to be breaking that. If someone adds another proto related header later, I wonder if they will know what this is about and make corresponding corrections here. Is it possible to add things in build_handwritten (because it's hand written)?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22076,381456849,2020-02-19T18:19:10Z,src/python/grpcio/grpc/_cython/_cygrpc/grpc.pxi,"@@ -26,6 +26,16 @@ ctypedef unsigned int       uint32_t ctypedef unsigned long long uint64_t  +cdef extern from ""grpc/support/sync.h"":",@pfreixes How are these used? I don't see a reference to them anywhere else in this PR.@veblush What's the current support status for `gpr`? Is it officially deprecated? Do you see any problems with using `absl` synchronization primitives here?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22076,381459123,2020-02-19T18:23:18Z,src/python/grpcio_tests/commands.py,"@@ -229,6 +229,52 @@ def run(self):         if not result.value.wasSuccessful():             sys.exit('Test failure') +class TestAsyncio(setuptools.Command):+    """"""Command to run tests w/asyncio.""""""++    BANNED_TESTS = (+        # Fork support is not compatible with Aio+        'fork._fork_interop_test.ForkInteropTest',+        'unit._cython._fork_test',++        # Next tests imply the usage by the gRPC library syscalls that do not+        # consider the existence of an installed iomgr.+        'unit._local_credentials_test.LocalCredentialsTest.test_local_tcp',++        # segfaults+        'unit._local_credentials_test.LocalCredentialsTest.test_uds',++        # needs investigation+        'health_check._health_servicer_test',++        # hangs forever+        'unit._cython._channel_test.ChannelTest.test_single_channel_lonely_connectivity',+        'unit._cython._channel_test.ChannelTest.test_multiple_channels_lonely_connectivity',+        'unit._abort_test.AbortTest.test_abort'+    )+    description = 'run tests with Asyncio IO manager.'+    user_options = []++    def initialize_options(self):+        pass++    def finalize_options(self):+        # distutils requires this override.+        pass++    def run(self):+        import grpc.experimental.aio+        grpc.experimental.aio.init_grpc_aio()++        import tests",I believe this block is identical to one in the `TestLite` class. Any opportunity to dedupe?,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22076,381459971,2020-02-19T18:24:45Z,src/python/grpcio_tests/commands.py,"@@ -229,6 +229,52 @@ def run(self):         if not result.value.wasSuccessful():             sys.exit('Test failure') +class TestAsyncio(setuptools.Command):+    """"""Command to run tests w/asyncio.""""""++    BANNED_TESTS = (",I believe this is also a duplicate. Any chance to pull out some commonalities?,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22076,381482900,2020-02-19T19:06:06Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/io_loop.pyx.pxi,"@@ -0,0 +1,87 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import sys+import threading++_RESOLUTION_MS = 0.001+cdef _IOLoop _io_loop = None++cdef class _IOLoop:++    def __init__(self):+        global _io_loop++        # Only one instantiation is exepected        +        assert _io_loop is None++        self._asyncio_loop = None+        self._io_ev = threading.Event()+        self._loop_started_cv = threading.Condition()+        self._loop_started = False++        self._thread = threading.Thread(target=self._run_forever, daemon=True)+        self._thread.start()++        # Some attributes are initialitzated when+        # the thread is really started, we wait till+        # the whole Asyncio loop is initialized and+        # ready to be used.+        with self._loop_started_cv:+            if not self._loop_started:+                self._loop_started_cv.wait()++        _io_loop = self+++    def _loop_started_cb(self):+        with self._loop_started_cv:+            self._loop_started = True+            self._loop_started_cv.notify_all()++    def _run_forever(self):+        self._asyncio_loop = asyncio.new_event_loop()+        asyncio.set_event_loop(self._asyncio_loop)+        self._asyncio_loop.call_soon(self._loop_started_cb)+        try:+            self._asyncio_loop.run_forever()+        except Exception as exp:+            print(""An error ocurred with the IO loop {}"".format(exp))+            # Without the IO loop running the program would become+            # unresponsive, proactively we close the process.+            sys.exit(1)++    cpdef void io_mark(self):+        # Wake up all threads that were waiting+        # for an IO event.+        self._io_ev.set()","I'm not sure I understand the usage of this `threading.Event`. Shouldn't calling `notify_all()` on a `threading.Condition` have the same effect? In fact, that's[ what `theading.Event` seems to be doing under the hood anyway](https://github.com/python/cpython/blob/f2ee21d858bc03dd801b97afe60ee2ea289e2fe9/Lib/threading.py#L565).",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22076,381485915,2020-02-19T19:11:46Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/io_loop.pyx.pxi,"@@ -0,0 +1,87 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import sys+import threading++_RESOLUTION_MS = 0.001+cdef _IOLoop _io_loop = None++cdef class _IOLoop:++    def __init__(self):+        global _io_loop++        # Only one instantiation is exepected        +        assert _io_loop is None++        self._asyncio_loop = None+        self._io_ev = threading.Event()+        self._loop_started_cv = threading.Condition()+        self._loop_started = False++        self._thread = threading.Thread(target=self._run_forever, daemon=True)+        self._thread.start()++        # Some attributes are initialitzated when+        # the thread is really started, we wait till+        # the whole Asyncio loop is initialized and+        # ready to be used.+        with self._loop_started_cv:+            if not self._loop_started:+                self._loop_started_cv.wait()++        _io_loop = self+++    def _loop_started_cb(self):+        with self._loop_started_cv:+            self._loop_started = True+            self._loop_started_cv.notify_all()++    def _run_forever(self):+        self._asyncio_loop = asyncio.new_event_loop()+        asyncio.set_event_loop(self._asyncio_loop)+        self._asyncio_loop.call_soon(self._loop_started_cb)+        try:+            self._asyncio_loop.run_forever()+        except Exception as exp:+            print(""An error ocurred with the IO loop {}"".format(exp))+            # Without the IO loop running the program would become+            # unresponsive, proactively we close the process.+            sys.exit(1)++    cpdef void io_mark(self):+        # Wake up all threads that were waiting+        # for an IO event.+        self._io_ev.set()++        # Clear the status, further threads will get+        # block.+        self._io_ev.clear()++    cdef void io_wait(self, size_t timeout_ms):+        if threading.get_ident() == self._thread.ident:",This needs to be a more explicit error condition. We need to bubble an error or at the very least log one.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22076,381487291,2020-02-19T19:14:15Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/io_loop.pyx.pxi,"@@ -0,0 +1,87 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import sys+import threading++_RESOLUTION_MS = 0.001+cdef _IOLoop _io_loop = None++cdef class _IOLoop:++    def __init__(self):+        global _io_loop++        # Only one instantiation is exepected        +        assert _io_loop is None++        self._asyncio_loop = None+        self._io_ev = threading.Event()+        self._loop_started_cv = threading.Condition()+        self._loop_started = False++        self._thread = threading.Thread(target=self._run_forever, daemon=True)+        self._thread.start()++        # Some attributes are initialitzated when+        # the thread is really started, we wait till+        # the whole Asyncio loop is initialized and+        # ready to be used.+        with self._loop_started_cv:+            if not self._loop_started:+                self._loop_started_cv.wait()++        _io_loop = self+++    def _loop_started_cb(self):+        with self._loop_started_cv:+            self._loop_started = True+            self._loop_started_cv.notify_all()++    def _run_forever(self):+        self._asyncio_loop = asyncio.new_event_loop()+        asyncio.set_event_loop(self._asyncio_loop)+        self._asyncio_loop.call_soon(self._loop_started_cb)+        try:+            self._asyncio_loop.run_forever()+        except Exception as exp:+            print(""An error ocurred with the IO loop {}"".format(exp))+            # Without the IO loop running the program would become+            # unresponsive, proactively we close the process.+            sys.exit(1)","When the event loop ran on the main thread, people had the ability to catch exceptions that bubbled up and handle them in an application-appropriate manner. Have we lost a significant capability by moving to a different thread?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22076,381514343,2020-02-19T20:05:05Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/io_loop.pyx.pxi,"@@ -0,0 +1,87 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import sys+import threading++_RESOLUTION_MS = 0.001+cdef _IOLoop _io_loop = None++cdef class _IOLoop:++    def __init__(self):+        global _io_loop++        # Only one instantiation is exepected        +        assert _io_loop is None++        self._asyncio_loop = None+        self._io_ev = threading.Event()+        self._loop_started_cv = threading.Condition()+        self._loop_started = False++        self._thread = threading.Thread(target=self._run_forever, daemon=True)+        self._thread.start()++        # Some attributes are initialitzated when+        # the thread is really started, we wait till+        # the whole Asyncio loop is initialized and+        # ready to be used.+        with self._loop_started_cv:+            if not self._loop_started:+                self._loop_started_cv.wait()++        _io_loop = self+++    def _loop_started_cb(self):+        with self._loop_started_cv:+            self._loop_started = True+            self._loop_started_cv.notify_all()++    def _run_forever(self):+        self._asyncio_loop = asyncio.new_event_loop()+        asyncio.set_event_loop(self._asyncio_loop)+        self._asyncio_loop.call_soon(self._loop_started_cb)+        try:+            self._asyncio_loop.run_forever()+        except Exception as exp:+            print(""An error ocurred with the IO loop {}"".format(exp))+            # Without the IO loop running the program would become+            # unresponsive, proactively we close the process.+            sys.exit(1)++    cpdef void io_mark(self):+        # Wake up all threads that were waiting+        # for an IO event.+        self._io_ev.set()++        # Clear the status, further threads will get+        # block.+        self._io_ev.clear()++    cdef void io_wait(self, size_t timeout_ms):+        if threading.get_ident() == self._thread.ident:+            # Reentrance is not allowed, otherwise we will be+            # blocking the Asyncio loop.+            return++        if timeout_ms > 0:+            self._io_ev.wait(timeout_ms * _RESOLUTION_MS)++    cdef object asyncio_loop(self):+        return self._asyncio_loop++cdef _IOLoop _current_io_loop():","optional: It can be a `@classmethod`, like getting singletons. The single instance of `_IOLoop` is a class variable, and can be set by `__new__` and access by `_IOLoop.get()`.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22076,381515144,2020-02-19T20:06:43Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi,"@@ -25,9 +25,15 @@ cdef class CallbackFailureHandler:         self._exception_type = exception_type      cdef handle(self, object future):-        future.set_exception(self._exception_type(+        excp = self._exception_type(",nit: s/excp/exception/,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22076,381598538,2020-02-19T23:04:20Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/io_loop.pyx.pxi,"@@ -0,0 +1,87 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import sys+import threading++_RESOLUTION_MS = 0.001+cdef _IOLoop _io_loop = None++cdef class _IOLoop:++    def __init__(self):+        global _io_loop++        # Only one instantiation is exepected        +        assert _io_loop is None++        self._asyncio_loop = None+        self._io_ev = threading.Event()+        self._loop_started_cv = threading.Condition()+        self._loop_started = False++        self._thread = threading.Thread(target=self._run_forever, daemon=True)+        self._thread.start()++        # Some attributes are initialitzated when+        # the thread is really started, we wait till+        # the whole Asyncio loop is initialized and+        # ready to be used.+        with self._loop_started_cv:+            if not self._loop_started:+                self._loop_started_cv.wait()++        _io_loop = self+++    def _loop_started_cb(self):+        with self._loop_started_cv:+            self._loop_started = True+            self._loop_started_cv.notify_all()++    def _run_forever(self):+        self._asyncio_loop = asyncio.new_event_loop()+        asyncio.set_event_loop(self._asyncio_loop)","Nops, third-party event loops install their policy [1] which would make Asyncio aware which is the factory for creating new loops.[1] https://github.com/pfreixes/grpcio_stress/blob/master/sync_unary.py#L113",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22076,381600311,2020-02-19T23:09:04Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/io_loop.pyx.pxi,"@@ -0,0 +1,87 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import sys+import threading++_RESOLUTION_MS = 0.001+cdef _IOLoop _io_loop = None++cdef class _IOLoop:++    def __init__(self):+        global _io_loop++        # Only one instantiation is exepected        +        assert _io_loop is None++        self._asyncio_loop = None+        self._io_ev = threading.Event()+        self._loop_started_cv = threading.Condition()+        self._loop_started = False++        self._thread = threading.Thread(target=self._run_forever, daemon=True)+        self._thread.start()++        # Some attributes are initialitzated when+        # the thread is really started, we wait till+        # the whole Asyncio loop is initialized and+        # ready to be used.+        with self._loop_started_cv:+            if not self._loop_started:+                self._loop_started_cv.wait()++        _io_loop = self+++    def _loop_started_cb(self):+        with self._loop_started_cv:+            self._loop_started = True+            self._loop_started_cv.notify_all()++    def _run_forever(self):+        self._asyncio_loop = asyncio.new_event_loop()+        asyncio.set_event_loop(self._asyncio_loop)+        self._asyncio_loop.call_soon(self._loop_started_cb)+        try:+            self._asyncio_loop.run_forever()+        except Exception as exp:+            print(""An error ocurred with the IO loop {}"".format(exp))+            # Without the IO loop running the program would become+            # unresponsive, proactively we close the process.+            sys.exit(1)","Theoretically, if I'm not missing something I would say no. Exceptions are still raised through the futures. So, any exception happening in the IO loop that is bound to a future that is consumed in another thread will get that exception.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22076,381603525,2020-02-19T23:18:13Z,src/python/grpcio_tests/commands.py,"@@ -229,6 +229,52 @@ def run(self):         if not result.value.wasSuccessful():             sys.exit('Test failure') +class TestAsyncio(setuptools.Command):+    """"""Command to run tests w/asyncio.""""""++    BANNED_TESTS = (","Just followed the same structure as other `Test???` classes - there are many, I would prefer to follow the same strategy which is IMO super explicit.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22076,381604484,2020-02-19T23:20:59Z,src/python/grpcio_tests/commands.py,"@@ -229,6 +229,52 @@ def run(self):         if not result.value.wasSuccessful():             sys.exit('Test failure') +class TestAsyncio(setuptools.Command):+    """"""Command to run tests w/asyncio.""""""++    BANNED_TESTS = (+        # Fork support is not compatible with Aio+        'fork._fork_interop_test.ForkInteropTest',+        'unit._cython._fork_test',++        # Next tests imply the usage by the gRPC library syscalls that do not+        # consider the existence of an installed iomgr.+        'unit._local_credentials_test.LocalCredentialsTest.test_local_tcp',++        # segfaults+        'unit._local_credentials_test.LocalCredentialsTest.test_uds',++        # needs investigation+        'health_check._health_servicer_test',++        # hangs forever+        'unit._cython._channel_test.ChannelTest.test_single_channel_lonely_connectivity',+        'unit._cython._channel_test.ChannelTest.test_multiple_channels_lonely_connectivity',+        'unit._abort_test.AbortTest.test_abort'+    )+    description = 'run tests with Asyncio IO manager.'+    user_options = []++    def initialize_options(self):+        pass++    def finalize_options(self):+        # distutils requires this override.+        pass++    def run(self):+        import grpc.experimental.aio+        grpc.experimental.aio.init_grpc_aio()++        import tests","I would prefer for now use its own test with some bits duplicated, I like the idea for now for having this small duplication, which IMO gives more readability.",
503812,voidzcy,https://api.github.com/repos/grpc/grpc/pulls/22063,381871837,2020-02-20T09:17:31Z,src/core/ext/filters/client_channel/xds/xds_api.cc,"@@ -260,7 +273,7 @@ envoy_api_v2_DiscoveryRequest* CreateDiscoveryRequest(   if (build_version != nullptr) {     envoy_api_v2_core_Node* node_msg =         envoy_api_v2_DiscoveryRequest_mutable_node(request, arena);-    PopulateNode(arena, node, build_version, node_msg);+    PopulateNode(arena, node, build_version, """", node_msg);","Why ""PROXYLESS_CLIENT_HOSTNAME"" is set to empty string for xDS requests? Does it matter? Can it be the same `Node` as used for LRS? Or can the `Node` for xDS requests not have that metadata field?",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22076,382020083,2020-02-20T14:08:20Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/io_loop.pyx.pxi,"@@ -0,0 +1,87 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import sys+import threading++_RESOLUTION_MS = 0.001+cdef _IOLoop _io_loop = None++cdef class _IOLoop:++    def __init__(self):+        global _io_loop++        # Only one instantiation is exepected        +        assert _io_loop is None++        self._asyncio_loop = None+        self._io_ev = threading.Event()+        self._loop_started_cv = threading.Condition()+        self._loop_started = False++        self._thread = threading.Thread(target=self._run_forever, daemon=True)+        self._thread.start()++        # Some attributes are initialitzated when+        # the thread is really started, we wait till+        # the whole Asyncio loop is initialized and+        # ready to be used.+        with self._loop_started_cv:+            if not self._loop_started:+                self._loop_started_cv.wait()++        _io_loop = self+++    def _loop_started_cb(self):+        with self._loop_started_cv:+            self._loop_started = True+            self._loop_started_cv.notify_all()++    def _run_forever(self):+        self._asyncio_loop = asyncio.new_event_loop()+        asyncio.set_event_loop(self._asyncio_loop)+        self._asyncio_loop.call_soon(self._loop_started_cb)+        try:+            self._asyncio_loop.run_forever()+        except Exception as exp:+            print(""An error ocurred with the IO loop {}"".format(exp))+            # Without the IO loop running the program would become+            # unresponsive, proactively we close the process.+            sys.exit(1)++    cpdef void io_mark(self):+        # Wake up all threads that were waiting+        # for an IO event.+        self._io_ev.set()++        # Clear the status, further threads will get+        # block.+        self._io_ev.clear()++    cdef void io_wait(self, size_t timeout_ms):+        if threading.get_ident() == self._thread.ident:+            # Reentrance is not allowed, otherwise we will be+            # blocking the Asyncio loop.+            return++        if timeout_ms > 0:+            self._io_ev.wait(timeout_ms * _RESOLUTION_MS)++    cdef object asyncio_loop(self):+        return self._asyncio_loop++cdef _IOLoop _current_io_loop():","Unfortunately the `classmethod` is not yet available in Cython, so everything would need to be done by using a `staticmethod`. The reason for moving this function as an individual method was for making it usable in its C format from any Python object.For using the `__new__` we would need to use a Python class If I'm not wrong.If you believe that having a more clear encapsulation is a good trade-off happy to change it.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22063,382059053,2020-02-20T15:08:12Z,src/core/ext/filters/client_channel/xds/xds_api.cc,"@@ -260,7 +273,7 @@ envoy_api_v2_DiscoveryRequest* CreateDiscoveryRequest(   if (build_version != nullptr) {     envoy_api_v2_core_Node* node_msg =         envoy_api_v2_DiscoveryRequest_mutable_node(request, arena);-    PopulateNode(arena, node, build_version, node_msg);+    PopulateNode(arena, node, build_version, """", node_msg);",This metadata field should not be present in xDS requests (which is what this code does -- `PopulateNode()` will only populate this field if the passed-in string is non-empty).This metadata field is needed only for LRS.,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22076,382074111,2020-02-20T15:29:58Z,src/python/grpcio_tests/commands.py,"@@ -229,6 +229,52 @@ def run(self):         if not result.value.wasSuccessful():             sys.exit('Test failure') +class TestAsyncio(setuptools.Command):+    """"""Command to run tests w/asyncio.""""""++    BANNED_TESTS = (+        # Fork support is not compatible with Aio+        'fork._fork_interop_test.ForkInteropTest',+        'unit._cython._fork_test',++        # Next tests imply the usage by the gRPC library syscalls that do not+        # consider the existence of an installed iomgr.+        'unit._local_credentials_test.LocalCredentialsTest.test_local_tcp',++        # segfaults+        'unit._local_credentials_test.LocalCredentialsTest.test_uds',++        # needs investigation+        'health_check._health_servicer_test',++        # hangs forever+        'unit._cython._channel_test.ChannelTest.test_single_channel_lonely_connectivity',+        'unit._cython._channel_test.ChannelTest.test_multiple_channels_lonely_connectivity',+        'unit._abort_test.AbortTest.test_abort'+    )+    description = 'run tests with Asyncio IO manager.'+    user_options = []++    def initialize_options(self):+        pass++    def finalize_options(self):+        # distutils requires this override.+        pass++    def run(self):+        import grpc.experimental.aio",Updated the docstring of the Class and also changed the name of the class for a more meaningful name.,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/21929,382152443,2020-02-20T17:36:55Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -0,0 +1,709 @@+#!/usr/bin/env python+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import subprocess+import yaml+import xml.etree.ElementTree as ET+import os+import sys++_ROOT = os.path.abspath(os.path.join(os.path.dirname(sys.argv[0]), '../..'))+os.chdir(_ROOT)++def _bazel_query_xml_tree(query):+    """"""Get xml output of bazel query invocation, parsed as XML tree""""""+    output = subprocess.check_output(['tools/bazel', 'query', '--noimplicit_deps', '--output', 'xml', query])+    return ET.fromstring(output)+++def _rule_dict_from_xml_node(rule_xml_node):+    result = {+        'class': rule_xml_node.attrib.get('class'),+        'name': rule_xml_node.attrib.get('name'),+        'srcs': [],+        'hdrs': [],+        'deps': [],+        'data': [],+        'tags': [],+        'args': [],+        'generator_function': None,+        'size': None,+    }+    for child in rule_xml_node:+        # all the metadata we want is stored under ""list"" tags+        if child.tag == 'list':+            list_name = child.attrib['name']+            if list_name in ['srcs', 'hdrs', 'deps', 'data', 'tags', 'args']:+                result[list_name] += [ item.attrib['value'] for item in child ]+        if child.tag == 'string':+            string_name = child.attrib['name']+            if string_name in ['generator_function', 'size']:+                result[string_name] = child.attrib['value']+    return result+++def _extract_rules_from_bazel_xml(xml_tree):+    result = {}+    for child in xml_tree:+        if child.tag == 'rule':+            rule_dict = _rule_dict_from_xml_node(child)+            rule_clazz = rule_dict['class']+            rule_name = rule_dict['name']+            if rule_clazz in ['cc_library', 'cc_binary', 'cc_test', 'cc_proto_library', 'proto_library']:+                if rule_name in result:+                    raise Exception('Rule %s already present' % rule_name)+                result[rule_name] = rule_dict+    return result+++def _get_bazel_label(target_name):+    if ':' in target_name:+        return '//%s' % target_name+    else:+        return '//:%s' % target_name+++def _extract_source_file_path(label):+    """"""Gets relative path to source file from bazel deps listing""""""+    if label.startswith('//'):+        label = label[len('//'):]+    # labels in form //:src/core/lib/surface/call_test_only.h+    if label.startswith(':'):+        label = label[len(':'):]+    # labels in form //test/core/util:port.cc+    label = label.replace(':', '/')+    return label+++def _extract_public_headers(bazel_rule):+    """"""Gets list of public headers from a bazel rule""""""+    result = []+    for dep in bazel_rule['hdrs']:+        if dep.startswith('//:include/') and dep.endswith('.h'):+            result.append(_extract_source_file_path(dep))+    return list(sorted(result))+++def _extract_nonpublic_headers(bazel_rule):+    """"""Gets list of non-public headers from a bazel rule""""""+    result = []+    for dep in bazel_rule['hdrs']:+        if dep.startswith('//') and not dep.startswith('//:include/') and dep.endswith('.h'):+            result.append(_extract_source_file_path(dep))+    return list(sorted(result))+++def _extract_sources(bazel_rule):+    """"""Gets list of source files from a bazel rule""""""+    result = []+    for dep in bazel_rule['srcs']:+        if dep.startswith('//') and (dep.endswith('.cc') or dep.endswith('.c') or dep.endswith('.proto')):+            result.append(_extract_source_file_path(dep))+    return list(sorted(result))+++def _extract_deps(bazel_rule):+    """"""Gets list of deps from from a bazel rule""""""+    return list(sorted(bazel_rule['deps']))+++def _create_target_from_bazel_rule(target_name, bazel_rules):+    # extract the deps from bazel+    bazel_rule = bazel_rules[_get_bazel_label(target_name)]+    result = {+        'name': target_name,+        '_PUBLIC_HEADERS_BAZEL': _extract_public_headers(bazel_rule),+        '_HEADERS_BAZEL': _extract_nonpublic_headers(bazel_rule),+        '_SRC_BAZEL': _extract_sources(bazel_rule),+        '_DEPS_BAZEL': _extract_deps(bazel_rule),+    }+    return result+++def _sort_by_build_order(lib_names, lib_dict, verbose = False):+    """"""Sort library names to form correct build order. Use metadata from lib_dict""""""+    # we find correct build order by performing a topological sort+    # expected output: if library B depends on A, A should be listed first+    +    # all libs that are not in the dictionary are considered external.+    external_deps = list(sorted(filter(lambda lib_name: lib_name not in lib_dict, lib_names)))+    if verbose:+      print('topo_ordering ' + str(lib_names))+      print('    external_deps ' + str(external_deps))++    result = list(external_deps)  # external deps will be listed first+    while len(result) < len(lib_names):+        more_results = []+        for lib in lib_names:+            if lib not in result:+                # TODO: this is a big hack!+                dep_set = set(lib_dict[lib].get('transitive_deps', []))+                dep_set.update(set(lib_dict[lib]['deps']))+                dep_set = dep_set.intersection(lib_names)+                # if lib only depends on what's already built, at it to the results+                if not dep_set.difference(set(result)):+                    more_results.append(lib)+        if not more_results:+            raise Exception('Cannot sort topologically, there seems to be a cyclic dependency')+        if verbose:+            print('    adding ' + str(more_results))+        result = result + list(sorted(more_results))  # when build order doesn't matter, sort lexicographically+    return result+++def _populate_transitive_deps(bazel_rules):+    """"""Add 'transitive_deps' field for each of the rules""""""+    transitive_deps = {}+    for rule_name in bazel_rules.iterkeys():+        transitive_deps[rule_name] = set(bazel_rules[rule_name]['deps'])+    +    while True:+        deps_added = 0+        for rule_name in bazel_rules.iterkeys():+            old_deps = transitive_deps[rule_name]+            new_deps = set(old_deps)+            for dep_name in old_deps:+                new_deps.update(transitive_deps.get(dep_name, set()))+            deps_added += len(new_deps) - len(old_deps)+            transitive_deps[rule_name] = new_deps+        # if none of the transitive dep sets has changed, we're done+        if deps_added == 0:+            break+    +    for rule_name, bazel_rule in bazel_rules.iteritems():+        bazel_rule['transitive_deps'] = list(sorted(transitive_deps[rule_name]))+++def _external_dep_name_from_bazel_dependency(bazel_dep):+    """"""Returns name of dependency if external bazel dependency is provided or None""""""+    if bazel_dep.startswith('@com_google_absl//'):+        # special case for add dependency on one of the absl libraries (there is not just one absl library)+        prefixlen = len('@com_google_absl//')+        return bazel_dep[prefixlen:]+    elif bazel_dep == '//external:upb_lib':+        return 'upb'+    elif bazel_dep == '//external:benchmark':+        return 'benchmark'+    else:+        # all the other external deps such as gflags, protobuf, cares, zlib+        # don't need to be listed explicitly, they are handled automatically+        # by the build system (make, cmake)+        return None+++def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):+    # Some of the libraries defined by bazel won't be exposed in build.yaml+    # We call these ""intermediate"" dependencies. This method expands +    # the intermediate deps for given target (populates library's+    # headers, sources and dicts as if the intermediate dependency never existed)+    +    # use this dictionary to translate from bazel labels to dep names+    bazel_label_to_dep_name = {}+    for dep_name in public_dep_names:+        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name++    target_name = target_dict['name']+    bazel_deps = target_dict['_DEPS_BAZEL']++    # initial values+    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])+    headers = set(target_dict['_HEADERS_BAZEL'])+    src = set(target_dict['_SRC_BAZEL'])+    deps = set()++    expansion_blacklist = set()+    to_expand = set(bazel_deps)+    while to_expand:++        # start with the last dependency to be built+        build_order = _sort_by_build_order(list(to_expand), bazel_rules)++        bazel_dep = build_order[-1]+        to_expand.remove(bazel_dep)++        is_public = bazel_dep in bazel_label_to_dep_name+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(bazel_dep)++        if is_public:+            # this is not an intermediate dependency we so we add it+            # to the list of public dependencies to the list, in the right format+            deps.add(bazel_label_to_dep_name[bazel_dep])++            # we do not want to expand any intermediate libraries that are already included+            # by the dependency we just added+            expansion_blacklist.update(bazel_rules[bazel_dep]['transitive_deps'])++        elif external_dep_name_maybe:+            deps.add(external_dep_name_maybe)++        elif bazel_dep.startswith('//external:') or not bazel_dep.startswith('//'):+            # all the other external deps can be skipped+            pass++        elif bazel_dep in expansion_blacklist:+            # do not expand if a public dependency that depends on this has already been expanded+            pass++        else:+            if bazel_dep in bazel_rules:+                # this is an intermediate library, expand it+                public_headers.update(_extract_public_headers(bazel_rules[bazel_dep]))+                headers.update(_extract_nonpublic_headers(bazel_rules[bazel_dep]))+                src.update(_extract_sources(bazel_rules[bazel_dep]))+                +                new_deps = _extract_deps(bazel_rules[bazel_dep])+                to_expand.update(new_deps)+            else:+                raise Exception(bazel_dep + ' not in bazel_rules')+    +    # make the 'deps' field transitive, but only list non-intermediate deps and selected external deps+    bazel_transitive_deps = bazel_rules[_get_bazel_label(target_name)]['transitive_deps']+    for transitive_bazel_dep in bazel_transitive_deps:+        public_name = bazel_label_to_dep_name.get(transitive_bazel_dep, None)+        if public_name:+            deps.add(public_name)+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(transitive_bazel_dep)+        if external_dep_name_maybe:+            # expanding all absl libraries is technically correct but creates too much noise+            if not external_dep_name_maybe.startswith('absl'):+                deps.add(external_dep_name_maybe)++    target_dict['public_headers'] = list(sorted(public_headers))+    target_dict['headers'] = list(sorted(headers))+    target_dict['src'] = list(sorted(src))+    target_dict['deps'] = list(sorted(deps))+++def _generate_build_metadata(build_extra_metadata, bazel_rules):+    lib_names = build_extra_metadata.keys()+    result = {}+    +    for lib_name in lib_names:+        lib_dict = _create_target_from_bazel_rule(lib_name, bazel_rules)++        _expand_intermediate_deps(lib_dict, lib_names, bazel_rules)    ++        # populate extra properties from build metadata+        lib_dict.update(build_extra_metadata.get(lib_name, {}))++        # store to results+        result[lib_name] = lib_dict++    # rename some targets to something else+    # this needs to be made after we're done with most of processing logic+    # otherwise the already-renamed libraries will have different names than expected+    for lib_name in lib_names:+        to_name = build_extra_metadata.get(lib_name, {}).get('_RENAME', None)+        if to_name:+            # store lib under the new name and also change its 'name' property+            if to_name in result:+                raise Exception('Cannot rename target ' + lib_name + ', ' + to_name + ' already exists.')+            lib_dict = result.pop(lib_name)+            lib_dict['name'] = to_name+            result[to_name] = lib_dict++            # dep names need to be updated as well+            for lib_dict_to_update in result.values():+                lib_dict_to_update['deps'] = list(map(lambda dep: to_name if dep == lib_name else dep, lib_dict_to_update['deps']))++    # make sure deps are listed in reverse topological order (e.g. ""grpc gpr"" and not ""gpr grpc"")+    for lib_dict in result.itervalues():+        lib_dict['deps'] = list(reversed(_sort_by_build_order(lib_dict['deps'], result)))++    return result+++def _convert_to_build_yaml_like(lib_dict):+    lib_names = list(filter(lambda lib_name: lib_dict[lib_name].get('_TYPE', 'library') == 'library' , lib_dict.keys()))+    target_names = list(filter(lambda lib_name: lib_dict[lib_name].get('_TYPE', 'library') == 'target' , lib_dict.keys()))+    test_names = list(filter(lambda lib_name: lib_dict[lib_name].get('_TYPE', 'library') == 'test' , lib_dict.keys()))++    # make sure libraries come in build order (seems to be required by Makefile)+    lib_names = _sort_by_build_order(lib_names, lib_dict)+    target_names = _sort_by_build_order(target_names, lib_dict)+    test_names = _sort_by_build_order(test_names, lib_dict)++    # list libraries and targets in predefined order+    lib_list = list(map(lambda lib_name: lib_dict[lib_name], lib_names))+    target_list = list(map(lambda lib_name: lib_dict[lib_name], target_names))+    test_list = list(map(lambda lib_name: lib_dict[lib_name], test_names))+    +    # get rid of temporary private fields prefixed with ""_"" and some other useless fields+    for lib in lib_list:+        for field_to_remove in filter(lambda k: k.startswith('_'), lib.keys()):+            lib.pop(field_to_remove, None)+    for target in target_list:+        for field_to_remove in filter(lambda k: k.startswith('_'), target.keys()):+            target.pop(field_to_remove, None)+        target.pop('public_headers', None)  # public headers make no sense for targets+    for test in test_list:+        for field_to_remove in filter(lambda k: k.startswith('_'), test.keys()):+            test.pop(field_to_remove, None)+        test.pop('public_headers', None)  # public headers make no sense for tests+    +    build_yaml_like = {+        'libs': lib_list,+        'filegroups': [],+        'targets': target_list,+        'tests': test_list,+    }+    return build_yaml_like+++def _extract_cc_tests(bazel_rules):+    """"""Gets list of cc_test tests from bazel rules""""""+    result = []+    for bazel_rule in bazel_rules.itervalues():+        if bazel_rule['class'] == 'cc_test':+            test_name = bazel_rule['name']+            if test_name.startswith('//'):+                prefixlen = len('//')+                result.append(test_name[prefixlen:])+    return list(sorted(result))++def _filter_cc_tests(tests):+    """"""Filters out tests that we don't want or we cannot build them reasonably""""""+    +    # most qps tests are autogenerated, we are fine without them+    tests = list(filter(lambda test: not test.startswith('test/cpp/qps:'), tests))++    # we have trouble with census dependency outside of bazel+    tests = list(filter(lambda test: not test.startswith('test/cpp/ext/filters/census:'), tests))+    tests = list(filter(lambda test: not test.startswith('test/cpp/microbenchmarks:bm_opencensus_plugin'), tests))++    # missing opencensus/stats/stats.h+    tests = list(filter(lambda test: not test.startswith('test/cpp/end2end:server_load_reporting_end2end_test'), tests))+    tests = list(filter(lambda test: not test.startswith('test/cpp/server/load_reporter:lb_load_reporter_test'), tests))+    +    # The test uses --running_under_bazel cmdline argument+    # To avoid the trouble needing to adjust it, we just skip the test+    tests = list(filter(lambda test: not test.startswith('test/cpp/naming:resolver_component_tests_runner_invoker'), tests))++    # the test requires 'client_crash_test_server' to be built+    tests = list(filter(lambda test: not test.startswith('test/cpp/end2end:time_change_test'), tests))++    # the test requires 'client_crash_test_server' to be built+    tests = list(filter(lambda test: not test.startswith('test/cpp/end2end:client_crash_test'), tests))++    # the test requires 'server_crash_test_client' to be built+    tests = list(filter(lambda test: not test.startswith('test/cpp/end2end:server_crash_test'), tests))++    # test never existed under build.yaml and it fails -> skip it+    tests = list(filter(lambda test: not test.startswith('test/core/tsi:ssl_session_cache_test'), tests))++    return tests+++def _generate_build_extra_metadata_for_tests(tests, bazel_rules):+    test_metadata = {}+    for test in tests:+        test_dict = { 'build': 'test', '_TYPE': 'target' }++        bazel_rule = bazel_rules[_get_bazel_label(test)]++        bazel_tags = bazel_rule['tags']+        if 'manual' in bazel_tags:+            # don't run the tests marked as ""manual""+            test_dict['run'] = False++        if 'grpc_fuzzer' == bazel_rule['generator_function']:+            # currently we hand-list fuzzers instead of generating them automatically+            # because there's no way to obtain maxlen property from bazel BUILD file.+            print('skipping fuzzer ' + test)+            continue++        # if any tags that restrict platform compatibility are present,+        # generate the ""platforms"" field accordingly+        # TODO(jtattermusch): there is also a ""no_linux"" tag, but we cannot take+        # it into account as it is applied by grpc_cc_test when poller expansion+        # is made (for tests where uses_polling=True). So for now, we just+        # assume all tests are compatible with linux and ignore the ""no_linux"" tag+        # completely.+        known_platform_tags = set(['no_windows', 'no_mac'])+        if set(bazel_tags).intersection(known_platform_tags):+            platforms = []+            # assume all tests are compatible with linux and posix+            platforms.append('linux')+            platforms.append('posix')  # there is no posix-specific tag in bazel BUILD+            if not 'no_mac' in bazel_tags:+                platforms.append('mac')+            if not 'no_windows' in bazel_tags:+                platforms.append('windows')+            test_dict['platforms'] = platforms++        if '//external:benchmark' in bazel_rule['transitive_deps']:+            test_dict['benchmark'] = True+            test_dict['defaults'] = 'benchmark'++        cmdline_args = bazel_rule['args']+        if cmdline_args:+            test_dict['args'] = list(cmdline_args)++        uses_gtest = '//external:gtest' in bazel_rule['transitive_deps']+        if uses_gtest:+            test_dict['gtest'] = True++        if test.startswith('test/cpp') or uses_gtest:+            test_dict['language'] = 'c++'+            +        elif test.startswith('test/core'):+            test_dict['language'] = 'c'+        else:+            raise Exception('wrong test' + test)++        # short test name without the path.+        # There can be name collisions, but we will resolve them later+        simple_test_name = os.path.basename(_extract_source_file_path(test))+        test_dict['_RENAME'] = simple_test_name++        test_metadata[test] = test_dict++    # detect duplicate test names+    tests_by_simple_name = {}+    for test_name, test_dict in test_metadata.iteritems():+        simple_test_name = test_dict['_RENAME']+        if not simple_test_name in tests_by_simple_name:+            tests_by_simple_name[simple_test_name] = []+        tests_by_simple_name[simple_test_name].append(test_name)++    # choose alternative names for tests with a name collision+    for collision_list in tests_by_simple_name.itervalues():+        if len(collision_list) > 1:+            for test_name in collision_list:+                long_name = test_name.replace('/', '_').replace(':', '_')+                print('short name of ""%s"" collides with another test, renaming to %s' % (test_name, long_name))+                test_metadata[test_name]['_RENAME'] = long_name++    # TODO(jtattermusch): in bazel, add ""_test"" suffix to the test names+    # test does not have ""_test"" suffix: fling+    # test does not have ""_test"" suffix: fling_stream+    # test does not have ""_test"" suffix: client_ssl+    # test does not have ""_test"" suffix: handshake_server_with_readahead_handshaker+    # test does not have ""_test"" suffix: handshake_verify_peer_options+    # test does not have ""_test"" suffix: server_ssl++    return test_metadata++# extra metadata that will be used to construct build.yaml+# there are mostly extra properties that we weren't able to obtain from the bazel build+# _TYPE: whether this is library, target or test+# _RENAME: whether this target should be renamed to a different name (to match expectations of make and cmake builds)+# NOTE: secure is 'check' by default, so setting secure = False below does matter+_BUILD_EXTRA_METADATA = {",How about having a separate file like `build_metadata.yaml` to keep this script isolated?,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21929,382171557,2020-02-20T18:13:51Z,templates/gRPC-C++.podspec.template,"@@ -60,42 +60,43 @@         files.update(lib.get(field, []))     return list(sorted(files)) -  def list_filegroup_files(expect_filegroups, groups):-    out = []-    for filegroup_name in expect_filegroups:-      filegroup = filegroup_maps[filegroup_name]-      for group in groups:-        out += filegroup.get(group, [])-    return list(sorted(set(out)))-   def filter_grpcpp(files):     return sorted([file for file in files if not file.startswith(""include/grpc++"")]) -  grpc_private_files = list_lib_files(""grpc"", (""headers"", ""src""))-  grpc_public_headers = list_lib_files(""grpc"", (""public_headers"",))-  grpc_private_headers = list_lib_files(""grpc"", (""headers"",))+  # TODO(jtattermusch): get rid of this hack+  address_sorting_unwanted_files = list_lib_files(""address_sorting"", (""public_headers"", ""headers"", ""src""))++  grpc_private_files = list(sorted(set(list_lib_files(""grpc"", (""headers"", ""src""))) - set(address_sorting_unwanted_files)))+  grpc_public_headers = list(sorted(set(list_lib_files(""grpc"", (""public_headers"",))) - set(address_sorting_unwanted_files)))+  grpc_private_headers = list(sorted(set(list_lib_files(""grpc"", (""headers"",))) - set(address_sorting_unwanted_files)))++  # TODO(jtattermusch): build.yaml no longer has filegroups, so the files here are just hand-listed","Potentially we could add this in build_handwritten.yaml, I'll look into that.I understand the desire to have the list of file for grpc_cronet in a special place (I'll try to clean up), but in practice this approach this doesn't really change much IMHO. AFAIK The grpc_cronet was only built as part of the objC build even before and the podspecs were hand-massaging and filtering the list of the files anyway.  ",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22076,382184739,2020-02-20T18:39:34Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/io_loop.pyx.pxi,"@@ -0,0 +1,83 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import os+import sys+import threading++_SECONDS_PER_MILLISECOND = 0.001+cdef _IOLoop _io_loop = None++cdef class _IOLoop:++    def __init__(self):+        global _io_loop++        # Only one instantiation is exepected        +        assert _io_loop is None++        self._asyncio_loop = None+        self._io_ev = threading.Event()+        self._loop_started_ev = threading.Event()++        self._thread = threading.Thread(target=self._run_forever, daemon=True)+        self._thread.start()++        # Some attributes are initialitzated when+        # the thread is really started, we wait till+        # the whole Asyncio loop is initialized and+        # ready to be used.+        self._loop_started_ev.wait()++        _io_loop = self++    def _loop_started_cb(self):+        self._loop_started_ev.set()++    def _run_forever(self):+        self._asyncio_loop = asyncio.new_event_loop()+        asyncio.set_event_loop(self._asyncio_loop)+        self._asyncio_loop.call_soon(self._loop_started_cb)+        try:+            self._asyncio_loop.run_forever()+        except Exception as exp:+            sys.stderr(""An error ocurred with the IO loop {}{}"".format(exp, os.linesep))",I missed this one. Should we use `_LOGGER.error` or `_LOGGER.exception`?,
7281574,nicolasnoble,https://api.github.com/repos/grpc/grpc/pulls/22093,382242771,2020-02-20T20:34:38Z,src/core/tsi/ssl_transport_security.cc,"@@ -1122,7 +1122,7 @@ static void tsi_ssl_handshaker_factory_init( tsi_result tsi_ssl_get_cert_chain_contents(STACK_OF(X509) * peer_chain,                                            tsi_peer_property* property) {   BIO* bio = BIO_new(BIO_s_mem());-  for (int i = 0; i < sk_X509_num(peer_chain); i++) {+  for (size_t i = 0; i < sk_X509_num(peer_chain); i++) {","```Cdecltype(sk_X509_num(peer_chain)) i, num = sk_X509_num(peer_chain);for (i = 0; i < num; i++) {```",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22076,382248459,2020-02-20T20:47:31Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/io_loop.pyx.pxi,"@@ -0,0 +1,83 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import os+import sys+import threading++_SECONDS_PER_MILLISECOND = 0.001+cdef _IOLoop _io_loop = None++cdef class _IOLoop:++    def __init__(self):+        global _io_loop++        # Only one instantiation is exepected        +        assert _io_loop is None++        self._asyncio_loop = None+        self._io_ev = threading.Event()+        self._loop_started_ev = threading.Event()++        self._thread = threading.Thread(target=self._run_forever, daemon=True)+        self._thread.start()++        # Some attributes are initialitzated when+        # the thread is really started, we wait till+        # the whole Asyncio loop is initialized and+        # ready to be used.+        self._loop_started_ev.wait()++        _io_loop = self++    def _loop_started_cb(self):+        self._loop_started_ev.set()++    def _run_forever(self):+        self._asyncio_loop = asyncio.new_event_loop()+        asyncio.set_event_loop(self._asyncio_loop)+        self._asyncio_loop.call_soon(self._loop_started_cb)+        try:+            self._asyncio_loop.run_forever()+        except Exception as exp:+            sys.stderr(""An error ocurred with the IO loop {}{}"".format(exp, os.linesep))","facepalm!Ive used the `sys.stderr` because for an unknown reason `print` within Cyhton does not have support for the `file` kw, no idea the reason why.I can use the `_LOGGER.exception` as it's suggested by Lidi. do you agree?",
900411,mehrdada,https://api.github.com/repos/grpc/grpc/pulls/22093,382305177,2020-02-20T22:56:00Z,src/core/tsi/ssl_transport_security.cc,"@@ -1122,7 +1122,7 @@ static void tsi_ssl_handshaker_factory_init( tsi_result tsi_ssl_get_cert_chain_contents(STACK_OF(X509) * peer_chain,                                            tsi_peer_property* property) {   BIO* bio = BIO_new(BIO_s_mem());-  for (int i = 0; i < sk_X509_num(peer_chain); i++) {+  for (size_t i = 0; i < sk_X509_num(peer_chain); i++) {",how about ```cppfor (size_t i = 0; i != sk_X509_num(peer_chain); i++) {```,
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/22082,382311352,2020-02-20T23:13:10Z,src/core/lib/security/security_connector/local/local_security_connector.cc,"@@ -70,39 +70,19 @@ void local_check_peer(tsi_peer peer, grpc_endpoint* ep,                       grpc_core::RefCountedPtr<grpc_auth_context>* auth_context,                       grpc_closure* on_peer_checked,                       grpc_local_connect_type type) {-  int fd = grpc_endpoint_get_fd(ep);-  grpc_resolved_address resolved_addr;-  memset(&resolved_addr, 0, sizeof(resolved_addr));-  resolved_addr.len = GRPC_MAX_SOCKADDR_SIZE;   bool is_endpoint_local = false;-  if (getsockname(fd, reinterpret_cast<grpc_sockaddr*>(resolved_addr.addr),-                  &resolved_addr.len) == 0) {-    grpc_resolved_address addr_normalized;-    grpc_resolved_address* addr =-        grpc_sockaddr_is_v4mapped(&resolved_addr, &addr_normalized)-            ? &addr_normalized-            : &resolved_addr;-    grpc_sockaddr* sock_addr = reinterpret_cast<grpc_sockaddr*>(&addr->addr);-    // UDS-    if (type == UDS && grpc_is_unix_socket(addr)) {++  char* peer_address = ep->vtable->get_peer(ep);+  if (type == UDS && strncmp(""uds"", peer_address, 3)) {+    is_endpoint_local = true;+  } else if (type == LOCAL_TCP) {+    if (strncmp(""ipv4:127.0.0.1"", peer_address, 14)) {+      is_endpoint_local = true;+    } else if (strncmp(""ipv6:[::1]"", peer_address, 10)) {",https://github.com/grpc/grpc/blob/master/src/core/lib/iomgr/sockaddr_utils.cc#L219 is the place that bake `peer_address`.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/21904,382725831,2020-02-21T18:03:48Z,tools/run_tests/performance/build_performance.sh,"@@ -72,6 +73,9 @@ do     # python workers are only run with python2.7 and building with multiple python versions is costly     python tools/run_tests/run_tests.py -l ""$language"" -c ""$CONFIG"" --compiler python2.7 --build_only -j 8",Will do. Added to my list.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21929,382868069,2020-02-22T00:51:12Z,tools/distrib/sanitize.sh,"@@ -20,7 +20,7 @@ cd $(dirname $0)/../.. DIFF_COMMAND=""git diff --name-only HEAD | grep -v ^third_party/""  if [ ""x$1"" == 'x--pre-commit' ]; then-  if eval $DIFF_COMMAND | grep '^build.yaml$'; then+  if eval $DIFF_COMMAND | grep '^build_handwritten.yaml$'; then","Good point but actually adding BUILD in repo root won't be enough as the bazel metatadata also comes from other BUILD files as well as some .bzl files. I think the best option is to nuke the `--pre-commit` flag, I doubt anyone actually uses it (and I found any other occurrences of it in our repo).",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22104,382870569,2020-02-22T01:09:21Z,src/python/grpcio/grpc/_cython/BUILD.bazel,"@@ -2,13 +2,21 @@ package(default_visibility = [""//visibility:public""])  load(""//bazel:cython_library.bzl"", ""pyx_library"") +genrule(+    name = ""copy_roots_pem"",+    srcs = [""//:etc/roots.pem""],+    outs = [""_credentials/roots.pem""],+    cmd = ""cp $(SRCS) $(@)"",+)+ pyx_library(     name = ""cygrpc"",     srcs = glob([         ""**/*.pxi"",         ""cygrpc.pxd"",         ""cygrpc.pyx"",     ]),+    data = ["":copy_roots_pem""],","nit: We have credentials in https://github.com/grpc/grpc/tree/master/src/python/grpcio_tests/tests/unit/credentials. We could reuse them. It might be complex, and it is okay to be complete in separate PR.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/22104,382871254,2020-02-22T01:15:06Z,src/python/grpcio_tests/tests/interop/xds_interop_client.py,"@@ -0,0 +1,201 @@+# Copyright 2020 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import signal+import threading+import time+import sys++from typing import DefaultDict, List, Set+import collections++from concurrent import futures++import grpc++from src.proto.grpc.testing import test_pb2+from src.proto.grpc.testing import test_pb2_grpc+from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import empty_pb2+++# TODO: Back with a LoadBalancerStatsResponse proto?+class _StatsWatcher:+    _start: int+    _end: int+    _rpcs_needed: int+    _rpcs_by_peer: DefaultDict[str, int]+    _no_remote_peer: int+    _lock: threading.Lock+    _condition: threading.Condition++    def __init__(self, start: int, end: int):+        self._start = start+        self._end = end+        self._rpcs_needed = end - start+        self._rpcs_by_peer = collections.defaultdict(int)+        self._lock = threading.Lock()+        self._condition = threading.Condition(self._lock)+        self._no_remote_peer = 0++    def on_rpc_complete(self, request_id: int, peer: str) -> None:+        """"""Records statistics for a single RPC.""""""+        if self._start <= request_id < self._end:+            with self._lock:+                if not peer:+                    self._no_remote_peer += 1+                else:+                    self._rpcs_by_peer[peer] += 1+                self._rpcs_needed -= 1+                self._condition.notify()++    def await_rpc_stats_response(self, timeout_sec: int+                                ) -> messages_pb2.LoadBalancerStatsResponse:+        """"""Blocks until a full response has been collected.""""""+        with self._lock:+            self._condition.wait_for(lambda: not self._rpcs_needed,+                                     timeout=float(timeout_sec))+            response = messages_pb2.LoadBalancerStatsResponse()+            for peer, count in self._rpcs_by_peer.items():+                response.rpcs_by_peer[peer] = count+            response.num_failures = self._no_remote_peer + self._rpcs_needed+        return response+++_global_lock = threading.Lock()+_stop_event = threading.Event()+_global_rpc_id: int = 0+_watchers: Set[_StatsWatcher] = set()+_global_server = None+++def _handle_sigint(sig, frame):+    _stop_event.set()+    _global_server.stop(None)+++class _LoadBalancerStatsServicer(test_pb2_grpc.LoadBalancerStatsServiceServicer+                                ):++    def __init__(self):+        super(_LoadBalancerStatsServicer).__init__()++    def GetClientStats(self, request: messages_pb2.LoadBalancerStatsRequest,+                       context: grpc.ServicerContext+                      ) -> messages_pb2.LoadBalancerStatsResponse:+        print(""Received stats request."")+        sys.stdout.flush()+        start = None+        end = None+        watcher = None+        with _global_lock:+            start = _global_rpc_id + 1+            end = start + request.num_rpcs+            watcher = _StatsWatcher(start, end)+            _watchers.add(watcher)+        response = watcher.await_rpc_stats_response(request.timeout_sec)+        with _global_lock:+            _watchers.remove(watcher)+        return response+++# TODO: Accept finer-grained arguments.+def _run_single_channel(args: argparse.Namespace):+    global _global_rpc_id  # pylint: disable=global-statement+    duration_per_query = 1.0 / float(args.qps)+    with grpc.insecure_channel(args.server) as channel:+        stub = test_pb2_grpc.TestServiceStub(channel)+        while not _stop_event.is_set():+            request_id = None+            with _global_lock:+                request_id = _global_rpc_id+                _global_rpc_id += 1+            print(f""Sending request to backend: {request_id}"")+            sys.stdout.flush()+            start = time.time()+            end = start + duration_per_query+            call, _ = stub.UnaryCall.with_call(messages_pb2.SimpleRequest(),+                                               timeout=float(+                                                   args.rpc_timeout_sec))+            print(f""Got result {request_id}"")+            sys.stdout.flush()+            with _global_lock:+                for watcher in _watchers:+                    # TODO: Implement a peer details getter.+                    peer = f""192.168.1.{request_id % 255}""","The peer for the `LoadBalancerStatsResponse.rpcs_by_peer` should actually come from the server's response (so `SimpleResponse.hostname`, as done [here](https://github.com/grpc/grpc/blob/5312a3aeb2c06e6d8c518365e4a701456bd653c3/test/cpp/interop/xds_interop_client.cc#L142)). Getting the peer from the context (ala `context.peer()` in the C++ client) is only used if `-print_response` is true, which is only used by an internal test that should soon be deprecated, so I don't think there's any need for Python to implement this flag or pull the peer from the context at all.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22104,382873028,2020-02-22T01:31:08Z,src/python/grpcio_tests/tests/interop/xds_interop_client.py,"@@ -0,0 +1,201 @@+# Copyright 2020 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import signal+import threading+import time+import sys++from typing import DefaultDict, List, Set+import collections++from concurrent import futures++import grpc++from src.proto.grpc.testing import test_pb2+from src.proto.grpc.testing import test_pb2_grpc+from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import empty_pb2+++# TODO: Back with a LoadBalancerStatsResponse proto?+class _StatsWatcher:+    _start: int+    _end: int+    _rpcs_needed: int+    _rpcs_by_peer: DefaultDict[str, int]+    _no_remote_peer: int+    _lock: threading.Lock+    _condition: threading.Condition",nit: `threading.Condition` creates a lock if got given. We could use that one `with self._condition:`.,
5928725,Akrog,https://api.github.com/repos/grpc/grpc/pulls/22062,383341962,2020-02-24T15:42:50Z,src/python/grpcio/grpc/_cython/_cygrpc/grpc_eventlet.pxd.pxi,"@@ -0,0 +1,40 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+# distutils: language=c++++cdef class ETimerWrapper:","OK, I'll change the E prefix of all classes to Eventlet.",
5928725,Akrog,https://api.github.com/repos/grpc/grpc/pulls/22062,383364039,2020-02-24T16:17:07Z,src/python/grpcio/grpc/_cython/_cygrpc/grpc_eventlet.pyx.pxi,"@@ -0,0 +1,691 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+# distutils: language=c+++#+#+#+# IMPLEMENTATION DETAILS:+# =======================+#+# Below code implements the eventlet custom IO manager code, while gevent's+# implementation was used as reference, many modifications were necessary,+# since these two libraries use greenthreads differently, and we had to+# workaround some of eventlet's quirks resulting in less readable code.+#+# These comments try to provide context to some of the implementation+# decisions.+#+# Event class+# -----------+#+# Eventlet's Event class is not implemented using any event notification+# mechanism (ie: epoll), but with greenthread switching and cannot be used+# between different native threads.  In our case it means that we cannot use+# them to kick a poll from a different native thread. That's why we have our+# own implementation.+#+# Greenthread pools+# -----------------+#+# Eventlet has a Hub for each native thread with a greenthread main loop that+# automatically handles the polling of readers and writers, timers, firing the+# different greenthreas, etc.  This means that even if we use a shared+# GreenPool between the main native thread (MT) and the ThreadPoolExecutor+# native thread (TPE), in the end the GreenPool will have greenthreads running+# on both hubs.  If we are not careful where we run some of our greenthreads we+# could en up in a deadlock, mostly with the kick and poll.+#+# The deadlock happends when a kick from the TPE schedules the switch to the+# poller on the TPE hub, because then the poller code that was started on the+# MT and is waiting for a kick continues on the TPE (different thread), so we+# end up with the TM having a gRPC lock and waiting for the GIL, and the TPE+# waiting on the gRPC lock and having the GIL.+#+# That is why we schedule some switch calls on the poller hub instead of the+# current hub on the kick.+#+# MAINLOOP+# --------+#+# We use a socket pair for the kick when signalling the poller because the+# eventlet's MAINLOOP code waits using a poll call and setting the timeout to+# the time when the next greenthread that needs to be run.  This means new+# greenthreads that are scheduled in the hub from another native thread will+# not be picked up until the poll call from the hub exits.  By having a+# greenthread waiting on a socket read operation we can wake the MAINLOOP from+# another thread so that the new calls that have been added to the hub are+# taken into account without delay.+#+# Socket close+# ------------+#+# Eventlet does not signal socket readers/writers when the socket is closed+# from another greenthread, (sometimes we seem to get an OSError, but not+# always), so we have to code the signaling ourselves by raising exceptions on+# the different greenthreads.  If we don't, we won't be able to stop the+# server, since the accept call never returns and the accept callback is never+# called.+#+# We also have to make sure that we only call the close callback after all the+# other readers/writers callbacks have been completed.+#+# Class methods+# -------------+#+# Methods e_timer_finished and e_signal_closed could be class methods instead,+# but since they are cdef methods, it seems that these behave as static methods+# when added to the hub, so we have to manually pass the ""self"" parameter on+# the timer/spawn call or it will complain it is missing the parameter at+# runtime.  Since that is the case, we just avoid having the extra attribute+# access by using the global methods.+#+# Global variables+# ----------------+#+# We don't set defaults for global variables at the beginning of our code, they+# are just created in different methods:+# - In init_grpc_eventlet:+#     * e_eventlet_lib+#     * e_socket_lib+#     * eventlet_spawn+#     * e_connection_backlog+#     * e_greenlet+#     * e_LocalTimer+#     * e_GlobalTimer+#     * SOCKET_CLOSED_EXC+# - In eventlet_init_poll:+#     * e_poller_waiters+#     * e_poller_wsock+#     * e_poller_hub+#+# The exception is e_poller_is_set, which is a non Python global variable.+#+# Notes+# -----+#+# Names of parameters in methods called by eventlet_spawn ARE IMPORTANT.  Since+# we are calling cdef code directly in eventlet_spawn, instead of using an+# intermediary python method, all parameters of the same class IN ALL these+# callback methods must have the same name, otherwise compiler will fail+# because it will generate multiple wrapers for the same class.  This includes+# Python classes such as object, tuple, etc.+#+# Blocking threads in user's code will affect the performance of the gRPC+# library, since we are spawning the async calls on greenthreads on the same+# hub.++cimport cpython+import errno+from libc cimport string+++cdef int e_poller_is_set = 0+++#############################+### socket implementation ###+#############################++cdef class ESocketWrapper:+    def __cinit__(self):+        fork_handlers_and_grpc_init()+        self.accepting_socket = NULL+        self.socket = None+        self.users = []+        self.c_socket = NULL+        self.c_buffer = NULL+        self.len = 0++    def __dealloc__(self):+        grpc_shutdown_blocking()++    cdef call_close_cb(ESocketWrapper self):+        # Close callback must be called after all other socket callbacks+        if not self.users:+            # This makes sure there's only 1 caller (in case we are waiting on+            # read and write or if we also get the OSError exception) and also+            # prevents races.+            cb, self.close_cb = self.close_cb, <grpc_custom_close_callback>0+            if cb:+                cb(self.c_socket)+                return True+++cdef void e_signal_closed(ESocketWrapper socket_wrapper, current):+    # If thread hasn't done its callback yet, then abort it+    if current in socket_wrapper.users:+        try:+            current.throw(SOCKET_CLOSED_EXC)+        # Throw returns raising GreenletExit (it's not an Exception instance)+        except:+            return++    # If the thread finished before we could send it the signal, then try to do+    # the close callback, and if we do the call, then kick the poll+    if socket_wrapper.call_close_cb():+        eventlet_kick_poll()+++cdef grpc_error* eventlet_socket_init(grpc_custom_socket* socket,+                                      int domain) with gil:+    # Python doesn't support AF_UNSPEC sockets, so we defer creation until+    # bind/connect when we know the type and can set the socket attribute+    sw = ESocketWrapper()+    sw.c_socket = socket+    socket.impl = <void*>sw+    cpython.Py_INCREF(sw)+    return <grpc_error*>0+++cdef tuple eventlet_get_socket_and_addr(const grpc_sockaddr *addr,+                                        size_t addr_len):+    if sockaddr_is_ipv4(addr, addr_len):+        py_socket = e_socket_lib.socket(e_socket_lib.AF_INET)+    else:+        py_socket = e_socket_lib.socket(e_socket_lib.AF_INET6)++    py_socket.setsockopt(e_socket_lib.SOL_SOCKET, e_socket_lib.SO_REUSEADDR, 1)+    py_socket.setsockopt(e_socket_lib.IPPROTO_TCP, e_socket_lib.TCP_NODELAY,+                         True)++    return py_socket, sockaddr_to_tuple(addr, addr_len)+++cdef eventlet_socket_connect_async(ESocketWrapper socket_wrapper,+                                   tuple addr_tuple):+    try:+        current = e_greenlet.getcurrent()+        socket_wrapper.users.append(current)+        socket_wrapper.socket.connect(addr_tuple)+        socket_wrapper.connect_cb(socket_wrapper.c_socket, <grpc_error*>0)+        socket_wrapper.users.remove(current)+    except (IOError, OSError) as exc:+        socket_wrapper.users.remove(current)+        socket_wrapper.connect_cb(socket_wrapper.c_socket,+                                  socket_error('connect', str(exc)))+        socket_wrapper.call_close_cb()+    eventlet_kick_poll()+++cdef void eventlet_socket_connect(grpc_custom_socket* socket,+                                  const grpc_sockaddr* addr, size_t addr_len,+                                  grpc_custom_connect_callback cb) with gil:+    socket_wrapper = <ESocketWrapper>socket.impl+    socket_wrapper.connect_cb = cb+    socket_wrapper.socket, addr_tuple = eventlet_get_socket_and_addr(addr,+                                                                     addr_len)+    eventlet_spawn(eventlet_socket_connect_async, socket_wrapper, addr_tuple)+++cdef void eventlet_socket_destroy(grpc_custom_socket* socket):+    cpython.Py_DECREF(<ESocketWrapper>socket.impl)+++cdef void eventlet_socket_shutdown(grpc_custom_socket* socket) with gil:+    try:+        (<ESocketWrapper>socket.impl).socket.shutdown(e_socket_lib.SHUT_RDWR)+    except IOError as io_error:+        if io_error.errno != errno.ENOTCONN:+            raise io_error+    except Exception:+        pass+++cdef void eventlet_socket_close(grpc_custom_socket* socket,+                                grpc_custom_close_callback cb) with gil:+    socket_wrapper = (<ESocketWrapper>socket.impl)+    # Is None after eventlet_socket_init and before bind/connect+    if socket_wrapper.socket is not None:+        socket_wrapper.socket.close()+        # Eventlet does not raise an exception to polling greenthreads, so they+        # may be left waiting forever. Raise it ourselves.+        if socket_wrapper.users:+            socket_wrapper.close_cb = cb+            for greenlet in socket_wrapper.users:+                eventlet_spawn(e_signal_closed, socket_wrapper, greenlet)+            return+    cb(socket)+    eventlet_kick_poll()+++cdef eventlet_socket_write_async(ESocketWrapper socket_wrapper, write_bytes):+    try:+        current = e_greenlet.getcurrent()+        socket_wrapper.users.append(current)+        socket_wrapper.socket.sendall(write_bytes)+        socket_wrapper.write_cb(socket_wrapper.c_socket, <grpc_error*>0)+        socket_wrapper.users.remove(current)+    except (IOError, OSError) as exc:+        socket_wrapper.users.remove(current)+        socket_wrapper.write_cb(socket_wrapper.c_socket,+                                socket_error('send', str(exc)))+        socket_wrapper.call_close_cb()+    eventlet_kick_poll()+++cdef void eventlet_socket_write(grpc_custom_socket* socket,+                                grpc_slice_buffer* buffer,+                                grpc_custom_write_callback cb) with gil:+    cdef char* start+    sw = <ESocketWrapper>socket.impl+    sw.write_cb = cb+    data = bytearray()+    for i in range(buffer.count):+        start = grpc_slice_buffer_start(buffer, i)+        length = grpc_slice_buffer_length(buffer, i)+        data.extend(<bytes>start[:length])+    eventlet_spawn(eventlet_socket_write_async, sw, data)+++cdef eventlet_socket_read_async(ESocketWrapper socket_wrapper):+    cdef char* buff_char_arr+    try:+        current = e_greenlet.getcurrent()+        socket_wrapper.users.append(current)+        buff_str = socket_wrapper.socket.recv(socket_wrapper.len)+        buff_char_arr = buff_str+        string.memcpy(<void*>socket_wrapper.c_buffer, buff_char_arr,+                      len(buff_str))+        socket_wrapper.read_cb(socket_wrapper.c_socket, len(buff_str),+                               <grpc_error*>0)+        socket_wrapper.users.remove(current)+    except (IOError, OSError) as exc:+        socket_wrapper.users.remove(current)+        socket_wrapper.read_cb(<grpc_custom_socket*>socket_wrapper.c_socket,+                               -1, socket_error('recv', str(exc)))+        socket_wrapper.call_close_cb()+    eventlet_kick_poll()+++cdef void eventlet_socket_read(grpc_custom_socket* socket, char* buffer,+                               size_t length,+                               grpc_custom_read_callback cb) with gil:+    sw = <ESocketWrapper>socket.impl+    sw.read_cb = cb+    sw.c_buffer = buffer+    sw.len = length+    eventlet_spawn(eventlet_socket_read_async, sw)+++cdef grpc_error* eventlet_socket_getpeername(grpc_custom_socket* socket,+                                             const grpc_sockaddr* addr,+                                             int* length) with gil:+    cdef grpc_resolved_address c_addr++    peer = (<ESocketWrapper>socket.impl).socket.getpeername()+    hostname = str_to_bytes(peer[0])+    grpc_string_to_sockaddr(&c_addr, hostname, peer[1])+    string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)+    length[0] = c_addr.len+    return <grpc_error*>0+++cdef grpc_error* eventlet_socket_getsockname(grpc_custom_socket* socket,+                                             const grpc_sockaddr* addr,+                                             int* length) with gil:+    cdef grpc_resolved_address c_addr+    if (<ESocketWrapper>socket.impl).socket is None:+        peer = ('0.0.0.0', 0)+    else:+        peer = (<ESocketWrapper>socket.impl).socket.getsockname()+    hostname = str_to_bytes(peer[0])+    grpc_string_to_sockaddr(&c_addr, hostname, peer[1])+    string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)+    length[0] = c_addr.len+    return <grpc_error*>0+++cdef grpc_error* eventlet_socket_bind(grpc_custom_socket* socket,+                                      const grpc_sockaddr* addr, size_t len,+                                      int flags) with gil:+    py_socket, addr_tuple = eventlet_get_socket_and_addr(addr, len)+    try:+        py_socket.bind(addr_tuple)+    except Exception as exc:+        return socket_error('bind', str(exc))+    (<ESocketWrapper>socket.impl).socket = py_socket+    return <grpc_error*>0+++cdef grpc_error* eventlet_socket_listen(grpc_custom_socket* socket) with gil:+    (<ESocketWrapper>socket.impl).socket.listen(e_connection_backlog)+    return <grpc_error*>0+++cdef void eventlet_socket_accept_async(ESocketWrapper socket_wrapper):+    try:+        current = e_greenlet.getcurrent()+        socket_wrapper.users.append(current)+        conn, address = socket_wrapper.socket.accept()+        sw = ESocketWrapper()+        sw.c_socket = socket_wrapper.accepting_socket+        sw.socket = conn+        sw.c_socket.impl = <void*>sw+        cpython.Py_INCREF(sw)+        socket_wrapper.accept_cb(socket_wrapper.c_socket, sw.c_socket,+                                 <grpc_error*>0)+        socket_wrapper.users.remove(current)+    except (IOError, OSError) as exc:+        socket_wrapper.users.remove(current)+        socket_wrapper.accept_cb(<grpc_custom_socket*>socket_wrapper.c_socket,+                                 NULL, socket_error('accept', str(exc)))+        socket_wrapper.call_close_cb()+    eventlet_kick_poll()+++cdef void eventlet_socket_accept(grpc_custom_socket* socket,+                                 grpc_custom_socket* client,+                                 grpc_custom_accept_callback cb) with gil:+    sw = <ESocketWrapper>socket.impl+    sw.accepting_socket = client+    sw.accept_cb = cb+    eventlet_spawn(eventlet_socket_accept_async, sw)+++###############################+### resolver implementation ###+###############################++cdef class EResolveWrapper:+    def __cinit__(self):+        fork_handlers_and_grpc_init()+        self.c_resolver = NULL+        self.c_host = NULL+        self.c_port = NULL++    def __dealloc__(self):+        grpc_shutdown_blocking()+++cdef eventlet_socket_resolve_async_callback(EResolveWrapper resolve_wrapper):+    try:+        # Eventlet doesn't handle bytes, so we may need conversion. Issue+        # https://github.com/eventlet/eventlet/issues/599+        res = e_socket_lib.getaddrinfo(_decode(resolve_wrapper.c_host),+                                       resolve_wrapper.c_port)+        grpc_custom_resolve_callback(resolve_wrapper.c_resolver,+                                     tuples_to_resolvaddr(res),+                                     <grpc_error*>0)+    except Exception as exc:+        grpc_custom_resolve_callback(resolve_wrapper.c_resolver,+                                     <grpc_resolved_addresses*>0,+                                     socket_error('getaddrinfo', str(exc)))+    eventlet_kick_poll()+++cdef void eventlet_socket_resolve_async(grpc_custom_resolver* r, char* host,+                                        char* port) with gil:+    rw = EResolveWrapper()+    rw.c_resolver = r+    rw.c_host, rw.c_port = host, port+    eventlet_spawn(eventlet_socket_resolve_async_callback, rw)+++cdef grpc_error* eventlet_socket_resolve(char* host, char* port,+                                         grpc_resolved_addresses** res+                                         ) with gil:+    try:+        # Eventlet doesn't handle bytes, so we may need conversion. Issue+        # https://github.com/eventlet/eventlet/issues/599+        result = e_socket_lib.getaddrinfo(_decode(host), port)+        res[0] = tuples_to_resolvaddr(result)+        return <grpc_error*>0+    except Exception as exc:+        return socket_error('getaddrinfo', str(exc))+++############################+### timer implementation ###+############################++cdef e_timer_finished(ETimerWrapper time_wrapper):+    # This method cannot be an instance method because the gRPC custom+    # timer callback will call `eventlet_timer_stop`, where we decref the+    # wrapper and free the instance.+    grpc_custom_timer_callback(time_wrapper.c_timer, <grpc_error*>0)+    eventlet_kick_poll()+++cdef class ETimerWrapper:+    def __cinit__(self, timeout_ms):+        fork_handlers_and_grpc_init()+        self.c_timer = NULL+        self.timer = e_GlobalTimer(timeout_ms / 1000.0, e_timer_finished, self)++    def stop(self):+        self.timer.cancel()++    def start(self):+        self.timer.schedule()++    def __dealloc__(self):+        grpc_shutdown_blocking()+++cdef void eventlet_timer_start(grpc_custom_timer* t) with gil:+    wrapper = ETimerWrapper(t.timeout_ms)+    t.timer = <void *>wrapper+    cpython.Py_INCREF(wrapper)+    wrapper.c_timer = t+    wrapper.start()+++cdef void eventlet_timer_stop(grpc_custom_timer* t) with gil:+    wrapper = <object>t.timer+    wrapper.stop()+    cpython.Py_DECREF(wrapper)+++##############################+### pollset implementation ###+##############################++def e_sock_waiter(rsock):+    """"""Wait on a socket so we can wake the MAINLOOP from other threads.""""""+    do_run = 1+    while do_run:+        do_run = int(rsock.recv(1))+    # We'll only get here after eventlet_destroy_poll has been called+    rsock.close()+++cdef void eventlet_init_poll() with gil:+    """"""Initialize poller's global variables.++    Initialized global variables are e_poller_waiters, e_poller_is_set,+    e_poller_wsock, and e_poller_hub+    """"""+    global e_poller_waiters, e_poller_is_set, e_poller_wsock, e_poller_hub++    e_poller_waiters = []+    e_poller_is_set = 0++    # Connected socket pair used to wake up poller greenthreads+    socket = e_eventlet_lib.patcher.original('socket')+    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)+    sock.bind(('127.0.0.1', 0))+    sock.listen(1)+    csock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)+    csock.connect(sock.getsockname())+    csock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, True)+    e_poller_wsock, _addr = sock.accept()+    e_poller_wsock.settimeout(None)+    e_poller_wsock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, True)+    sock.close()+    rsock = e_eventlet_lib.greenio.GreenSocket(csock)+    rsock.settimeout(None)++    e_poller_hub = e_eventlet_lib.hubs.get_hub()++    # Start the greenthread in charge of waking up the MAINLOOP of the poller+    # thread.+    g = e_greenlet(e_sock_waiter, parent=e_poller_hub.greenlet)+    e_poller_hub.schedule_call_global(0, g.switch, rsock)+++cdef void eventlet_destroy_poll() with gil:+    global e_poller_waiters, e_poller_wsock, e_poller_hub++    e_poller_wsock.sendall(b'0')+    # Socket will get closed when freed+    e_poller_wsock = e_poller_hub = e_poller_waiters = None+++cdef void eventlet_kick_poll() with gil:+    """"""Signal that a callback has been completed.++    This will release all polling greenthreads, if there are any, or just flag+    the next thread that polls.++    All asynchronous calls must do a kick before finishing, and after the+    callback, to signal the gRPC that they have completed.+    """"""+    global e_poller_is_set++    if not e_poller_is_set:+        e_poller_is_set = 1+        # Avoid duplicate waiter.switch calls on concurrent kicks using+        # try...except and pop instead of a while loop.+        try:+            while 1:+                waiter = e_poller_waiters.pop()+                # Cancel the timer and add a call to return `True` to each of+                # the pollers that are waiting on `e_poller_hub.switch()` in+                # the eventlet_run_poll method.+                waiter.grpc_poll_timer.cancel()+                e_poller_hub.add_timer(e_GlobalTimer(0, waiter.switch, True))+        except IndexError:+            pass+        # If we are on a different hub/native thread, then the poller's hub may+        # be waiting on a poll call with a timeout (default timeout is 60+        # seconds), and won't notice the new calls that need to be scheduled+        # until the poll timeouts.  To avoid this we send data using the socket+        # and force the poll to exit so the MAINLOOP will notice the new calls+        # it has to handle.+        if e_poller_hub is not e_eventlet_lib.hubs.get_hub():+            e_poller_wsock.sendall(b'1')+++def e_poll_timeout(waiter):+    """"""Handle the timeout of a poller greenthread.""""""+    try:+        # If the waiter is no longer there, we had an unlikely race with the+        # kick, and we don't need to signal the timeout.+        e_poller_waiters.remove(waiter)+        waiter.grpc_poll_timer.cancel()+        # This will switch execution to the `e_poller_hub.switch` line in the+        # eventlet_run_poll returning False to express we timed out.+        waiter.switch(False)+    except ValueError:+        pass+++cdef void eventlet_run_poll(size_t timeout_ms) with gil:+    """"""Wait for completion of any of the callbacks.""""""+    global e_poller_is_set++    # NOTE: When timeouts are 0 they are usually coming from the MAINLOOP, so+    # we cannot yield, and when the poller flag is set there's no need to wait.+    # We could check that `e_greenlet.getcurrent()` is not+    # `e_eventlet_lib.hubs.get_hub().greenlet` to know we are not in the+    # MAINLOOP, but that check is slower that checking `timeout_ms` is not 0.+    if timeout_ms and not e_poller_is_set:+        current = e_greenlet.getcurrent()+        timer = e_LocalTimer(timeout_ms / 1000.0, e_poll_timeout, current)++        # Store the timer so we can cancel it on timeout and signal+        current.grpc_poll_timer = timer++        # Add to list of polling greenthreads that kick uses to wake them up.+        e_poller_waiters.append(current)+        e_poller_hub.add_timer(timer)++        # Now switch to the MAINLOOP.  We'll return to this greenthread once+        # the timeout switches back with False or signal with True.  Skip+        # resetting the poller flag if we didn't receive the signal.+        if not e_poller_hub.switch():+            return++    e_poller_is_set = 0+++###################+### Initializer ###+###################++cdef grpc_socket_vtable eventlet_socket_vtable+cdef grpc_custom_resolver_vtable eventlet_resolver_vtable+cdef grpc_custom_timer_vtable eventlet_timer_vtable+cdef grpc_custom_poller_vtable eventlet_pollset_vtable+++def eventlet_async_callback_func(cb, args):+    # Method used by the gRPC code to run async callback functions+    eventlet_spawn(cb, *args)+++def init_grpc_eventlet(connection_backlog=50):+    """"""Initialize Eventlet's custom IO manager.++    Here we initialize the following global variables: e_eventlet_lib,+    e_socket_lib, eventlet_spawn, e_connection_backlog, e_greenlet,+    e_LocalTimer, e_GlobalTimer, and SOCKET_CLOSED_EXC+    """"""+    global e_eventlet_lib, e_socket_lib, eventlet_spawn, e_connection_backlog+    global e_greenlet, e_LocalTimer, e_GlobalTimer, SOCKET_CLOSED_EXC++    # Lazily import libraries+    import eventlet+    from eventlet.hubs import timer+    from eventlet.support import greenlets++    e_eventlet_lib = eventlet+    SOCKET_CLOSED_EXC = IOError('Socket closed')+    e_socket_lib = eventlet.green.socket+    e_LocalTimer = timer.LocalTimer+    e_GlobalTimer = timer.Timer",The difference between local and global timers is that a local timer will not fire if the parent greenthread is no longer running.  To make things simpler I'll just add a `try...except` clause and cancel the timeout instead of relying on Eventlet's behavior of local timers.,
5928725,Akrog,https://api.github.com/repos/grpc/grpc/pulls/22062,383370648,2020-02-24T16:27:18Z,src/python/grpcio/grpc/_cython/_cygrpc/grpc_eventlet.pyx.pxi,"@@ -0,0 +1,691 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+# distutils: language=c+++#+#+#+# IMPLEMENTATION DETAILS:+# =======================+#+# Below code implements the eventlet custom IO manager code, while gevent's+# implementation was used as reference, many modifications were necessary,+# since these two libraries use greenthreads differently, and we had to+# workaround some of eventlet's quirks resulting in less readable code.+#+# These comments try to provide context to some of the implementation+# decisions.+#+# Event class+# -----------+#+# Eventlet's Event class is not implemented using any event notification+# mechanism (ie: epoll), but with greenthread switching and cannot be used+# between different native threads.  In our case it means that we cannot use+# them to kick a poll from a different native thread. That's why we have our+# own implementation.+#+# Greenthread pools+# -----------------+#+# Eventlet has a Hub for each native thread with a greenthread main loop that+# automatically handles the polling of readers and writers, timers, firing the+# different greenthreas, etc.  This means that even if we use a shared+# GreenPool between the main native thread (MT) and the ThreadPoolExecutor+# native thread (TPE), in the end the GreenPool will have greenthreads running+# on both hubs.  If we are not careful where we run some of our greenthreads we+# could en up in a deadlock, mostly with the kick and poll.+#+# The deadlock happends when a kick from the TPE schedules the switch to the+# poller on the TPE hub, because then the poller code that was started on the+# MT and is waiting for a kick continues on the TPE (different thread), so we+# end up with the TM having a gRPC lock and waiting for the GIL, and the TPE+# waiting on the gRPC lock and having the GIL.+#+# That is why we schedule some switch calls on the poller hub instead of the+# current hub on the kick.+#+# MAINLOOP+# --------+#+# We use a socket pair for the kick when signalling the poller because the+# eventlet's MAINLOOP code waits using a poll call and setting the timeout to+# the time when the next greenthread that needs to be run.  This means new+# greenthreads that are scheduled in the hub from another native thread will+# not be picked up until the poll call from the hub exits.  By having a+# greenthread waiting on a socket read operation we can wake the MAINLOOP from+# another thread so that the new calls that have been added to the hub are+# taken into account without delay.+#+# Socket close+# ------------+#+# Eventlet does not signal socket readers/writers when the socket is closed+# from another greenthread, (sometimes we seem to get an OSError, but not+# always), so we have to code the signaling ourselves by raising exceptions on+# the different greenthreads.  If we don't, we won't be able to stop the+# server, since the accept call never returns and the accept callback is never+# called.+#+# We also have to make sure that we only call the close callback after all the+# other readers/writers callbacks have been completed.+#+# Class methods+# -------------+#+# Methods e_timer_finished and e_signal_closed could be class methods instead,+# but since they are cdef methods, it seems that these behave as static methods+# when added to the hub, so we have to manually pass the ""self"" parameter on+# the timer/spawn call or it will complain it is missing the parameter at+# runtime.  Since that is the case, we just avoid having the extra attribute+# access by using the global methods.+#+# Global variables+# ----------------+#+# We don't set defaults for global variables at the beginning of our code, they+# are just created in different methods:+# - In init_grpc_eventlet:+#     * e_eventlet_lib+#     * e_socket_lib+#     * eventlet_spawn+#     * e_connection_backlog+#     * e_greenlet+#     * e_LocalTimer+#     * e_GlobalTimer+#     * SOCKET_CLOSED_EXC+# - In eventlet_init_poll:+#     * e_poller_waiters+#     * e_poller_wsock+#     * e_poller_hub+#+# The exception is e_poller_is_set, which is a non Python global variable.+#+# Notes+# -----+#+# Names of parameters in methods called by eventlet_spawn ARE IMPORTANT.  Since+# we are calling cdef code directly in eventlet_spawn, instead of using an+# intermediary python method, all parameters of the same class IN ALL these+# callback methods must have the same name, otherwise compiler will fail+# because it will generate multiple wrapers for the same class.  This includes+# Python classes such as object, tuple, etc.+#+# Blocking threads in user's code will affect the performance of the gRPC+# library, since we are spawning the async calls on greenthreads on the same+# hub.++cimport cpython+import errno+from libc cimport string+++cdef int e_poller_is_set = 0+++#############################+### socket implementation ###+#############################++cdef class ESocketWrapper:+    def __cinit__(self):+        fork_handlers_and_grpc_init()+        self.accepting_socket = NULL+        self.socket = None+        self.users = []+        self.c_socket = NULL+        self.c_buffer = NULL+        self.len = 0++    def __dealloc__(self):+        grpc_shutdown_blocking()++    cdef call_close_cb(ESocketWrapper self):+        # Close callback must be called after all other socket callbacks+        if not self.users:+            # This makes sure there's only 1 caller (in case we are waiting on+            # read and write or if we also get the OSError exception) and also+            # prevents races.+            cb, self.close_cb = self.close_cb, <grpc_custom_close_callback>0+            if cb:+                cb(self.c_socket)+                return True+++cdef void e_signal_closed(ESocketWrapper socket_wrapper, current):+    # If thread hasn't done its callback yet, then abort it+    if current in socket_wrapper.users:+        try:+            current.throw(SOCKET_CLOSED_EXC)+        # Throw returns raising GreenletExit (it's not an Exception instance)+        except:+            return++    # If the thread finished before we could send it the signal, then try to do+    # the close callback, and if we do the call, then kick the poll+    if socket_wrapper.call_close_cb():+        eventlet_kick_poll()+++cdef grpc_error* eventlet_socket_init(grpc_custom_socket* socket,+                                      int domain) with gil:+    # Python doesn't support AF_UNSPEC sockets, so we defer creation until+    # bind/connect when we know the type and can set the socket attribute+    sw = ESocketWrapper()+    sw.c_socket = socket+    socket.impl = <void*>sw+    cpython.Py_INCREF(sw)+    return <grpc_error*>0+++cdef tuple eventlet_get_socket_and_addr(const grpc_sockaddr *addr,+                                        size_t addr_len):+    if sockaddr_is_ipv4(addr, addr_len):+        py_socket = e_socket_lib.socket(e_socket_lib.AF_INET)+    else:+        py_socket = e_socket_lib.socket(e_socket_lib.AF_INET6)++    py_socket.setsockopt(e_socket_lib.SOL_SOCKET, e_socket_lib.SO_REUSEADDR, 1)+    py_socket.setsockopt(e_socket_lib.IPPROTO_TCP, e_socket_lib.TCP_NODELAY,+                         True)++    return py_socket, sockaddr_to_tuple(addr, addr_len)+++cdef eventlet_socket_connect_async(ESocketWrapper socket_wrapper,+                                   tuple addr_tuple):+    try:+        current = e_greenlet.getcurrent()+        socket_wrapper.users.append(current)+        socket_wrapper.socket.connect(addr_tuple)+        socket_wrapper.connect_cb(socket_wrapper.c_socket, <grpc_error*>0)+        socket_wrapper.users.remove(current)+    except (IOError, OSError) as exc:+        socket_wrapper.users.remove(current)+        socket_wrapper.connect_cb(socket_wrapper.c_socket,+                                  socket_error('connect', str(exc)))+        socket_wrapper.call_close_cb()+    eventlet_kick_poll()+++cdef void eventlet_socket_connect(grpc_custom_socket* socket,+                                  const grpc_sockaddr* addr, size_t addr_len,+                                  grpc_custom_connect_callback cb) with gil:+    socket_wrapper = <ESocketWrapper>socket.impl+    socket_wrapper.connect_cb = cb+    socket_wrapper.socket, addr_tuple = eventlet_get_socket_and_addr(addr,+                                                                     addr_len)+    eventlet_spawn(eventlet_socket_connect_async, socket_wrapper, addr_tuple)+++cdef void eventlet_socket_destroy(grpc_custom_socket* socket):+    cpython.Py_DECREF(<ESocketWrapper>socket.impl)+++cdef void eventlet_socket_shutdown(grpc_custom_socket* socket) with gil:+    try:+        (<ESocketWrapper>socket.impl).socket.shutdown(e_socket_lib.SHUT_RDWR)+    except IOError as io_error:+        if io_error.errno != errno.ENOTCONN:+            raise io_error+    except Exception:+        pass+++cdef void eventlet_socket_close(grpc_custom_socket* socket,+                                grpc_custom_close_callback cb) with gil:+    socket_wrapper = (<ESocketWrapper>socket.impl)+    # Is None after eventlet_socket_init and before bind/connect+    if socket_wrapper.socket is not None:+        socket_wrapper.socket.close()+        # Eventlet does not raise an exception to polling greenthreads, so they+        # may be left waiting forever. Raise it ourselves.+        if socket_wrapper.users:+            socket_wrapper.close_cb = cb+            for greenlet in socket_wrapper.users:+                eventlet_spawn(e_signal_closed, socket_wrapper, greenlet)+            return+    cb(socket)+    eventlet_kick_poll()+++cdef eventlet_socket_write_async(ESocketWrapper socket_wrapper, write_bytes):+    try:+        current = e_greenlet.getcurrent()+        socket_wrapper.users.append(current)+        socket_wrapper.socket.sendall(write_bytes)+        socket_wrapper.write_cb(socket_wrapper.c_socket, <grpc_error*>0)+        socket_wrapper.users.remove(current)+    except (IOError, OSError) as exc:+        socket_wrapper.users.remove(current)+        socket_wrapper.write_cb(socket_wrapper.c_socket,+                                socket_error('send', str(exc)))+        socket_wrapper.call_close_cb()+    eventlet_kick_poll()+++cdef void eventlet_socket_write(grpc_custom_socket* socket,+                                grpc_slice_buffer* buffer,+                                grpc_custom_write_callback cb) with gil:+    cdef char* start+    sw = <ESocketWrapper>socket.impl+    sw.write_cb = cb+    data = bytearray()+    for i in range(buffer.count):+        start = grpc_slice_buffer_start(buffer, i)+        length = grpc_slice_buffer_length(buffer, i)+        data.extend(<bytes>start[:length])+    eventlet_spawn(eventlet_socket_write_async, sw, data)+++cdef eventlet_socket_read_async(ESocketWrapper socket_wrapper):+    cdef char* buff_char_arr+    try:+        current = e_greenlet.getcurrent()+        socket_wrapper.users.append(current)+        buff_str = socket_wrapper.socket.recv(socket_wrapper.len)+        buff_char_arr = buff_str+        string.memcpy(<void*>socket_wrapper.c_buffer, buff_char_arr,+                      len(buff_str))+        socket_wrapper.read_cb(socket_wrapper.c_socket, len(buff_str),+                               <grpc_error*>0)+        socket_wrapper.users.remove(current)+    except (IOError, OSError) as exc:+        socket_wrapper.users.remove(current)+        socket_wrapper.read_cb(<grpc_custom_socket*>socket_wrapper.c_socket,+                               -1, socket_error('recv', str(exc)))+        socket_wrapper.call_close_cb()+    eventlet_kick_poll()+++cdef void eventlet_socket_read(grpc_custom_socket* socket, char* buffer,+                               size_t length,+                               grpc_custom_read_callback cb) with gil:+    sw = <ESocketWrapper>socket.impl+    sw.read_cb = cb+    sw.c_buffer = buffer+    sw.len = length+    eventlet_spawn(eventlet_socket_read_async, sw)+++cdef grpc_error* eventlet_socket_getpeername(grpc_custom_socket* socket,+                                             const grpc_sockaddr* addr,+                                             int* length) with gil:+    cdef grpc_resolved_address c_addr++    peer = (<ESocketWrapper>socket.impl).socket.getpeername()+    hostname = str_to_bytes(peer[0])+    grpc_string_to_sockaddr(&c_addr, hostname, peer[1])+    string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)+    length[0] = c_addr.len+    return <grpc_error*>0+++cdef grpc_error* eventlet_socket_getsockname(grpc_custom_socket* socket,+                                             const grpc_sockaddr* addr,+                                             int* length) with gil:+    cdef grpc_resolved_address c_addr+    if (<ESocketWrapper>socket.impl).socket is None:+        peer = ('0.0.0.0', 0)+    else:+        peer = (<ESocketWrapper>socket.impl).socket.getsockname()+    hostname = str_to_bytes(peer[0])+    grpc_string_to_sockaddr(&c_addr, hostname, peer[1])+    string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)+    length[0] = c_addr.len+    return <grpc_error*>0+++cdef grpc_error* eventlet_socket_bind(grpc_custom_socket* socket,+                                      const grpc_sockaddr* addr, size_t len,+                                      int flags) with gil:+    py_socket, addr_tuple = eventlet_get_socket_and_addr(addr, len)+    try:+        py_socket.bind(addr_tuple)+    except Exception as exc:+        return socket_error('bind', str(exc))+    (<ESocketWrapper>socket.impl).socket = py_socket+    return <grpc_error*>0+++cdef grpc_error* eventlet_socket_listen(grpc_custom_socket* socket) with gil:+    (<ESocketWrapper>socket.impl).socket.listen(e_connection_backlog)+    return <grpc_error*>0+++cdef void eventlet_socket_accept_async(ESocketWrapper socket_wrapper):+    try:+        current = e_greenlet.getcurrent()+        socket_wrapper.users.append(current)+        conn, address = socket_wrapper.socket.accept()+        sw = ESocketWrapper()+        sw.c_socket = socket_wrapper.accepting_socket+        sw.socket = conn+        sw.c_socket.impl = <void*>sw+        cpython.Py_INCREF(sw)+        socket_wrapper.accept_cb(socket_wrapper.c_socket, sw.c_socket,+                                 <grpc_error*>0)+        socket_wrapper.users.remove(current)+    except (IOError, OSError) as exc:+        socket_wrapper.users.remove(current)+        socket_wrapper.accept_cb(<grpc_custom_socket*>socket_wrapper.c_socket,+                                 NULL, socket_error('accept', str(exc)))+        socket_wrapper.call_close_cb()+    eventlet_kick_poll()+++cdef void eventlet_socket_accept(grpc_custom_socket* socket,+                                 grpc_custom_socket* client,+                                 grpc_custom_accept_callback cb) with gil:+    sw = <ESocketWrapper>socket.impl+    sw.accepting_socket = client+    sw.accept_cb = cb+    eventlet_spawn(eventlet_socket_accept_async, sw)+++###############################+### resolver implementation ###+###############################++cdef class EResolveWrapper:+    def __cinit__(self):+        fork_handlers_and_grpc_init()+        self.c_resolver = NULL+        self.c_host = NULL+        self.c_port = NULL++    def __dealloc__(self):+        grpc_shutdown_blocking()+++cdef eventlet_socket_resolve_async_callback(EResolveWrapper resolve_wrapper):+    try:+        # Eventlet doesn't handle bytes, so we may need conversion. Issue+        # https://github.com/eventlet/eventlet/issues/599+        res = e_socket_lib.getaddrinfo(_decode(resolve_wrapper.c_host),+                                       resolve_wrapper.c_port)+        grpc_custom_resolve_callback(resolve_wrapper.c_resolver,+                                     tuples_to_resolvaddr(res),+                                     <grpc_error*>0)+    except Exception as exc:+        grpc_custom_resolve_callback(resolve_wrapper.c_resolver,+                                     <grpc_resolved_addresses*>0,+                                     socket_error('getaddrinfo', str(exc)))+    eventlet_kick_poll()+++cdef void eventlet_socket_resolve_async(grpc_custom_resolver* r, char* host,+                                        char* port) with gil:+    rw = EResolveWrapper()+    rw.c_resolver = r+    rw.c_host, rw.c_port = host, port+    eventlet_spawn(eventlet_socket_resolve_async_callback, rw)+++cdef grpc_error* eventlet_socket_resolve(char* host, char* port,+                                         grpc_resolved_addresses** res+                                         ) with gil:+    try:+        # Eventlet doesn't handle bytes, so we may need conversion. Issue+        # https://github.com/eventlet/eventlet/issues/599+        result = e_socket_lib.getaddrinfo(_decode(host), port)+        res[0] = tuples_to_resolvaddr(result)+        return <grpc_error*>0+    except Exception as exc:+        return socket_error('getaddrinfo', str(exc))+++############################+### timer implementation ###+############################++cdef e_timer_finished(ETimerWrapper time_wrapper):+    # This method cannot be an instance method because the gRPC custom+    # timer callback will call `eventlet_timer_stop`, where we decref the+    # wrapper and free the instance.+    grpc_custom_timer_callback(time_wrapper.c_timer, <grpc_error*>0)+    eventlet_kick_poll()+++cdef class ETimerWrapper:+    def __cinit__(self, timeout_ms):+        fork_handlers_and_grpc_init()+        self.c_timer = NULL+        self.timer = e_GlobalTimer(timeout_ms / 1000.0, e_timer_finished, self)++    def stop(self):+        self.timer.cancel()++    def start(self):+        self.timer.schedule()++    def __dealloc__(self):+        grpc_shutdown_blocking()+++cdef void eventlet_timer_start(grpc_custom_timer* t) with gil:+    wrapper = ETimerWrapper(t.timeout_ms)+    t.timer = <void *>wrapper+    cpython.Py_INCREF(wrapper)+    wrapper.c_timer = t+    wrapper.start()+++cdef void eventlet_timer_stop(grpc_custom_timer* t) with gil:+    wrapper = <object>t.timer+    wrapper.stop()+    cpython.Py_DECREF(wrapper)+++##############################+### pollset implementation ###+##############################++def e_sock_waiter(rsock):+    """"""Wait on a socket so we can wake the MAINLOOP from other threads.""""""+    do_run = 1+    while do_run:+        do_run = int(rsock.recv(1))+    # We'll only get here after eventlet_destroy_poll has been called+    rsock.close()+++cdef void eventlet_init_poll() with gil:+    """"""Initialize poller's global variables.++    Initialized global variables are e_poller_waiters, e_poller_is_set,+    e_poller_wsock, and e_poller_hub+    """"""+    global e_poller_waiters, e_poller_is_set, e_poller_wsock, e_poller_hub++    e_poller_waiters = []+    e_poller_is_set = 0++    # Connected socket pair used to wake up poller greenthreads+    socket = e_eventlet_lib.patcher.original('socket')+    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)+    sock.bind(('127.0.0.1', 0))+    sock.listen(1)+    csock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)+    csock.connect(sock.getsockname())+    csock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, True)+    e_poller_wsock, _addr = sock.accept()+    e_poller_wsock.settimeout(None)+    e_poller_wsock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, True)","You are reading it right.Afaik the only mechanism that eventlet has to wake up a MAINLOOP from another native thread that is waiting is by making the poll select exit due to an I/O operation.  That's why on the kick we check to see if we are actually running on a different native thread and we need to use the socket I/O mechanism to wake it:```        if e_poller_hub is not e_eventlet_lib.hubs.get_hub():            e_poller_wsock.sendall(b'1')```I looked into the Eventlet implementation of the MAINLOOP as well as their polling implementation, and I couldn't find any alternative, so I did it using sockets, which is also how Eventlet does the [signalling in the tpool code](https://github.com/eventlet/eventlet/blob/ac7917c7241bc29696cf65dd7b9757df9c9bc955/eventlet/tpool.py#L96).",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22104,383444714,2020-02-24T18:44:25Z,src/python/grpcio_tests/tests/interop/xds_interop_client.py,"@@ -0,0 +1,201 @@+# Copyright 2020 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import signal+import threading+import time+import sys++from typing import DefaultDict, List, Set+import collections++from concurrent import futures++import grpc++from src.proto.grpc.testing import test_pb2+from src.proto.grpc.testing import test_pb2_grpc+from src.proto.grpc.testing import messages_pb2+from src.proto.grpc.testing import empty_pb2+++# TODO: Back with a LoadBalancerStatsResponse proto?+class _StatsWatcher:+    _start: int+    _end: int+    _rpcs_needed: int+    _rpcs_by_peer: DefaultDict[str, int]+    _no_remote_peer: int+    _lock: threading.Lock+    _condition: threading.Condition++    def __init__(self, start: int, end: int):+        self._start = start+        self._end = end+        self._rpcs_needed = end - start+        self._rpcs_by_peer = collections.defaultdict(int)+        self._lock = threading.Lock()+        self._condition = threading.Condition(self._lock)+        self._no_remote_peer = 0++    def on_rpc_complete(self, request_id: int, peer: str) -> None:+        """"""Records statistics for a single RPC.""""""+        if self._start <= request_id < self._end:+            with self._lock:+                if not peer:+                    self._no_remote_peer += 1+                else:+                    self._rpcs_by_peer[peer] += 1+                self._rpcs_needed -= 1+                self._condition.notify()++    def await_rpc_stats_response(self, timeout_sec: int+                                ) -> messages_pb2.LoadBalancerStatsResponse:+        """"""Blocks until a full response has been collected.""""""+        with self._lock:+            self._condition.wait_for(lambda: not self._rpcs_needed,+                                     timeout=float(timeout_sec))+            response = messages_pb2.LoadBalancerStatsResponse()+            for peer, count in self._rpcs_by_peer.items():+                response.rpcs_by_peer[peer] = count+            response.num_failures = self._no_remote_peer + self._rpcs_needed+        return response+++_global_lock = threading.Lock()+_stop_event = threading.Event()+_global_rpc_id: int = 0+_watchers: Set[_StatsWatcher] = set()+_global_server = None+++def _handle_sigint(sig, frame):+    _stop_event.set()+    _global_server.stop(None)+++class _LoadBalancerStatsServicer(test_pb2_grpc.LoadBalancerStatsServiceServicer+                                ):++    def __init__(self):+        super(_LoadBalancerStatsServicer).__init__()++    def GetClientStats(self, request: messages_pb2.LoadBalancerStatsRequest,+                       context: grpc.ServicerContext+                      ) -> messages_pb2.LoadBalancerStatsResponse:+        print(""Received stats request."")+        sys.stdout.flush()+        start = None+        end = None+        watcher = None+        with _global_lock:+            start = _global_rpc_id + 1+            end = start + request.num_rpcs+            watcher = _StatsWatcher(start, end)+            _watchers.add(watcher)+        response = watcher.await_rpc_stats_response(request.timeout_sec)+        with _global_lock:+            _watchers.remove(watcher)+        return response+++# TODO: Accept finer-grained arguments.+def _run_single_channel(args: argparse.Namespace):+    global _global_rpc_id  # pylint: disable=global-statement+    duration_per_query = 1.0 / float(args.qps)+    with grpc.insecure_channel(args.server) as channel:+        stub = test_pb2_grpc.TestServiceStub(channel)+        while not _stop_event.is_set():+            request_id = None+            with _global_lock:+                request_id = _global_rpc_id+                _global_rpc_id += 1+            print(f""Sending request to backend: {request_id}"")+            sys.stdout.flush()+            start = time.time()+            end = start + duration_per_query+            call, _ = stub.UnaryCall.with_call(messages_pb2.SimpleRequest(),+                                               timeout=float(+                                                   args.rpc_timeout_sec))+            print(f""Got result {request_id}"")+            sys.stdout.flush()+            with _global_lock:+                for watcher in _watchers:+                    # TODO: Implement a peer details getter.+                    peer = f""192.168.1.{request_id % 255}""+                    watcher.on_rpc_complete(request_id, peer)+            if args.print_response:+                if call.code() == grpc.StatusCode.OK:+                    print(""Successful response."")+                    sys.stdout.flush()+                else:+                    print(f""RPC failed: {call}"")+                    sys.stdout.flush()+            now = time.time()+            while now < end:+                time.sleep(end - now)","Having a bursty traffic pattern would differ from the other clients and require a more complicated implementation. Unless there's a compelling reason for it, I'd prefer not to.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22032,383738307,2020-02-25T08:59:59Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -214,15 +215,34 @@ cdef class _ServicerContext:         self._rpc_state.disable_next_compression = True  -cdef _find_method_handler(str method, tuple metadata, list generic_handlers):+async def _run_interceptor(object interceptors, object query_handler,+                           object handler_call_details):+    interceptor = next(interceptors, None)+    if interceptor:+        continuation = functools.partial(_run_interceptor, interceptors,+                                         query_handler)+        return await interceptor.intercept_service(continuation, handler_call_details)+    else:+        return query_handler(handler_call_details)+++async def _find_method_handler(str method, tuple metadata, list generic_handlers,+                          tuple interceptors):+    def query_handlers(handler_call_details):+        for generic_handler in generic_handlers:+            method_handler = generic_handler.service(handler_call_details)+            if method_handler is not None:+                return method_handler+        return None+     cdef _HandlerCallDetails handler_call_details = _HandlerCallDetails(method,                                                                         metadata)--    for generic_handler in generic_handlers:-        method_handler = generic_handler.service(handler_call_details)-        if method_handler is not None:-            return method_handler-    return None+    # interceptor+    if interceptors:+        return await _run_interceptor(iter(interceptors), query_handlers,","Should we return and not execute, the `_run_interceptor` coroutine? This would mean that the execution will eventually happen at the very last step and the interceptor for example in the case of a unary call will be here [1]Indeed we could name this method like `_wrap_call_interceptor` or something like that.Without doing this, future interceptors that would take care of observability for measuring the time that handler took won't work since they won't be able to measure the real execution of the handler.Also moving this to the right place, will eventually will help us for extending the interceptors for having visibility of the servicer context [2] at the interceptor level.The only friction point of doing this that I can see, if Im not missing something is the access to the metadata. So the interceptor won't be able to have direct access to the metadata and mutate them, but could have access to the service context that will make the metadata available for the interceptor.WDYT @lidizheng should we aim for moving the execution of the interceptor to the very last step?Also, I'm wondering if since we are not most likely commit to having this feature for the alpha release we could start thinking on how a nex iteration of the servers interceptors could be done for knowing the response code of the method handler. Would this change help us to do so? [1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi#L242[2] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi#L244",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/21984,384061238,2020-02-25T18:58:34Z,include/grpc/grpc_security.h,"@@ -425,6 +425,8 @@ typedef struct {       size_t* num_creds_md, grpc_status_code* status,       const char** error_details); +  char* (*debug_string)(void* state);","This one is complicated. Changing the shape of the grpc_metadata_credentials_plugin clearly breaks the core ABI (which is fine) but doesn't technically break the API so long as this function never gets used unless the user actually requests an invocation of DebugString. That said, the underlying code is not actually in charge of what credentials plugins get used so it can't be sure if the plugin actually supports the debug_string or not. Thus, I'm going to rule this an API breaker. As a result, you need to bump the core API version and mention this fact in the gRFC.... (unless you can convince me otherwise)We also need to see a comment about this function just like there are comments about get_metadata and destroy.",
10605667,chwarr,https://api.github.com/repos/grpc/grpc/pulls/21984,384115102,2020-02-25T20:43:27Z,src/core/lib/security/credentials/oauth2/oauth2_credentials.cc,"@@ -540,6 +554,14 @@ class StsTokenFetcherCredentials    ~StsTokenFetcherCredentials() override { grpc_uri_destroy(sts_url_); } +  std::string debug_string() const override {+    return absl::StrFormat(+        ""StsTokenFetcherCredentials{Path:%s,Authority:%s,%s}"",+        static_cast<char*>(sts_url_->path),",Are these casts needed? [`sts_url_`](https://github.com/grpc/grpc/blob/659cba3511983de869aebe74a07b6f150f87433b/src/core/lib/security/credentials/oauth2/oauth2_credentials.cc#L628) looks like it's an instance of [`grpc_uri`](https://github.com/grpc/grpc/blob/53657b5de385ffc54e33899b3f2a87ff78d2952b/src/core/lib/uri/uri_parser.h#L38). The `path` and `authority` fields are both `char*` there.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22032,384245655,2020-02-26T02:36:27Z,src/python/grpcio/grpc/experimental/aio/_server.py,"@@ -41,6 +42,13 @@ def __init__(self, thread_pool: Optional[Executor],                  maximum_concurrent_rpcs: Optional[int],                  compression: Optional[grpc.Compression]):         self._loop = asyncio.get_event_loop()+        if interceptors:+            invalid_interceptors = [interceptor for interceptor in interceptors+                                    if not isinstance(interceptor,+                                                      ServerInterceptor)]+            if invalid_interceptors:+                raise ValueError('Interceptor must be ServerInterceptor, the '+                                 f'following are invalid: {invalid_interceptors}')","In Python, yes; in Cython, no.If possible, I would like to use this Python 3 feature (simple is better than complex).Cython doesn't understand f-string if it is compiled with Python < 3.6. We have learnt this in through test failures.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22032,384250563,2020-02-26T02:57:33Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -214,15 +215,34 @@ cdef class _ServicerContext:         self._rpc_state.disable_next_compression = True  -cdef _find_method_handler(str method, tuple metadata, list generic_handlers):+async def _run_interceptor(object interceptors, object query_handler,+                           object handler_call_details):+    interceptor = next(interceptors, None)+    if interceptor:+        continuation = functools.partial(_run_interceptor, interceptors,+                                         query_handler)+        return await interceptor.intercept_service(continuation, handler_call_details)+    else:+        return query_handler(handler_call_details)+++async def _find_method_handler(str method, tuple metadata, list generic_handlers,+                          tuple interceptors):+    def query_handlers(handler_call_details):+        for generic_handler in generic_handlers:+            method_handler = generic_handler.service(handler_call_details)+            if method_handler is not None:+                return method_handler+        return None+     cdef _HandlerCallDetails handler_call_details = _HandlerCallDetails(method,                                                                         metadata)--    for generic_handler in generic_handlers:-        method_handler = generic_handler.service(handler_call_details)-        if method_handler is not None:-            return method_handler-    return None+    # interceptor+    if interceptors:+        return await _run_interceptor(iter(interceptors), query_handlers,","I agree that the server interceptors should be more powerful. In fact, it would be great, if it has similar functionality as the client-side.Back to your proposal, it might introduce a regression if users are selecting method handlers according to the invocation metadata (client-sent initial metadata). This feature is supported in the existing stack, even without interceptors (although I wonder if any user uses it).If we decided to optimize the design of generic handlers (basically let the Core route the requests), then users cannot use this feature anyway. In this case, yes, I think this proposal is valid.---> should we aim for moving the execution of the interceptor to the very last step? ... could be done for knowing the response code of the method handler.If we want a proper full-featured server interceptor, we can make interceptors wrapping entire method handler (similar to client-side). If we only executed it till the last step, users can only either inject logic before or after the RPC (or I might be wrong).If we only want an interceptor design better than the existing stack, I believe the move does improve server interceptors capability.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22032,384251257,2020-02-26T03:00:27Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -214,15 +215,34 @@ cdef class _ServicerContext:         self._rpc_state.disable_next_compression = True  -cdef _find_method_handler(str method, tuple metadata, list generic_handlers):+async def _run_interceptor(object interceptors, object query_handler,+                      object handler_call_details):+    interceptor = next(interceptors, None)+    if interceptor:+        continuation = functools.partial(_run_interceptor, interceptors,+                                         query_handler)+        return await interceptor.intercept_service(continuation, handler_call_details)+    else:+        return query_handler(handler_call_details)+++async def _find_method_handler(str method, tuple metadata, list generic_handlers,+                          tuple interceptors):+    def query_handlers(handler_call_details):+        for generic_handler in generic_handlers:+            method_handler = generic_handler.service(handler_call_details)+            if method_handler is not None:+                return method_handler+        return None+     cdef _HandlerCallDetails handler_call_details = _HandlerCallDetails(method,                                                                         metadata)--    for generic_handler in generic_handlers:-        method_handler = generic_handler.service(handler_call_details)-        if method_handler is not None:-            return method_handler-    return None+    # interceptor+    if interceptors:+        return await _run_interceptor(iter(interceptors), query_handlers,+                                      handler_call_details)+    else:+        return query_handlers(handler_call_details)","It depends on whether we allow users to change the method.If allowed, the method handler needs to be ran as last function in the chain, otherwise users' interception might not work.If not allowed, this is a valid optimization.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/22139,384288373,2020-02-26T05:59:11Z,tools/run_tests/run_xds_tests.py,"@@ -31,10 +32,20 @@ from src.proto.grpc.testing import messages_pb2 from src.proto.grpc.testing import test_pb2_grpc -logger = logging.getLogger(__name__)+logger = logging.getLogger() console_handler = logging.StreamHandler() logger.addHandler(console_handler) ++def parse_port_range(port_arg):","Was there a better way to do this? Didn't really want to introduce two new flags (so `--*_port` along with `--port_min` and `--port_max`, as then the user has to know which combination takes precedence)",
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/22076,384522521,2020-02-26T14:22:56Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/io_loop.pyx.pxi,"@@ -0,0 +1,83 @@+# Copyright 2019 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import os+import sys+import threading++_SECONDS_PER_MILLISECOND = 0.001+cdef _IOLoop _io_loop = None++cdef class _IOLoop:++    def __init__(self):+        global _io_loop++        # Only one instantiation is exepected        +        assert _io_loop is None","I'd add something similar to what the comment says to the assertion message, to make it more understanable for developers```suggestion        assert _io_loop is None, ""More than one loop found""```",
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/21984,384767570,2020-02-26T21:11:03Z,src/core/lib/security/credentials/iam/iam_credentials.cc,"@@ -30,6 +28,10 @@ #include <grpc/support/string_util.h> #include <grpc/support/sync.h> +#include ""absl/strings/str_format.h""+#include ""src/core/lib/gprpp/ref_counted_ptr.h""",The include order of this file is weird. But I think you somehow had a merge issue to have duplicate includes.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22082,385379522,2020-02-27T21:24:08Z,src/core/lib/security/security_connector/local/local_security_connector.cc,"@@ -70,39 +70,19 @@ void local_check_peer(tsi_peer peer, grpc_endpoint* ep,                       grpc_core::RefCountedPtr<grpc_auth_context>* auth_context,                       grpc_closure* on_peer_checked,                       grpc_local_connect_type type) {-  int fd = grpc_endpoint_get_fd(ep);-  grpc_resolved_address resolved_addr;-  memset(&resolved_addr, 0, sizeof(resolved_addr));-  resolved_addr.len = GRPC_MAX_SOCKADDR_SIZE;   bool is_endpoint_local = false;-  if (getsockname(fd, reinterpret_cast<grpc_sockaddr*>(resolved_addr.addr),-                  &resolved_addr.len) == 0) {-    grpc_resolved_address addr_normalized;-    grpc_resolved_address* addr =-        grpc_sockaddr_is_v4mapped(&resolved_addr, &addr_normalized)-            ? &addr_normalized-            : &resolved_addr;-    grpc_sockaddr* sock_addr = reinterpret_cast<grpc_sockaddr*>(&addr->addr);-    // UDS-    if (type == UDS && grpc_is_unix_socket(addr)) {++  char* peer_address = ep->vtable->get_peer(ep);+  if (type == UDS && strncmp(""uds"", peer_address, 3)) {+    is_endpoint_local = true;+  } else if (type == LOCAL_TCP) {+    if (strncmp(""ipv4:127.0.0.1"", peer_address, 14)) {",It is made by [grpc_sockaddr_to_uri](https://github.com/grpc/grpc/blob/master/src/core/lib/iomgr/sockaddr_utils.cc#L219). I think it is stable.,
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/22101,385382770,2020-02-27T21:30:59Z,src/core/ext/filters/client_channel/lb_policy/child_policy_handler.cc,"@@ -0,0 +1,277 @@+//+// Copyright 2018 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/filters/client_channel/lb_policy/child_policy_handler.h""++#include ""absl/strings/str_cat.h""++#include ""src/core/ext/filters/client_channel/lb_policy_registry.h""++namespace grpc_core {++//+// ChildPolicyHandler::Helper+//++class ChildPolicyHandler::Helper+    : public LoadBalancingPolicy::ChannelControlHelper {+ public:+  explicit Helper(RefCountedPtr<ChildPolicyHandler> parent)+      : parent_(std::move(parent)) {}++  ~Helper() { parent_.reset(DEBUG_LOCATION, ""Helper""); }++  RefCountedPtr<SubchannelInterface> CreateSubchannel(+      const grpc_channel_args& args) override {+    if (parent_->shutting_down_) return nullptr;+    if (!CalledByCurrentChild() && !CalledByPendingChild()) return nullptr;+    return parent_->channel_control_helper()->CreateSubchannel(args);+  }++  void UpdateState(grpc_connectivity_state state,+                   std::unique_ptr<SubchannelPicker> picker) override {+    if (parent_->shutting_down_) return;+    // If this request is from the pending child policy, ignore it until+    // it reports READY, at which point we swap it into place.+    if (CalledByPendingChild()) {+      if (GRPC_TRACE_FLAG_ENABLED(*(parent_->tracer_))) {+        gpr_log(GPR_INFO,+                ""[child_policy_handler %p] helper %p: pending child policy %p ""+                ""reports state=%s"",+                parent_.get(), this, child_, ConnectivityStateName(state));+      }+      if (state != GRPC_CHANNEL_READY) return;+      grpc_pollset_set_del_pollset_set(+          parent_->child_policy_->interested_parties(),+          parent_->interested_parties());+      parent_->child_policy_ = std::move(parent_->pending_child_policy_);+    } else if (!CalledByCurrentChild()) {","This is very nit, but would it be a little bit less awkward if you change the negative condition to a positive condition? When I read this line I was like ""why would we even care if it's not a pending and not a current child"", then after 3 seconds it became ""oh yea we do not care about this case indeed"". Changing to a positive condition would make it flow easier I guess.",X
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/22101,385392055,2020-02-27T21:50:59Z,src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc,"@@ -1889,21 +1744,27 @@ class GrpcLbFactory : public LoadBalancingPolicyFactory {       return MakeRefCounted<GrpcLbConfig>(nullptr);     }     std::vector<grpc_error*> error_list;-    RefCountedPtr<LoadBalancingPolicy::Config> child_policy;+    Json child_policy_config_json;     auto it = json.object_value().find(""childPolicy"");-    if (it != json.object_value().end()) {-      grpc_error* parse_error = GRPC_ERROR_NONE;-      child_policy = LoadBalancingPolicyRegistry::ParseLoadBalancingConfig(-          it->second, &parse_error);-      if (parse_error != GRPC_ERROR_NONE) {-        std::vector<grpc_error*> child_errors;-        child_errors.push_back(parse_error);-        error_list.push_back(-            GRPC_ERROR_CREATE_FROM_VECTOR(""field:childPolicy"", &child_errors));-      }+    if (it == json.object_value().end()) {+      child_policy_config_json = Json::Array{Json::Object{+          {""round_robin"", Json::Object()},+      }};+    } else {+      child_policy_config_json = it->second;",This could be a big copy. Consider use pointer for `child_policy_config_json`?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22082,385403727,2020-02-27T22:17:04Z,src/core/lib/security/security_connector/local/local_security_connector.cc,"@@ -70,39 +70,19 @@ void local_check_peer(tsi_peer peer, grpc_endpoint* ep,                       grpc_core::RefCountedPtr<grpc_auth_context>* auth_context,                       grpc_closure* on_peer_checked,                       grpc_local_connect_type type) {-  int fd = grpc_endpoint_get_fd(ep);-  grpc_resolved_address resolved_addr;-  memset(&resolved_addr, 0, sizeof(resolved_addr));-  resolved_addr.len = GRPC_MAX_SOCKADDR_SIZE;   bool is_endpoint_local = false;-  if (getsockname(fd, reinterpret_cast<grpc_sockaddr*>(resolved_addr.addr),-                  &resolved_addr.len) == 0) {-    grpc_resolved_address addr_normalized;-    grpc_resolved_address* addr =-        grpc_sockaddr_is_v4mapped(&resolved_addr, &addr_normalized)-            ? &addr_normalized-            : &resolved_addr;-    grpc_sockaddr* sock_addr = reinterpret_cast<grpc_sockaddr*>(&addr->addr);-    // UDS-    if (type == UDS && grpc_is_unix_socket(addr)) {++  char* peer_address = ep->vtable->get_peer(ep);+  if (type == UDS && strncmp(""uds"", peer_address, 3)) {+    is_endpoint_local = true;+  } else if (type == LOCAL_TCP) {+    if (strncmp(""ipv4:127.0.0.1"", peer_address, 14)) {+      is_endpoint_local = true;+    } else if (strncmp(""ipv6:[::1]"", peer_address, 10)) {","I agree with @yihuazhang here -- it does not make sense to use anything other than the fd here.I think this problem is a symptom of the fact that the asyncio code was implemented as a custom iomgr instead of via the EventManager interface.  The custom iomgr interface is an incomplete abstraction, and the fact that it pretends to support endpoints but its endpoints don't actually provide the full endpoint API (which includes access to the underlying fd) causes problems exactly like this one.  This is one of the many reasons why I want to get rid of the iomgr API.  If asyncio was implemented via the EventManager interface, then I don't think this would be a problem.Given that asyncio is currently implemented the wrong way, if you want to add some sort of fallback in local credentials for the case where the fd is not available, that's fine as a temporary solution.  But we should not stop using the fd in cases that are not broken this way.  And we should definitely be actively working toward an EventManager-based approach for asyncio to make this kind of temporary hack go away.",X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22168,385426026,2020-02-27T23:16:45Z,test/core/security/grpc_tls_credentials_options_test.cc,"@@ -0,0 +1,64 @@+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include ""src/core/lib/security/credentials/tls/grpc_tls_credentials_options.h""+#include ""test/core/end2end/data/ssl_test_data.h""++#include <gmock/gmock.h>+#include <grpc/support/alloc.h>+#include <grpc/support/log.h>+#include <grpc/support/string_util.h>+#include <gtest/gtest.h>++namespace testing {++static void SetKeyMaterials(grpc_tls_key_materials_config* config) {+  grpc_ssl_pem_key_cert_pair** key_cert_pair =+      static_cast<grpc_ssl_pem_key_cert_pair**>(+          gpr_zalloc(sizeof(grpc_ssl_pem_key_cert_pair*)));+  key_cert_pair[0] = static_cast<grpc_ssl_pem_key_cert_pair*>(+      gpr_zalloc(sizeof(grpc_ssl_pem_key_cert_pair)));+  key_cert_pair[0]->private_key = gpr_strdup(test_server1_key);+  key_cert_pair[0]->cert_chain = gpr_strdup(test_server1_cert);+  grpc_tls_key_materials_config_set_key_materials(+      config, gpr_strdup(test_root_cert),+      (const grpc_ssl_pem_key_cert_pair**)key_cert_pair, 1);+}","Just want to give you a heads up that the C core ownership semantics are going to be changed([reference](https://github.com/grpc/grpc/issues/20161)). This means C core wouldn't steal the ownership, and we probably have to manually free grpc_ssl_pem_key_cert_pair here in this test. The PR is https://github.com/grpc/grpc/pull/22171 (Not ready for review yet).For now I think we are fine to merge these tests.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22101,385427632,2020-02-27T23:21:35Z,src/core/ext/filters/client_channel/lb_policy/child_policy_handler.cc,"@@ -0,0 +1,277 @@+//+// Copyright 2018 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/filters/client_channel/lb_policy/child_policy_handler.h""++#include ""absl/strings/str_cat.h""++#include ""src/core/ext/filters/client_channel/lb_policy_registry.h""++namespace grpc_core {++//+// ChildPolicyHandler::Helper+//++class ChildPolicyHandler::Helper+    : public LoadBalancingPolicy::ChannelControlHelper {+ public:+  explicit Helper(RefCountedPtr<ChildPolicyHandler> parent)+      : parent_(std::move(parent)) {}++  ~Helper() { parent_.reset(DEBUG_LOCATION, ""Helper""); }++  RefCountedPtr<SubchannelInterface> CreateSubchannel(+      const grpc_channel_args& args) override {+    if (parent_->shutting_down_) return nullptr;+    if (!CalledByCurrentChild() && !CalledByPendingChild()) return nullptr;+    return parent_->channel_control_helper()->CreateSubchannel(args);+  }++  void UpdateState(grpc_connectivity_state state,+                   std::unique_ptr<SubchannelPicker> picker) override {+    if (parent_->shutting_down_) return;+    // If this request is from the pending child policy, ignore it until+    // it reports READY, at which point we swap it into place.+    if (CalledByPendingChild()) {+      if (GRPC_TRACE_FLAG_ENABLED(*(parent_->tracer_))) {+        gpr_log(GPR_INFO,+                ""[child_policy_handler %p] helper %p: pending child policy %p ""+                ""reports state=%s"",+                parent_.get(), this, child_, ConnectivityStateName(state));+      }+      if (state != GRPC_CHANNEL_READY) return;+      grpc_pollset_set_del_pollset_set(+          parent_->child_policy_->interested_parties(),+          parent_->interested_parties());+      parent_->child_policy_ = std::move(parent_->pending_child_policy_);+    } else if (!CalledByCurrentChild()) {","Can you give an example of what you mean by ""a positive condition""?  I'm not sure what you mean.Note that I can't just change this to check that it *is* from the current child and then move line 67 inside of the body here, because we want line 67 to run even if the case of a pending child that has just gone READY and been swapped into being the current child.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22101,385429017,2020-02-27T23:26:02Z,src/core/ext/filters/client_channel/lb_policy/child_policy_handler.cc,"@@ -0,0 +1,277 @@+//+// Copyright 2018 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/filters/client_channel/lb_policy/child_policy_handler.h""++#include ""absl/strings/str_cat.h""++#include ""src/core/ext/filters/client_channel/lb_policy_registry.h""++namespace grpc_core {++//+// ChildPolicyHandler::Helper+//++class ChildPolicyHandler::Helper+    : public LoadBalancingPolicy::ChannelControlHelper {+ public:+  explicit Helper(RefCountedPtr<ChildPolicyHandler> parent)+      : parent_(std::move(parent)) {}++  ~Helper() { parent_.reset(DEBUG_LOCATION, ""Helper""); }++  RefCountedPtr<SubchannelInterface> CreateSubchannel(+      const grpc_channel_args& args) override {+    if (parent_->shutting_down_) return nullptr;+    if (!CalledByCurrentChild() && !CalledByPendingChild()) return nullptr;+    return parent_->channel_control_helper()->CreateSubchannel(args);+  }++  void UpdateState(grpc_connectivity_state state,+                   std::unique_ptr<SubchannelPicker> picker) override {+    if (parent_->shutting_down_) return;+    // If this request is from the pending child policy, ignore it until+    // it reports READY, at which point we swap it into place.+    if (CalledByPendingChild()) {+      if (GRPC_TRACE_FLAG_ENABLED(*(parent_->tracer_))) {+        gpr_log(GPR_INFO,+                ""[child_policy_handler %p] helper %p: pending child policy %p ""+                ""reports state=%s"",+                parent_.get(), this, child_, ConnectivityStateName(state));+      }+      if (state != GRPC_CHANNEL_READY) return;+      grpc_pollset_set_del_pollset_set(+          parent_->child_policy_->interested_parties(),+          parent_->interested_parties());+      parent_->child_policy_ = std::move(parent_->pending_child_policy_);+    } else if (!CalledByCurrentChild()) {+      // This request is from an outdated child, so ignore it.+      return;+    }+    parent_->channel_control_helper()->UpdateState(state, std::move(picker));+  }++  void RequestReresolution() override {+    if (parent_->shutting_down_) return;+    const LoadBalancingPolicy* latest_child_policy =+        parent_->pending_child_policy_ != nullptr+            ? parent_->pending_child_policy_.get()+            : parent_->child_policy_.get();+    if (child_ != latest_child_policy) return;+    if (GRPC_TRACE_FLAG_ENABLED(*(parent_->tracer_))) {+      gpr_log(GPR_INFO, ""[child_policy_handler %p] started name re-resolving"",+              parent_.get());+    }+    parent_->channel_control_helper()->RequestReresolution();+  }++  void AddTraceEvent(TraceSeverity severity, StringView message) override {+    if (parent_->shutting_down_) return;+    if (!CalledByPendingChild() && !CalledByCurrentChild()) return;+    parent_->channel_control_helper()->AddTraceEvent(severity, message);+  }++  void set_child(LoadBalancingPolicy* child) { child_ = child; }++ private:+  bool CalledByPendingChild() const {+    GPR_ASSERT(child_ != nullptr);+    return child_ == parent_->pending_child_policy_.get();+  }++  bool CalledByCurrentChild() const {+    GPR_ASSERT(child_ != nullptr);+    return child_ == parent_->child_policy_.get();+  };++  RefCountedPtr<ChildPolicyHandler> parent_;+  LoadBalancingPolicy* child_ = nullptr;+};++//+// ChildPolicyHandler+//++void ChildPolicyHandler::ShutdownLocked() {+  if (GRPC_TRACE_FLAG_ENABLED(*tracer_)) {+    gpr_log(GPR_INFO, ""[child_policy_handler %p] shutting down"", this);+  }+  shutting_down_ = true;+  if (child_policy_ != nullptr) {+    if (GRPC_TRACE_FLAG_ENABLED(*tracer_)) {+      gpr_log(GPR_INFO, ""[child_policy_handler %p] shutting down lb_policy %p"",+              this, child_policy_.get());+    }+    grpc_pollset_set_del_pollset_set(child_policy_->interested_parties(),+                                     interested_parties());+    child_policy_.reset();+  }+  if (pending_child_policy_ != nullptr) {+    if (GRPC_TRACE_FLAG_ENABLED(*tracer_)) {+      gpr_log(GPR_INFO,+              ""[child_policy_handler %p] shutting down pending lb_policy %p"",+              this, pending_child_policy_.get());+    }+    grpc_pollset_set_del_pollset_set(+        pending_child_policy_->interested_parties(), interested_parties());+    pending_child_policy_.reset();+  }+}++void ChildPolicyHandler::UpdateLocked(UpdateArgs args) {+  // If the child policy name changes, we need to create a new child+  // policy.  When this happens, we leave child_policy_ as-is and store+  // the new child policy in pending_child_policy_.  Once the new child+  // policy transitions into state READY, we swap it into child_policy_,+  // replacing the original child policy.  So pending_child_policy_ is+  // non-null only between when we apply an update that changes the child+  // policy name and when the new child reports state READY.+  //+  // Updates can arrive at any point during this transition.  We always+  // apply updates relative to the most recently created child policy,+  // even if the most recent one is still in pending_child_policy_.  This+  // is true both when applying the updates to an existing child policy+  // and when determining whether we need to create a new policy.+  //+  // As a result of this, there are several cases to consider here:+  //+  // 1. We have no existing child policy (i.e., we have started up but+  //    have not yet received a serverlist from the balancer or gone+  //    into fallback mode; in this case, both child_policy_ and+  //    pending_child_policy_ are null).  In this case, we create a+  //    new child policy and store it in child_policy_.+  //+  // 2. We have an existing child policy and have no pending child policy+  //    from a previous update (i.e., either there has not been a+  //    previous update that changed the policy name, or we have already+  //    finished swapping in the new policy; in this case, child_policy_+  //    is non-null but pending_child_policy_ is null).  In this case:+  //    a. If child_policy_->name() equals child_policy_name, then we+  //       update the existing child policy.+  //    b. If child_policy_->name() does not equal child_policy_name,+  //       we create a new policy.  The policy will be stored in+  //       pending_child_policy_ and will later be swapped into+  //       child_policy_ by the helper when the new child transitions+  //       into state READY.+  //+  // 3. We have an existing child policy and have a pending child policy+  //    from a previous update (i.e., a previous update set+  //    pending_child_policy_ as per case 2b above and that policy has+  //    not yet transitioned into state READY and been swapped into+  //    child_policy_; in this case, both child_policy_ and+  //    pending_child_policy_ are non-null).  In this case:+  //    a. If pending_child_policy_->name() equals child_policy_name,+  //       then we update the existing pending child policy.+  //    b. If pending_child_policy->name() does not equal+  //       child_policy_name, then we create a new policy.  The new+  //       policy is stored in pending_child_policy_ (replacing the one+  //       that was there before, which will be immediately shut down)+  //       and will later be swapped into child_policy_ by the helper+  //       when the new child transitions into state READY.+  const char* child_policy_name = args.config->name();+  const bool create_policy =+      // case 1+      child_policy_ == nullptr ||+      // case 2b+      (pending_child_policy_ == nullptr &&+       strcmp(child_policy_->name(), child_policy_name) != 0) ||+      // case 3b+      (pending_child_policy_ != nullptr &&+       strcmp(pending_child_policy_->name(), child_policy_name) != 0);+  LoadBalancingPolicy* policy_to_update = nullptr;+  if (create_policy) {+    // Cases 1, 2b, and 3b: create a new child policy.+    // If child_policy_ is null, we set it (case 1), else we set+    // pending_child_policy_ (cases 2b and 3b).+    if (GRPC_TRACE_FLAG_ENABLED(*tracer_)) {+      gpr_log(GPR_INFO,+              ""[child_policy_handler %p] creating new %schild policy %s"", this,+              child_policy_ == nullptr ? """" : ""pending "", child_policy_name);+    }+    auto& lb_policy =+        child_policy_ == nullptr ? child_policy_ : pending_child_policy_;+    lb_policy = CreateChildPolicy(child_policy_name, *args.args);","The handing of `grpc_channel_args` is inconsistent all across our codebase, unfortunately.  It's one of our oldest types, long predating the conversion from C to C++, so most of our older interfaces take it as a pointer.  However, the C++ convention is to pass read-only arguments as const references, so that's what we do in newer code.At some point, I would like to find time to convert `grpc_channel_args` to C++ and fix all of our code to use a more idiomatic interface.  But until then, these inconsistencies will have to remain.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22101,385430695,2020-02-27T23:31:37Z,src/core/ext/filters/client_channel/resolving_lb_policy.h,"@@ -92,10 +92,9 @@ class ResolvingLoadBalancingPolicy : public LoadBalancingPolicy {   void OnResolverError(grpc_error* error);   void CreateOrUpdateLbPolicyLocked(       RefCountedPtr<LoadBalancingPolicy::Config> lb_policy_config,-      Resolver::Result result, TraceStringVector* trace_strings);+      Resolver::Result result);   OrphanablePtr<LoadBalancingPolicy> CreateLbPolicyLocked(-      const char* lb_policy_name, const grpc_channel_args& args,-      TraceStringVector* trace_strings);+      const grpc_channel_args& args);",See my reply elsewhere.  This is the normal C++ convention for passing a read-only parameter.,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22168,385434976,2020-02-27T23:45:32Z,src/core/lib/security/security_connector/tls/tls_security_connector.cc,"@@ -62,34 +62,41 @@ tsi_ssl_pem_key_cert_pair* ConvertToTsiPemKeyCertPair(  }  // namespace -/** -- Util function to fetch TLS server/channel credentials. -- */ grpc_status_code TlsFetchKeyMaterials(     const grpc_core::RefCountedPtr<grpc_tls_key_materials_config>&         key_materials_config,","May I know what are the benefits we can provide when defining this field as ""a const reference to a RefCountedPtr""?It gives me a sense that `key_materials_config` is somewhat similar with `reload_status`, in a way that they are both provided by the caller, and populated inside this function.Since reload_status is a plain pointer, can we just make key_materials_config a RefCountedPtr?",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22168,385479171,2020-02-28T02:31:12Z,src/core/lib/security/security_connector/tls/tls_security_connector.cc,"@@ -62,34 +62,41 @@ tsi_ssl_pem_key_cert_pair* ConvertToTsiPemKeyCertPair(  }  // namespace -/** -- Util function to fetch TLS server/channel credentials. -- */ grpc_status_code TlsFetchKeyMaterials(     const grpc_core::RefCountedPtr<grpc_tls_key_materials_config>&         key_materials_config,","Sorry, I probably should rephrase my last sentence to ""Since reload_status is a plain pointer, can we make key_materials_config a RefCountedPtr, or even a plain pointer?""I think by using a RefCountedPtr, we could imply that `TlsFetchKeyMaterials` shares the ownership of `key_materials_config`, which looks correct because we need to modify this parameter in `TlsFetchKeyMaterials`. So I think it's better using RefCountedPtr than a plain pointer.What I don't understand is why we need a const reference...I might be over picky about this, but I really want to get an idea of the philosophy when designing these APIs... ",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22168,385481823,2020-02-28T02:43:21Z,src/core/lib/security/security_connector/tls/tls_security_connector.cc,"@@ -62,34 +62,41 @@ tsi_ssl_pem_key_cert_pair* ConvertToTsiPemKeyCertPair(  }  // namespace -/** -- Util function to fetch TLS server/channel credentials. -- */ grpc_status_code TlsFetchKeyMaterials(     const grpc_core::RefCountedPtr<grpc_tls_key_materials_config>&         key_materials_config,","I found this post explains this pretty well: https://herbsutter.com/2013/06/05/gotw-91-solution-smart-pointer-parameters/To my understanding, we use something like ""const shared_ptr &"" when we want to imply the sharing semantics, but don't want the pointer to be copied. I think I understand it now...Sorry for the noise here, @matthewstevenson88 ! I will resolve this right away :-) ",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/22168,385760316,2020-02-28T15:31:03Z,src/core/lib/security/security_connector/tls/tls_security_connector.h,"@@ -145,10 +145,32 @@ class TlsServerSecurityConnector final : public grpc_server_security_connector { };  // ---- Functions below are exposed for testing only -----------------------++/** The |TlsFetchKeyMaterials| API ensures that |key_materials_config| has a+ *  non-empty pem-key-cert pair list. This is done as follows:+ *  - if |options| is equipped with a credential reload config, then this+ *    methods uses credential reloading to populate |key_materials_config|, and+ *    afterwards it populates |reload_status| with the status of this operation.+ *    In particular, any data stored in |key_materials_config| is overwritten.+ *  - if |options| has no credential reload config, then:+ *    - if |key_materials_config| already has a non-empty pem-key-cert pair+ *      list or is called by a client, then the method returns |GRPC_STATUS_OK|.+ *    - if |key_materials_config| has an empty pem-key-cert pair list and is+ *      called by a server, then the method return an error code.+ *+ *  The arguments are detailed below:+ *  - key_materials_config: a key material config that will be populated by the",Sorry I don't follow what you mean. Would you like me to explain the kind of data that the `key_materials_config` holds?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22101,385781957,2020-02-28T16:07:44Z,src/core/ext/filters/client_channel/lb_policy/child_policy_handler.cc,"@@ -0,0 +1,277 @@+//+// Copyright 2018 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/filters/client_channel/lb_policy/child_policy_handler.h""++#include ""absl/strings/str_cat.h""++#include ""src/core/ext/filters/client_channel/lb_policy_registry.h""++namespace grpc_core {++//+// ChildPolicyHandler::Helper+//++class ChildPolicyHandler::Helper+    : public LoadBalancingPolicy::ChannelControlHelper {+ public:+  explicit Helper(RefCountedPtr<ChildPolicyHandler> parent)+      : parent_(std::move(parent)) {}++  ~Helper() { parent_.reset(DEBUG_LOCATION, ""Helper""); }++  RefCountedPtr<SubchannelInterface> CreateSubchannel(+      const grpc_channel_args& args) override {+    if (parent_->shutting_down_) return nullptr;+    if (!CalledByCurrentChild() && !CalledByPendingChild()) return nullptr;+    return parent_->channel_control_helper()->CreateSubchannel(args);+  }++  void UpdateState(grpc_connectivity_state state,+                   std::unique_ptr<SubchannelPicker> picker) override {+    if (parent_->shutting_down_) return;+    // If this request is from the pending child policy, ignore it until+    // it reports READY, at which point we swap it into place.+    if (CalledByPendingChild()) {+      if (GRPC_TRACE_FLAG_ENABLED(*(parent_->tracer_))) {+        gpr_log(GPR_INFO,+                ""[child_policy_handler %p] helper %p: pending child policy %p ""+                ""reports state=%s"",+                parent_.get(), this, child_, ConnectivityStateName(state));+      }+      if (state != GRPC_CHANNEL_READY) return;+      grpc_pollset_set_del_pollset_set(+          parent_->child_policy_->interested_parties(),+          parent_->interested_parties());+      parent_->child_policy_ = std::move(parent_->pending_child_policy_);+    } else if (!CalledByCurrentChild()) {+      // This request is from an outdated child, so ignore it.+      return;+    }+    parent_->channel_control_helper()->UpdateState(state, std::move(picker));+  }++  void RequestReresolution() override {+    if (parent_->shutting_down_) return;+    const LoadBalancingPolicy* latest_child_policy =+        parent_->pending_child_policy_ != nullptr+            ? parent_->pending_child_policy_.get()+            : parent_->child_policy_.get();+    if (child_ != latest_child_policy) return;+    if (GRPC_TRACE_FLAG_ENABLED(*(parent_->tracer_))) {+      gpr_log(GPR_INFO, ""[child_policy_handler %p] started name re-resolving"",+              parent_.get());+    }+    parent_->channel_control_helper()->RequestReresolution();+  }++  void AddTraceEvent(TraceSeverity severity, StringView message) override {+    if (parent_->shutting_down_) return;+    if (!CalledByPendingChild() && !CalledByCurrentChild()) return;+    parent_->channel_control_helper()->AddTraceEvent(severity, message);+  }++  void set_child(LoadBalancingPolicy* child) { child_ = child; }++ private:+  bool CalledByPendingChild() const {+    GPR_ASSERT(child_ != nullptr);+    return child_ == parent_->pending_child_policy_.get();+  }++  bool CalledByCurrentChild() const {+    GPR_ASSERT(child_ != nullptr);+    return child_ == parent_->child_policy_.get();+  };++  RefCountedPtr<ChildPolicyHandler> parent_;+  LoadBalancingPolicy* child_ = nullptr;+};++//+// ChildPolicyHandler+//++void ChildPolicyHandler::ShutdownLocked() {+  if (GRPC_TRACE_FLAG_ENABLED(*tracer_)) {+    gpr_log(GPR_INFO, ""[child_policy_handler %p] shutting down"", this);+  }+  shutting_down_ = true;+  if (child_policy_ != nullptr) {+    if (GRPC_TRACE_FLAG_ENABLED(*tracer_)) {+      gpr_log(GPR_INFO, ""[child_policy_handler %p] shutting down lb_policy %p"",+              this, child_policy_.get());+    }+    grpc_pollset_set_del_pollset_set(child_policy_->interested_parties(),+                                     interested_parties());+    child_policy_.reset();+  }+  if (pending_child_policy_ != nullptr) {+    if (GRPC_TRACE_FLAG_ENABLED(*tracer_)) {+      gpr_log(GPR_INFO,+              ""[child_policy_handler %p] shutting down pending lb_policy %p"",+              this, pending_child_policy_.get());+    }+    grpc_pollset_set_del_pollset_set(+        pending_child_policy_->interested_parties(), interested_parties());+    pending_child_policy_.reset();+  }+}++void ChildPolicyHandler::UpdateLocked(UpdateArgs args) {+  // If the child policy name changes, we need to create a new child+  // policy.  When this happens, we leave child_policy_ as-is and store+  // the new child policy in pending_child_policy_.  Once the new child+  // policy transitions into state READY, we swap it into child_policy_,+  // replacing the original child policy.  So pending_child_policy_ is+  // non-null only between when we apply an update that changes the child+  // policy name and when the new child reports state READY.+  //+  // Updates can arrive at any point during this transition.  We always+  // apply updates relative to the most recently created child policy,+  // even if the most recent one is still in pending_child_policy_.  This+  // is true both when applying the updates to an existing child policy+  // and when determining whether we need to create a new policy.+  //+  // As a result of this, there are several cases to consider here:+  //+  // 1. We have no existing child policy (i.e., we have started up but+  //    have not yet received a serverlist from the balancer or gone+  //    into fallback mode; in this case, both child_policy_ and+  //    pending_child_policy_ are null).  In this case, we create a+  //    new child policy and store it in child_policy_.+  //+  // 2. We have an existing child policy and have no pending child policy+  //    from a previous update (i.e., either there has not been a+  //    previous update that changed the policy name, or we have already+  //    finished swapping in the new policy; in this case, child_policy_+  //    is non-null but pending_child_policy_ is null).  In this case:+  //    a. If child_policy_->name() equals child_policy_name, then we+  //       update the existing child policy.+  //    b. If child_policy_->name() does not equal child_policy_name,+  //       we create a new policy.  The policy will be stored in+  //       pending_child_policy_ and will later be swapped into+  //       child_policy_ by the helper when the new child transitions+  //       into state READY.+  //+  // 3. We have an existing child policy and have a pending child policy+  //    from a previous update (i.e., a previous update set+  //    pending_child_policy_ as per case 2b above and that policy has+  //    not yet transitioned into state READY and been swapped into+  //    child_policy_; in this case, both child_policy_ and+  //    pending_child_policy_ are non-null).  In this case:+  //    a. If pending_child_policy_->name() equals child_policy_name,+  //       then we update the existing pending child policy.+  //    b. If pending_child_policy->name() does not equal+  //       child_policy_name, then we create a new policy.  The new+  //       policy is stored in pending_child_policy_ (replacing the one+  //       that was there before, which will be immediately shut down)+  //       and will later be swapped into child_policy_ by the helper+  //       when the new child transitions into state READY.+  const char* child_policy_name = args.config->name();+  const bool create_policy =+      // case 1+      child_policy_ == nullptr ||+      // case 2b+      (pending_child_policy_ == nullptr &&+       strcmp(child_policy_->name(), child_policy_name) != 0) ||+      // case 3b+      (pending_child_policy_ != nullptr &&+       strcmp(pending_child_policy_->name(), child_policy_name) != 0);+  LoadBalancingPolicy* policy_to_update = nullptr;+  if (create_policy) {+    // Cases 1, 2b, and 3b: create a new child policy.+    // If child_policy_ is null, we set it (case 1), else we set+    // pending_child_policy_ (cases 2b and 3b).+    if (GRPC_TRACE_FLAG_ENABLED(*tracer_)) {+      gpr_log(GPR_INFO,+              ""[child_policy_handler %p] creating new %schild policy %s"", this,+              child_policy_ == nullptr ? """" : ""pending "", child_policy_name);+    }+    auto& lb_policy =+        child_policy_ == nullptr ? child_policy_ : pending_child_policy_;+    lb_policy = CreateChildPolicy(child_policy_name, *args.args);","It is in the style guide, but confusingly, it's buried in the middle of a section about output parameters, even though it's talking about input parameters:https://google.github.io/styleguide/cppguide.html#Output_Parameters""Input parameters are usually values or const references""",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22171,385924572,2020-02-28T21:13:08Z,src/core/lib/security/credentials/tls/grpc_tls_credentials_options.h,"@@ -45,11 +45,19 @@ struct grpc_tls_key_materials_config   void set_pem_root_certs(grpc_core::UniquePtr<char> pem_root_certs) {     pem_root_certs_ = std::move(pem_root_certs);   }+  void set_pem_root_certs(const char* pem_root_certs) {+    // make a copy of pem_root_certs.+    grpc_core::UniquePtr<char> pem_root_ptr(gpr_strdup(pem_root_certs));","My guess: this is to make sure we only keep one copy of the certificate, since [the certificate string could take a large piece of memory](https://github.com/grpc/grpc/issues/17743). It might be inefficient if we use `std::string` and the caller doesn't handle it correctly, and causing a copy of the string. Besides root certs, the peer certs and private key are actually [also defined this way](https://github.com/grpc/grpc/blob/master/src/core/lib/security/security_connector/ssl_utils.h#L175-L176) .@yihuazhang please correct me if I was wrong, or I misunderstood something. Thanks!",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/22190,385981612,2020-02-29T00:38:28Z,examples/cpp/metadata/greeter_server.cc,"@@ -44,7 +44,7 @@ class GreeterServiceImpl final : public Greeter::Service {      // Get the client's initial metadata     std::cout << ""Client metadata: "" << std::endl;-    const std::multimap<grpc::string_ref, grpc::string_ref> metadata = context->client_metadata();+    const std::multimap<std::string_ref, std::string_ref> metadata = context->client_metadata();",Oh my bad! I'm fixing it.,
900411,mehrdada,https://api.github.com/repos/grpc/grpc/pulls/22183,386542934,2020-03-02T17:38:31Z,tools/internal_ci/helper_scripts/prepare_build_macos_rc,"@@ -29,6 +29,9 @@ ulimit -a pip install --user google-api-python-client oauth2client export GOOGLE_APPLICATION_CREDENTIALS=${KOKORO_GFILE_DIR}/GrpcTesting-d0eeee2db331.json +# Upgrade pip to understand manylinux2010 wheels+pip install -U pip",Perhaps pin `pip`/`virtualenv` so the build environment doesn't accidentally change underneath you?,
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/22171,386596272,2020-03-02T19:18:09Z,src/core/lib/security/credentials/tls/grpc_tls_credentials_options.h,"@@ -45,11 +45,19 @@ struct grpc_tls_key_materials_config   void set_pem_root_certs(grpc_core::UniquePtr<char> pem_root_certs) {     pem_root_certs_ = std::move(pem_root_certs);   }+  void set_pem_root_certs(const char* pem_root_certs) {+    // make a copy of pem_root_certs.+    grpc_core::UniquePtr<char> pem_root_ptr(gpr_strdup(pem_root_certs));","+1 on avoiding unnecessary memory copy. When initially writing the TLS code, it was also not possible to use C++ standard library in C-core. ",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/22171,386627243,2020-03-02T20:19:12Z,src/core/lib/security/security_connector/tls/tls_security_connector.cc,"@@ -334,14 +334,12 @@ grpc_security_status TlsChannelSecurityConnector::InitializeHandshakerFactory(       static_cast<const TlsCredentials*>(channel_creds());   grpc_tls_key_materials_config* key_materials_config =       creds->options().key_materials_config();-  /* Copy key materials config from credential options. */+  // key_materials_config_->set_key_materials will handle the copying of the key",It seems only copying `pem_root_certs` but not  `pem_key_cert_pair_list`.,X
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/22171,386633036,2020-03-02T20:30:58Z,src/core/lib/security/credentials/tls/grpc_tls_credentials_options.cc,"@@ -29,10 +29,28 @@  /** -- gRPC TLS key materials config API implementation. -- **/ void grpc_tls_key_materials_config::set_key_materials(-    grpc_core::UniquePtr<char> pem_root_certs,-    PemKeyCertPairList pem_key_cert_pair_list) {-  pem_key_cert_pair_list_ = std::move(pem_key_cert_pair_list);-  pem_root_certs_ = std::move(pem_root_certs);+    const char* pem_root_certs,+    const grpc_ssl_pem_key_cert_pair** pem_key_cert_pairs,+    size_t num_key_cert_pairs) {+  this->set_pem_root_certs(pem_root_certs);+  grpc_tls_key_materials_config::PemKeyCertPairList cert_pair_list;+  for (size_t i = 0; i < num_key_cert_pairs; i++) {+    auto current_pair = static_cast<grpc_ssl_pem_key_cert_pair*>(+        gpr_zalloc(sizeof(grpc_ssl_pem_key_cert_pair)));+    current_pair->cert_chain = gpr_strdup(pem_key_cert_pairs[i]->cert_chain);+    current_pair->private_key = gpr_strdup(pem_key_cert_pairs[i]->private_key);+    cert_pair_list.emplace_back(grpc_core::PemKeyCertPair(current_pair));+  }+  pem_key_cert_pair_list_ = std::move(cert_pair_list);+}++void grpc_tls_key_materials_config::set_key_materials(",The semantics seems a little weird in a sense that we only do copy for `pem_root_certs` but do move on `pem_key_cert_pair_list`. Should not we make them consistent?,X
4779759,dapengzhang0,https://api.github.com/repos/grpc/grpc/pulls/22193,387221063,2020-03-03T18:47:36Z,tools/run_tests/run_xds_tests.py,"@@ -51,24 +51,38 @@ def parse_port_range(port_arg): argp.add_argument(     '--gcp_suffix',     default='',-    help='Optional suffix for all generated GCP resource names. Useful to ensure '-    'distinct names across test runs.')-argp.add_argument('--test_case',-                  default=None,-                  choices=['all', 'ping_pong', 'round_robin'])+    help='Optional suffix for all generated GCP resource names. Useful to '+    'ensure distinct names across test runs.')+argp.add_argument(+    '--test_case',+    default=None,",The default will always fail. Default to 'all' or a fixed list instead?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22221,387286443,2020-03-03T20:52:32Z,doc/service_config.md,"@@ -8,134 +8,30 @@ parameters to be automatically used by all clients of their service.  # Format -The service config is a JSON string of the following form:--```-{-  // [deprecated] Load balancing policy name (case insensitive).-  // Currently, the only selectable client-side policy provided with gRPC-  // is 'round_robin', but third parties may add their own policies.-  // This field is optional; if unset, the default behavior is to pick-  // the first available backend. If set, the load balancing policy should be-  // supported by the client, otherwise the service config is considered-  // invalid.-  // If the policy name is set via the client API, that value overrides-  // the value specified here.-  //-  // Note that if the resolver returns at least one balancer address (as-  // opposed to backend addresses), gRPC will use grpclb (see-  // https://github.com/grpc/grpc/blob/master/doc/load-balancing.md),-  // regardless of what LB policy is requested either here or via the-  // client API.-  'loadBalancingPolicy': string,--  // Per-method configuration.  Optional.-  'methodConfig': [-    {-      // The names of the methods to which this method config applies. There-      // must be at least one name. Each name entry must be unique across the-      // entire service config. If the 'method' field is empty, then this-      // method config specifies the defaults for all methods for the specified-      // service.-      //-      // For example, let's say that the service config contains the following-      // method config entries:-      //-      // 'methodConfig': [-      //   { 'name': [ { 'service': 'MyService' } ] ... },-      //   { 'name': [ { 'service': 'MyService', 'method': 'Foo' } ] ... }-      // ]-      //-      // For a request for MyService/Foo, we will use the second entry, because-      // it exactly matches the service and method name.-      // For a request for MyService/Bar, we will use the first entry, because-      // it provides the default for all methods of MyService.-      'name': [-        {-          // RPC service name.  Required.-          // If using gRPC with protobuf as the IDL, then this will be of-          // the form ""pkg.service_name"", where ""pkg"" is the package name-          // defined in the proto file.-          'service': string,--          // RPC method name.  Optional (see above).-          'method': string,-        }-      ],--      // Optional. Whether RPCs sent to this method should wait until the-      // connection is ready by default. If false, the RPC will abort-      // immediately if there is a transient failure connecting to the server.-      // Otherwise, gRPC will attempt to connect until the deadline is-      // exceeded.-      //-      // The value specified via the gRPC client API will override the value-      // set here. However, note that setting the value in the client API will-      // also affect transient errors encountered during name resolution,-      // which cannot be caught by the value here, since the service config-      // is obtained by the gRPC client via name resolution.-      'waitForReady': bool,--      // Optional. The default timeout in seconds for RPCs sent to this method.-      // This can be overridden in code. If no reply is received in the-      // specified amount of time, the request is aborted and a-      // deadline-exceeded error status is returned to the caller.-      //-      // The actual deadline used will be the minimum of the value specified-      // here and the value set by the application via the gRPC client API.-      // If either one is not set, then the other will be used.-      // If neither is set, then the request has no deadline.-      //-      // The format of the value is that of the 'Duration' type defined here:-      // https://developers.google.com/protocol-buffers/docs/proto3#json-      'timeout': string,--      // Optional. The maximum allowed payload size for an individual request-      // or object in a stream (client->server) in bytes. The size which is-      // measured is the serialized, uncompressed payload in bytes. This-      // applies both to streaming and non-streaming requests.-      //-      // The actual value used is the minimum of the value specified here and-      // the value set by the application via the gRPC client API.-      // If either one is not set, then the other will be used.-      // If neither is set, then the built-in default is used.-      //-      // If a client attempts to send an object larger than this value, it-      // will not be sent and the client will see an error.-      // Note that 0 is a valid value, meaning that the request message must-      // be empty.-      'maxRequestMessageBytes': number,--      // Optional. The maximum allowed payload size for an individual response-      // or object in a stream (server->client) in bytes. The size which is-      // measured is the serialized, uncompressed payload in bytes. This-      // applies both to streaming and non-streaming requests.-      //-      // The actual value used is the minimum of the value specified here and-      // the value set by the application via the gRPC client API.-      // If either one is not set, then the other will be used.-      // If neither is set, then the built-in default is used.-      //-      // If a server attempts to send an object larger than this value, it-      // will not be sent, and the client will see an error.-      // Note that 0 is a valid value, meaning that the response message must-      // be empty.-      'maxResponseMessageBytes': number-    }-  ]-}-```--Note that new per-method parameters may be added in the future as new-functionality is introduced.","The format that was previously shown here was badly out of date, and I don't want to have to remember to update it every time we update the proto file, which is the authoritative source of truth.  But I have added a short example here in both protobuf and JSON form to illustrate how it should look.",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22171,387366113,2020-03-03T23:57:34Z,src/core/lib/security/credentials/tls/grpc_tls_credentials_options.cc,"@@ -29,10 +29,28 @@  /** -- gRPC TLS key materials config API implementation. -- **/ void grpc_tls_key_materials_config::set_key_materials(-    grpc_core::UniquePtr<char> pem_root_certs,-    PemKeyCertPairList pem_key_cert_pair_list) {-  pem_key_cert_pair_list_ = std::move(pem_key_cert_pair_list);-  pem_root_certs_ = std::move(pem_root_certs);+    const char* pem_root_certs,+    const grpc_ssl_pem_key_cert_pair** pem_key_cert_pairs,+    size_t num_key_cert_pairs) {+  this->set_pem_root_certs(pem_root_certs);+  grpc_tls_key_materials_config::PemKeyCertPairList cert_pair_list;+  for (size_t i = 0; i < num_key_cert_pairs; i++) {+    auto current_pair = static_cast<grpc_ssl_pem_key_cert_pair*>(+        gpr_zalloc(sizeof(grpc_ssl_pem_key_cert_pair)));+    current_pair->cert_chain = gpr_strdup(pem_key_cert_pairs[i]->cert_chain);+    current_pair->private_key = gpr_strdup(pem_key_cert_pairs[i]->private_key);+    cert_pair_list.emplace_back(grpc_core::PemKeyCertPair(current_pair));+  }+  pem_key_cert_pair_list_ = std::move(cert_pair_list);+}++void grpc_tls_key_materials_config::set_key_materials(","This creates a dup_list, which is a copy of the list. Then I moved the ownership of the copy to `pem_key_cert_pair_list_`, while leaving the original one untouched.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/22193,387909200,2020-03-04T20:13:59Z,tools/run_tests/run_xds_tests.py,"@@ -177,65 +184,232 @@ def get_client_stats(num_rpcs, timeout_sec):             raise Exception('GetClientStats RPC failed')  -def wait_until_only_given_backends_receive_load(backends, timeout_sec):+def wait_until_only_given_instances_receive_load(backends,+                                                 timeout_sec,+                                                 num_rpcs=100,+                                                 allow_failures=False):     start_time = time.time()     error_msg = None+    logger.debug('Waiting for %d sec until backends %s  receive load' %+                 (timeout_sec, backends))     while time.time() - start_time <= timeout_sec:         error_msg = None-        stats = get_client_stats(max(len(backends), 1), timeout_sec)+        stats = get_client_stats(num_rpcs, timeout_sec)         rpcs_by_peer = stats.rpcs_by_peer         for backend in backends:             if backend not in rpcs_by_peer:                 error_msg = 'Backend %s did not receive load' % backend                 break         if not error_msg and len(rpcs_by_peer) > len(backends):             error_msg = 'Unexpected backend received load: %s' % rpcs_by_peer+        if not allow_failures and stats.num_failures > 0:+            error_msg = '%d RPCs failed' % stats.num_failures         if not error_msg:             return     raise Exception(error_msg)  -def test_ping_pong(backends, num_rpcs, stats_timeout_sec):+def test_backends_restart(gcp, backend_service, instance_group):+    instance_names = get_instance_names(gcp, instance_group)+    num_instances = len(instance_names)+    start_time = time.time()+    wait_until_only_given_instances_receive_load(instance_names,+                                                 _WAIT_FOR_STATS_SEC)+    stats = get_client_stats(_NUM_TEST_RPCS, _WAIT_FOR_STATS_SEC)+    try:+        resize_instance_group(gcp, instance_group, 0)+        wait_until_only_given_instances_receive_load([],+                                                     _WAIT_FOR_BACKEND_SEC,+                                                     allow_failures=True)+    finally:+        resize_instance_group(gcp, instance_group, num_instances)+    wait_for_healthy_backends(gcp, backend_service, instance_group)+    new_instance_names = get_instance_names(gcp, instance_group)+    wait_until_only_given_instances_receive_load(new_instance_names,+                                                 _WAIT_FOR_BACKEND_SEC)+    new_stats = get_client_stats(_NUM_TEST_RPCS, _WAIT_FOR_STATS_SEC)+    original_distribution = list(stats.rpcs_by_peer.values())+    original_distribution.sort()+    new_distribution = list(new_stats.rpcs_by_peer.values())+    new_distribution.sort()+    if original_distribution != new_distribution:+        raise Exception('Distributions do not match: ', stats, new_stats)+++def test_change_backend_service(gcp, original_backend_service, instance_group,+                                alternate_backend_service,+                                same_zone_instance_group):+    original_backend_instances = get_instance_names(gcp, instance_group)+    alternate_backend_instances = get_instance_names(gcp,+                                                     same_zone_instance_group)+    patch_backend_instances(gcp, alternate_backend_service,+                            [same_zone_instance_group])+    wait_for_healthy_backends(gcp, original_backend_service, instance_group)+    wait_for_healthy_backends(gcp, alternate_backend_service,+                              same_zone_instance_group)+    wait_until_only_given_instances_receive_load(original_backend_instances,+                                                 _WAIT_FOR_STATS_SEC)+    try:+        patch_url_map_backend_service(gcp, alternate_backend_service)+        stats = get_client_stats(_NUM_TEST_RPCS, _WAIT_FOR_STATS_SEC)+        if stats.num_failures > 0:+            raise Exception('Unexpected failure: %s', stats)+        wait_until_only_given_instances_receive_load(+            alternate_backend_instances, _WAIT_FOR_STATS_SEC)+    finally:+        patch_url_map_backend_service(gcp, original_backend_service)+        patch_backend_instances(gcp, alternate_backend_service, [])+++def test_new_instance_group_receives_traffic(gcp, backend_service,+                                             instance_group,+                                             same_zone_instance_group):+    instance_names = get_instance_names(gcp, instance_group)+    wait_until_only_given_instances_receive_load(instance_names,+                                                 _WAIT_FOR_STATS_SEC)+    try:+        patch_backend_instances(gcp,+                                backend_service,+                                [instance_group, same_zone_instance_group],+                                balancing_mode='RATE')+        wait_for_healthy_backends(gcp, backend_service, instance_group)+        wait_for_healthy_backends(gcp, backend_service,+                                  same_zone_instance_group)+        combined_instance_names = instance_names + get_instance_names(+            gcp, same_zone_instance_group)+        wait_until_only_given_instances_receive_load(combined_instance_names,+                                                     _WAIT_FOR_BACKEND_SEC)+    finally:+        patch_backend_instances(gcp, backend_service, [instance_group])+++def test_ping_pong(gcp, backend_service, instance_group):+    wait_for_healthy_backends(gcp, backend_service, instance_group)+    instance_names = get_instance_names(gcp, instance_group)     start_time = time.time()     error_msg = None-    while time.time() - start_time <= stats_timeout_sec:+    while time.time() - start_time <= _WAIT_FOR_STATS_SEC:         error_msg = None-        stats = get_client_stats(num_rpcs, stats_timeout_sec)+        stats = get_client_stats(_NUM_TEST_RPCS, _WAIT_FOR_STATS_SEC)         rpcs_by_peer = stats.rpcs_by_peer-        for backend in backends:-            if backend not in rpcs_by_peer:-                error_msg = 'Backend %s did not receive load' % backend+        for instance in instance_names:+            if instance not in rpcs_by_peer:+                error_msg = 'Instance %s did not receive load' % instance                 break-        if not error_msg and len(rpcs_by_peer) > len(backends):-            error_msg = 'Unexpected backend received load: %s' % rpcs_by_peer+        if not error_msg and len(rpcs_by_peer) > len(instance_names):+            error_msg = 'Unexpected instance received load: %s' % rpcs_by_peer         if not error_msg:             return     raise Exception(error_msg)  -def test_round_robin(backends, num_rpcs, stats_timeout_sec):+def test_remove_instance_group(gcp, backend_service, instance_group,+                               same_zone_instance_group):+    try:+        patch_backend_instances(gcp,+                                backend_service,+                                [instance_group, same_zone_instance_group],+                                balancing_mode='RATE')+        wait_for_healthy_backends(gcp, backend_service, instance_group)+        wait_for_healthy_backends(gcp, backend_service,+                                  same_zone_instance_group)+        instance_names = get_instance_names(gcp, instance_group)+        same_zone_instance_names = get_instance_names(gcp,+                                                      same_zone_instance_group)+        wait_until_only_given_instances_receive_load(+            instance_names + same_zone_instance_names, _WAIT_FOR_BACKEND_SEC)+        patch_backend_instances(gcp,+                                backend_service, [same_zone_instance_group],+                                balancing_mode='RATE')+        wait_until_only_given_instances_receive_load(same_zone_instance_names,+                                                     _WAIT_FOR_BACKEND_SEC)+    finally:+        patch_backend_instances(gcp, backend_service, [instance_group])+        wait_until_only_given_instances_receive_load(instance_names,+                                                     _WAIT_FOR_BACKEND_SEC)+++def test_round_robin(gcp, backend_service, instance_group):+    wait_for_healthy_backends(gcp, backend_service, instance_group)+    instance_names = get_instance_names(gcp, instance_group)     threshold = 1-    wait_until_only_given_backends_receive_load(backends, stats_timeout_sec)-    stats = get_client_stats(num_rpcs, stats_timeout_sec)+    wait_until_only_given_instances_receive_load(instance_names,+                                                 _WAIT_FOR_STATS_SEC)+    stats = get_client_stats(_NUM_TEST_RPCS, _WAIT_FOR_STATS_SEC)     requests_received = [stats.rpcs_by_peer[x] for x in stats.rpcs_by_peer]     total_requests_received = sum(         [stats.rpcs_by_peer[x] for x in stats.rpcs_by_peer])-    if total_requests_received != num_rpcs:+    if total_requests_received != _NUM_TEST_RPCS:         raise Exception('Unexpected RPC failures', stats)-    expected_requests = total_requests_received / len(backends)-    for backend in backends:-        if abs(stats.rpcs_by_peer[backend] - expected_requests) > threshold:+    expected_requests = total_requests_received / len(instance_names)+    for instance in instance_names:+        if abs(stats.rpcs_by_peer[instance] - expected_requests) > threshold:             raise Exception(-                'RPC peer distribution differs from expected by more than %d for backend %s (%s)',-                threshold, backend, stats)+                'RPC peer distribution differs from expected by more than %d '+                'for instance %s (%s)', threshold, instance, stats)  -def create_instance_template(compute, project, name, grpc_port):+def test_secondary_locality_gets_no_requests_on_partial_primary_failure(+    gcp, backend_service, primary_instance_group,+    secondary_zone_instance_group):+    try:+        patch_backend_instances(+            gcp, backend_service,+            [primary_instance_group, secondary_zone_instance_group])+        wait_for_healthy_backends(gcp, backend_service, primary_instance_group)+        wait_for_healthy_backends(gcp, backend_service,+                                  secondary_zone_instance_group)+        primary_instance_names = get_instance_names(gcp, instance_group)+        secondary_instance_names = get_instance_names(+            gcp, secondary_zone_instance_group)+        wait_until_only_given_instances_receive_load(primary_instance_names,+                                                     _WAIT_FOR_STATS_SEC)+        original_size = len(primary_instance_names)+        resize_instance_group(gcp, primary_instance_group, original_size - 1)+        remaining_instance_names = get_instance_names(gcp,+                                                      primary_instance_group)+        wait_until_only_given_instances_receive_load(remaining_instance_names,+                                                     _WAIT_FOR_BACKEND_SEC)+    finally:+        patch_backend_instances(gcp, backend_service, [primary_instance_group])+        resize_instance_group(gcp, primary_instance_group, original_size)+++def test_secondary_locality_gets_requests_on_primary_failure(+    gcp, backend_service, primary_instance_group,+    secondary_zone_instance_group):+    try:+        patch_backend_instances(+            gcp, backend_service,+            [primary_instance_group, secondary_zone_instance_group])+        wait_for_healthy_backends(gcp, backend_service, primary_instance_group)+        wait_for_healthy_backends(gcp, backend_service,+                                  secondary_zone_instance_group)+        primary_instance_names = get_instance_names(gcp, instance_group)+        secondary_instance_names = get_instance_names(+            gcp, secondary_zone_instance_group)+        wait_until_only_given_instances_receive_load(primary_instance_names,+                                                     _WAIT_FOR_BACKEND_SEC)+        original_size = len(primary_instance_names)+        resize_instance_group(gcp, primary_instance_group, 0)+        wait_until_only_given_instances_receive_load(secondary_instance_names,+                                                     _WAIT_FOR_BACKEND_SEC)++        resize_instance_group(gcp, primary_instance_group, original_size)+        new_instance_names = get_instance_names(gcp, primary_instance_group)+        wait_for_healthy_backends(gcp, backend_service, primary_instance_group)+        wait_until_only_given_instances_receive_load(new_instance_names,+                                                     _WAIT_FOR_BACKEND_SEC)+    finally:+        patch_backend_instances(gcp, backend_service, [primary_instance_group])+++def create_instance_template(gcp, name, network, source_image):","The class-as-state-holder and modifying it via external methods is a common pattern in (at least gRPC) Python's implementation, so just copied it over here (and I think it sort of makes sense in this context, since I'm basically using `gcpstate` as a struct to avoid having huge parameters lists to all of the functions)",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/22193,387910133,2020-03-04T20:15:56Z,tools/run_tests/run_xds_tests.py,"@@ -557,114 +802,226 @@ def start_xds_client(service_port):     return client_process  +class InstanceGroup(object):++    def __init__(self, name, url, zone):+        self.name = name+        self.url = url+        self.zone = zone+++class GcpResource(object):++    def __init__(self, name, url):+        self.name = name+        self.url = url+++class GcpState(object):++    def __init__(self, compute, project):+        self.compute = compute+        self.project = project+        self.health_check = None+        self.health_check_firewall_rule = None+        self.backend_services = []+        self.url_map = None+        self.target_http_proxy = None+        self.global_forwarding_rule = None+        self.service_port = None+        self.instance_template = None+        self.instance_groups = []++    def clean_up(self):+        if self.global_forwarding_rule:+            delete_global_forwarding_rule(self)+        if self.target_http_proxy:+            delete_target_http_proxy(self)+        if self.url_map:+            delete_url_map(self)+        delete_backend_services(self)+        if self.health_check_firewall_rule:+            delete_firewall(self)+        if self.health_check:+            delete_health_check(self)+        delete_instance_groups(self)+        if self.instance_template:+            delete_instance_template(self)++ if args.compute_discovery_document:     with open(args.compute_discovery_document, 'r') as discovery_doc:         compute = googleapiclient.discovery.build_from_document(             discovery_doc.read()) else:     compute = googleapiclient.discovery.build('compute', 'v1') -service_port = None client_process = None  try:-    instance_group_url = None+    gcp = GcpState(compute, args.project_id)+    health_check_name = _BASE_HEALTH_CHECK_NAME + args.gcp_suffix+    firewall_name = _BASE_FIREWALL_RULE_NAME + args.gcp_suffix+    backend_service_name = _BASE_BACKEND_SERVICE_NAME + args.gcp_suffix+    alternate_backend_service_name = _BASE_BACKEND_SERVICE_NAME + '-alternate' + args.gcp_suffix+    url_map_name = _BASE_URL_MAP_NAME + args.gcp_suffix+    service_host_name = _BASE_SERVICE_HOST + args.gcp_suffix+    target_http_proxy_name = _BASE_TARGET_PROXY_NAME + args.gcp_suffix+    forwarding_rule_name = _BASE_FORWARDING_RULE_NAME + args.gcp_suffix+    template_name = _BASE_TARGET_PROXY_NAME + args.gcp_suffix+    instance_group_name = _BASE_INSTANCE_GROUP_NAME + args.gcp_suffix+    same_zone_instance_group_name = _BASE_INSTANCE_GROUP_NAME + '-same-zone' + args.gcp_suffix+    secondary_zone_instance_group_name = _BASE_INSTANCE_GROUP_NAME + '-secondary-zone' + args.gcp_suffix     try:-        health_check_url = create_health_check(compute, PROJECT_ID,-                                               HEALTH_CHECK_NAME)-        create_health_check_firewall_rule(compute, PROJECT_ID,-                                          FIREWALL_RULE_NAME)-        backend_service_url = create_backend_service(compute, PROJECT_ID,-                                                     BACKEND_SERVICE_NAME,-                                                     health_check_url)-        url_map_url = create_url_map(compute, PROJECT_ID, URL_MAP_NAME,-                                     backend_service_url, SERVICE_HOST)-        target_http_proxy_url = create_target_http_proxy(-            compute, PROJECT_ID, TARGET_PROXY_NAME, url_map_url)+        create_health_check(gcp, health_check_name)+        create_health_check_firewall_rule(gcp, firewall_name)+        backend_service = add_backend_service(gcp, backend_service_name)+        alternate_backend_service = add_backend_service(+            gcp, alternate_backend_service_name)+        create_url_map(gcp, url_map_name, backend_service, service_host_name)+        create_target_http_proxy(gcp, target_http_proxy_name)         potential_service_ports = list(args.service_port_range)         random.shuffle(potential_service_ports)         for port in potential_service_ports:             try:-                create_global_forwarding_rule(-                    compute,-                    PROJECT_ID,-                    FORWARDING_RULE_NAME,-                    port,-                    target_http_proxy_url,-                )-                service_port = port+                create_global_forwarding_rule(gcp, forwarding_rule_name, port)+                gcp.service_port = port                 break             except googleapiclient.errors.HttpError as http_error:                 logger.warning(-                    'Got error %s when attempting to create forwarding rule to port %d. Retrying with another port.'-                    % (http_error, port))-        if not service_port:+                    'Got error %s when attempting to create forwarding rule to '+                    'port %d. Retrying with another port.' % (http_error, port))+        if not gcp.service_port:             raise Exception('Failed to pick a service port in the range %s' %                             args.service_port_range)-        template_url = create_instance_template(compute, PROJECT_ID,-                                                TEMPLATE_NAME, service_port)-        instance_group_url = create_instance_group(compute, PROJECT_ID, ZONE,-                                                   INSTANCE_GROUP_NAME,-                                                   INSTANCE_GROUP_SIZE,-                                                   service_port, template_url)-        add_instances_to_backend(compute, PROJECT_ID, BACKEND_SERVICE_NAME,-                                 instance_group_url)+        create_instance_template(gcp, template_name, args.network,+                                 args.source_image)+        instance_group = add_instance_group(gcp, args.zone, instance_group_name,+                                            _INSTANCE_GROUP_SIZE)+        patch_backend_instances(gcp, backend_service, [instance_group])+        same_zone_instance_group = add_instance_group(+            gcp, args.zone, same_zone_instance_group_name, _INSTANCE_GROUP_SIZE)+        secondary_zone_instance_group = add_instance_group(+            gcp, args.secondary_zone, secondary_zone_instance_group_name,+            _INSTANCE_GROUP_SIZE)     except googleapiclient.errors.HttpError as http_error:-        if TOLERATE_GCP_ERRORS:+        if args.tolerate_gcp_errors:","The HttpError class doesn't provide any details other than the string, and I'd prefer not to do string matching...this whole error recovery is quite brittle as-is, but otherwise it is incredibly slow to manually run/debug this script as you'd have to recreate the entire GCP resources each local run. I'm comfortable leaving this as-is for now as it's only for manual runs.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22234,388612507,2020-03-05T22:50:02Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi,"@@ -13,16 +13,31 @@ # limitations under the License.  -cdef bint _grpc_aio_initialized = 0+cdef bint _grpc_aio_initialized = False+# NOTE(lidiz) Theoretically, applications can run in multiple event loops as+# long as they are in the same thread with same magic. However, I don't think+# we should support this use case. So, the gRPC Python Async Stack should use+# a single event loop picked by ""init_grpc_aio"".+cdef object _grpc_aio_loop   def init_grpc_aio():     global _grpc_aio_initialized+    global _grpc_aio_loop      if _grpc_aio_initialized:         return+    else:+        _grpc_aio_initialized = True +    # Anchors the event loop that the gRPC library going to use.+    _grpc_aio_loop = asyncio.get_event_loop()++    # Activates asyncio IO manager     install_asyncio_iomgr()++    # TODO(lidiz) we need a the grpc_shutdown_blocking() counterpart for this",This seems like a pretty big deal. It deserves its own Github issue.,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22234,388790193,2020-03-06T09:17:33Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi,"@@ -13,16 +13,32 @@ # limitations under the License.  -cdef bint _grpc_aio_initialized = 0+cdef bint _grpc_aio_initialized = False+# NOTE(lidiz) Theoretically, applications can run in multiple event loops as+# long as they are in the same thread with same magic. However, I don't think+# we should support this use case. So, the gRPC Python Async Stack should use+# a single event loop picked by ""init_grpc_aio"".+cdef object _grpc_aio_loop","Not sure about this change, Asyncio runs one loop per thread. If the user spins up a new Thread, hence a new Asyncio loop this will lead to an invalid usage of call_soon - explicitly or implicitly because was called behind the scenes by a create_task- when we try to wake up a future that was created in the new loop.There is an outstanding question about how the driver must work in a multithread and multievent loop environment, and TBH we do not need to give an answer to this right now since I would say that this is a not well spread and used pattern (the multithread and multiloop environments)Maybe we should create a Github issue and add it into the Alpha, Beta or GA release. I would be tempted to have this solved by the Beta release.Meanwhile, are we happy about having this pattern that might raise exceptions if the user starts using multiple threads and multiple loops? or should we add a guard for avoiding this pattern and simply raise a RuntimeError when it happens?",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22234,389084248,2020-03-06T18:59:46Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi,"@@ -13,16 +13,32 @@ # limitations under the License.  -cdef bint _grpc_aio_initialized = 0+cdef bint _grpc_aio_initialized = False+# NOTE(lidiz) Theoretically, applications can run in multiple event loops as+# long as they are in the same thread with same magic. However, I don't think+# we should support this use case. So, the gRPC Python Async Stack should use+# a single event loop picked by ""init_grpc_aio"".+cdef object _grpc_aio_loop","> There is an outstanding question about how the driver must work in a multithread and multievent loop environment, and TBH we do not need to give an answer to this right now since I would say that this is a not well spread and used pattern (the multithread and multiloop environments)`asyncio` functions are not allowed to invoke **from other threads** or **from another event loop**, not only powerful functions like `loop.create_task` and `loop.call_soon`, but also `Task.cancel`, `Task.done`, etc.. Otherwise, an exception will be raised by `asyncio` module:```RuntimeError: Non-thread-safe operation invoked on an event loop other than the current one```If we activate the AsyncIO IO manager, the IO operations will inevitably be invoked in other thread, which have given us a hard time to make sync stack and async stack compatible. The first step to avoid the above `RuntimeError` will be picking an event loop.About multiple event loop objects in one thread, I don't think it is a valid use case... Each individual servers and channels can only live in one event loop, and it might take some effort to make them working at full-capacity under multi-loop environment.> Meanwhile, are we happy about having this pattern that might raise exceptions if the user starts using multiple threads and multiple loops? or should we add a guard for avoiding this pattern and simply raise a RuntimeError when it happens?`asyncio` module raises `RuntimeError` for us.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22255,389162840,2020-03-06T21:54:15Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -252,7 +252,25 @@ def __init__(self, target: str, options: ChannelArgumentType,         calls = []         call_tasks = []         for task in tasks:-            stack = task.get_stack(limit=1)+            try:+                stack = task.get_stack(limit=1)+            except AttributeError as attribute_error:+                # NOTE(lidiz) tl;dr: If the Task is created with a CPython+                # object, it will trigger AttributeError.+                #+                # In the global finalizer, the event loop schedules+                # a CPython PyAsyncGenAThrow object.+                # https://github.com/python/cpython/blob/00e45877e33d32bb61aa13a2033e3bba370bda4d/Lib/asyncio/base_events.py#L484+                #+                # However, the PyAsyncGenAThrow object is written in C and+                # failed to include the normal Python frame objects. Hence,+                # this exception is a false negative, and it is safe to ignore+                # the failure (someone should fix it in CPython!).+                # https://github.com/python/cpython/blob/a025d4ca99fb4c652465368e0b4eb03cf4b316b9/Objects/genobject.c#L1989+                if 'frame' in str(attribute_error):","Can we add a TODO with a Github issue pointing out that we should remote the workaround once we drop support for 3.7? That will be years from now, so it's good to document that debt.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22255,389165770,2020-03-06T22:02:29Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -252,7 +252,25 @@ def __init__(self, target: str, options: ChannelArgumentType,         calls = []         call_tasks = []         for task in tasks:-            stack = task.get_stack(limit=1)+            try:+                stack = task.get_stack(limit=1)+            except AttributeError as attribute_error:+                # NOTE(lidiz) tl;dr: If the Task is created with a CPython+                # object, it will trigger AttributeError.+                #+                # In the global finalizer, the event loop schedules+                # a CPython PyAsyncGenAThrow object.+                # https://github.com/python/cpython/blob/00e45877e33d32bb61aa13a2033e3bba370bda4d/Lib/asyncio/base_events.py#L484+                #+                # However, the PyAsyncGenAThrow object is written in C and+                # failed to include the normal Python frame objects. Hence,+                # this exception is a false negative, and it is safe to ignore+                # the failure (someone should fix it in CPython!).+                # https://github.com/python/cpython/blob/a025d4ca99fb4c652465368e0b4eb03cf4b316b9/Objects/genobject.c#L1989+                if 'frame' in str(attribute_error):","It will be available for `3.8.3` and `3.9`, so we will have to wait for the drop support of `3.8`. It's definitely years to come... Since an GitHub issue isn't going to last that long, I think comment in code is better. Let's hope someone will update this path 5-10 years later.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22218,389207214,2020-03-07T00:44:18Z,src/compiler/python_generator.cc,"@@ -570,6 +570,92 @@ bool PrivateGenerator::PrintAddServicerToServer(   return true; } +/* Prints out a service class used as a container for static methods pertaining+ * to a class. This class has the exact name of service written in the "".proto""+ * file, with no suffixes. Since this class merely acts as a namespace, it+ * should never be instantiated.+ */+bool PrivateGenerator::PrintServiceClass(+    const grpc::string& package_qualified_service_name,+    const grpc_generator::Service* service, grpc_generator::Printer* out) {","nit: Should we call it service class? It is a stub, but is named as a service... Do you think something like `SimpleStubClass` is a viable name?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22218,389213103,2020-03-07T01:25:43Z,src/python/grpcio_tests/commands.py,"@@ -193,6 +193,7 @@ class TestGevent(setuptools.Command):         'unit._server_ssl_cert_config_test',         # TODO(https://github.com/grpc/grpc/issues/14901) enable this test         'protoc_plugin._python_plugin_test.PythonPluginTest',+        'protoc_plugin._python_plugin_test.SimpleStubsPluginTest',","[The other test in this file also doesn't work.](https://github.com/grpc/grpc/blob/d8623ba548cc9d65d98bf136d1bd52b7aba1c219/src/python/grpcio_tests/commands.py#L195) After seeing that the failure was due to the setup in the rest of the file, I didn't debug it further.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22218,389213262,2020-03-07T01:26:50Z,src/python/grpcio_tests/tests/protoc_plugin/_python_plugin_test.py,"@@ -503,5 +504,118 @@ def half_duplex_request_iterator():         service.server.stop(None)  +@unittest.skipIf(sys.version_info[0] < 3, ""Unsupported on Python 2."")+class SimpleStubsPluginTest(unittest.TestCase):+    servicer_methods = _ServicerMethods()",Yes. But it's not the pattern that the rest of the tests in this file are using. I decided to prioritize meshing well with the rest of the file over going with something slightly more conventional.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22218,389213966,2020-03-07T01:33:07Z,src/python/grpcio_tests/tests/protoc_plugin/_python_plugin_test.py,"@@ -503,5 +504,118 @@ def half_duplex_request_iterator():         service.server.stop(None)  +@unittest.skipIf(sys.version_info[0] < 3, ""Unsupported on Python 2."")+class SimpleStubsPluginTest(unittest.TestCase):+    servicer_methods = _ServicerMethods()++    class Servicer(service_pb2_grpc.TestServiceServicer):++        def UnaryCall(self, request, context):+            return SimpleStubsPluginTest.servicer_methods.UnaryCall(+                request, context)++        def StreamingOutputCall(self, request, context):+            return SimpleStubsPluginTest.servicer_methods.StreamingOutputCall(+                request, context)++        def StreamingInputCall(self, request_iterator, context):+            return SimpleStubsPluginTest.servicer_methods.StreamingInputCall(+                request_iterator, context)++        def FullDuplexCall(self, request_iterator, context):+            return SimpleStubsPluginTest.servicer_methods.FullDuplexCall(+                request_iterator, context)++        def HalfDuplexCall(self, request_iterator, context):+            return SimpleStubsPluginTest.servicer_methods.HalfDuplexCall(+                request_iterator, context)++    def setUp(self):+        super(SimpleStubsPluginTest, self).setUp()+        self._server = test_common.test_server()+        service_pb2_grpc.add_TestServiceServicer_to_server(+            self.Servicer(), self._server)+        self._port = self._server.add_insecure_port('[::]:0')+        self._server.start()+        self._target = 'localhost:{}'.format(self._port)++    def tearDown(self):+        self._server.stop(None)+        super(SimpleStubsPluginTest, self).tearDown()++    def testUnaryCall(self):+        request = request_pb2.SimpleRequest(response_size=13)+        response = service_pb2_grpc.TestService.UnaryCall(+            request,+            self._target,+            channel_credentials=grpc.experimental.insecure_channel_credentials(+            ),+            wait_for_ready=True)","That's rather vague. How do you mean? Were you expecting insecure to be default? Were you expecting the name to be shorter? FWIW, the default is currently actually `LocalCredentials`, which I was originally using here. But I ran into  https://github.com/grpc/grpc/issues/22257, which is why I'm explicitly specifying here. Eric and Sanjay both have weighed in that explicit is better than implicit for security reasons.`wait_for_ready` is necessary in general to make this test not flaky. Otherwise, there's a race condition between the server thread coming ready and the main test thread sending the request. I don't think relying on the implementation detail that channels are pooled is a good idea. `wait_for_ready` is already a standard solution for this. However, we *could* flip the default to `True` for `wait_for_ready`, but we should probably have a bit more of a conversation about that first.Let's discuss these two points on Monday.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22218,389214506,2020-03-07T01:38:00Z,src/compiler/python_generator.cc,"@@ -570,6 +570,92 @@ bool PrivateGenerator::PrintAddServicerToServer(   return true; } +/* Prints out a service class used as a container for static methods pertaining+ * to a class. This class has the exact name of service written in the "".proto""+ * file, with no suffixes. Since this class merely acts as a namespace, it+ * should never be instantiated.+ */+bool PrivateGenerator::PrintServiceClass(+    const grpc::string& package_qualified_service_name,+    const grpc_generator::Service* service, grpc_generator::Printer* out) {+  StringMap dict;+  dict[""Service""] = service->name();+  out->Print(""\n\n"");+  out->Print("" # This class is part of an EXPERIMENTAL API.\n"");+  out->Print(dict, ""class $Service$(object):\n"");+  {+    IndentScope class_indent(out);+    StringVector service_comments = service->GetAllComments();+    PrintAllComments(service_comments, out);","Thought I was following a pattern from the existing generated code, but it turns out to be something only in the beta generated code. I'll switch this over.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22218,389215240,2020-03-07T01:43:39Z,src/compiler/python_generator.cc,"@@ -570,6 +570,92 @@ bool PrivateGenerator::PrintAddServicerToServer(   return true; } +/* Prints out a service class used as a container for static methods pertaining+ * to a class. This class has the exact name of service written in the "".proto""+ * file, with no suffixes. Since this class merely acts as a namespace, it+ * should never be instantiated.+ */+bool PrivateGenerator::PrintServiceClass(+    const grpc::string& package_qualified_service_name,+    const grpc_generator::Service* service, grpc_generator::Printer* out) {","I'm not married to the name here since this class is just an implementation detail. My hope was that in the future, this class will also serve as a place to add things that pertain to the server. It's called ""Service"" here because it corresponds one-to-one to your Protobuf-level `service` definition. If you feel strongly, I'll change it. Thoughts?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22218,389215444,2020-03-07T01:45:36Z,src/compiler/python_generator.cc,"@@ -570,6 +570,92 @@ bool PrivateGenerator::PrintAddServicerToServer(   return true; } +/* Prints out a service class used as a container for static methods pertaining+ * to a class. This class has the exact name of service written in the "".proto""+ * file, with no suffixes. Since this class merely acts as a namespace, it+ * should never be instantiated.+ */+bool PrivateGenerator::PrintServiceClass(+    const grpc::string& package_qualified_service_name,+    const grpc_generator::Service* service, grpc_generator::Printer* out) {",I'm fine with the naming if we are going to add servicer definition someday. Should we create a TODO issue for it? Or point it to an existing design doc?,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22161,389740906,2020-03-09T14:51:30Z,tools/run_tests/performance/scenario_config.py,"@@ -446,6 +446,20 @@ def scenarios(self):                 server_threads_per_cq=2,                 categories=inproc_categories + [SCALABLE]) +            yield _ping_pong_scenario(+                'cpp_protobuf_async_unary_qps_unconstrained_multiple_channels_%s'","What's the difference between the ""regular"" cpp_protobuf_async_unary_qps_unconstrained scenario and this one?It seems that you tweaked the number of channels to 8 (any other important changes?)Problems:- While it's definitely a nice find that increasing the number of channels improves the throughput, I don't see much point in adding yet another benchmarking scenario (we do have plenty of them, the number for most of them aren't really being tracked and they are not displayed in the dashboard) - with the current state, this is basically just a way to make the benchmarks take one bit longer.- why 8 channels and async_server_threads=8? It seems oddly specific. Is this hand tuned for 8core machines? Keep in mind that for C++ we also have 32-core machines and in some sense they are more interesting because they show the scalability bottlenecks.A few things to investigate:- It would be good to see if this leads to improvements with 32 core setup as well (which is where C++ is performing worse than Java in our OSS benchmarks and we don't quite know why).- If setting channels=N in 32-core machines helps, could this performance gain be translated into better throughput with ""default"" settings but using multiple subchannels internally? (IMHO that should have the same effect as using multiple channels explicitly).- btw, does C-core automatically use multiple subchannels (=connections) for high throughput scenarios? If so, it might be worth checking if the settings are the same internally and externally. (because we do see quite different numbers in OSS and internally).CC @vjpai ",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22234,389798603,2020-03-09T16:15:39Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi,"@@ -13,16 +13,32 @@ # limitations under the License.  -cdef bint _grpc_aio_initialized = 0+cdef bint _grpc_aio_initialized = False+# NOTE(lidiz) Theoretically, applications can run in multiple event loops as+# long as they are in the same thread with same magic. However, I don't think+# we should support this use case. So, the gRPC Python Async Stack should use+# a single event loop picked by ""init_grpc_aio"".+cdef object _grpc_aio_loop","I think that we are on the same page about not sharing RPC objects through different threads/loops if we eventually we support many threads and many loops, we should explicitly prescript to the users to not share objects across different loops and threads, otherwise `RuntimeErros` will start appearing.Also, we are on the same page that the scenario in Asyncio when you have multiple threads is by having 1 loop per thread, other options are not even supported by Asyncio.But at the moment that we are saving at the very beginning which Asyncio loop will be used during the whole application, we explicitly forbidding the pattern for having many threads and many loops, , where theoretically the gRPC library might operate - saying theoretically because has been never tested.My point about raising our own function was for raising explicitly an exception that will tell the user that nowadays the `Aio` module does not work in a multi-thread environment, and neither a multi-loop since we are sticking to the same Asyncio loop here.Also, since we were reducing the options for the user I was wondering if in the future - short, middle e or even a long term - we could start investigating how we could remove this restriction, so giving the chance of running the `Aio` driver in multiple threads and loops environment.While removing this freedom to our users right now can be seen as none nocive, we could end up in a situation where users have a lot of friction for not having this solved.Question: What will happen in test scenarios where tests are executed in a new thread? The following issue might appear: Tests running in a different thread could raise a `RuntimeError` exception if the gRPC Asyncio loop was stick to the parent thread.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22234,389859939,2020-03-09T17:51:51Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi,"@@ -13,16 +13,32 @@ # limitations under the License.  -cdef bint _grpc_aio_initialized = 0+cdef bint _grpc_aio_initialized = False+# NOTE(lidiz) Theoretically, applications can run in multiple event loops as+# long as they are in the same thread with same magic. However, I don't think+# we should support this use case. So, the gRPC Python Async Stack should use+# a single event loop picked by ""init_grpc_aio"".+cdef object _grpc_aio_loop","> But at the moment that we are saving at the very beginning which Asyncio loop will be used during the whole application, we explicitly forbidding the pattern for having **many threads and many loops**, , where theoretically the gRPC library might operate - saying theoretically because has been never tested.I agree. I think supporting ""many-thread and many-loop"" environment is manageable with the https://github.com/grpc/grpc/pull/22258. However, with custom IO manager, we will need more engineering effort.To make this library less error-prone, I think you are right that we should imply less restrictions. I created a TODO issue for the support of new environment https://github.com/grpc/grpc/issues/22272. Can we push this PR forward, and solve the many-thread many-loop problem in a new one (it would need test cases and won't be trivial)?> What will happen in test scenarios where tests are executed in a new thread?This PR let gRPC pick a thread while initializing async mode. So, if the unit tests are initialized in a new thread, then they should be fine to run. If each test case is executed in a different thread, the exception will be thrown.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22161,389867598,2020-03-09T18:04:51Z,tools/run_tests/performance/scenario_config.py,"@@ -446,6 +446,20 @@ def scenarios(self):                 server_threads_per_cq=2,                 categories=inproc_categories + [SCALABLE]) +            yield _ping_pong_scenario(+                'cpp_protobuf_async_unary_qps_unconstrained_multiple_channels_%s'+                % secstr,+                rpc_type='UNARY',+                client_type='ASYNC_CLIENT',+                server_type='ASYNC_SERVER',+                unconstrained_client='async',+                secure=secure,+                channels=8,","btw, the ""unconstrained"" tests seem to be using 64 channels by default:https://github.com/grpc/grpc/blob/be7df211f99a6cd3497bfd55203cb9e6801545f7/tools/run_tests/performance/scenario_config.py#L45, so I'm not sure this setting is useful. (or maybe we're actually using too many channel?)",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22161,389868739,2020-03-09T18:06:57Z,tools/run_tests/performance/scenario_config.py,"@@ -446,6 +446,20 @@ def scenarios(self):                 server_threads_per_cq=2,                 categories=inproc_categories + [SCALABLE]) +            yield _ping_pong_scenario(+                'cpp_protobuf_async_unary_qps_unconstrained_multiple_channels_%s'+                % secstr,+                rpc_type='UNARY',+                client_type='ASYNC_CLIENT',+                server_type='ASYNC_SERVER',+                unconstrained_client='async',+                secure=secure,+                channels=8,","If `channels` are not specified, it is 64 by default. https://github.com/grpc/grpc/blob/be7df211f99a6cd3497bfd55203cb9e6801545f7/tools/run_tests/performance/scenario_config.py#L187However, each channel map to a completion queue, and a completion queue maybe bound to one-or-more threads.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22258,389902725,2020-03-09T19:09:49Z,tools/bazel.rc,"@@ -89,3 +89,5 @@ build:basicprof --copt=-DGRPC_BASIC_PROFILER build:basicprof --copt=-DGRPC_TIMERS_RDTSC  build:python_single_threaded_unary_stream --test_env=""GRPC_SINGLE_THREADED_UNARY_STREAM=true""++build:python_poller_engine --test_env=""GRPC_ASYNCIO_ENGINE=poller""",:+1: for this pattern.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22258,389962180,2020-03-09T21:07:28Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/iomgr/timer.pyx.pxi,"@@ -16,30 +16,32 @@ cdef class _AsyncioTimer:     def __cinit__(self):         self._grpc_timer = NULL-        self._timer_handler = None-        self._active = 0+        self._timer_future = None+        self._active = False+        cpython.Py_INCREF(self)","Yes. In Line 34, and Line 47, the `_AsyncioTimer` is ""decref""ed. This is the same pattern we used for `CallbackWrapper` that we manually modify refcount to simulate the behavior that Core is holding its reference. Otherwise, it will segfault.",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22258,389963400,2020-03-09T21:10:06Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pxd.pxi,"@@ -0,0 +1,32 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++cdef class BaseCompletionQueue:++    cdef grpc_completion_queue* c_ptr(self)++cdef class PollerCompletionQueue(BaseCompletionQueue):+    cdef grpc_completion_queue *_cq",can be this attribute moved to the base class?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22258,389972973,2020-03-09T21:30:34Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi,"@@ -13,25 +13,97 @@ # limitations under the License.  -cdef bint _grpc_aio_initialized = 0+cdef bint _grpc_aio_initialized = False+# NOTE(lidiz) Theoretically, applications can run in multiple event loops as+# long as they are in the same thread with same magic. This is not a supported+# use case. So, the gRPC Python Async Stack should use a single event loop+# picked by ""init_grpc_aio"".+cdef object _grpc_aio_loop  # asyncio.AbstractEventLoop+cdef int64_t _event_loop_thread_ident+cdef str _GRPC_ASYNCIO_ENGINE = os.environ.get('GRPC_ASYNCIO_ENGINE', 'default').lower()+grpc_aio_engine = None+cdef object _grpc_initialization_lock = threading.Lock()+++class AsyncIOEngine(enum.Enum):+    DEFAULT = 'default'","does add any value having an enum option for `default` one when this is technically the `CUSTOM_IO_MANAGER`? maybe more the default variable as a separated variable, like:```_DEFAULT_IO_ENGINE = AsyncIOEngine.CUSTOM_IO_POLLER```",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22218,390026519,2020-03-10T00:04:57Z,src/python/grpcio_tests/tests/protoc_plugin/_python_plugin_test.py,"@@ -503,5 +504,118 @@ def half_duplex_request_iterator():         service.server.stop(None)  +@unittest.skipIf(sys.version_info[0] < 3, ""Unsupported on Python 2."")+class SimpleStubsPluginTest(unittest.TestCase):+    servicer_methods = _ServicerMethods()++    class Servicer(service_pb2_grpc.TestServiceServicer):++        def UnaryCall(self, request, context):+            return SimpleStubsPluginTest.servicer_methods.UnaryCall(+                request, context)++        def StreamingOutputCall(self, request, context):+            return SimpleStubsPluginTest.servicer_methods.StreamingOutputCall(+                request, context)++        def StreamingInputCall(self, request_iterator, context):+            return SimpleStubsPluginTest.servicer_methods.StreamingInputCall(+                request_iterator, context)++        def FullDuplexCall(self, request_iterator, context):+            return SimpleStubsPluginTest.servicer_methods.FullDuplexCall(+                request_iterator, context)++        def HalfDuplexCall(self, request_iterator, context):+            return SimpleStubsPluginTest.servicer_methods.HalfDuplexCall(+                request_iterator, context)++    def setUp(self):+        super(SimpleStubsPluginTest, self).setUp()+        self._server = test_common.test_server()+        service_pb2_grpc.add_TestServiceServicer_to_server(+            self.Servicer(), self._server)+        self._port = self._server.add_insecure_port('[::]:0')+        self._server.start()+        self._target = 'localhost:{}'.format(self._port)++    def tearDown(self):+        self._server.stop(None)+        super(SimpleStubsPluginTest, self).tearDown()++    def testUnaryCall(self):+        request = request_pb2.SimpleRequest(response_size=13)+        response = service_pb2_grpc.TestService.UnaryCall(+            request,+            self._target,+            channel_credentials=grpc.experimental.insecure_channel_credentials(+            ),+            wait_for_ready=True)",Temporarily removed the default and added a TODO. I'm looking into some longer term solutions to this problem.,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22218,390026784,2020-03-10T00:05:49Z,src/compiler/python_generator.cc,"@@ -570,6 +570,92 @@ bool PrivateGenerator::PrintAddServicerToServer(   return true; } +/* Prints out a service class used as a container for static methods pertaining+ * to a class. This class has the exact name of service written in the "".proto""+ * file, with no suffixes. Since this class merely acts as a namespace, it+ * should never be instantiated.+ */+bool PrivateGenerator::PrintServiceClass(+    const grpc::string& package_qualified_service_name,+    const grpc_generator::Service* service, grpc_generator::Printer* out) {+  StringMap dict;+  dict[""Service""] = service->name();+  out->Print(""\n\n"");+  out->Print("" # This class is part of an EXPERIMENTAL API.\n"");+  out->Print(dict, ""class $Service$(object):\n"");+  {+    IndentScope class_indent(out);+    StringVector service_comments = service->GetAllComments();+    PrintAllComments(service_comments, out);","Fixed for not just this class, but all generated code.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22258,390176178,2020-03-10T09:11:20Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi,"@@ -13,25 +13,97 @@ # limitations under the License.  -cdef bint _grpc_aio_initialized = 0+cdef bint _grpc_aio_initialized = False+# NOTE(lidiz) Theoretically, applications can run in multiple event loops as+# long as they are in the same thread with same magic. This is not a supported+# use case. So, the gRPC Python Async Stack should use a single event loop+# picked by ""init_grpc_aio"".+cdef object _grpc_aio_loop  # asyncio.AbstractEventLoop+cdef int64_t _event_loop_thread_ident+cdef str _GRPC_ASYNCIO_ENGINE = os.environ.get('GRPC_ASYNCIO_ENGINE', 'default').lower()+grpc_aio_engine = None+cdef object _grpc_initialization_lock = threading.Lock()+++class AsyncIOEngine(enum.Enum):+    DEFAULT = 'default'+    CUSTOM_IO_MANAGER = 'custom'+    POLLER = 'poller'   def init_grpc_aio():     global _grpc_aio_initialized+    global _grpc_aio_loop+    global _event_loop_thread_ident+    global grpc_aio_engine++    with _grpc_initialization_lock:+        # Marks this function as called+        if _grpc_aio_initialized:+            return+        else:+            _grpc_aio_initialized = True++        # Picks the engine for gRPC AsyncIO Stack+        for engine_type in AsyncIOEngine:+            if engine_type.value == _GRPC_ASYNCIO_ENGINE:+                grpc_aio_engine = engine_type+                break+        if grpc_aio_engine is None or grpc_aio_engine is AsyncIOEngine.DEFAULT:+            grpc_aio_engine = AsyncIOEngine.CUSTOM_IO_MANAGER++        # Anchors the event loop that the gRPC library going to use.+        _grpc_aio_loop = asyncio.get_event_loop()+        _event_loop_thread_ident = threading.current_thread().ident++        if grpc_aio_engine is AsyncIOEngine.CUSTOM_IO_MANAGER:+            # Activates asyncio IO manager.+            # NOTE(lidiz) Custom IO manager must be activated before the first+            # `grpc_init()`. Otherwise, some special configurations in Core won't+            # pick up the change, and resulted in SEGFAULT or ABORT.+            install_asyncio_iomgr()++            # TODO(https://github.com/grpc/grpc/issues/22244) we need a the+            # grpc_shutdown_blocking() counterpart for this call. Otherwise, the gRPC+            # library won't shutdown cleanly.+            grpc_init()++            # Timers are triggered by the Asyncio loop. We disable+            # the background thread that is being used by the native+            # gRPC iomgr.+            grpc_timer_manager_set_threading(False)++            # gRPC callbaks are executed within the same thread used by the Asyncio+            # event loop, as it is being done by the other Asyncio callbacks.+            Executor.SetThreadingAll(False)+        else:+            # TODO(https://github.com/grpc/grpc/issues/22244) we need a the+            # grpc_shutdown_blocking() counterpart for this call. Otherwise, the gRPC+            # library won't shutdown cleanly.+            grpc_init()+++def grpc_aio_loop():+    """"""Returns the one-and-only gRPC Aio event loop.""""""+    return _grpc_aio_loop -    if _grpc_aio_initialized:-        return -    install_asyncio_iomgr()-    grpc_init()+def grpc_schedule_coroutine(object coro):+    """"""Thread-safely schedules coroutine to gRPC Aio event loop. -    # Timers are triggered by the Asyncio loop. We disable-    # the background thread that is being used by the native-    # gRPC iomgr.-    grpc_timer_manager_set_threading(False)+    If invoked within the same thread as the event loop, return an+    Asyncio.Task. Otherwise, return a concurrent.futures.Future (the sync+    Future). For non-asyncio threads, sync Future objects are probably easier+    to handle (without worrying other thread-safety stuff).+    """"""+    if _event_loop_thread_ident != threading.current_thread().ident:+        return asyncio.run_coroutine_threadsafe(coro, _grpc_aio_loop)+    else:+        return _grpc_aio_loop.create_task(coro) -    # gRPC callbaks are executed within the same thread used by the Asyncio-    # event loop, as it is being done by the other Asyncio callbacks.-    Executor.SetThreadingAll(False) -    _grpc_aio_initialized = 1+def grpc_call_soon_threadsafe(object func, *args):+    # TODO(lidiz) After we are confident, we can drop this assert. Otherwsie,+    # we should limit this function to non-grpc-event-loop thread.+    assert _event_loop_thread_ident != threading.current_thread().ident","Just my personal taste, not blocking the PR, I would prefer to defer the addition of the `assert` once we know that the function could be called wrongly from the same thread, till this does not happen I would prefer to no have to have the `assert` which right now is not really useful.This could be done by narrowing the scope of the function and moving the function to the `PollerCompletionQueue` class or just removing the assert for now.As I said, personal taste. ",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22258,390485232,2020-03-10T17:26:23Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/grpc_aio.pyx.pxi,"@@ -13,25 +13,97 @@ # limitations under the License.  -cdef bint _grpc_aio_initialized = 0+cdef bint _grpc_aio_initialized = False+# NOTE(lidiz) Theoretically, applications can run in multiple event loops as+# long as they are in the same thread with same magic. This is not a supported+# use case. So, the gRPC Python Async Stack should use a single event loop+# picked by ""init_grpc_aio"".+cdef object _grpc_aio_loop  # asyncio.AbstractEventLoop+cdef int64_t _event_loop_thread_ident+cdef str _GRPC_ASYNCIO_ENGINE = os.environ.get('GRPC_ASYNCIO_ENGINE', 'default').lower()+grpc_aio_engine = None+cdef object _grpc_initialization_lock = threading.Lock()+++class AsyncIOEngine(enum.Enum):+    DEFAULT = 'default'+    CUSTOM_IO_MANAGER = 'custom'+    POLLER = 'poller'   def init_grpc_aio():     global _grpc_aio_initialized+    global _grpc_aio_loop+    global _event_loop_thread_ident+    global grpc_aio_engine++    with _grpc_initialization_lock:+        # Marks this function as called+        if _grpc_aio_initialized:+            return+        else:+            _grpc_aio_initialized = True++        # Picks the engine for gRPC AsyncIO Stack+        for engine_type in AsyncIOEngine:+            if engine_type.value == _GRPC_ASYNCIO_ENGINE:+                grpc_aio_engine = engine_type+                break+        if grpc_aio_engine is None or grpc_aio_engine is AsyncIOEngine.DEFAULT:+            grpc_aio_engine = AsyncIOEngine.CUSTOM_IO_MANAGER++        # Anchors the event loop that the gRPC library going to use.+        _grpc_aio_loop = asyncio.get_event_loop()+        _event_loop_thread_ident = threading.current_thread().ident++        if grpc_aio_engine is AsyncIOEngine.CUSTOM_IO_MANAGER:+            # Activates asyncio IO manager.+            # NOTE(lidiz) Custom IO manager must be activated before the first+            # `grpc_init()`. Otherwise, some special configurations in Core won't+            # pick up the change, and resulted in SEGFAULT or ABORT.+            install_asyncio_iomgr()++            # TODO(https://github.com/grpc/grpc/issues/22244) we need a the+            # grpc_shutdown_blocking() counterpart for this call. Otherwise, the gRPC+            # library won't shutdown cleanly.+            grpc_init()++            # Timers are triggered by the Asyncio loop. We disable+            # the background thread that is being used by the native+            # gRPC iomgr.+            grpc_timer_manager_set_threading(False)++            # gRPC callbaks are executed within the same thread used by the Asyncio+            # event loop, as it is being done by the other Asyncio callbacks.+            Executor.SetThreadingAll(False)+        else:+            # TODO(https://github.com/grpc/grpc/issues/22244) we need a the+            # grpc_shutdown_blocking() counterpart for this call. Otherwise, the gRPC+            # library won't shutdown cleanly.+            grpc_init()+++def grpc_aio_loop():+    """"""Returns the one-and-only gRPC Aio event loop.""""""+    return _grpc_aio_loop -    if _grpc_aio_initialized:-        return -    install_asyncio_iomgr()-    grpc_init()+def grpc_schedule_coroutine(object coro):+    """"""Thread-safely schedules coroutine to gRPC Aio event loop. -    # Timers are triggered by the Asyncio loop. We disable-    # the background thread that is being used by the native-    # gRPC iomgr.-    grpc_timer_manager_set_threading(False)+    If invoked within the same thread as the event loop, return an+    Asyncio.Task. Otherwise, return a concurrent.futures.Future (the sync+    Future). For non-asyncio threads, sync Future objects are probably easier+    to handle (without worrying other thread-safety stuff).+    """"""+    if _event_loop_thread_ident != threading.current_thread().ident:+        return asyncio.run_coroutine_threadsafe(coro, _grpc_aio_loop)+    else:+        return _grpc_aio_loop.create_task(coro) -    # gRPC callbaks are executed within the same thread used by the Asyncio-    # event loop, as it is being done by the other Asyncio callbacks.-    Executor.SetThreadingAll(False) -    _grpc_aio_initialized = 1+def grpc_call_soon_threadsafe(object func, *args):+    # TODO(lidiz) After we are confident, we can drop this assert. Otherwsie,+    # we should limit this function to non-grpc-event-loop thread.+    assert _event_loop_thread_ident != threading.current_thread().ident","As you suggested, if we are moving toward 1-cq per process pattern, we will need to keep closer track of which loop object does the future belong to. Hence, the `aio_loop_call_soon_threadsafe` won't be needed, as we will invoke `call_soon_threadsafe` directly on the loop object.I will update it in following PR.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22161,390807901,2020-03-11T08:26:49Z,tools/run_tests/performance/scenario_config.py,"@@ -446,6 +446,20 @@ def scenarios(self):                 server_threads_per_cq=2,                 categories=inproc_categories + [SCALABLE]) +            yield _ping_pong_scenario(+                'cpp_protobuf_async_unary_qps_unconstrained_multiple_channels_%s'","> During the experiment, I found the issue with the OSS cpp scenario is majorly its configuration. If it works for 8-core, we can perform the same hand tuning for 32-core as well. Cpp folks should have better knowledge to explain the details of its qps worker..That's a really good find but since the same scenario config is used for both 8core and 32core workers, this PR as such isn't as useful.Also, we'd want the default scenario to perform well, not the special one.The naming ""_multiple_channels_"" makes no sense because the ""default"" unconstrained scenario actually uses more channels than this one.Do you know what is the number of channels that we use for the internal scenario?",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22306,391308066,2020-03-11T22:32:59Z,src/python/grpcio/grpc/experimental/aio/_metadata.py,"@@ -0,0 +1,72 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Implementation of the metadata abstraction for gRPC Asyncio Python.""""""+from typing import List, Tuple, AnyStr, Iterator, Any+from collections import abc, OrderedDict+++class Metadata(abc.Mapping):+    """"""Metadata abstraction for the asynchronous calls and interceptors.++    The metadata is a mapping from str -> List[str]++    Traits+        * Multiple entries are allowed for the same key+        * The order of the values by key is preserved+        * Getting by an element by key, retrieves the first mapped value+        * Supports an immutable view of the data+    """"""++    def __init__(self, *args) -> None:+        self._metadata = OrderedDict()+        for md_key, md_value in args:+            self.add(md_key, md_value)++    def add(self, key: str, value: str) -> None:+        key = key.lower()+        self._metadata.setdefault(key, [])+        self._metadata[key].append(value)++    def __len__(self) -> int:+        return len(self._metadata)++    def __getitem__(self, key: str) -> str:+        try:+            first, *_ = self._metadata[key.lower()]",This seems copying entire list. Can we simply get the first element out?,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22306,391310088,2020-03-11T22:38:27Z,src/python/grpcio/grpc/experimental/aio/_metadata.py,"@@ -0,0 +1,72 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Implementation of the metadata abstraction for gRPC Asyncio Python.""""""+from typing import List, Tuple, AnyStr, Iterator, Any+from collections import abc, OrderedDict+++class Metadata(abc.Mapping):+    """"""Metadata abstraction for the asynchronous calls and interceptors.++    The metadata is a mapping from str -> List[str]++    Traits+        * Multiple entries are allowed for the same key+        * The order of the values by key is preserved+        * Getting by an element by key, retrieves the first mapped value+        * Supports an immutable view of the data","What if the user want to modify a value / delete a value?This class seems not intended to be entirely immutable, that's good for usability. Do we need to have a read-only mode? Does `aiohttp` provides that?",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22306,391660538,2020-03-12T14:30:10Z,src/python/grpcio/grpc/experimental/aio/_metadata.py,"@@ -0,0 +1,72 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Implementation of the metadata abstraction for gRPC Asyncio Python.""""""+from typing import List, Tuple, AnyStr, Iterator, Any+from collections import abc, OrderedDict+++class Metadata(abc.Mapping):+    """"""Metadata abstraction for the asynchronous calls and interceptors.++    The metadata is a mapping from str -> List[str]++    Traits+        * Multiple entries are allowed for the same key+        * The order of the values by key is preserved+        * Getting by an element by key, retrieves the first mapped value+        * Supports an immutable view of the data+    """"""++    def __init__(self, *args) -> None:+        self._metadata = OrderedDict()+        for md_key, md_value in args:+            self.add(md_key, md_value)++    def add(self, key: str, value: str) -> None:+        key = key.lower()+        self._metadata.setdefault(key, [])+        self._metadata[key].append(value)++    def __len__(self) -> int:+        return len(self._metadata)++    def __getitem__(self, key: str) -> str:+        try:+            first, *_ = self._metadata[key.lower()]+            return first+        except ValueError as e:+            raise KeyError(""{0!r}"".format(key)) from e++    def __iter__(self) -> Iterator[Tuple[AnyStr, AnyStr]]:+        for key, values in self._metadata.items():+            for value in values:+                yield (key, value)++    def view(self) -> Tuple[AnyStr, AnyStr]:+        return tuple(self)++    def get_all(self, key: str) -> List[str]:+        """"""For compatibility with other Metadata abstraction objects (like in Java),+        this would return all items under the desired <key>.+        """"""+        return self._metadata.get(key.lower(), [])","question, should we return an empty list or a `KeyErorr` exception if the key could not be found? what other libraries do?",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22311,392301617,2020-03-13T15:32:09Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi,"@@ -35,6 +35,8 @@ cdef class CallbackWrapper:     def __cinit__(self, object future, CallbackFailureHandler failure_handler):         self.context.functor.functor_run = self.functor_run         self.context.waiter = <cpython.PyObject*>future+        # TODO(lidiz) switch to future.get_loop() which is available 3.7+.+        self.context.loop = <cpython.PyObject*>future._loop","Technically we could and maybe we should make the plumbing of the loop at the up call level, so instead of inferring the loop by accessing the none public attribute create the context by passing as a parameter the loop. It's a pity that `future.get_loop()` is only available in Python versions 3.7 and beyond.But TBH we might take the risk since we are gonna support only Python 3.6 and its hard to believe that this internal attribute will be changed in patch release of the 3.6. But then we would need to do something like:```pythonif python_version > (3, 6):    loop = future.get_loop()else:    loop = future._loop```",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22311,392308758,2020-03-13T15:44:13Z,src/python/grpcio_tests/tests_aio/unit/compatibility_test.py,"@@ -0,0 +1,215 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Testing the compatibility between AsyncIO stack and the old stack.""""""++import asyncio+import logging+import os+import unittest+import threading+from concurrent.futures import ThreadPoolExecutor+import time+import random+from typing import Callable, Sequence, Tuple++import grpc+from grpc.experimental import aio+from grpc._cython import cygrpc++from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants+from tests.unit.framework.common import get_socket+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit import _common++_NUM_STREAM_RESPONSES = 5+_REQUEST_PAYLOAD_SIZE = 7+_RESPONSE_PAYLOAD_SIZE = 42+++def _unique_options() -> Sequence[Tuple[str, float]]:+    return (('iv', random.random()),)+++@unittest.skipIf(cygrpc.grpc_aio_engine() != cygrpc.AsyncIOEngine.POLLER,+                 'Compatible mode needs POLLER completion queue.')+class TestCompatibility(AioTestBase):++    async def setUp(self):+        address, self._async_server = await start_test_server()+        # Create async stub+        self._async_channel = aio.insecure_channel(address,+                                                   options=_unique_options())+        self._async_stub = test_pb2_grpc.TestServiceStub(self._async_channel)++        # Create sync stub+        self._sync_channel = grpc.insecure_channel(address,+                                                   options=_unique_options())+        self._sync_stub = test_pb2_grpc.TestServiceStub(self._sync_channel)++    async def tearDown(self):+        self._sync_channel.close()+        await self._async_channel.close()+        await self._async_server.stop(None)++    async def _run_in_another_thread(self, func: Callable[[], None]):+        work_done = asyncio.Event()++        def thread_work():+            func()+            self.loop.call_soon_threadsafe(work_done.set)++        thread = threading.Thread(target=thread_work)+        thread.daemon = True",constructor parameter of `Thread`?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22311,392392873,2020-03-13T18:12:49Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/common.pyx.pxi,"@@ -99,3 +99,16 @@ class AbortError(BaseError):  class InternalError(BaseError):     """"""Raised upon unexpected errors in native code.""""""+++def schedule_coro_threadsafe(object coro, object loop):+    try:+        return loop.create_task(coro)+    except RuntimeError as runtime_error:+        if 'Non-thread-safe operation' in str(runtime_error):",Is there a less heuristic way of classifying this exception? Human-readable strings probably aren't part of the API contract.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22311,392467670,2020-03-13T20:50:15Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/common.pyx.pxi,"@@ -99,3 +99,16 @@ class AbortError(BaseError):  class InternalError(BaseError):     """"""Raised upon unexpected errors in native code.""""""+++def schedule_coro_threadsafe(object coro, object loop):+    try:+        return loop.create_task(coro)+    except RuntimeError as runtime_error:+        if 'Non-thread-safe operation' in str(runtime_error):","I don't have a better solution than this one, since they are simply raising a `RuntimeError`. https://github.com/python/cpython/blob/374d998b507d34a6c0a3816a163926a8ba0c483f/Lib/asyncio/base_events.py#L785Alternatives are 1) track thread ident ourselves; 2) act upon all `RuntimeError`. Former one introduces extra complexity, and latter one seems error-prone.",
14166415,sanjaypujare,https://api.github.com/repos/grpc/grpc/pulls/22308,392720902,2020-03-15T22:42:37Z,include/grpc/grpc_security.h,"@@ -834,30 +838,73 @@ typedef void (*grpc_tls_on_credential_reload_done_cb)(     grpc_tls_credential_reload_arg* arg);  /** A struct containing all information necessary to schedule/cancel a-    credential reload request.-    - cb and cb_user_data represent a gRPC-provided-      callback and an argument passed to it.-    - key_materials_config is an in/output parameter containing currently-      used/newly reloaded credentials. If credential reload does not result in-      a new credential, key_materials_config should not be modified. The same-      key_materials_config object can be updated if new key materials is-      available.-    - status and error_details are used to hold information about-      errors occurred when a credential reload request is scheduled/cancelled.-    - config is a pointer to the unique grpc_tls_credential_reload_config-      instance that this argument corresponds to.-    - context is a pointer to a wrapped language implementation of this-      grpc_tls_credential_reload_arg instance.-    - destroy_context is a pointer to a caller-provided method that cleans-      up any data associated with the context pointer.+    credential reload request. To help understand each fields, we mark each+    field with labels. Here are their explanations+    -------------------------label explanation----------------------------------+    [USER-MANAGED]: fields that are managed directly by the end users+    [SYSTEM-PROVIDED]: fields that needed to retrieve implementations in C core+    and would be provided by the gRPC stack+    [WRAP-LANG]: fields that needed to retrieve the particular implementation in+    wrap language+    [ASYNC_ONLY]: fields that only used when in asynchronous mode. If a field+    doesn't have this label, we can assume it should be applied to both+    synchronous and asynchronous mode.+    Note that if a field is marked both as [USER-MANAGED] and [SYSTEM-PROVIDED],+    it indicates that the field is initiated by the gRPC stack, but users+    need to properly manage them to correctly perform credential reloading.+    ----------------------label explanation ends--------------------------------+    - cb[USER-MANAGED][ASYNC_ONLY]: a callback-function provided for users to+      indicate an asynchronous reloading operation is complete. After calling+      this function, gRPC stack would be notified and continue rest of the+      authentication.+    - cb_user_data[SYSTEM-PROVIDED][ASYNC_ONLY]: a pointer used to retrieve the+      corresponding security connector implementation. This is needed because+      we need to use the |peer_checked| closure in security connector after+      users call |cb|.+    - key_materials_config[USER-MANAGED][SYSTEM-PROVIDED]: an in/output+      parameter containing currently used/newly reloaded credentials. Users can+      assume it always points to a non-null object.+      If users set |status| to |GRPC_SSL_CERTIFICATE_CONFIG_RELOAD_UNCHANGED|,+      key_materials_config should not be modified.+      If set to |GRPC_SSL_CERTIFICATE_CONFIG_RELOAD_NEW|, key_materials_config+      object can be updated by the new credentials.+      Note that, key_materials_config might be shared by multiple connections,+      because each key_materials_config is attached to a security_connector, and+          1. on client side, if retry mechanism happens, one security_connector+          can map to multiple connections+          2. on server side, all connections share the same security_connector+      But the reload logic is guaranteed to be invoked in every connection.+    - status[USER-MANAGED]: a per-connection parameter to indicate the finish+      status of a schedule/cancel request.+      If set to |GRPC_SSL_CERTIFICATE_CONFIG_RELOAD_UNCHANGED|, gRPC stack+      would take current |grpc_tls_key_materials_config| as credentials;+      If set to |GRPC_SSL_CERTIFICATE_CONFIG_RELOAD_NEW|, gRPC stack would+      perform credential reloading, using newly loaded |key_materials_config|;+      If set to |GRPC_SSL_CERTIFICATE_CONFIG_RELOAD_FAIL|, users also need to+      set |error_details|.+    - err_details[USER-MANAGED][SYSTEM-PROVIDED]: a per-connection",A suggestion: both status and err_details are used together in `grpc_tls_credential_reload_arg` and `grpc_tls_server_authorization_check_arg` . Why not make status part of `grpc_tls_err_details` (and rename it to `grpc_tls_status_details`) so have one less field in both these complex structs?,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22359,392957970,2020-03-16T11:43:12Z,tools/run_tests/python_utils/upload_rbe_results.py,"@@ -189,8 +216,9 @@ def _get_resultstore_data(api_key, invocation_id):         elif 'tests' not in action['testAction']['testSuite']:             continue         else:-            test_cases = action['testAction']['testSuite']['tests'][0][","not that surprisingly, it turns out that some  'tests' elements have multiple groups of tests. The original version of the code just skips them. This only actually happens for a few tests so the overall quality of the old bigquery data is still pretty good.",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22308,393290129,2020-03-16T20:25:27Z,src/core/lib/security/security_connector/tls/tls_security_connector.cc,"@@ -104,14 +105,14 @@ grpc_status_code TlsFetchKeyMaterials(         gpr_log(GPR_DEBUG, ""Credential does not change after reload."");       } else if (arg->status == GRPC_SSL_CERTIFICATE_CONFIG_RELOAD_FAIL) {         gpr_log(GPR_ERROR, ""Credential reload failed with an error:"");-        if (arg->error_details != nullptr) {-          gpr_log(GPR_ERROR, ""%s"", arg->error_details);+        if (arg->error_details->has_error()) {","Yeah, Thanks for pointing this out! Currently we don't have the test for this. I will add a unit test for the class. ",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22308,393330522,2020-03-16T21:52:54Z,src/core/lib/security/security_connector/tls/tls_security_connector.cc,"@@ -88,6 +88,7 @@ grpc_status_code TlsFetchKeyMaterials(   if (credential_reload_config != nullptr) {     grpc_tls_credential_reload_arg* arg = new grpc_tls_credential_reload_arg();     arg->key_materials_config = key_materials_config.get();+    arg->error_details = new grpc_tls_err_details();","making a full object would require the `arg` to have a constructor, which is the next step I am going to take. My goal is to make the `arg` a class instead of a struct here. ",
14166415,sanjaypujare,https://api.github.com/repos/grpc/grpc/pulls/22308,393370342,2020-03-16T23:42:45Z,src/core/lib/security/security_connector/tls/tls_security_connector.cc,"@@ -88,6 +88,7 @@ grpc_status_code TlsFetchKeyMaterials(   if (credential_reload_config != nullptr) {     grpc_tls_credential_reload_arg* arg = new grpc_tls_credential_reload_arg();     arg->key_materials_config = key_materials_config.get();+    arg->error_details = new grpc_tls_err_details();",Good point but I don't see that as a problem. Also the class vs struct distinction seems quite irrelevant at this point unless I am missing something.,
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/22306,393759876,2020-03-17T15:22:30Z,src/python/grpcio/grpc/experimental/aio/_metadata.py,"@@ -0,0 +1,72 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Implementation of the metadata abstraction for gRPC Asyncio Python.""""""+from typing import List, Tuple, AnyStr, Iterator, Any+from collections import abc, OrderedDict+++class Metadata(abc.Mapping):+    """"""Metadata abstraction for the asynchronous calls and interceptors.++    The metadata is a mapping from str -> List[str]++    Traits+        * Multiple entries are allowed for the same key+        * The order of the values by key is preserved+        * Getting by an element by key, retrieves the first mapped value+        * Supports an immutable view of the data+    """"""++    def __init__(self, *args) -> None:+        self._metadata = OrderedDict()+        for md_key, md_value in args:+            self.add(md_key, md_value)++    def add(self, key: str, value: str) -> None:+        key = key.lower()+        self._metadata.setdefault(key, [])+        self._metadata[key].append(value)++    def __len__(self) -> int:+        return len(self._metadata)++    def __getitem__(self, key: str) -> str:+        try:+            first, *_ = self._metadata[key.lower()]+            return first+        except ValueError as e:+            raise KeyError(""{0!r}"".format(key)) from e++    def __iter__(self) -> Iterator[Tuple[AnyStr, AnyStr]]:+        for key, values in self._metadata.items():+            for value in values:+                yield (key, value)++    def view(self) -> Tuple[AnyStr, AnyStr]:+        return tuple(self)++    def get_all(self, key: str) -> List[str]:+        """"""For compatibility with other Metadata abstraction objects (like in Java),+        this would return all items under the desired <key>.+        """"""+        return self._metadata.get(key.lower(), [])","For consistency with ``dict.get`` and being this object a wrapper over a list of dictionaries, it would make sense to return a ``[]`` here, and keep the ``KeyError`` for the ``__getitem__``",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/22363,393799722,2020-03-17T16:15:19Z,test/core/util/port_isolated_runtime_environment.cc,"@@ -43,14 +43,26 @@ static int get_random_port_offset() { static int s_initial_offset = get_random_port_offset(); static gpr_atm s_pick_counter = 0; -int grpc_pick_unused_port_or_die(void) {+static int pick_unused_port_or_die(void) {",Can you rename this to `grpc_pick_unused_port_or_die_impl` like the same function in port.cc.,
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/22375,393835974,2020-03-17T17:07:56Z,src/core/tsi/ssl_transport_security.cc,"@@ -1955,16 +1954,15 @@ tsi_result tsi_create_ssl_server_handshaker_factory_with_options(    for (i = 0; i < options->num_key_cert_pairs; i++) {     do {-#if defined(OPENSSL_NO_TLS1_2_METHOD) || OPENSSL_API_COMPAT >= 0x10100000L       impl->ssl_contexts[i] = SSL_CTX_new(TLS_method());-#else-      impl->ssl_contexts[i] = SSL_CTX_new(TLSv1_2_method());-#endif       if (impl->ssl_contexts[i] == nullptr) {         gpr_log(GPR_ERROR, ""Could not create ssl context."");         result = TSI_OUT_OF_RESOURCES;         break;       }+#if OPENSSL_VERSION_NUMBER >= 0x10100000+      SSL_CTX_set_min_proto_version(impl->ssl_contexts[i], TLS1_2_VERSION);","If we don't set, then it is really up to the SSL library, most likely version TLS 1.1 or below.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22259,393846972,2020-03-17T17:25:33Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pxd.pxi,"@@ -12,18 +12,45 @@ # See the License for the specific language governing permissions and # limitations under the License. +cdef extern from ""<queue>"" namespace ""std"" nogil:",I think you can just do [`from libcpp.queue cimport queue`.](https://github.com/cython/cython/blob/master/Cython/Includes/libcpp/queue.pxd),
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22259,393866167,2020-03-17T17:55:50Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pxd.pxi,"@@ -12,18 +12,45 @@ # See the License for the specific language governing permissions and # limitations under the License. +cdef extern from ""<queue>"" namespace ""std"" nogil:","If we use ""cimport"", we will have a build failure on some platform due to C++ exception handling. So, I avoided [`except +`](https://github.com/cython/cython/blob/master/Cython/Includes/libcpp/queue.pxd#L3). Comments added:```# NOTE(lidiz) Unfortunately, we can't use ""cimport"" here because Cython# links it with exception handling. It introduces new dependencies.```",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22308,393881123,2020-03-17T18:20:19Z,src/core/lib/security/security_connector/tls/tls_security_connector.cc,"@@ -88,6 +88,7 @@ grpc_status_code TlsFetchKeyMaterials(   if (credential_reload_config != nullptr) {     grpc_tls_credential_reload_arg* arg = new grpc_tls_credential_reload_arg();     arg->key_materials_config = key_materials_config.get();+    arg->error_details = new grpc_tls_err_details();","Making `error_details` an object member of `arg` would require `grpc_security.h` to have a new dependency of `grpc_tls_credentials_options.h`, which I think is not allowed here, because `grpc_security.h` is a wrapper interface and shouldn't depend on any other C core header files. IIUC, nearly most of the classes defined here are of the same pointer type, because of the same reason. When defining as pointer type, we could forward declare this class and then use it in  `grpc_security.h`, without need to depend on C core classes, since we don't need to know the how much memory the class that pointer points to is going to take. However, if it's a object, we have to know beforehand, and hence need that dependency.The main purpose of this PR is to fix that memory leak issue, so I think probably I would try to figure out the solution to this intricate problem later on :-) ",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22308,393897747,2020-03-17T18:48:30Z,include/grpc/grpc_security.h,"@@ -834,30 +838,73 @@ typedef void (*grpc_tls_on_credential_reload_done_cb)(     grpc_tls_credential_reload_arg* arg);  /** A struct containing all information necessary to schedule/cancel a-    credential reload request.-    - cb and cb_user_data represent a gRPC-provided-      callback and an argument passed to it.-    - key_materials_config is an in/output parameter containing currently-      used/newly reloaded credentials. If credential reload does not result in-      a new credential, key_materials_config should not be modified. The same-      key_materials_config object can be updated if new key materials is-      available.-    - status and error_details are used to hold information about-      errors occurred when a credential reload request is scheduled/cancelled.-    - config is a pointer to the unique grpc_tls_credential_reload_config-      instance that this argument corresponds to.-    - context is a pointer to a wrapped language implementation of this-      grpc_tls_credential_reload_arg instance.-    - destroy_context is a pointer to a caller-provided method that cleans-      up any data associated with the context pointer.+    credential reload request. To help understand each fields, we mark each+    field with labels. Here are their explanations+    -------------------------label explanation----------------------------------+    [USER-MANAGED]: fields that are managed directly by the end users+    [SYSTEM-PROVIDED]: fields that needed to retrieve implementations in C core+    and would be provided by the gRPC stack+    [WRAP-LANG]: fields that needed to retrieve the particular implementation in+    wrap language+    [ASYNC_ONLY]: fields that only used when in asynchronous mode. If a field+    doesn't have this label, we can assume it should be applied to both+    synchronous and asynchronous mode.+    Note that if a field is marked both as [USER-MANAGED] and [SYSTEM-PROVIDED],+    it indicates that the field is initiated by the gRPC stack, but users+    need to properly manage them to correctly perform credential reloading.+    ----------------------label explanation ends--------------------------------+    - cb[USER-MANAGED][ASYNC_ONLY]: a callback-function provided for users to+      indicate an asynchronous reloading operation is complete. After calling+      this function, gRPC stack would be notified and continue rest of the+      authentication.+    - cb_user_data[SYSTEM-PROVIDED][ASYNC_ONLY]: a pointer used to retrieve the+      corresponding security connector implementation. This is needed because+      we need to use the |peer_checked| closure in security connector after","Nah, peer_checked is the callback function attached to secuity_connector: https://github.com/grpc/grpc/blob/master/src/core/lib/security/security_connector/tls/tls_security_connector.h#L100The reason why we need this field is, we need to use this  `on_peer_checked` at some point of time, so we keep a pointer to the security_connector, and use that to get `on_peer_checked`.May I know if there are any suggestions for rephrasing this?",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22308,393902896,2020-03-17T18:57:52Z,include/grpc/grpc_security.h,"@@ -834,30 +838,73 @@ typedef void (*grpc_tls_on_credential_reload_done_cb)(     grpc_tls_credential_reload_arg* arg);  /** A struct containing all information necessary to schedule/cancel a-    credential reload request.-    - cb and cb_user_data represent a gRPC-provided-      callback and an argument passed to it.-    - key_materials_config is an in/output parameter containing currently-      used/newly reloaded credentials. If credential reload does not result in-      a new credential, key_materials_config should not be modified. The same-      key_materials_config object can be updated if new key materials is-      available.-    - status and error_details are used to hold information about-      errors occurred when a credential reload request is scheduled/cancelled.-    - config is a pointer to the unique grpc_tls_credential_reload_config-      instance that this argument corresponds to.-    - context is a pointer to a wrapped language implementation of this-      grpc_tls_credential_reload_arg instance.-    - destroy_context is a pointer to a caller-provided method that cleans-      up any data associated with the context pointer.+    credential reload request. To help understand each fields, we mark each+    field with labels. Here are their explanations+    -------------------------label explanation----------------------------------+    [USER-MANAGED]: fields that are managed directly by the end users+    [SYSTEM-PROVIDED]: fields that needed to retrieve implementations in C core+    and would be provided by the gRPC stack+    [WRAP-LANG]: fields that needed to retrieve the particular implementation in+    wrap language+    [ASYNC_ONLY]: fields that only used when in asynchronous mode. If a field+    doesn't have this label, we can assume it should be applied to both+    synchronous and asynchronous mode.+    Note that if a field is marked both as [USER-MANAGED] and [SYSTEM-PROVIDED],+    it indicates that the field is initiated by the gRPC stack, but users+    need to properly manage them to correctly perform credential reloading.+    ----------------------label explanation ends--------------------------------+    - cb[USER-MANAGED][ASYNC_ONLY]: a callback-function provided for users to+      indicate an asynchronous reloading operation is complete. After calling+      this function, gRPC stack would be notified and continue rest of the+      authentication.+    - cb_user_data[SYSTEM-PROVIDED][ASYNC_ONLY]: a pointer used to retrieve the+      corresponding security connector implementation. This is needed because+      we need to use the |peer_checked| closure in security connector after+      users call |cb|.+    - key_materials_config[USER-MANAGED][SYSTEM-PROVIDED]: an in/output+      parameter containing currently used/newly reloaded credentials. Users can+      assume it always points to a non-null object.+      If users set |status| to |GRPC_SSL_CERTIFICATE_CONFIG_RELOAD_UNCHANGED|,+      key_materials_config should not be modified.+      If set to |GRPC_SSL_CERTIFICATE_CONFIG_RELOAD_NEW|, key_materials_config+      object can be updated by the new credentials.+      Note that, key_materials_config might be shared by multiple connections,+      because each key_materials_config is attached to a security_connector, and+          1. on client side, if retry mechanism happens, one security_connector+          can map to multiple connections+          2. on server side, all connections share the same security_connector+      But the reload logic is guaranteed to be invoked in every connection.+    - status[USER-MANAGED]: a per-connection parameter to indicate the finish+      status of a schedule/cancel request.+      If set to |GRPC_SSL_CERTIFICATE_CONFIG_RELOAD_UNCHANGED|, gRPC stack+      would take current |grpc_tls_key_materials_config| as credentials;+      If set to |GRPC_SSL_CERTIFICATE_CONFIG_RELOAD_NEW|, gRPC stack would+      perform credential reloading, using newly loaded |key_materials_config|;+      If set to |GRPC_SSL_CERTIFICATE_CONFIG_RELOAD_FAIL|, users also need to+      set |error_details|.+    - err_details[USER-MANAGED][SYSTEM-PROVIDED]: a per-connection","> I thought this is set by the callback to indicate the error happened. This needs to be clarified because it is confusing. Also grpc_tls_err_details struct is defined in a different file grpc_tls_credentials_options.h but if the callback is going to set the error string in the object they also need access to that definition, right?Yes, I marked it both as  [USER-MANAGED] and [SYSTEM-PROVIDED], because from a user's point of view, this field should be managed properly to indicate a failure;  from a maintainer's pointer of view, this field should be properly set in gRPC stack to give it for users to use. I was mixing the semantics here. Probably using a label is not a good idea...",X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22308,393912761,2020-03-17T19:17:04Z,include/grpc/grpc_security.h,"@@ -834,30 +838,73 @@ typedef void (*grpc_tls_on_credential_reload_done_cb)(     grpc_tls_credential_reload_arg* arg);  /** A struct containing all information necessary to schedule/cancel a-    credential reload request.-    - cb and cb_user_data represent a gRPC-provided-      callback and an argument passed to it.-    - key_materials_config is an in/output parameter containing currently-      used/newly reloaded credentials. If credential reload does not result in-      a new credential, key_materials_config should not be modified. The same-      key_materials_config object can be updated if new key materials is-      available.-    - status and error_details are used to hold information about-      errors occurred when a credential reload request is scheduled/cancelled.-    - config is a pointer to the unique grpc_tls_credential_reload_config-      instance that this argument corresponds to.-    - context is a pointer to a wrapped language implementation of this-      grpc_tls_credential_reload_arg instance.-    - destroy_context is a pointer to a caller-provided method that cleans-      up any data associated with the context pointer.+    credential reload request. To help understand each fields, we mark each+    field with labels. Here are their explanations+    -------------------------label explanation----------------------------------+    [USER-MANAGED]: fields that are managed directly by the end users+    [SYSTEM-PROVIDED]: fields that needed to retrieve implementations in C core+    and would be provided by the gRPC stack+    [WRAP-LANG]: fields that needed to retrieve the particular implementation in","Yes...For now the design is to assume we have an equivalent `arg` ""class"" wrapper implementation in wrapping languages, and we have a field in `arg` and `config` to point back to the wrapper implementation.This remains my biggest concern as last time I talked to @yihuazhang , for the following reasons:1. not all wrap languages have ""class"" definition, and 2. we probably couldn't assume too much about how other wrap languages implement these interfacesI am now trying to figure out how Python is going to wrap this, which might provide another view other than C++. This field might need to change in the future.  @jiangtaoli2016  FYI.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/22104,393988253,2020-03-17T21:43:58Z,tools/internal_ci/linux/grpc_xds_bazel_test_in_docker.sh,"@@ -45,12 +45,24 @@ touch ""$TOOLS_DIR""/src/proto/grpc/testing/__init__.py     ""$PROTO_SOURCE_DIR""/messages.proto \     ""$PROTO_SOURCE_DIR""/empty.proto -bazel build test/cpp/interop:xds_interop_client+export GRPC_VERBOSITY=debug+export GRPC_TRACE=xds_client,xds_resolver,cds_lb,xds_lb -GRPC_VERBOSITY=debug GRPC_TRACE=xds_client,xds_resolver,cds_lb,xds_lb ""$PYTHON"" \","This should still have verbose logging enabled.More importantly, I'm not sure about combining Python and C++ in the same job. The runs for this test already take long enough that I don't think we want the same kokoro job to execute C++ and Python sequentially. Python should probably just have its own stand-alone xDS Kokoro job and corresponding script here on Github - at the least, C++ and Python should only be in the same job if they are configured to separately report status for Python and C++ for dashboards etc.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22259,393996537,2020-03-17T22:02:40Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi,"@@ -30,41 +40,70 @@ cdef class BaseCompletionQueue: cdef class PollerCompletionQueue(BaseCompletionQueue):      def __cinit__(self):+        self._loop = asyncio.get_event_loop()         self._cq = grpc_completion_queue_create_for_next(NULL)         self._shutdown = False         self._poller_thread = threading.Thread(target=self._poll_wrapper, daemon=True)         self._poller_thread.start() -    cdef void _poll(self) except *:+        self._read_socket, self._write_socket = socket.socketpair()+        self._write_fd = self._write_socket.fileno()+        self._loop.add_reader(self._read_socket, self._handle_events)++        self._queue = cpp_event_queue()++    cdef void _poll(self) nogil:         cdef grpc_event event         cdef CallbackContext *context          while not self._shutdown:-            with nogil:-                event = grpc_completion_queue_next(self._cq,-                                                   _GPR_INF_FUTURE,-                                                   NULL)+            event = grpc_completion_queue_next(self._cq,+                                                _GPR_INF_FUTURE,+                                                NULL)              if event.type == GRPC_QUEUE_TIMEOUT:-                raise AssertionError(""Core should not return GRPC_QUEUE_TIMEOUT!"")+                with gil:+                    raise AssertionError(""Core should not return GRPC_QUEUE_TIMEOUT!"")             elif event.type == GRPC_QUEUE_SHUTDOWN:                 self._shutdown = True             else:-                context = <CallbackContext *>event.tag-                loop = <object>context.loop-                loop.call_soon_threadsafe(-                    _handle_callback_wrapper,-                    <CallbackWrapper>context.callback_wrapper,-                    event.success)+                self._queue.push(event)+                _unified_socket_write(self._write_fd)      def _poll_wrapper(self):-        self._poll()+        with nogil:+            self._poll() -    cdef void shutdown(self) nogil:+    cdef shutdown(self):+        self._loop.remove_reader(self._read_socket)         # TODO(https://github.com/grpc/grpc/issues/22365) perform graceful shutdown         grpc_completion_queue_shutdown(self._cq)         grpc_completion_queue_destroy(self._cq) +    def _handle_events(self):+        cdef bytes data = self._read_socket.recv(1)+        cdef grpc_event event+        cdef CallbackContext *context++        while not self._queue.empty():+            event = self._queue.front()+            self._queue.pop()",if this is not thread-safe we should guard the reading against having race conditions.,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22104,394022964,2020-03-17T23:12:14Z,tools/internal_ci/linux/grpc_xds_bazel_test_in_docker.sh,"@@ -45,12 +45,24 @@ touch ""$TOOLS_DIR""/src/proto/grpc/testing/__init__.py     ""$PROTO_SOURCE_DIR""/messages.proto \     ""$PROTO_SOURCE_DIR""/empty.proto -bazel build test/cpp/interop:xds_interop_client+export GRPC_VERBOSITY=debug+export GRPC_TRACE=xds_client,xds_resolver,cds_lb,xds_lb -GRPC_VERBOSITY=debug GRPC_TRACE=xds_client,xds_resolver,cds_lb,xds_lb ""$PYTHON"" \",I think I've split things up the way you want (along with a pending internal change). PTALA.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22333,394161176,2020-03-18T07:58:27Z,test/core/end2end/tests/disappearing_server.cc,"@@ -210,7 +210,9 @@ static void disappearing_server_test(grpc_end2end_test_config config) {  void disappearing_server(grpc_end2end_test_config config) {   GPR_ASSERT(config.feature_mask & FEATURE_MASK_SUPPORTS_DELAYED_CONNECTION);+#ifndef GPR_WINDOWS /* b/148110727 for more details */","It would be better to disable the combo of ""proxy fixture"" and ""disappearing server"" test in https://github.com/grpc/grpc/blob/master/test/core/end2end/generate_tests.bzl, but I don't see a straighforward way of doing this, so this is good enough.",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/22375,394551774,2020-03-18T18:20:53Z,src/core/tsi/ssl_transport_security.cc,"@@ -1955,16 +1954,15 @@ tsi_result tsi_create_ssl_server_handshaker_factory_with_options(    for (i = 0; i < options->num_key_cert_pairs; i++) {     do {-#if defined(OPENSSL_NO_TLS1_2_METHOD) || OPENSSL_API_COMPAT >= 0x10100000L       impl->ssl_contexts[i] = SSL_CTX_new(TLS_method());-#else-      impl->ssl_contexts[i] = SSL_CTX_new(TLSv1_2_method());-#endif       if (impl->ssl_contexts[i] == nullptr) {         gpr_log(GPR_ERROR, ""Could not create ssl context."");         result = TSI_OUT_OF_RESOURCES;         break;       }+#if OPENSSL_VERSION_NUMBER >= 0x10100000+      SSL_CTX_set_min_proto_version(impl->ssl_contexts[i], TLS1_2_VERSION);","In previous code, if user does not config any thing, it will use TLSv1_2_method(). The only way to use TLS 1.1 before was set OPENSSL_NO_TLS1_2_METHOD and use an SSL library thats does not support TLS 1.2 or above. The question is whether we want to provide better security (to prevent downgrade attack) or be full backward compatible.Let me remove this and add a todo.",
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/22306,394877233,2020-03-19T09:02:36Z,src/python/grpcio/grpc/experimental/aio/_metadata.py,"@@ -0,0 +1,72 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Implementation of the metadata abstraction for gRPC Asyncio Python.""""""+from typing import List, Tuple, AnyStr, Iterator, Any+from collections import abc, OrderedDict+++class Metadata(abc.Mapping):+    """"""Metadata abstraction for the asynchronous calls and interceptors.++    The metadata is a mapping from str -> List[str]++    Traits+        * Multiple entries are allowed for the same key+        * The order of the values by key is preserved+        * Getting by an element by key, retrieves the first mapped value+        * Supports an immutable view of the data","I've added the last point about modifying data.This ought to be covered by the ``__setitem__``, ``__delitem__``, and ``set_all`` methods.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22306,395116625,2020-03-19T15:32:06Z,src/python/grpcio/grpc/experimental/aio/_metadata.py,"@@ -0,0 +1,78 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Implementation of the metadata abstraction for gRPC Asyncio Python.""""""+from typing import List, Tuple, AnyStr, Iterator, Any+from collections import abc, OrderedDict+++class Metadata(abc.Mapping):+    """"""Metadata abstraction for the asynchronous calls and interceptors.++    The metadata is a mapping from str -> List[str]++    Traits+        * Multiple entries are allowed for the same key+        * The order of the values by key is preserved+        * Getting by an element by key, retrieves the first mapped value+        * Supports an immutable view of the data+        * Allows partial mutation on the data without recreating the new object from scratch.+    """"""++    def __init__(self, *args: Tuple[str, AnyStr]) -> None:+        self._metadata = OrderedDict()","Nothing urgent since its hidden to the user, we can explore customizing and maybe improving the data-structure used under the hood, later on. So not a stopper for me!",
394885,soheilhy,https://api.github.com/repos/grpc/grpc/pulls/22372,395359120,2020-03-19T22:40:55Z,src/core/lib/iomgr/tcp_posix.cc,"@@ -1809,7 +1809,8 @@ grpc_endpoint* grpc_tcp_create(grpc_fd* em_fd,   if (setsockopt(tcp->fd, SOL_TCP, TCP_INQ, &one, sizeof(one)) == 0) {     tcp->inq_capable = true;   } else {-    gpr_log(GPR_DEBUG, ""cannot set inq fd=%d errno=%d"", tcp->fd, errno);+    const char* msg = ""dbg-info, non-fatal: cannot set inq fd=%d errno=%d"";",Oh I see. Thank you for the context. I'm ok with removing this LOG line all together :-)  It's not critical and we can get the data with ftrace.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22390,395515629,2020-03-20T09:20:12Z,src/core/tsi/test_creds/README,"@@ -19,44 +19,68 @@ Valid test credentials: The ca is self-signed: ---------------------- -$ openssl req -x509 -new -newkey rsa:1024 -nodes -out ca.pem -config ca-openssl.cnf -days 3650 -extensions v3_req+$ openssl req -x509 -new -newkey rsa:2048 -nodes -keyout ca.key -out ca.pem \+  -config ca-openssl.cnf -days 3650 -extensions v3_req When prompted for certificate information, everything is default.  client is issued by CA: ----------------------- -$ openssl genrsa -out client.key.rsa 1024+$ openssl genrsa -out client.key.rsa 2048 $ openssl pkcs8 -topk8 -in client.key.rsa -out client.key -nocrypt-$ rm client.key.rsa $ openssl req -new -key client.key -out client.csr  When prompted for certificate information, everything is default except the common name which is set to testclient. -$ openssl ca -in client.csr -out client.pem+$ openssl x509 -req -CA ca.pem -CAkey ca.key -CAcreateserial -in client.csr \+  -out client.pem -days 3650  server0 is issued by CA: ------------------------ -$ openssl genrsa -out server0.key.rsa 1024+$ openssl genrsa -out server0.key.rsa 2048 $ openssl pkcs8 -topk8 -in server0.key.rsa -out server0.key -nocrypt-$ rm server0.key.rsa $ openssl req -new -key server0.key -out server0.csr  When prompted for certificate information, everything is default except the common name which is set to *.test.google.com.au. -$ openssl ca -in server0.csr -out server0.pem+$ openssl x509 -req -CA ca.pem -CAkey ca.key -CAcreateserial -in server0.csr \+  -out server0.pem -days 3650  server1 is issued by CA with a special config for subject alternative names: ---------------------------------------------------------------------------- -$ openssl genrsa -out server1.key.rsa 1024+$ openssl genrsa -out server1.key.rsa 2048 $ openssl pkcs8 -topk8 -in server1.key.rsa -out server1.key -nocrypt-$ rm server1.key.rsa $ openssl req -new -key server1.key -out server1.csr -config server1-openssl.cnf  When prompted for certificate information, everything is default except the common name which is set to *.test.google.com. -$ openssl ca -in server1.csr -out server1.pem+$ openssl x509 -req -CA ca.pem -CAkey ca.key -CAcreateserial -in server1.csr \+  -out server1.pem -extensions req_ext -extfile server1-openssl.cnf -days 3650++Clean up:+---------+$ rm *.rsa+$ rm *.csr+$ rm ca.srl++Sync up with other repositories+===============================++You will need to copy the test credentials to the following places","I think list is impossible to keep up-to-date so I'm not sure if it's useful to add it here.(if we do, it will be out of date very soon). Add a note that this list is incomplete and that these are just some of the locations we have to check?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22390,395764819,2020-03-20T16:49:01Z,src/core/tsi/test_creds/README,"@@ -19,44 +19,68 @@ Valid test credentials: The ca is self-signed: ---------------------- -$ openssl req -x509 -new -newkey rsa:1024 -nodes -out ca.pem -config ca-openssl.cnf -days 3650 -extensions v3_req+$ openssl req -x509 -new -newkey rsa:2048 -nodes -keyout ca.key -out ca.pem \+  -config ca-openssl.cnf -days 3650 -extensions v3_req When prompted for certificate information, everything is default.  client is issued by CA: ----------------------- -$ openssl genrsa -out client.key.rsa 1024+$ openssl genrsa -out client.key.rsa 2048 $ openssl pkcs8 -topk8 -in client.key.rsa -out client.key -nocrypt-$ rm client.key.rsa $ openssl req -new -key client.key -out client.csr  When prompted for certificate information, everything is default except the common name which is set to testclient. -$ openssl ca -in client.csr -out client.pem+$ openssl x509 -req -CA ca.pem -CAkey ca.key -CAcreateserial -in client.csr \+  -out client.pem -days 3650  server0 is issued by CA: ------------------------ -$ openssl genrsa -out server0.key.rsa 1024+$ openssl genrsa -out server0.key.rsa 2048 $ openssl pkcs8 -topk8 -in server0.key.rsa -out server0.key -nocrypt-$ rm server0.key.rsa $ openssl req -new -key server0.key -out server0.csr  When prompted for certificate information, everything is default except the common name which is set to *.test.google.com.au. -$ openssl ca -in server0.csr -out server0.pem+$ openssl x509 -req -CA ca.pem -CAkey ca.key -CAcreateserial -in server0.csr \+  -out server0.pem -days 3650  server1 is issued by CA with a special config for subject alternative names: ---------------------------------------------------------------------------- -$ openssl genrsa -out server1.key.rsa 1024+$ openssl genrsa -out server1.key.rsa 2048 $ openssl pkcs8 -topk8 -in server1.key.rsa -out server1.key -nocrypt-$ rm server1.key.rsa $ openssl req -new -key server1.key -out server1.csr -config server1-openssl.cnf  When prompted for certificate information, everything is default except the common name which is set to *.test.google.com. -$ openssl ca -in server1.csr -out server1.pem+$ openssl x509 -req -CA ca.pem -CAkey ca.key -CAcreateserial -in server1.csr \+  -out server1.pem -extensions req_ext -extfile server1-openssl.cnf -days 3650++Clean up:+---------+$ rm *.rsa+$ rm *.csr+$ rm ca.srl++Sync up with other repositories+===============================++You will need to copy the test credentials to the following places","Sure, I absolutely believe you've done a good job searching through the existing repos - I'm just saying that providing the list of locations in an .md file as a documentation doesn't make much sense, because the list of locations can change any time and it is unrealistic to expect that people will update this list over time. So it's better to just say - ""copies of these keys exist in multiple locations across all the grpc repos, so you need to be careful when updating the keys"" (and perhaps provide some locations as examples, rather than a definitive list).",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/22390,395780100,2020-03-20T17:15:49Z,src/core/tsi/test_creds/README,"@@ -19,44 +19,68 @@ Valid test credentials: The ca is self-signed: ---------------------- -$ openssl req -x509 -new -newkey rsa:1024 -nodes -out ca.pem -config ca-openssl.cnf -days 3650 -extensions v3_req+$ openssl req -x509 -new -newkey rsa:2048 -nodes -keyout ca.key -out ca.pem \+  -config ca-openssl.cnf -days 3650 -extensions v3_req When prompted for certificate information, everything is default.  client is issued by CA: ----------------------- -$ openssl genrsa -out client.key.rsa 1024+$ openssl genrsa -out client.key.rsa 2048 $ openssl pkcs8 -topk8 -in client.key.rsa -out client.key -nocrypt-$ rm client.key.rsa $ openssl req -new -key client.key -out client.csr  When prompted for certificate information, everything is default except the common name which is set to testclient. -$ openssl ca -in client.csr -out client.pem+$ openssl x509 -req -CA ca.pem -CAkey ca.key -CAcreateserial -in client.csr \+  -out client.pem -days 3650  server0 is issued by CA: ------------------------ -$ openssl genrsa -out server0.key.rsa 1024+$ openssl genrsa -out server0.key.rsa 2048 $ openssl pkcs8 -topk8 -in server0.key.rsa -out server0.key -nocrypt-$ rm server0.key.rsa $ openssl req -new -key server0.key -out server0.csr  When prompted for certificate information, everything is default except the common name which is set to *.test.google.com.au. -$ openssl ca -in server0.csr -out server0.pem+$ openssl x509 -req -CA ca.pem -CAkey ca.key -CAcreateserial -in server0.csr \+  -out server0.pem -days 3650  server1 is issued by CA with a special config for subject alternative names: ---------------------------------------------------------------------------- -$ openssl genrsa -out server1.key.rsa 1024+$ openssl genrsa -out server1.key.rsa 2048 $ openssl pkcs8 -topk8 -in server1.key.rsa -out server1.key -nocrypt-$ rm server1.key.rsa $ openssl req -new -key server1.key -out server1.csr -config server1-openssl.cnf  When prompted for certificate information, everything is default except the common name which is set to *.test.google.com. -$ openssl ca -in server1.csr -out server1.pem+$ openssl x509 -req -CA ca.pem -CAkey ca.key -CAcreateserial -in server1.csr \+  -out server1.pem -extensions req_ext -extfile server1-openssl.cnf -days 3650++Clean up:+---------+$ rm *.rsa+$ rm *.csr+$ rm ca.srl++Sync up with other repositories+===============================++You will need to copy the test credentials to the following places","Agree. README updated. The reason I listed the locations is because it took me a while to find all copies. It will save time for whoever does the key update next time. Hopefully, that is after 10 years when keys are expired.",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/22308,395839404,2020-03-20T19:08:49Z,include/grpc/grpc_security.h,"@@ -808,49 +808,81 @@ GRPCAPI int grpc_tls_key_materials_config_set_key_materials(  /** Set grpc_tls_key_materials_config instance with a provided version number,     which is used to keep track of the version of key materials.-    It returns 1 on success and 0 on failure. It is used for-    experimental purpose for now and subject to change.+    It returns 1 on success and 0 on failure.+    Note the API is experimental now and subject to change.  */ GRPCAPI int grpc_tls_key_materials_config_set_version(     grpc_tls_key_materials_config* config, int version);  /** Get the version number of a grpc_tls_key_materials_config instance.     It returns the version number on success and -1 on failure.-    It is used for experimental purpose for now and subject to change.+    Note the API is experimental now and subject to change.  */ GRPCAPI int grpc_tls_key_materials_config_get_version(     grpc_tls_key_materials_config* config);  /** --- TLS credential reload config. ----    It is used for experimental purpose for now and subject to change.*/+    Note the API is experimental now and subject to change.*/  typedef struct grpc_tls_credential_reload_arg grpc_tls_credential_reload_arg;  /** A callback function provided by gRPC to handle the result of credential     reload. It is used when schedule API is implemented asynchronously and-    serves to bring the control back to grpc C core. It is used for-    experimental purpose for now and subject to change. */+    serves to bring the control back to grpc C core.+    Note the API is experimental now and subject to change. */ typedef void (*grpc_tls_on_credential_reload_done_cb)(     grpc_tls_credential_reload_arg* arg);  /** A struct containing all information necessary to schedule/cancel a     credential reload request.-    - cb and cb_user_data represent a gRPC-provided-      callback and an argument passed to it.-    - key_materials_config is an in/output parameter containing currently-      used/newly reloaded credentials. If credential reload does not result in-      a new credential, key_materials_config should not be modified. The same-      key_materials_config object can be updated if new key materials is-      available.-    - status and error_details are used to hold information about-      errors occurred when a credential reload request is scheduled/cancelled.-    - config is a pointer to the unique grpc_tls_credential_reload_config+    Note that we currently only support synchronous credential reloading. All+    the fields related to asynchronous credential reloading can be ignored for+    now.+    - cb: a callback-function provided by gRPC stack for users to indicate an+      asynchronous reloading operation is complete. Users are expected to call+      this function when asynchronous reloading is done.+      This field is only applicable in the asynchronous mode which is currently+      not implemented.+    - cb_user_data: a pointer used to retrieve some implementations in C core.+      This is needed because after users call |cb|, we will use this field to+      notify the gRPC stack to continue the handshake.+      Users are not expected to interact with this field directly.+    - key_materials_config: an in/output parameter containing currently+      used/newly reloaded credentials. This field is provided by the gRPC stack,+      and users can use it to handle credential reloading.+      Note that, key_materials_config is not a per-connection parameter, which+      means it might be shared by multiple connections.+    - status: a per-connection parameter to indicate the finish status of a","Does the ""user"" need to know if it is a per-connection parameter or not? From his perspective, he only cares about if a single reload request succeeds or not. ",
14166415,sanjaypujare,https://api.github.com/repos/grpc/grpc/pulls/22308,395859678,2020-03-20T19:53:36Z,include/grpc/grpc_security.h,"@@ -808,49 +808,81 @@ GRPCAPI int grpc_tls_key_materials_config_set_key_materials(  /** Set grpc_tls_key_materials_config instance with a provided version number,     which is used to keep track of the version of key materials.-    It returns 1 on success and 0 on failure. It is used for-    experimental purpose for now and subject to change.+    It returns 1 on success and 0 on failure.+    Note the API is experimental now and subject to change.  */ GRPCAPI int grpc_tls_key_materials_config_set_version(     grpc_tls_key_materials_config* config, int version);  /** Get the version number of a grpc_tls_key_materials_config instance.     It returns the version number on success and -1 on failure.-    It is used for experimental purpose for now and subject to change.+    Note the API is experimental now and subject to change.  */ GRPCAPI int grpc_tls_key_materials_config_get_version(     grpc_tls_key_materials_config* config);  /** --- TLS credential reload config. ----    It is used for experimental purpose for now and subject to change.*/+    Note the API is experimental now and subject to change.*/  typedef struct grpc_tls_credential_reload_arg grpc_tls_credential_reload_arg;  /** A callback function provided by gRPC to handle the result of credential     reload. It is used when schedule API is implemented asynchronously and-    serves to bring the control back to grpc C core. It is used for-    experimental purpose for now and subject to change. */+    serves to bring the control back to grpc C core.+    Note the API is experimental now and subject to change. */ typedef void (*grpc_tls_on_credential_reload_done_cb)(     grpc_tls_credential_reload_arg* arg);  /** A struct containing all information necessary to schedule/cancel a     credential reload request.-    - cb and cb_user_data represent a gRPC-provided-      callback and an argument passed to it.-    - key_materials_config is an in/output parameter containing currently-      used/newly reloaded credentials. If credential reload does not result in-      a new credential, key_materials_config should not be modified. The same-      key_materials_config object can be updated if new key materials is-      available.-    - status and error_details are used to hold information about-      errors occurred when a credential reload request is scheduled/cancelled.-    - config is a pointer to the unique grpc_tls_credential_reload_config+    Note that we currently only support synchronous credential reloading. All+    the fields related to asynchronous credential reloading can be ignored for+    now.+    - cb: a callback-function provided by gRPC stack for users to indicate an+      asynchronous reloading operation is complete. Users are expected to call","I would say ""user"" is one who is using the reload config feature so is calling the reload_config_create function. But I agree that with this particular use-case/API we have multiple actors and that adds to the complexity of the documentation.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22306,397350957,2020-03-24T17:53:24Z,src/python/grpcio_tests/tests/unit/_metadata_test.py,"@@ -19,6 +19,7 @@  import grpc from grpc import _channel+from grpc.experimental.aio._metadata import Metadata",Let's expose `Metadata` as one of the public API.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22460,397440189,2020-03-24T20:28:56Z,examples/python/xds/server.py,"@@ -0,0 +1,78 @@+# Copyright 2020 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""The Python implementation of the GRPC helloworld.Greeter server.""""""++from concurrent import futures+import argparse+import logging+import socket++import grpc++import helloworld_pb2+import helloworld_pb2_grpc++from grpc_reflection.v1alpha import reflection+from grpc_health.v1 import health+from grpc_health.v1 import health_pb2+from grpc_health.v1 import health_pb2_grpc++_DESCRIPTION = ""A general purpose dummy server.""+++class Greeter(helloworld_pb2_grpc.GreeterServicer):++    def __init__(self, hostname):+        self._hostname = hostname if hostname else socket.gethostname()++    def SayHello(self, request, context):+        return helloworld_pb2.HelloReply(+            message=f""Hello {request.name} from {self._hostname}!"")+++def serve(port, hostname):+    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))+    helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(hostname), server)+    health_servicer = health.HealthServicer(+        experimental_non_blocking=True,+        experimental_thread_pool=futures.ThreadPoolExecutor(max_workers=4))","Since examples tends to be copied a lot, maybe we should use a bigger number here? Will the threads become exhausted after 4 Watch RPC?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22306,397981421,2020-03-25T16:12:30Z,src/python/grpcio_tests/tests_aio/unit/_metadata_test.py,"@@ -0,0 +1,125 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests for the metadata abstraction that's used in the asynchronous driver.""""""+import logging+import unittest++from grpc.experimental.aio._metadata import Metadata",```pythonfrom grpc.experimental.aio import Metadata```,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22232,398047697,2020-03-25T17:42:43Z,src/core/ext/filters/client_channel/service_config.cc,"@@ -156,91 +170,80 @@ grpc_error* ServiceConfig::ParsePerMethodParams() {             ""field:methodConfig error:not of type Object""));         continue;       }-      grpc_error* error = ParseJsonMethodConfigToServiceConfigVectorTable(-          method_config, &entries);+      grpc_error* error = ParseJsonMethodConfig(method_config);       if (error != GRPC_ERROR_NONE) {         error_list.push_back(error);       }     }   }-  if (!entries.empty()) {-    parsed_method_configs_table_ =-        SliceHashTable<const ParsedConfigVector*>::Create(-            entries.size(), entries.data(), nullptr);-  }   return GRPC_ERROR_CREATE_FROM_VECTOR(""Method Params"", &error_list); } -UniquePtr<char> ServiceConfig::ParseJsonMethodName(const Json& json,-                                                   grpc_error** error) {+std::string ServiceConfig::ParseJsonMethodName(const Json& json,+                                               grpc_error** error) {   if (json.type() != Json::Type::OBJECT) {     *error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(         ""field:name error:type is not object"");-    return nullptr;+    return """";   }   // Find service name.+  const std::string* service_name = nullptr;   auto it = json.object_value().find(""service"");-  if (it == json.object_value().end()) {-    *error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(-        ""field:name error: field:service error:not found"");-    return nullptr;  // Required field.-  }-  if (it->second.type() != Json::Type::STRING) {-    *error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(-        ""field:name error: field:service error:not of type string"");-    return nullptr;-  }-  if (it->second.string_value().empty()) {-    *error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(-        ""field:name error: field:service error:empty value"");-    return nullptr;+  if (it != json.object_value().end() &&+      it->second.type() != Json::Type::JSON_NULL) {+    if (it->second.type() != Json::Type::STRING) {+      *error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(+          ""field:name error: field:service error:not of type string"");+      return """";+    }+    if (!it->second.string_value().empty()) {+      service_name = &it->second.string_value();+    }   }-  const char* service_name = it->second.string_value().c_str();-  const char* method_name = nullptr;+  const std::string* method_name = nullptr;   // Find method name.   it = json.object_value().find(""method"");-  if (it != json.object_value().end()) {+  if (it != json.object_value().end() &&+      it->second.type() != Json::Type::JSON_NULL) {     if (it->second.type() != Json::Type::STRING) {       *error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(           ""field:name error: field:method error:not of type string"");-      return nullptr;+      return """";+    }+    if (!it->second.string_value().empty()) {+      method_name = &it->second.string_value();     }-    if (it->second.string_value().empty()) {+  }+  // If neither service nor method are specified, it's the default.+  // Method name may not be specified without service name.+  if (service_name == nullptr) {+    if (method_name != nullptr) {       *error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(-          ""field:name error: field:method error:empty value"");-      return nullptr;+          ""field:name error:method name populated without service name"");     }-    method_name = it->second.string_value().c_str();+    return """";   }-  char* path;-  gpr_asprintf(&path, ""/%s/%s"", service_name,-               method_name == nullptr ? """" : method_name);-  return grpc_core::UniquePtr<char>(path);+  // Construct path.+  return absl::StrCat(""/"", *service_name, ""/"",+                      method_name == nullptr ? """" : *method_name); }  const ServiceConfig::ParsedConfigVector*-ServiceConfig::GetMethodParsedConfigVector(const grpc_slice& path) {-  if (parsed_method_configs_table_.get() == nullptr) {-    return nullptr;-  }-  const auto* value = parsed_method_configs_table_->Get(path);+ServiceConfig::GetMethodParsedConfigVector(const grpc_slice& path) const {+  // Try looking up the full path in the map.+  auto it = parsed_method_configs_map_.find(path);+  if (it != parsed_method_configs_map_.end()) return it->second;   // If we didn't find a match for the path, try looking for a wildcard   // entry (i.e., change ""/service/method"" to ""/service/"").-  if (value == nullptr) {-    char* path_str = grpc_slice_to_c_string(path);-    const char* sep = strrchr(path_str, '/') + 1;-    const size_t len = (size_t)(sep - path_str);-    char* buf = (char*)gpr_malloc(len + 1);  // trailing NUL-    memcpy(buf, path_str, len);-    buf[len] = '\0';-    grpc_slice wildcard_path = grpc_slice_from_copied_string(buf);-    gpr_free(buf);-    value = parsed_method_configs_table_->Get(wildcard_path);-    grpc_slice_unref_internal(wildcard_path);-    gpr_free(path_str);-    if (value == nullptr) return nullptr;-  }-  return *value;+  UniquePtr<char> path_str(grpc_slice_to_c_string(path));+  char* sep = strrchr(path_str.get(), '/') + 1;+  if (sep == nullptr) return nullptr;  // Shouldn't ever happen.","In general, we should prefer to avoid adding assertions, even debug-only ones, if there's a resaonable alternative.  It's always better for library code to return an error than for it to crash the entire application.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22232,398049151,2020-03-25T17:44:45Z,src/core/ext/filters/client_channel/service_config.h,"@@ -180,8 +180,10 @@ class ServiceConfig : public RefCounted<ServiceConfig> {   // A map from the method name to the parsed config vector. Note that we are   // using a raw pointer and not a unique pointer so that we can use the same   // vector for multiple names.-  RefCountedPtr<SliceHashTable<const ParsedConfigVector*>>-      parsed_method_configs_table_;+  std::unordered_map<grpc_slice, const ParsedConfigVector*, SliceHash>+      parsed_method_configs_map_;","Yeah, that's my long-term goal as well.  But it's still being used in one other place in grpclb, and I'll have to spend some time to change that code to use something else before we can get rid of it.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22484,398892531,2020-03-26T21:05:39Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi,"@@ -128,7 +129,7 @@ async def _receive_message(GrpcCallWrapper grpc_call_wrapper,         # the callback (e.g. cancelled).         #         # Since they all indicates finish, they are better be merged.-        _LOGGER.debug(e)+        _LOGGER.debug('Failed to receive any message from Core')",why not `_LOGGER.exception(....)`?,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22495,400188499,2020-03-30T13:26:01Z,bazel/grpc_build_system.bzl,"@@ -168,7 +168,7 @@ def ios_cc_test(             deps = ios_test_deps,         ) -def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None):+def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None, rbe_linux_only_uses_blackhole_ipv6_address = False):",I feel that the `rbe_linux_only_uses_blackhole_ipv6_address` argument is a very adhoc option.I'm not a fan of making the build system (which is already complex) even more complex to satisfy a very specific need like this one. How many tests will use this option and what does this option enable?My feeling is that it would need to be extremely valuable to outweigh the increase of complexity in build system and test infrastructure associated with it.,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22495,400189336,2020-03-30T13:27:11Z,test/core/util/run_with_poller.sh,"@@ -16,4 +16,17 @@ set -ex export GRPC_POLL_STRATEGY=$1 shift+blackhole_address_setting=""$1""","I am not a fan of adding custom logic to the helper .sh scripts. Over time this is going to snowball and we're gonna end up with maintenance hell and a setup that so complex that no one understand it (and yes, it happened to use before - just look at all the .py and .sh testing scripts we have).",X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/22497,400407762,2020-03-30T18:34:02Z,test/core/end2end/generate_tests.bzl,"@@ -284,12 +286,21 @@ END2END_TESTS = {     ""retry_exceeds_buffer_size_in_subsequent_batch"": _test_options(         needs_client_channel = True,         proxyable = False,+        # TODO(jtattermusch): too long bazel test name makes the test flaky on Windows RBE+        # See b/151617965+        short_name = ""retry_exceeds_buffer_size_in_subseq"",",Name would be fine enough since it sounds a temporary workaround.,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/22481,400605374,2020-03-31T02:27:19Z,test/cpp/end2end/grpclb_end2end_test.cc,"@@ -514,11 +522,10 @@ class GrpclbEnd2endTest : public ::testing::Test {    struct AddressData {     int port;-    bool is_balancer;     grpc::string balancer_name;   }; -  grpc_core::ServerAddressList CreateLbAddressesFromAddressDataList(+  static grpc_core::ServerAddressList CreateLbAddressesFromAddressDataList(","nit: it's may be confusing to unconditionally associate a ""balancer name"" arg with each one of these addresses, since `CreateLbAddressesFromAddressDataList` is used to create not only balancer addresses but also fallback addresses.How about pass in a bool here, `lb_addresses` ?",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/22495,400622635,2020-03-31T03:32:46Z,bazel/grpc_build_system.bzl,"@@ -168,7 +168,7 @@ def ios_cc_test(             deps = ios_test_deps,         ) -def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None):+def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None, rbe_linux_only_uses_blackhole_ipv6_address = False):","This primarily addresses [this comment](https://github.com/grpc/grpc/blob/master/test/cpp/client/destroy_grpclb_channel_with_active_connect_stress_test.cc#L55) in a test.For background though, there's an internal customer with a production workload that reported a crash. The crash was only reproduceable if using ""grpclb"", and the load balancer address failed to connect i.e. the connect call experienced some latency and timed out. So I added https://github.com/grpc/grpc/blob/master/test/cpp/client/destroy_grpclb_channel_with_active_connect_stress_test.cc, which reproduced the crash (before the fix) 100% on my local workstation and in other environments, but unfortunately not on linux RBE because of the nature of how the network stack is configured for RBE containers. This change is meant to clean up that specific test, and get it to work in a tightly controlled manner (i.e. in such a way that we can guarantee that it always exercises a grpclb packet loss scenario). I could see this general kind of thing could come up again though.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22495,400820109,2020-03-31T10:54:13Z,WORKSPACE,"@@ -33,8 +33,9 @@ load(""@bazel_toolchains//rules:rbe_repo.bzl"", ""rbe_autoconfig"") rbe_autoconfig(     name = ""rbe_default"",     exec_properties = create_exec_properties_dict(-        docker_add_capabilities = ""SYS_PTRACE"",+        docker_add_capabilities = ""SYS_PTRACE,NET_ADMIN"",         docker_privileged = True,+        docker_run_as_root = True,","Yes, I understand you cannot make it work without setting docker_run_as_root and NET_ADMIN, and it's fine to use these options for tests that actually need them. But I don't want to use these options across all of our test.I believe it is possible to set exec_properties on per-test case basis:For example, for some of our tests, we adjust the exec_properties to execute on an RBE worker pool with beefier VMs:https://github.com/grpc/grpc/blob/dfec5266ddb30a06e9fb0470f729362a36a6d22b/test/core/iomgr/BUILD#L43https://github.com/grpc/grpc/blob/dfec5266ddb30a06e9fb0470f729362a36a6d22b/bazel/custom_exec_properties.bzl#L17https://github.com/grpc/grpc/blob/cc43fd64ab97e45fcdd38759dd8d9eb273823b9e/WORKSPACE#L23Looks like you'd try to do something similar?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22495,400823747,2020-03-31T11:00:44Z,bazel/grpc_build_system.bzl,"@@ -168,7 +168,7 @@ def ios_cc_test(             deps = ios_test_deps,         ) -def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None):+def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None, rbe_linux_only_uses_blackhole_ipv6_address = False):","It's fine to not port to google3, but you'd still need to add a new dummy parameter `rbe_linux_only_uses_blackhole_ipv6_address = False` to the internal definition of grpc_cc_test() otherwise the import will fail (try cherrypicking into internal and you'll see).But the best option would be not having the new arg `rbe_linux_only_uses_blackhole_ipv6_address = False` at all.Ideally you'd be able to just use a custom sh_test and tweak the exec_properies just for that one test. ",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22503,401157996,2020-03-31T19:23:49Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -367,15 +369,21 @@ async def _handle_unary_unary_rpc(object method_handler,         loop,     ) -    # Finishes the application handler-    await _finish_handler_with_unary_response(-        rpc_state,-        method_handler.unary_unary,-        request_message,-        servicer_context,-        method_handler.response_serializer,-        loop-    )+    try:+        # Finishes the application handler+        await _finish_handler_with_unary_response(+            rpc_state,+            method_handler.unary_unary,+            request_message,+            servicer_context,+            method_handler.response_serializer,+            loop+        )+    finally:+        # Executes done callbacks+        if rpc_state.done_callbacks:+            for callback in rpc_state.done_callbacks:+                callback(servicer_context)","Im a bit lost with the `servicer_context` parameter, If I understood correctly from the discussion that you had with @gnossen main use case is a cancellation, so the RPC being canceled proactively by the client and by giving a chance to the handler to know when it was canceled by the execution of a callback.If so the parameter should not be the `RPCState`? why the `ServicerContext`? does this parameter allows you to know In what terminal state the RPC finished? for example the status code.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22503,401275388,2020-03-31T23:37:13Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -367,15 +369,21 @@ async def _handle_unary_unary_rpc(object method_handler,         loop,     ) -    # Finishes the application handler-    await _finish_handler_with_unary_response(-        rpc_state,-        method_handler.unary_unary,-        request_message,-        servicer_context,-        method_handler.response_serializer,-        loop-    )+    try:+        # Finishes the application handler+        await _finish_handler_with_unary_response(+            rpc_state,+            method_handler.unary_unary,+            request_message,+            servicer_context,+            method_handler.response_serializer,+            loop+        )+    finally:+        # Executes done callbacks+        if rpc_state.done_callbacks:+            for callback in rpc_state.done_callbacks:+                callback(servicer_context)","`RPCState` is a private class for servers, and the client should only see `ServicerContext`.I think the right path to go should be porting an `AioRpcError` like instance to the done callback.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22503,401608110,2020-04-01T13:19:28Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -367,15 +369,21 @@ async def _handle_unary_unary_rpc(object method_handler,         loop,     ) -    # Finishes the application handler-    await _finish_handler_with_unary_response(-        rpc_state,-        method_handler.unary_unary,-        request_message,-        servicer_context,-        method_handler.response_serializer,-        loop-    )+    try:+        # Finishes the application handler+        await _finish_handler_with_unary_response(+            rpc_state,+            method_handler.unary_unary,+            request_message,+            servicer_context,+            method_handler.response_serializer,+            loop+        )+    finally:+        # Executes done callbacks+        if rpc_state.done_callbacks:+            for callback in rpc_state.done_callbacks:+                callback(servicer_context)","Could we make the `RPCState` attribute of the `ServicerContext` accessible in read-only? So the user if adds a callback for knowing when a request has been canceled for doing whatever would need to know if the requests were really canceled since the `add_done_callback` would be triggered in all of the situations.Another solution would be provide a specific interface ONLY for when remote cancellation happens, so in that use case the user won't need to check the terminal status of the RPC since the status is gonna be yes or yes the cancellation one.BTW, personal taste. Other patterns like logging IMO should be implemented through interceptors/middleware.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21194,401842909,2020-04-01T19:03:29Z,templates/src/objective-c/BoringSSL-GRPC.podspec.template,"@@ -225,25 +223,28 @@       % endfor       EOF -      # The symbol prefixing mechanism is performed by redefining BoringSSL symbols with ""#define-      # SOME_BORINGSSL_SYMBOL GRPC_SHADOW_SOME_BORINGSSL_SYMBOL"". Unfortunately, some symbols are-      # already redefined as macros in BoringSSL headers in the form ""#define SOME_BORINGSSL_SYMBOL-      # SOME_BORINGSSL_SYMBOL"" Such type of redefinition will cause ""SOME_BORINGSSL_SYMBOL redefined""-      # error when using together with our prefix header. So the workaround in the below lines removes-      # all such type of #define directives.-      sed -i'.back' '/^#define \\([A-Za-z0-9_]*\\) \\1/d' include/openssl/*.h-      # Remove lines of the format below for the same reason above-      #     #define SOME_BORINGSSL_SYMBOL ${""\\""}-      #         SOME_BORINGSSL_SYMBOL-      sed -i'.back' '/^#define.*\\\\$/{N;/^#define \\([A-Za-z0-9_]*\\) *\\\\\\n *\\1/d;}' include/openssl/*.h+      # To avoid symbol conflict with OpenSSL, gRPC needs to rename all the BoringSSL symbols with a +      # prefix. This is done with BoringSSL's BORINGSSL_PREFIX mechanism+      # (https://github.com/google/boringssl/blob/75148d7abf12bdd1797fec3c5da9a21963703516/BUILDING.md#building-with-prefixed-symbols).+      # The required prefix header file boringssl_prefix_symbols.h is not part of BoringSSL repo at+      # this moment. It has to be generated by BoringSSL's users and be injected to BoringSSL build.+      # gRPC generates this file in script /tools/distrib/upgrade_boringssl_objc.sh. This script+      # outputs a gzip+base64 encoded version of boringssl_prefix_symbols.h because of Cocoapods'+      # limit on the 'prepare_command' field length. The encoded header is put at+      # /src/boringssl/boringssl_prefix_symbols.h.gz.b64. Here we decode the content and inject+      # the header to correcty location in BoringSSL.+      base64 -D <<EOF | gunzip > include/openssl/boringssl_prefix_symbols.h+      % for line in open(""src/boringssl/boringssl_prefix_symbols.h.gz.b64"", ""r"").readlines():","This should work, but the problem I meet right now is that python 2's `gzip` library has a bug that using `gzip.compress(...)` will report `AttributeError: 'module' object has no attribute 'compress'`. This does not happen on python 3. It happens on both my mac and my Linux. I'll check if Kokoro has the same error (still running as for now), but I guess a lot of the team's machines could have the same problem.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22503,401899296,2020-04-01T20:47:12Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -367,15 +369,21 @@ async def _handle_unary_unary_rpc(object method_handler,         loop,     ) -    # Finishes the application handler-    await _finish_handler_with_unary_response(-        rpc_state,-        method_handler.unary_unary,-        request_message,-        servicer_context,-        method_handler.response_serializer,-        loop-    )+    try:+        # Finishes the application handler+        await _finish_handler_with_unary_response(+            rpc_state,+            method_handler.unary_unary,+            request_message,+            servicer_context,+            method_handler.response_serializer,+            loop+        )+    finally:+        # Executes done callbacks+        if rpc_state.done_callbacks:+            for callback in rpc_state.done_callbacks:+                callback(servicer_context)","> Could we make the RPCState attribute of the ServicerContext accessible in read-only?The better design should expose smaller surface API. In done callback, the application can't alter the state of the RPC, so passing the powerful `ServicerContext` object might not be useful. On the other hand, if we allow users to access those value while RPC still running, they might misbehave. So, I hope we can come up with something clean that doesn't require us to introduce a new class.@pfreixes There is some overlapping between done_callback and interceptors. Since you have more hands-on experience on interceptors, do you think we should provide the done callbacks at all? If the server interceptors are complete, then all discussed scenarios can be fulfilled.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22539,401917104,2020-04-01T21:21:44Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -171,7 +171,7 @@ def __init__(self, cython_call: cygrpc._AioCall, metadata: MetadataType,                  loop: asyncio.AbstractEventLoop) -> None:         self._loop = loop         self._cython_call = cython_call-        self._metadata = metadata+        self._metadata = tuple(metadata)","If we're changing this, we should probably change the annotation on `_metadata` on line 164 to be a bit stronger. Its type should be `Tuple[MetadatumType]` instead of `MetadataType`, which is `Sequence[MetadatumType]`.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22539,401939059,2020-04-01T22:09:01Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -171,7 +171,7 @@ def __init__(self, cython_call: cygrpc._AioCall, metadata: MetadataType,                  loop: asyncio.AbstractEventLoop) -> None:         self._loop = loop         self._cython_call = cython_call-        self._metadata = metadata+        self._metadata = tuple(metadata)","Understood, but not what I'm referring to. Line 164 is the type annotation for the `_metadata` member, which now has type `Tuple` in all cases due to this new conversion operation.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22503,402157473,2020-04-02T08:58:44Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -367,15 +369,21 @@ async def _handle_unary_unary_rpc(object method_handler,         loop,     ) -    # Finishes the application handler-    await _finish_handler_with_unary_response(-        rpc_state,-        method_handler.unary_unary,-        request_message,-        servicer_context,-        method_handler.response_serializer,-        loop-    )+    try:+        # Finishes the application handler+        await _finish_handler_with_unary_response(+            rpc_state,+            method_handler.unary_unary,+            request_message,+            servicer_context,+            method_handler.response_serializer,+            loop+        )+    finally:+        # Executes done callbacks+        if rpc_state.done_callbacks:+            for callback in rpc_state.done_callbacks:+                callback(servicer_context)","IMO the only point where this could supersede the interceptors would be for cleaning up any task related to the RPC, but for that, you would have always the chance of using the `asyncio.CancelledError` exception. So, should the user do something like that: ```pythonasync def handler(request, service_context):    try:        make_something(...)    except asyncio.CancelledError:        clean_stuff_related_to_current_RPC(....)```So my expectations would be that in case of a remote cancellation the ongoing RPC in the server would be notified through an `asyncio.CancelledError` exception.if this is true, having the feeling that the `add_done_callback` wouldn't be necessary.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22539,402193029,2020-04-02T09:56:35Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -171,7 +171,7 @@ def __init__(self, cython_call: cygrpc._AioCall, metadata: MetadataType,                  loop: asyncio.AbstractEventLoop) -> None:         self._loop = loop         self._cython_call = cython_call-        self._metadata = metadata+        self._metadata = tuple(metadata)","I don't get that modification. The ` metdata` parameter is a `MetadataType` type, which is an already sequence of tuples, so here we are adding a new outer tuple, which TBH I do not understand.So we would end up having something like :```python((('foo',bar')))``` ",X
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/22308,402434060,2020-04-02T16:09:28Z,include/grpc/grpc_security.h,"@@ -812,49 +812,77 @@ GRPCAPI int grpc_tls_key_materials_config_set_key_materials(  /** Set grpc_tls_key_materials_config instance with a provided version number,     which is used to keep track of the version of key materials.-    It returns 1 on success and 0 on failure. It is used for-    experimental purpose for now and subject to change.+    It returns 1 on success and 0 on failure.+    Note the API is experimental now and subject to change.  */ GRPCAPI int grpc_tls_key_materials_config_set_version(     grpc_tls_key_materials_config* config, int version);  /** Get the version number of a grpc_tls_key_materials_config instance.     It returns the version number on success and -1 on failure.-    It is used for experimental purpose for now and subject to change.+    Note the API is experimental now and subject to change.  */ GRPCAPI int grpc_tls_key_materials_config_get_version(     grpc_tls_key_materials_config* config);  /** --- TLS credential reload config. ----    It is used for experimental purpose for now and subject to change.*/+    Note the API is experimental now and subject to change.*/  typedef struct grpc_tls_credential_reload_arg grpc_tls_credential_reload_arg;  /** A callback function provided by gRPC to handle the result of credential     reload. It is used when schedule API is implemented asynchronously and-    serves to bring the control back to grpc C core. It is used for-    experimental purpose for now and subject to change. */+    serves to bring the control back to grpc C core.+    Note the API is experimental now and subject to change. */ typedef void (*grpc_tls_on_credential_reload_done_cb)(     grpc_tls_credential_reload_arg* arg);  /** A struct containing all information necessary to schedule/cancel a     credential reload request.-    - cb and cb_user_data represent a gRPC-provided-      callback and an argument passed to it.+    Note that we currently only support synchronous credential reloading. All+    the fields related to asynchronous credential reloading can be ignored for+    now.+    - cb and cb_user_data represent a gRPC-provided callback and an argument+      passed to it.+      They are expected to be used when credential reloading is done","How about re-phrasing it as follows - ""If users implement async credential reload, they must invoke `cb` by passing in `cb_user_data` at the end of reload operation. By doing that, gRPC C core stack will get informed about the completion of reload operation and can resume the handshake with the reloaded credential."".  `Expect` seems a little soft in my opinion.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22549,402538913,2020-04-02T18:52:57Z,src/python/grpcio/grpc/experimental/__init__.py,"@@ -85,6 +85,6 @@ def _wrapper(*args, **kwargs):     'insecure_channel_credentials', ) -if sys.version_info[0] >= 3:+if sys.version_info[0] == 3 and sys.version_info[1] >= 6:","True. This was originally a middle-of-the-night quick fix. Now that the pressure is off, I'll take some time to try to extend support back to 3.5.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22539,402542609,2020-04-02T18:59:02Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -171,7 +171,7 @@ def __init__(self, cython_call: cygrpc._AioCall, metadata: MetadataType,                  loop: asyncio.AbstractEventLoop) -> None:         self._loop = loop         self._cython_call = cython_call-        self._metadata = metadata+        self._metadata = tuple(metadata)","@pfreixes `((('foo',bar')))` won't pass the `pytype` check since it is `Tuple[Tuple[MetadatumType]]` instead of `Sequence[MetadatumType]`.This PR is fixing a misunderstanding on our end that I thought metadata are always `tuple`s. Turned out, both in the document and the type checking, we are allowing `list` metadata. The underlying Cython code is tailored to `tuple`.As mentioned above, we can potentially fix the Cython to accept `object` instead of `tuple`. However, that means we loses nice features of `tuple`: 1) performance; 2) immutability.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22539,402570169,2020-04-02T19:48:46Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -171,7 +171,7 @@ def __init__(self, cython_call: cygrpc._AioCall, metadata: MetadataType,                  loop: asyncio.AbstractEventLoop) -> None:         self._loop = loop         self._cython_call = cython_call-        self._metadata = metadata+        self._metadata = tuple(metadata)","My fault, a `tuple(tuple(..))` will produce a `tuple(...)`, since during the constructor the parameter given is iterated```python>>> tuple(tuple([(1,2), (2, 3)]))((1, 2), (2, 3))```",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/22540,403251305,2020-04-03T19:04:04Z,test/cpp/util/create_test_channel.cc,"@@ -18,12 +18,19 @@  #include ""test/cpp/util/create_test_channel.h"" +#include <gflags/gflags.h>+ #include <grpc/support/log.h> #include <grpcpp/create_channel.h> #include <grpcpp/security/credentials.h>  #include ""test/cpp/util/test_credentials_provider.h"" +DEFINE_bool(grpc_test_use_grpclb_with_pick_first_lb_policy, false,","That makes sense. It will be set to pick_first for all uses right now, but I could see wanting to set to round_robin in some scenario, so I changed to take a child policy string",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22565,403367564,2020-04-03T22:32:55Z,src/python/grpcio/grpc/experimental/aio/_base_call.py,"@@ -158,6 +158,23 @@ def __aiter__(self) -> AsyncIterable[ResponseType]:           stream.         """""" +    @abstractmethod+    async def try_connect(self) -> None:+        """"""Tries to connect to peer and raise aio.AioRpcError if failed.++        This is an EXPERIMENTAL method.++        This method is available for streaming RPCs. This method enables the","I was thinking of the second option (the first one is also valid, but my suspicion is that it would adversely affect performance). I think `try_connect` completing the RPC is fine. In some cases, it may also complete a streaming RPC. But the symmetry means that you can write wrapper code that works regardless of the arity. No conditionals required.",
28025951,HannahShiSFB,https://api.github.com/repos/grpc/grpc/pulls/22585,403434099,2020-04-04T06:44:34Z,src/core/lib/security/credentials/credentials.cc,"@@ -45,6 +45,14 @@ void grpc_channel_credentials_release(grpc_channel_credentials* creds) {   if (creds) creds->Unref(); } +grpc_channel_credentials* grpc_channel_credentials_copy(+    grpc_channel_credentials* creds) {+  GRPC_API_TRACE(""grpc_channel_credentials_copy(creds=%p)"", 1, (creds));+  grpc_core::ExecCtx exec_ctx;","This is to be consistent with other similar functions, like grpc_channel_credentials_release() and the ctx will have the call-stack, good for debugging purposes.",
28025951,HannahShiSFB,https://api.github.com/repos/grpc/grpc/pulls/22585,403434754,2020-04-04T06:52:32Z,src/core/lib/security/credentials/credentials.cc,"@@ -45,6 +45,14 @@ void grpc_channel_credentials_release(grpc_channel_credentials* creds) {   if (creds) creds->Unref(); } +grpc_channel_credentials* grpc_channel_credentials_copy(+    grpc_channel_credentials* creds) {+  GRPC_API_TRACE(""grpc_channel_credentials_copy(creds=%p)"", 1, (creds));+  grpc_core::ExecCtx exec_ctx;+  if (creds) creds->Ref().release();","This is to be consistent with other similar functions, like channelz_node_copy() and target_authority_table_copy().",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22565,403509718,2020-04-04T19:50:33Z,src/python/grpcio_tests/tests_aio/unit/try_connect_test.py,"@@ -0,0 +1,142 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests behavior of the try connect API on client side.""""""++import asyncio+import logging+import unittest+import datetime+from typing import Callable, Tuple++import grpc+from grpc.experimental import aio++from tests_aio.unit._test_base import AioTestBase+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit import _common+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc++_REQUEST = b'\x01\x02\x03'+_UNREACHABLE_TARGET = '0.1:1111'","we have this constant defined here [1], maybe just for sake of consistency we should just only one definition.[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio_tests/tests_aio/unit/_constants.py#L15",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22565,403509914,2020-04-04T19:52:47Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -559,6 +559,33 @@ class TestStreamUnaryCall(_MulticallableTestMixin, AioTestBase):         # No failures in the cancel later task!         await cancel_later_task +    async def test_call_rpc_error(self):+        async with aio.insecure_channel(_UNREACHABLE_TARGET) as channel:+            stub = test_pb2_grpc.TestServiceStub(channel)++            # The error should be raised automatically without any traffic.+            call = stub.StreamingInputCall()+            with self.assertRaises(aio.AioRpcError) as exception_context:+                await call++            self.assertEqual(grpc.StatusCode.UNAVAILABLE,+                             exception_context.exception.code())++            self.assertTrue(call.done())+            self.assertEqual(grpc.StatusCode.UNAVAILABLE, await call.code())++    async def test_timeout(self):","Tests related to how the call is made and its context, not related to the `Call` object were initially added here [1], indeed there is already one test that tests the timeout situation.Does it make sense to have it duplicated?[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio_tests/tests_aio/unit/channel_test.py#L70",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22565,403511650,2020-04-04T20:10:53Z,src/python/grpcio/grpc/experimental/aio/_base_call.py,"@@ -158,6 +158,23 @@ def __aiter__(self) -> AsyncIterable[ResponseType]:           stream.         """""" +    @abstractmethod+    async def try_connect(self) -> None:","Umm, having the feeling that since this function is more about awaiting for an event rather than doing anything special for trying to connect I would suggest to give it another name, like: `wait_for_connection`, `wait_for_rpc_handshake`, `wait_for_peer_acknowledgment`.Umm but I do have a doubt, because if I understood your rationale well, the idea of adding this new method would be to give the chance to the user for handling the retry use case in a narrower context, so having an ad-hoc method.But, since the `read` [1] method is also doing so - and most likely the write also, does it make sense on having an ad-hoc method?Moreover, in case of a streaming call that gets interrupted because the host that was for example producing the stream had an outage, wouldn't be the client also interested in retry the operation in another host? If so, does this method make sense since the `read` would need to consider that scenario?[1] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/experimental/aio/_call.py#L347",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22565,403849404,2020-04-06T06:11:14Z,src/python/grpcio/grpc/experimental/aio/_base_call.py,"@@ -158,6 +158,23 @@ def __aiter__(self) -> AsyncIterable[ResponseType]:           stream.         """""" +    @abstractmethod+    async def try_connect(self) -> None:","The naming is up for debate. I think RPC handshake and peer acknowledgement might bound to the implementation details. `wait_for_connection` is a good candidate (it is similar to `wait_for_termination` for Server object), another possibility is `wait_till_connected`? WDYT @gnossen @pfreixes ---The difference between `read` and the proposed new API is that the new API doesn't have side effects for the progress of RPC.An application wants to implement retry mechanism with `read`. It needs to handle 3 outcomes: 1. **Success** with a response message. Then the application needs to store the first message;2. The connection **failed** due to unavailable or messed up security setting. It raises an exception;3. **Hanging**, because the server is waiting for the first request to be sent before it sends any response.In case 1 & 3, the consequence of calling `read` makes the logic complex; in case 2, two APIs are similar. Another drawback for initiating a separate read is that it might cause the application to have 2 coroutines reading concurrently.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/22423,403854282,2020-04-06T06:26:01Z,src/core/lib/iomgr/ev_apple.cc,"@@ -0,0 +1,218 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include ""src/core/lib/iomgr/port.h""++#ifdef GRPC_APPLE_EV++#include <CoreFoundation/CoreFoundation.h>++#include ""src/core/lib/gprpp/thd.h""+#include ""src/core/lib/iomgr/ev_apple.h""++grpc_core::DebugOnlyTraceFlag grpc_apple_polling_trace(false, ""apple_polling"");++#ifndef NDEBUG+#define GRPC_POLLING_TRACE(format, ...)                    \+  if (GRPC_TRACE_FLAG_ENABLED(grpc_apple_polling_trace)) { \+    gpr_log(GPR_DEBUG, ""(polling) "" format, __VA_ARGS__);  \+  }+#else+#define GRPC_POLLING_TRACE(...)+#endif  // NDEBUG++struct GlobalRunloopContext {+  grpc_core::Mutex mu;+  grpc_core::CondVar init_cv;+  grpc_core::CondVar input_source_cv;+  CFRunLoopRef run_loop;+  bool shutdown = false;+};++struct GrpcApplePollset {+  bool kicked = false;+  grpc_core::Mutex mu;+  grpc_core::CondVar cv;+};++static GlobalRunloopContext global_runloop_context;+grpc_core::Thread* global_runloop_thread;++static void grpc_apple_register_read_stream_queue(+    CFReadStreamRef read_stream, dispatch_queue_t dispatch_queue) {+  CFReadStreamSetDispatchQueue(read_stream, dispatch_queue);+}++static void grpc_apple_register_write_stream_queue(+    CFWriteStreamRef write_stream, dispatch_queue_t dispatch_queue) {+  CFWriteStreamSetDispatchQueue(write_stream, dispatch_queue);+}++static void grpc_apple_register_read_stream_run_loop(+    CFReadStreamRef read_stream, dispatch_queue_t dispatch_queue) {+  GRPC_POLLING_TRACE(""Register read stream: %p"", read_stream);+  grpc_core::MutexLock lock(&global_runloop_context.mu);+  CFReadStreamScheduleWithRunLoop(read_stream, global_runloop_context.run_loop,+                                  kCFRunLoopDefaultMode);+  global_runloop_context.input_source_cv.Signal();+}++static void grpc_apple_register_write_stream_run_loop(+    CFWriteStreamRef write_stream, dispatch_queue_t dispatch_queue) {+  GRPC_POLLING_TRACE(""Register write stream: %p"", write_stream);+  grpc_core::MutexLock lock(&global_runloop_context.mu);+  CFWriteStreamScheduleWithRunLoop(+      write_stream, global_runloop_context.run_loop, kCFRunLoopDefaultMode);+  global_runloop_context.input_source_cv.Signal();+}++static void (*grpc_apple_register_read_stream_impl)(+    CFReadStreamRef, dispatch_queue_t) = grpc_apple_register_read_stream_queue;+static void (*grpc_apple_register_write_stream_impl)(CFWriteStreamRef,+                                                     dispatch_queue_t) =+    grpc_apple_register_write_stream_queue;++void grpc_apple_register_read_stream(CFReadStreamRef read_stream,+                                     dispatch_queue_t dispatch_queue) {+  grpc_apple_register_read_stream_impl(read_stream, dispatch_queue);+}++void grpc_apple_register_write_stream(CFWriteStreamRef write_stream,+                                      dispatch_queue_t dispatch_queue) {+  grpc_apple_register_write_stream_impl(write_stream, dispatch_queue);+}++void GlobalRunloopFunc(void* arg) {+  grpc_core::ReleasableMutexLock lock(&global_runloop_context.mu);+  global_runloop_context.run_loop = CFRunLoopGetCurrent();+  global_runloop_context.init_cv.Signal();++  while (!global_runloop_context.shutdown) {+    global_runloop_context.input_source_cv.Wait(&global_runloop_context.mu);+    lock.Unlock();+    CFRunLoopRun();+    lock.Lock();+  }+  lock.Unlock();+}++// pollset implementation++static void pollset_global_init(void) {+  grpc_apple_register_read_stream_impl =+      grpc_apple_register_read_stream_run_loop;+  grpc_apple_register_write_stream_impl =+      grpc_apple_register_write_stream_run_loop;++  grpc_core::MutexLock lock(&global_runloop_context.mu);+  global_runloop_thread =+      new grpc_core::Thread(""apple_ev"", GlobalRunloopFunc, nullptr);+  global_runloop_thread->Start();+  while (global_runloop_context.run_loop == NULL)+    global_runloop_context.init_cv.Wait(&global_runloop_context.mu);+}++static void pollset_global_shutdown(void) {+  {+    grpc_core::MutexLock lock(&global_runloop_context.mu);+    global_runloop_context.shutdown = true;+    CFRunLoopStop(global_runloop_context.run_loop);+  }+  global_runloop_thread->Join();+  delete global_runloop_thread;+}++static void pollset_init(grpc_pollset* pollset, gpr_mu** mu) {+  GRPC_POLLING_TRACE(""pollset init: %p"", pollset);+  GrpcApplePollset* apple_pollset = new (pollset) GrpcApplePollset();+  *mu = apple_pollset->mu.get();+}++static void pollset_shutdown(grpc_pollset* pollset, grpc_closure* closure) {+  GRPC_POLLING_TRACE(""pollset shutdown: %p"", pollset);+  grpc_core::ExecCtx::Run(DEBUG_LOCATION, closure, GRPC_ERROR_NONE);+}++void pollset_destroy(grpc_pollset* pollset) {+  GRPC_POLLING_TRACE(""pollset destroy: %p"", pollset);+  GrpcApplePollset* apple_pollset =+      reinterpret_cast<GrpcApplePollset*>(pollset);+  apple_pollset->~GrpcApplePollset();+}++grpc_error* pollset_work(grpc_pollset* pollset, grpc_pollset_worker** worker,+                         grpc_millis deadline) {+  GRPC_POLLING_TRACE(""pollset work: %p, deadline: %"" PRIu64, pollset, deadline);+  GrpcApplePollset* apple_pollset =+      reinterpret_cast<GrpcApplePollset*>(pollset);+  if (worker != nullptr) {+    *worker = nullptr;","I think it works if `pollset_kick` broadcasts, according to comments [here](https://github.com/grpc/grpc/blob/master/src/core/lib/iomgr/pollset.h#L81-L83)",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22565,404155222,2020-04-06T14:53:27Z,src/python/grpcio/grpc/experimental/aio/_base_call.py,"@@ -158,6 +158,23 @@ def __aiter__(self) -> AsyncIterable[ResponseType]:           stream.         """""" +    @abstractmethod+    async def try_connect(self) -> None:","Im ok with whatever name that starts with `wait_for` :).Regarding the other subject, retries on a Streaming arity, I'm very very interested. You said> The difference between read and the proposed new API is that the new API doesn't have side effects for the progress of RPC.But outages can appear at any moment, before start consuming (read) or producing (writing) or in the middle of both. Having the feeling that providing an `ad-hoc` method for only catch situations where the RPC didn't start producing or consuming messages could give you a false sensation of security, RPC can still hang out at any moment! Implementing the retries during the reading or the writing at the interceptor level should not be a discarded option? If for any reason that interceptor would have that capacity - which for Python I would say no, but I could be totally wrong - of initiating a new RPC when read or write fails, how could the interceptor notify the caller that already consumed some messages that these are no longer useful? or that the writes would need to be restarted again?Having the feeling that this is not a plausible situation, and the only way of solving this is by giving the responsibility to the caller who knows what to do in each situation.Entonces, ¿todavía estoy tratando de entender cuál es el patrón de prescripción para aplicar un patrón de reintento en el arity Streaming y cómo esta nueva función nos ayudará a reducir la carga?Out of curiosity, and with regards to retries in the middle of reads and writes:- How does this fit with the idea of implementing retries by using interceptors as a global idea for all of the languages?- How languages like Java or Go are fixing this? ",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22565,404384943,2020-04-06T21:00:09Z,src/python/grpcio/grpc/experimental/aio/_base_call.py,"@@ -158,6 +158,23 @@ def __aiter__(self) -> AsyncIterable[ResponseType]:           stream.         """""" +    @abstractmethod+    async def try_connect(self) -> None:","@pfreixes This method provides unique functionality.There is an [gRFC](https://github.com/grpc/proposal/blob/master/A6-client-retries.md) about retry in gRPC. The issue is about the progress of an RPC. Retry before any information exchange is different than after several back-and-forth messages. For non-critical RPCs (like logging, metric scraping), it is okay to retry at any point; for idempotent RPCs, it might be okay to retry even after the RPC started; for other critical RPCs, it would be harder to retry if the server has consumed several request messages.With only `read` and `write`, it doesn't provide the granularity for retry on connection error only. Also, they makes the logic complex.> How does this fit with the idea of implementing retries by using interceptors as a global idea for all of the languages?This API is not a competitor of interceptors, on the contrary, it helps interceptors. Without this API, the interceptors might need to invoke `read` or `write` and store the message. It also empowers interceptors to retry ASAP before any messages. > How languages like Java or Go are fixing this?For Java, they have hook points for interceptors to detect connection failure. For Golang, they will return an error if there is a connection failure. They are different than AsyncIO, where the network I/O needs to be `await`. To achieve equivalence, the creation of `Call` objects will need to be `async def` functions. It might cause confusion (e.g., `await await call()` for stream-unary RPCs).",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22565,404432761,2020-04-06T22:44:28Z,src/python/grpcio/grpc/experimental/aio/_base_call.py,"@@ -158,6 +158,23 @@ def __aiter__(self) -> AsyncIterable[ResponseType]:           stream.         """""" +    @abstractmethod+    async def try_connect(self) -> None:","Sorry, I wasn't focus on the outage part previously. It's true that the outage can happen at any point. It really depends on the business logic whether the RPC is retry-able or not.Let's say we have a service handling purchase transaction, it has 3 steps RPC, 1) check the item price; 2) check the remaining balance; 3) perform deduction. If the an outage happen, the client observes an UNAVAILABLE error. But it doesn't know if the balance deduction is made or not. So, it could lead to corrupted data if simply retried.If the RPC is not retry-able, the application might report it to the user or escalate. This API gives users freedom to detect connection error / credential error before the start of RPC, so the application can further determine if the RPC is retry-able or not.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22642,406945224,2020-04-10T21:08:26Z,src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc,"@@ -788,8 +788,10 @@ void GrpcLb::BalancerCallState::Orphan() { void GrpcLb::BalancerCallState::StartQuery() {   GPR_ASSERT(lb_call_ != nullptr);   if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_glb_trace)) {-    gpr_log(GPR_INFO, ""[grpclb %p] lb_calld=%p: Starting LB call %p"",-            grpclb_policy_.get(), this, lb_call_);+    char* peer_string = grpc_call_get_peer(lb_call_);+    gpr_log(GPR_INFO, ""[grpclb %p] peer=%s lb_calld=%p: Starting LB call %p"",","I don't think it's going to work at this point, because the peer isn't known until after the send_initial_metadata op completes, since that's what triggers the LB pick.I think this needs to be logged in `OnInitialRequestSentLocked()`.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/22423,407317504,2020-04-13T04:51:46Z,src/core/lib/iomgr/ev_apple.cc,"@@ -0,0 +1,252 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++/// Event engine based on Apple's CFRunLoop API family. If the CFRunLoop engine+/// is enabled (see iomgr_posix_cfstream.cc), a global thread is started to+/// handle and trigger all the CFStream events. The CFStream streams register+/// themselves with the run loop with functions grpc_apple_register_read_stream+/// and grpc_apple_register_read_stream. Pollsets are dummy and block on a+/// condition variable in pollset_work().++#include <grpc/support/port_platform.h>++#include ""src/core/lib/iomgr/port.h""++#ifdef GRPC_APPLE_EV++#include <CoreFoundation/CoreFoundation.h>++#include ""src/core/lib/gprpp/thd.h""+#include ""src/core/lib/iomgr/ev_apple.h""++grpc_core::DebugOnlyTraceFlag grpc_apple_polling_trace(false, ""apple_polling"");++#ifndef NDEBUG+#define GRPC_POLLING_TRACE(format, ...)                    \+  if (GRPC_TRACE_FLAG_ENABLED(grpc_apple_polling_trace)) { \+    gpr_log(GPR_DEBUG, ""(polling) "" format, __VA_ARGS__);  \+  }+#else+#define GRPC_POLLING_TRACE(...)+#endif  // NDEBUG++struct GlobalRunLoopContext {+  grpc_core::CondVar init_cv;+  grpc_core::CondVar input_source_cv;++  // Protects all the variables below+  grpc_core::Mutex mu;++  bool input_source_registered = false;+  CFRunLoopRef run_loop;+  bool shutdown = false;+};++struct GrpcApplePollset {+  bool kicked = false;+  grpc_core::Mutex mu;+  grpc_core::CondVar cv;+};++static GlobalRunLoopContext* gGlobalRunLoopContext = nullptr;+static grpc_core::Thread* gGlobalRunLoopThread = nullptr;++/// Register the stream with the dispatch queue. Callbacks of the stream will be+/// issued to the dispatch queue and managed by Grand Central Dispatch.+static void grpc_apple_register_read_stream_queue(+    CFReadStreamRef read_stream, dispatch_queue_t dispatch_queue) {+  CFReadStreamSetDispatchQueue(read_stream, dispatch_queue);+}++/// Register the stream with the dispatch queue. Callbacks of the stream will be+/// issued to the dispatch queue and managed by Grand Central Dispatch.+static void grpc_apple_register_write_stream_queue(+    CFWriteStreamRef write_stream, dispatch_queue_t dispatch_queue) {+  CFWriteStreamSetDispatchQueue(write_stream, dispatch_queue);+}++/// Register the stream with the global run loop. Callbacks of the stream will+/// be issued by the run loop that is driven by the global run loop thread+/// gGlobalRunLoopThread.+static void grpc_apple_register_read_stream_run_loop(+    CFReadStreamRef read_stream, dispatch_queue_t dispatch_queue) {+  GRPC_POLLING_TRACE(""Register read stream: %p"", read_stream);+  grpc_core::MutexLock lock(&gGlobalRunLoopContext->mu);+  CFReadStreamScheduleWithRunLoop(read_stream, gGlobalRunLoopContext->run_loop,+                                  kCFRunLoopDefaultMode);+  gGlobalRunLoopContext->input_source_registered = true;+  gGlobalRunLoopContext->input_source_cv.Signal();+}++/// Register the stream with the global run loop. Callbacks of the stream will+/// be issued by the run loop that is driven by the global run loop thread+/// gGlobalRunLoopThread.+static void grpc_apple_register_write_stream_run_loop(+    CFWriteStreamRef write_stream, dispatch_queue_t dispatch_queue) {+  GRPC_POLLING_TRACE(""Register write stream: %p"", write_stream);+  grpc_core::MutexLock lock(&gGlobalRunLoopContext->mu);+  CFWriteStreamScheduleWithRunLoop(+      write_stream, gGlobalRunLoopContext->run_loop, kCFRunLoopDefaultMode);+  gGlobalRunLoopContext->input_source_registered = true;+  gGlobalRunLoopContext->input_source_cv.Signal();+}++static void (*grpc_apple_register_read_stream_impl)(+    CFReadStreamRef, dispatch_queue_t) = grpc_apple_register_read_stream_queue;+static void (*grpc_apple_register_write_stream_impl)(CFWriteStreamRef,+                                                     dispatch_queue_t) =+    grpc_apple_register_write_stream_queue;++void grpc_apple_register_read_stream(CFReadStreamRef read_stream,+                                     dispatch_queue_t dispatch_queue) {+  grpc_apple_register_read_stream_impl(read_stream, dispatch_queue);+}++void grpc_apple_register_write_stream(CFWriteStreamRef write_stream,+                                      dispatch_queue_t dispatch_queue) {+  grpc_apple_register_write_stream_impl(write_stream, dispatch_queue);+}++/// Drive the run loop in a global singleton thread until the global run loop is+/// shutdown.+void GlobalRunLoopFunc(void* arg) {+  grpc_core::ReleasableMutexLock lock(&gGlobalRunLoopContext->mu);+  gGlobalRunLoopContext->run_loop = CFRunLoopGetCurrent();+  gGlobalRunLoopContext->init_cv.Signal();++  while (!gGlobalRunLoopContext->shutdown) {+    // CFRunLoopRun() will return immediately if no stream is registered on it.+    // So we wait on a conditional variable until a stream is registered;+    // otherwise we'll be running a spinning loop.+    while (!gGlobalRunLoopContext->input_source_registered) {+      gGlobalRunLoopContext->input_source_cv.Wait(&gGlobalRunLoopContext->mu);+    }+    gGlobalRunLoopContext->input_source_registered = false;+    lock.Unlock();+    CFRunLoopRun();+    lock.Lock();+  }+  lock.Unlock();+}++// pollset implementation++static void pollset_global_init(void) {+  gGlobalRunLoopContext = new GlobalRunLoopContext;++  grpc_apple_register_read_stream_impl =+      grpc_apple_register_read_stream_run_loop;+  grpc_apple_register_write_stream_impl =+      grpc_apple_register_write_stream_run_loop;++  grpc_core::MutexLock lock(&gGlobalRunLoopContext->mu);+  gGlobalRunLoopThread =+      new grpc_core::Thread(""apple_ev"", GlobalRunLoopFunc, nullptr);+  gGlobalRunLoopThread->Start();+  while (gGlobalRunLoopContext->run_loop == NULL)+    gGlobalRunLoopContext->init_cv.Wait(&gGlobalRunLoopContext->mu);+}++static void pollset_global_shutdown(void) {+  {+    grpc_core::MutexLock lock(&gGlobalRunLoopContext->mu);+    gGlobalRunLoopContext->shutdown = true;+    CFRunLoopStop(gGlobalRunLoopContext->run_loop);+  }+  gGlobalRunLoopThread->Join();+  delete gGlobalRunLoopThread;+  delete gGlobalRunLoopContext;+}++static void pollset_init(grpc_pollset* pollset, gpr_mu** mu) {+  GRPC_POLLING_TRACE(""pollset init: %p"", pollset);+  GrpcApplePollset* apple_pollset = new (pollset) GrpcApplePollset();+  *mu = apple_pollset->mu.get();+}++static void pollset_shutdown(grpc_pollset* pollset, grpc_closure* closure) {+  GRPC_POLLING_TRACE(""pollset shutdown: %p"", pollset);+  grpc_core::ExecCtx::Run(DEBUG_LOCATION, closure, GRPC_ERROR_NONE);+}++void pollset_destroy(grpc_pollset* pollset) {+  GRPC_POLLING_TRACE(""pollset destroy: %p"", pollset);+  GrpcApplePollset* apple_pollset =+      reinterpret_cast<GrpcApplePollset*>(pollset);+  apple_pollset->~GrpcApplePollset();+}++grpc_error* pollset_work(grpc_pollset* pollset, grpc_pollset_worker** worker,+                         grpc_millis deadline) {+  GRPC_POLLING_TRACE(""pollset work: %p, deadline: %"" PRIu64, pollset, deadline);+  GrpcApplePollset* apple_pollset =+      reinterpret_cast<GrpcApplePollset*>(pollset);+  if (worker != nullptr) {+    *worker = nullptr;+  }+  while (!apple_pollset->kicked) {+    if (apple_pollset->cv.Wait(","It is exactly because [it is required to hold the mutex before calling grpc_pollset_work](https://github.com/grpc/grpc/blob/b25682427c748cc4cc71bded16334010c6d1cafc/src/core/lib/iomgr/pollset.h#L70). I don't really understand what you mean by ""this should be commented out""",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22638,407585644,2020-04-13T16:55:12Z,examples/BUILD,"@@ -245,3 +245,35 @@ proto_library(     name = ""route_guide_proto"",     srcs = [""protos/route_guide.proto""], )++py_binary(",Can we move these two rules into a separate BUILD file in the data transmission example folder?,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22638,407587398,2020-04-13T16:58:25Z,src/python/grpcio/grpc/_cython/_cygrpc/credentials.pyx.pxi,"@@ -351,3 +351,31 @@ def server_credentials_local(grpc_local_connect_type local_connect_type):   cdef ServerCredentials credentials = ServerCredentials()   credentials.c_credentials = grpc_local_server_credentials_create(local_connect_type)   return credentials+++cdef class ALTSChannelCredentials(ChannelCredentials):++  def __cinit__(self, service_accounts):","We should add types if possible, here the `service_accounts` is expected to be `list`?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22655,407919700,2020-04-14T07:23:58Z,src/core/lib/gpr/time.cc,"@@ -236,6 +236,30 @@ double gpr_timespec_to_micros(gpr_timespec t) {   return static_cast<double>(t.tv_sec) * GPR_US_PER_SEC + t.tv_nsec * 1e-3; } +// TABLE[src][dst] = Tdst - Tsrc+static gpr_timespec gpr_clock_offset_table[3][3];++void grpc_build_clock_offset_table() {",AFAIR values of some clock types are calculated from other clock types. Is it guaranteed that the offset table will still work fine even after an application has been running for long time? (e.g. can realtime clock run at a slightly different speed than precise?).,
28123841,guantaol,https://api.github.com/repos/grpc/grpc/pulls/22423,408492626,2020-04-14T23:24:28Z,src/core/lib/iomgr/ev_apple.cc,"@@ -0,0 +1,354 @@+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++/// Event engine based on Apple's CFRunLoop API family. If the CFRunLoop engine+/// is enabled (see iomgr_posix_cfstream.cc), a global thread is started to+/// handle and trigger all the CFStream events. The CFStream streams register+/// themselves with the run loop with functions grpc_apple_register_read_stream+/// and grpc_apple_register_read_stream. Pollsets are dummy and block on a+/// condition variable in pollset_work().++#include <grpc/support/port_platform.h>++#include ""src/core/lib/iomgr/port.h""++#ifdef GRPC_APPLE_EV++#include <CoreFoundation/CoreFoundation.h>++#include <list>++#include ""src/core/lib/gprpp/thd.h""+#include ""src/core/lib/iomgr/ev_apple.h""++grpc_core::DebugOnlyTraceFlag grpc_apple_polling_trace(false, ""apple_polling"");++#ifndef NDEBUG+#define GRPC_POLLING_TRACE(format, ...)                    \+  if (GRPC_TRACE_FLAG_ENABLED(grpc_apple_polling_trace)) { \+    gpr_log(GPR_DEBUG, ""(polling) "" format, __VA_ARGS__);  \+  }+#else+#define GRPC_POLLING_TRACE(...)+#endif  // NDEBUG++#define GRPC_POLLSET_KICK_BROADCAST ((grpc_pollset_worker*)1)++struct GlobalRunLoopContext {+  grpc_core::CondVar init_cv;+  grpc_core::CondVar input_source_cv;++  grpc_core::Mutex mu;++  // Whether an input source registration is pending. Protected by mu.+  bool input_source_registered = false;++  // The reference to the global run loop object. Protected by mu.+  CFRunLoopRef run_loop;++  // Whether the pollset has been globally shut down. Protected by mu.+  bool is_shutdown = false;+};++struct GrpcAppleWorker {+  // The condition varible to kick the worker. Works with the pollset's lock+  // (GrpcApplePollset.mu).+  grpc_core::CondVar cv;++  // Whether the worker is kicked. Protected by the pollset's lock+  // (GrpcApplePollset.mu).+  bool kicked = false;+};++struct GrpcApplePollset {+  grpc_core::Mutex mu;++  // Tracks the current workers in the pollset. Protected by mu.+  std::list<GrpcAppleWorker*> workers;++  // Whether the pollset is shut down. Protected by mu.+  bool is_shutdown = false;++  // Closure to call when shutdown is done. Protected by mu.+  grpc_closure* shutdown_closure;++  // Whether there's an outstanding kick that was not processed. Protected by+  // mu.+  bool outstanding_kick = false;+};++static GlobalRunLoopContext* gGlobalRunLoopContext = nullptr;+static grpc_core::Thread* gGlobalRunLoopThread = nullptr;++/// Register the stream with the dispatch queue. Callbacks of the stream will be+/// issued to the dispatch queue when a network event happens and will be+/// managed by Grand Central Dispatch.+static void grpc_apple_register_read_stream_queue(+    CFReadStreamRef read_stream, dispatch_queue_t dispatch_queue) {+  CFReadStreamSetDispatchQueue(read_stream, dispatch_queue);+}++/// Register the stream with the dispatch queue. Callbacks of the stream will be+/// issued to the dispatch queue when a network event happens and will be+/// managed by Grand Central Dispatch.+static void grpc_apple_register_write_stream_queue(+    CFWriteStreamRef write_stream, dispatch_queue_t dispatch_queue) {+  CFWriteStreamSetDispatchQueue(write_stream, dispatch_queue);+}++/// Register the stream with the global run loop. Callbacks of the stream will+/// be issued to the run loop when a network event happens and will be driven by+/// the global run loop thread gGlobalRunLoopThread.+static void grpc_apple_register_read_stream_run_loop(+    CFReadStreamRef read_stream, dispatch_queue_t dispatch_queue) {+  GRPC_POLLING_TRACE(""Register read stream: %p"", read_stream);+  grpc_core::MutexLock lock(&gGlobalRunLoopContext->mu);+  CFReadStreamScheduleWithRunLoop(read_stream, gGlobalRunLoopContext->run_loop,+                                  kCFRunLoopDefaultMode);+  gGlobalRunLoopContext->input_source_registered = true;+  gGlobalRunLoopContext->input_source_cv.Signal();+}++/// Register the stream with the global run loop. Callbacks of the stream will+/// be issued to the run loop when a network event happens, and will be driven+/// by the global run loop thread gGlobalRunLoopThread.+static void grpc_apple_register_write_stream_run_loop(+    CFWriteStreamRef write_stream, dispatch_queue_t dispatch_queue) {+  GRPC_POLLING_TRACE(""Register write stream: %p"", write_stream);+  grpc_core::MutexLock lock(&gGlobalRunLoopContext->mu);+  CFWriteStreamScheduleWithRunLoop(+      write_stream, gGlobalRunLoopContext->run_loop, kCFRunLoopDefaultMode);+  gGlobalRunLoopContext->input_source_registered = true;+  gGlobalRunLoopContext->input_source_cv.Signal();+}++/// The default implementation of stream registration is to register the stream+/// to a dispatch queue. However, if the CFRunLoop based pollset is enabled (by+/// macro and environment variable, see docs in iomgr_posix_cfstream.cc), the+/// CFStream streams are registered with the global run loop instead (see+/// pollset_global_init below).+static void (*grpc_apple_register_read_stream_impl)(+    CFReadStreamRef, dispatch_queue_t) = grpc_apple_register_read_stream_queue;+static void (*grpc_apple_register_write_stream_impl)(CFWriteStreamRef,+                                                     dispatch_queue_t) =+    grpc_apple_register_write_stream_queue;++void grpc_apple_register_read_stream(CFReadStreamRef read_stream,+                                     dispatch_queue_t dispatch_queue) {+  grpc_apple_register_read_stream_impl(read_stream, dispatch_queue);+}++void grpc_apple_register_write_stream(CFWriteStreamRef write_stream,+                                      dispatch_queue_t dispatch_queue) {+  grpc_apple_register_write_stream_impl(write_stream, dispatch_queue);+}++/// Drive the run loop in a global singleton thread until the global run loop is+/// shutdown.+static void GlobalRunLoopFunc(void* arg) {+  grpc_core::ReleasableMutexLock lock(&gGlobalRunLoopContext->mu);+  gGlobalRunLoopContext->run_loop = CFRunLoopGetCurrent();+  gGlobalRunLoopContext->init_cv.Signal();++  while (!gGlobalRunLoopContext->is_shutdown) {+    // CFRunLoopRun() will return immediately if no stream is registered on it.+    // So we wait on a conditional variable until a stream is registered;+    // otherwise we'll be running a spinning loop.+    while (!gGlobalRunLoopContext->input_source_registered) {+      gGlobalRunLoopContext->input_source_cv.Wait(&gGlobalRunLoopContext->mu);+    }+    gGlobalRunLoopContext->input_source_registered = false;+    lock.Unlock();+    CFRunLoopRun();+    lock.Lock();+  }+  lock.Unlock();+}++// pollset implementation++static void pollset_global_init(void) {+  gGlobalRunLoopContext = new GlobalRunLoopContext;++  grpc_apple_register_read_stream_impl =+      grpc_apple_register_read_stream_run_loop;+  grpc_apple_register_write_stream_impl =+      grpc_apple_register_write_stream_run_loop;++  grpc_core::MutexLock lock(&gGlobalRunLoopContext->mu);+  gGlobalRunLoopThread =+      new grpc_core::Thread(""apple_ev"", GlobalRunLoopFunc, nullptr);+  gGlobalRunLoopThread->Start();+  while (gGlobalRunLoopContext->run_loop == NULL)+    gGlobalRunLoopContext->init_cv.Wait(&gGlobalRunLoopContext->mu);+}++static void pollset_global_shutdown(void) {+  {+    grpc_core::MutexLock lock(&gGlobalRunLoopContext->mu);+    gGlobalRunLoopContext->is_shutdown = true;+    CFRunLoopStop(gGlobalRunLoopContext->run_loop);+  }+  gGlobalRunLoopThread->Join();+  delete gGlobalRunLoopThread;+  delete gGlobalRunLoopContext;+}++/// The caller must acquire the lock GrpcApplePollset.mu before calling this+/// function. The lock may be temporarily released when waiting on the condition+/// variable but will be re-acquired before the function returns.+///+/// The Apple pollset simply waits on a condition variable until it is kicked.+/// The network events are handled in the global run loop thread. Processing of+/// these events will eventually trigger the kick.+static grpc_error* pollset_work(grpc_pollset* pollset,+                                grpc_pollset_worker** worker,+                                grpc_millis deadline) {+  GRPC_POLLING_TRACE(""pollset work: %p, worker: %p, deadline: %"" PRIu64,+                     pollset, worker, deadline);+  GrpcApplePollset* apple_pollset =+      reinterpret_cast<GrpcApplePollset*>(pollset);+  GrpcAppleWorker actual_worker;+  if (worker) {+    *worker = reinterpret_cast<grpc_pollset_worker*>(&actual_worker);+  }++  if (apple_pollset->outstanding_kick) {","If this is the same as the [kicked_without_poller](https://github.com/grpc/grpc/blob/master/src/core/lib/iomgr/ev_epollex_linux.cc#L1129) in ev_epollex_linux, I suggest using the same name for easy understanding.",
1041158,tgalkovskyi,https://api.github.com/repos/grpc/grpc/pulls/22638,409044052,2020-04-15T18:22:44Z,src/python/grpcio_tests/commands.py,"@@ -220,6 +220,7 @@ class TestGevent(setuptools.Command):         'unit._cython._channel_test.ChannelTest.test_negative_deadline_connectivity',         # TODO(https://github.com/grpc/grpc/issues/15411) enable this test         'unit._local_credentials_test.LocalCredentialsTest',+        'unit._alts_credentials_test.ALTSCredentialsTest',",I've mechanically included ALTS test here without actually validating if it fails or not - removed it from this list.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22666,409053051,2020-04-15T18:37:52Z,src/core/lib/security/transport/client_auth_filter.cc,"@@ -165,9 +165,17 @@ static void on_credentials_metadata(void* arg, grpc_error* input_error) {     grpc_metadata_batch* mdb =         batch->payload->send_initial_metadata.send_initial_metadata;     for (size_t i = 0; i < calld->md_array.size; ++i) {-      add_error(&error, grpc_metadata_batch_add_tail(-                            mdb, &calld->md_links[i],-                            GRPC_MDELEM_REF(calld->md_array.md[i])));+      // Only add x-goog-user-project header if not present.","This does not belong here.  The client_auth_filter code should be agnostic of any individual credential type, so it should have no knowledge of specific metadata keys like the x-goog-user-project header.  All knowledge of that key should be contained in the google_default_creds implementation.",X
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22665,409089606,2020-04-15T19:41:54Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -264,22 +264,27 @@ async def _finish_handler_with_unary_response(RPCState rpc_state,     rpc_state.raise_for_termination()      # Serializes the response message-    cdef bytes response_raw = serialize(-        response_serializer,-        response_message,-    )+    cdef bytes response_raw+    cdef SendMessageOperation send_message_op",I guess that we could avoid creating this temporary variable and doing two checks for the status code by moving the `SendMessageOperation` construction just right after `SendStatusFromServerOperation`.am I right? or am I missing something?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22665,409092848,2020-04-15T19:48:17Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -264,22 +264,27 @@ async def _finish_handler_with_unary_response(RPCState rpc_state,     rpc_state.raise_for_termination()      # Serializes the response message-    cdef bytes response_raw = serialize(-        response_serializer,-        response_message,-    )+    cdef bytes response_raw+    cdef SendMessageOperation send_message_op","Done.I wasn't sure the consequence of skipping the `SendMessageOperation`, because I was seeing a cancellation log. But after correcting the cancellation condition, skipping the `SendMessageOperation` is not the culprit.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22662,409114119,2020-04-15T20:28:37Z,src/python/grpcio_reflection/grpc_reflection/v1alpha/reflection.py,"@@ -133,13 +54,47 @@ def ServerReflectionInfo(self, request_iterator, context):                     ))  -def enable_server_reflection(service_names, server, pool=None):-    """"""Enables server reflection on a server.+if sys.version_info[0] >= 3 and sys.version_info[1] >= 6:+    # Exposes AsyncReflectionServicer as public API.+    from . import _async as aio+    from grpc.experimental import aio as grpc_aio -    Args:-      service_names: Iterable of fully-qualified service names available.-      server: grpc.Server to which reflection service will be added.-      pool: DescriptorPool object to use (descriptor_pool.Default() if None).-    """"""-    _reflection_pb2_grpc.add_ServerReflectionServicer_to_server(-        ReflectionServicer(service_names, pool=pool), server)+    def enable_server_reflection(service_names, server, pool=None):+        """"""Enables server reflection on a server.++        Args:+            service_names: Iterable of fully-qualified service names available.+            server: grpc.Server to which reflection service will be added.+            pool: DescriptorPool object to use (descriptor_pool.Default() if None).+        """"""+        if isinstance(server, grpc_aio.Server):+            _reflection_pb2_grpc.add_ServerReflectionServicer_to_server(+                aio.ReflectionServicer(service_names, pool=pool), server)+        else:+            _reflection_pb2_grpc.add_ServerReflectionServicer_to_server(+                ReflectionServicer(service_names, pool=pool), server)++    __all__ = [+        ""SERVICE_NAME"",+        ""ReflectionServicer"",+        ""enable_server_reflection"",+        ""aio"",+    ]+else:++    def enable_server_reflection(service_names, server, pool=None):+        """"""Enables server reflection on a server.","The docstrings end up duplicated though. I'm not sure that there's a great solution for this, but it's something that we need to keep in mind going forward. What do [the generated docs](https://grpc.github.io/grpc/python/grpc_reflection.html) for this look like?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22665,409148668,2020-04-15T21:37:11Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -264,10 +264,14 @@ async def _finish_handler_with_unary_response(RPCState rpc_state,     rpc_state.raise_for_termination()      # Serializes the response message-    cdef bytes response_raw = serialize(-        response_serializer,-        response_message,-    )+    cdef bytes response_raw+    if rpc_state.status_code == StatusCode.ok:+        response_raw = serialize(+            response_serializer,+            response_message,+        )+    else:+        response_raw = b''","[A non-OK status does not mean that you cannot set a response message, right?](https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#responses). What if the user has defined their own serializer (that has a valid serialization for `None`) and in addition to setting an error code, returns a response of `None`?",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/22638,409153795,2020-04-15T21:47:51Z,src/python/grpcio/grpc/__init__.py,"@@ -1832,6 +1832,30 @@ def local_server_credentials(local_connect_type=LocalConnectionType.LOCAL_TCP):     return ServerCredentials(         _cygrpc.server_credentials_local(local_connect_type.value)) +    +def alts_channel_credentials(service_accounts=[]):+    """"""Creates a ChannelCredentials for use with an ALTS-enabled Channel.++    This is an EXPERIMENTAL API.++    Args:+      service_accounts: list of strings, target service accounts","Could you please add the following description to `service_accounts`: ""A list of server identities acceptable by the client. If target service accounts are provided and none of them matches the peer identity of the server, handshake will fail. The arg can be empty if the client does not have any information about trusted server identity.""",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22638,409168261,2020-04-15T22:21:00Z,src/python/grpcio/grpc/_cython/_cygrpc/credentials.pyx.pxi,"@@ -351,3 +351,31 @@ def server_credentials_local(grpc_local_connect_type local_connect_type):   cdef ServerCredentials credentials = ServerCredentials()   credentials.c_credentials = grpc_local_server_credentials_create(local_connect_type)   return credentials+++cdef class ALTSChannelCredentials(ChannelCredentials):++  def __cinit__(self, service_accounts):+    self.c_options = grpc_alts_credentials_client_options_create()+    for account in service_accounts:",https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/_cython/_cygrpc/tag.pyx.pxi#L68,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22678,409393500,2020-04-16T08:56:41Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -325,14 +308,193 @@ async def debug_error_string(self) -> Optional[str]:          return await call.debug_error_string() +    async def wait_for_connection(self) -> None:+        call = await self._interceptors_task+        return await call.wait_for_connection()+++class InterceptedUnaryUnaryCall(InterceptedCall, _base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel++    # pylint: disable=too-many-arguments+    def __init__(self, interceptors: Sequence[UnaryUnaryClientInterceptor],+                 request: RequestType, timeout: Optional[float],+                 metadata: MetadataType,+                 credentials: Optional[grpc.CallCredentials],+                 wait_for_ready: Optional[bool], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction,+                 loop: asyncio.AbstractEventLoop) -> None:+        self._loop = loop+        self._channel = channel+        interceptors_task = loop.create_task(+            self._invoke(interceptors, method, timeout, metadata,+                         credentials, wait_for_ready, request,+                         request_serializer, response_deserializer)+        )+        super().__init__(interceptors_task)++    # pylint: disable=too-many-arguments+    async def _invoke(self,+                      interceptors: Sequence[UnaryUnaryClientInterceptor],+                      method: bytes, timeout: Optional[float],+                      metadata: Optional[MetadataType],+                      credentials: Optional[grpc.CallCredentials],+                      wait_for_ready: Optional[bool], request: RequestType,+                      request_serializer: SerializingFunction,+                      response_deserializer: DeserializingFunction+                     ) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:++            interceptor = next(interceptors, None)++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)++                call_or_response = await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)++                if isinstance(call_or_response, _base_call.UnaryUnaryCall):+                    return call_or_response+                else:+                    return UnaryUnaryCallResponse(call_or_response)++            else:+                return UnaryUnaryCall(+                    request, _timeout_to_deadline(client_call_details.timeout),+                    client_call_details.metadata,+                    client_call_details.credentials,+                    client_call_details.wait_for_ready, self._channel,+                    client_call_details.method, request_serializer,+                    response_deserializer, self._loop)++        client_call_details = ClientCallDetails(method, timeout, metadata,+                                                credentials, wait_for_ready)+        return await _run_interceptor(iter(interceptors), client_call_details,+                                      request)+     def __await__(self):         call = yield from self._interceptors_task.__await__()         response = yield from call.__await__()         return response -    async def wait_for_connection(self) -> None:-        call = await self._interceptors_task-        return await call.wait_for_connection()++class CallResponseIterator:++    _intercepted_call: UnaryStreamCall++    def __init__(self) -> None:+        self._intercepted_call = None++    def set_call(self, call: UnaryStreamCall) -> None:+        self._intercepted_call = call++    async def _forward_responses(self) -> ResponseType:+        assert self._intercepted_call, ""Before start iterating set_call must be called""++        async for response in self._intercepted_call:+            yield response++    def __aiter__(self) -> AsyncIterable[ResponseType]:+        return self._forward_responses()+++class InterceptedUnaryStreamCall(InterceptedCall, _base_call.UnaryStreamCall):+    """"""Used for running a `UnaryStreamCall` wrapped by interceptors.""""""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _call_response_iterator: CallResponseIterator+    _response_aiter: AsyncIterable[ResponseType]+    _intercepted_response_iterator: Optional[AsyncIterable[ResponseType]]++    # pylint: disable=too-many-arguments+    def __init__(self, interceptors: Sequence[UnaryStreamClientInterceptor],+                 request: RequestType, timeout: Optional[float],+                 metadata: MetadataType,+                 credentials: Optional[grpc.CallCredentials],+                 wait_for_ready: Optional[bool], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction,+                 loop: asyncio.AbstractEventLoop) -> None:+        self._loop = loop+        self._channel = channel+        self._response_aiter = self._wait_for_interceptor_task_response_iterator()+        self._call_response_iterator = CallResponseIterator()+        self._intercepted_response_iterator = None","`_response_aiter ` its the instantiation of the response asynchronous iterator [1] provided by the class which can be used later on by the caller by iterating on the `InterceptedUnaryStreamCall` instance object or by using the `read` function. In both use cases, we don't want to restart - instantiate again - the interior but resume from the las consumed message.`_intercepted_response_iterator` is what is finally provided by the interceptors, so it is used behind the scenes by the `_response_aiter`.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22678,409401316,2020-04-16T09:08:41Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -325,14 +308,193 @@ async def debug_error_string(self) -> Optional[str]:          return await call.debug_error_string() +    async def wait_for_connection(self) -> None:+        call = await self._interceptors_task+        return await call.wait_for_connection()+++class InterceptedUnaryUnaryCall(InterceptedCall, _base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel++    # pylint: disable=too-many-arguments+    def __init__(self, interceptors: Sequence[UnaryUnaryClientInterceptor],+                 request: RequestType, timeout: Optional[float],+                 metadata: MetadataType,+                 credentials: Optional[grpc.CallCredentials],+                 wait_for_ready: Optional[bool], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction,+                 loop: asyncio.AbstractEventLoop) -> None:+        self._loop = loop+        self._channel = channel+        interceptors_task = loop.create_task(+            self._invoke(interceptors, method, timeout, metadata,+                         credentials, wait_for_ready, request,+                         request_serializer, response_deserializer)+        )+        super().__init__(interceptors_task)++    # pylint: disable=too-many-arguments+    async def _invoke(self,+                      interceptors: Sequence[UnaryUnaryClientInterceptor],+                      method: bytes, timeout: Optional[float],+                      metadata: Optional[MetadataType],+                      credentials: Optional[grpc.CallCredentials],+                      wait_for_ready: Optional[bool], request: RequestType,+                      request_serializer: SerializingFunction,+                      response_deserializer: DeserializingFunction+                     ) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:++            interceptor = next(interceptors, None)++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)++                call_or_response = await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)++                if isinstance(call_or_response, _base_call.UnaryUnaryCall):+                    return call_or_response+                else:+                    return UnaryUnaryCallResponse(call_or_response)++            else:+                return UnaryUnaryCall(+                    request, _timeout_to_deadline(client_call_details.timeout),+                    client_call_details.metadata,+                    client_call_details.credentials,+                    client_call_details.wait_for_ready, self._channel,+                    client_call_details.method, request_serializer,+                    response_deserializer, self._loop)++        client_call_details = ClientCallDetails(method, timeout, metadata,+                                                credentials, wait_for_ready)+        return await _run_interceptor(iter(interceptors), client_call_details,+                                      request)+     def __await__(self):         call = yield from self._interceptors_task.__await__()         response = yield from call.__await__()         return response -    async def wait_for_connection(self) -> None:-        call = await self._interceptors_task-        return await call.wait_for_connection()++class CallResponseIterator:++    _intercepted_call: UnaryStreamCall++    def __init__(self) -> None:+        self._intercepted_call = None++    def set_call(self, call: UnaryStreamCall) -> None:+        self._intercepted_call = call++    async def _forward_responses(self) -> ResponseType:+        assert self._intercepted_call, ""Before start iterating set_call must be called""++        async for response in self._intercepted_call:+            yield response++    def __aiter__(self) -> AsyncIterable[ResponseType]:+        return self._forward_responses()+++class InterceptedUnaryStreamCall(InterceptedCall, _base_call.UnaryStreamCall):+    """"""Used for running a `UnaryStreamCall` wrapped by interceptors.""""""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _call_response_iterator: CallResponseIterator+    _response_aiter: AsyncIterable[ResponseType]+    _intercepted_response_iterator: Optional[AsyncIterable[ResponseType]]++    # pylint: disable=too-many-arguments+    def __init__(self, interceptors: Sequence[UnaryStreamClientInterceptor],+                 request: RequestType, timeout: Optional[float],+                 metadata: MetadataType,+                 credentials: Optional[grpc.CallCredentials],+                 wait_for_ready: Optional[bool], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction,+                 loop: asyncio.AbstractEventLoop) -> None:+        self._loop = loop+        self._channel = channel+        self._response_aiter = self._wait_for_interceptor_task_response_iterator()+        self._call_response_iterator = CallResponseIterator()+        self._intercepted_response_iterator = None+        interceptors_task = loop.create_task(+            self._invoke(interceptors, method, timeout, metadata,+                         credentials, wait_for_ready, request,+                         request_serializer, response_deserializer)+        )+        super().__init__(interceptors_task)++    # pylint: disable=too-many-arguments+    async def _invoke(self,+                      interceptors: Sequence[UnaryUnaryClientInterceptor],+                      method: bytes, timeout: Optional[float],+                      metadata: Optional[MetadataType],+                      credentials: Optional[grpc.CallCredentials],+                      wait_for_ready: Optional[bool], request: RequestType,+                      request_serializer: SerializingFunction,+                      response_deserializer: DeserializingFunction+                     ) -> UnaryStreamCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryStreamClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType,+                response_iterator: AsyncIterable[ResponseType]+               ) -> _base_call.UnaryUnaryCall:++            interceptor = next(interceptors, None)++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)++                return await interceptor.intercept_unary_stream(+                    continuation, client_call_details, request, response_iterator)+            else:+                call = UnaryStreamCall(+                    request, _timeout_to_deadline(client_call_details.timeout),+                    client_call_details.metadata,+                    client_call_details.credentials,+                    client_call_details.wait_for_ready, self._channel,+                    client_call_details.method, request_serializer,+                    response_deserializer, self._loop)++                self._call_response_iterator.set_call(call)","Is about how the interface is being designed, we provide the following signature:```python    async def intercept_unary_stream(            self, continuation: Callable[[ClientCallDetails, RequestType, AsyncIterable[ResponseType]],```So any interceptor can interpose its own iterator by using the iterator that is provided as a parameter. The `CallResponseIterator` allows us to provide this initial iterator, which would be finally configured once we have the call, by making the `set_call`.We could follow another pattern by asking the user for wrapping the call that has been returned and returning a wrapped call. For doing the second option we would need to provide a new interface for allowing this wrapping, because the object returned would need to publish the same contract as a generic call.Also, consider that once we start implementing the Streaming Input arities we could end up by following the same pattern, so providing a new parameter called `request_iterator`. So presenting both interfaces, `request_iterator` and `response_iterator`, in the same way, would be a less cognitive burden for the user.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22678,409816182,2020-04-16T20:03:29Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -325,14 +308,193 @@ async def debug_error_string(self) -> Optional[str]:          return await call.debug_error_string() +    async def wait_for_connection(self) -> None:+        call = await self._interceptors_task+        return await call.wait_for_connection()+++class InterceptedUnaryUnaryCall(InterceptedCall, _base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel++    # pylint: disable=too-many-arguments+    def __init__(self, interceptors: Sequence[UnaryUnaryClientInterceptor],+                 request: RequestType, timeout: Optional[float],+                 metadata: MetadataType,+                 credentials: Optional[grpc.CallCredentials],+                 wait_for_ready: Optional[bool], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction,+                 loop: asyncio.AbstractEventLoop) -> None:+        self._loop = loop+        self._channel = channel+        interceptors_task = loop.create_task(+            self._invoke(interceptors, method, timeout, metadata,+                         credentials, wait_for_ready, request,+                         request_serializer, response_deserializer)+        )+        super().__init__(interceptors_task)++    # pylint: disable=too-many-arguments+    async def _invoke(self,+                      interceptors: Sequence[UnaryUnaryClientInterceptor],+                      method: bytes, timeout: Optional[float],+                      metadata: Optional[MetadataType],+                      credentials: Optional[grpc.CallCredentials],+                      wait_for_ready: Optional[bool], request: RequestType,+                      request_serializer: SerializingFunction,+                      response_deserializer: DeserializingFunction+                     ) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:++            interceptor = next(interceptors, None)++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)++                call_or_response = await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)++                if isinstance(call_or_response, _base_call.UnaryUnaryCall):+                    return call_or_response+                else:+                    return UnaryUnaryCallResponse(call_or_response)++            else:+                return UnaryUnaryCall(+                    request, _timeout_to_deadline(client_call_details.timeout),+                    client_call_details.metadata,+                    client_call_details.credentials,+                    client_call_details.wait_for_ready, self._channel,+                    client_call_details.method, request_serializer,+                    response_deserializer, self._loop)++        client_call_details = ClientCallDetails(method, timeout, metadata,+                                                credentials, wait_for_ready)+        return await _run_interceptor(iter(interceptors), client_call_details,+                                      request)+     def __await__(self):         call = yield from self._interceptors_task.__await__()         response = yield from call.__await__()         return response -    async def wait_for_connection(self) -> None:-        call = await self._interceptors_task-        return await call.wait_for_connection()++class CallResponseIterator:++    _intercepted_call: UnaryStreamCall++    def __init__(self) -> None:+        self._intercepted_call = None++    def set_call(self, call: UnaryStreamCall) -> None:+        self._intercepted_call = call++    async def _forward_responses(self) -> ResponseType:+        assert self._intercepted_call, ""Before start iterating set_call must be called""++        async for response in self._intercepted_call:+            yield response++    def __aiter__(self) -> AsyncIterable[ResponseType]:+        return self._forward_responses()+++class InterceptedUnaryStreamCall(InterceptedCall, _base_call.UnaryStreamCall):+    """"""Used for running a `UnaryStreamCall` wrapped by interceptors.""""""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _call_response_iterator: CallResponseIterator+    _response_aiter: AsyncIterable[ResponseType]+    _intercepted_response_iterator: Optional[AsyncIterable[ResponseType]]++    # pylint: disable=too-many-arguments+    def __init__(self, interceptors: Sequence[UnaryStreamClientInterceptor],+                 request: RequestType, timeout: Optional[float],+                 metadata: MetadataType,+                 credentials: Optional[grpc.CallCredentials],+                 wait_for_ready: Optional[bool], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction,+                 loop: asyncio.AbstractEventLoop) -> None:+        self._loop = loop+        self._channel = channel+        self._response_aiter = self._wait_for_interceptor_task_response_iterator()+        self._call_response_iterator = CallResponseIterator()+        self._intercepted_response_iterator = None+        interceptors_task = loop.create_task(+            self._invoke(interceptors, method, timeout, metadata,+                         credentials, wait_for_ready, request,+                         request_serializer, response_deserializer)+        )+        super().__init__(interceptors_task)++    # pylint: disable=too-many-arguments+    async def _invoke(self,+                      interceptors: Sequence[UnaryUnaryClientInterceptor],+                      method: bytes, timeout: Optional[float],+                      metadata: Optional[MetadataType],+                      credentials: Optional[grpc.CallCredentials],+                      wait_for_ready: Optional[bool], request: RequestType,+                      request_serializer: SerializingFunction,+                      response_deserializer: DeserializingFunction+                     ) -> UnaryStreamCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryStreamClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType,+                response_iterator: AsyncIterable[ResponseType]+               ) -> _base_call.UnaryUnaryCall:++            interceptor = next(interceptors, None)++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)++                return await interceptor.intercept_unary_stream(+                    continuation, client_call_details, request, response_iterator)+            else:+                call = UnaryStreamCall(+                    request, _timeout_to_deadline(client_call_details.timeout),+                    client_call_details.metadata,+                    client_call_details.credentials,+                    client_call_details.wait_for_ready, self._channel,+                    client_call_details.method, request_serializer,+                    response_deserializer, self._loop)++                self._call_response_iterator.set_call(call)","Ive found out a use case where the current implementation it's not enough for the user. With the current implementation, the user is not able to access to the call since the call is not yet available until the continuation has been called.The user would be forced to make the proper wiring for not losing the call context, for example:```pythonclass ResponseIterator:    def __init__(self, response_iterator):        self._response_iterator = response_iterator    def set_call(self., call):       self.call = call    async def my_iter(self):        async for response in self._response_iterator:            yield response        print(await call.status_code())    def __aiter__(self):        return  my_iter()async def intercept_unary_stream(..., response_iterator):    my_response_itearator = ResponseIterator(response_iterator)    call = await continuation(..., my_response_itearator)    my_response_iterator.set_call(call)    return call```In the example, the user needs to implement their own wiring for having visibility of the status of the call once the iterator is finished.That makes me think that we would need to provide this kind of glue, so a way that the user can wrapper easily the iterator without losing call context.I will give it a try, by providing our own wrapper.BTW:  we do have a problem with the `add_done_callback` sooner or later we would need to accept coroutines as callbacks, otherwise, retrieve call attributes within the context of a NORMAL FUNCTION is quite complicated",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22668,410321106,2020-04-17T16:05:08Z,src/core/lib/iomgr/error.h,"@@ -127,7 +127,11 @@ typedef enum { #define GRPC_ERROR_OOM ((grpc_error*)2) #define GRPC_ERROR_RESERVED_2 ((grpc_error*)3) #define GRPC_ERROR_CANCELLED ((grpc_error*)4)-#define GRPC_ERROR_SPECIAL_MAX GRPC_ERROR_CANCELLED+#define GRPC_ERROR_RESERVED_3 ((grpc_error*)5)+// GRPC_ERROR_EOS is used only by transports for send ops that failed because+// the stream was closed for writes.+#define GRPC_ERROR_EOS ((grpc_error*)6)","That doesn't sound quite right.  Consider the case where you're using `grpc_error_add_child()` to wrap an error on the way back up the stack.  Currently, that function treats `GRPC_ERROR_NONE` specially, but in this case you would want to also treat `GRPC_ERROR_EOS` specially, since neither of those values actually represent an error.Actually, thinking about this further, because this condition is not actually an error, it does seem wrong to indicate it via a `grpc_error` value.  Doing it this way will basically mean that any filter that intercepts `on_complete` on the way back up the stack will need to know to ignore both `GRPC_ERROR_NONE` and `GRPC_ERROR_EOS`.What was wrong with representing this via the `stream_write_closed` field in the transport API?  That actually seems like a much more natural representation.",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/22668,410395990,2020-04-17T18:23:59Z,src/core/lib/iomgr/error.h,"@@ -127,7 +127,11 @@ typedef enum { #define GRPC_ERROR_OOM ((grpc_error*)2) #define GRPC_ERROR_RESERVED_2 ((grpc_error*)3) #define GRPC_ERROR_CANCELLED ((grpc_error*)4)-#define GRPC_ERROR_SPECIAL_MAX GRPC_ERROR_CANCELLED+#define GRPC_ERROR_RESERVED_3 ((grpc_error*)5)+// GRPC_ERROR_EOS is used only by transports for send ops that failed because+// the stream was closed for writes.+#define GRPC_ERROR_EOS ((grpc_error*)6)","Well, GRPC_ERROR_EOS is an error since the ops need to be failed because of this error. It's just that we do not want to cancel the call so that we can avoid overwriting the status. There was nothing wrong with the stream_write_closed representation. I thought that GRPC_ERROR_EOS would be a better way since we do not need to add a field to the batch and we can simply use a special type of error to achieve the same purpose (similar to an int return value with varying error codes.) ",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21194,410715253,2020-04-18T15:54:26Z,tools/distrib/generate_boringssl_prefix_header.sh,"@@ -0,0 +1,62 @@+#!/bin/bash+# Copyright 2018 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# Generate the list of boringssl symbols that need to be renamed based on the+# current boringssl submodule. The script should be run after a boringssl+# upgrade in third_party/boringssl-with-bazel. Note that after the script is+# run, you will typically need to manually upgrade the BoringSSL-GRPC podspec+# (templates/src/objective-c/BoringSSL-GRPC.podspec.template) version and the+# corresponding version number in gRPC-Core podspec+# (templates/gRPC-Core.podspec.template).++set -ev++BORINGSSL_ROOT=third_party/boringssl-with-bazel/src++cd ""$(dirname $0)""+cd ../../$BORINGSSL_ROOT++BORINGSSL_COMMIT=$(git rev-parse HEAD)+BORINGSSL_PREFIX_HEADERS_DIR=src/boringssl++# generate the prefix header+rm -rf build+mkdir -p build+cd build+cmake ..+make -j++[ -f ssl/libssl.a ] || { echo ""Failed to build libssl.a"" ; exit 1 ; }+[ -f crypto/libcrypto.a ] || { echo ""Failed to build libcrypto.a"" ; exit 1 ; }++go run ../util/read_symbols.go ssl/libssl.a > ./symbols.txt+go run ../util/read_symbols.go crypto/libcrypto.a >> ./symbols.txt++# generates boringssl_prefix_symbols.h+cmake .. -DBORINGSSL_PREFIX=GRPC -DBORINGSSL_PREFIX_SYMBOLS=symbols.txt+make boringssl_prefix_symbols",Looks like this is based on https://github.com/google/boringssl/blob/master/BUILDING.md#building-with-prefixed-symbolsplease do include this link as explanation of what's happening.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/21194,410716278,2020-04-18T16:04:38Z,templates/src/objective-c/BoringSSL-GRPC.podspec.template,"@@ -209,25 +219,26 @@         }       EOF -      # The symbol prefixing mechanism is performed by redefining BoringSSL symbols with ""#define-      # SOME_BORINGSSL_SYMBOL GRPC_SHADOW_SOME_BORINGSSL_SYMBOL"". Unfortunately, some symbols are-      # already redefined as macros in BoringSSL headers in the form ""#define SOME_BORINGSSL_SYMBOL-      # SOME_BORINGSSL_SYMBOL"" Such type of redefinition will cause ""SOME_BORINGSSL_SYMBOL redefined""-      # error when using together with our prefix header. So the workaround in the below lines removes-      # all such type of #define directives.-      sed -i'.back' '/^#define \\([A-Za-z0-9_]*\\) \\1/d' src/include/openssl/*.h-      # Remove lines of the format below for the same reason above-      #     #define SOME_BORINGSSL_SYMBOL ${""\\""}-      #         SOME_BORINGSSL_SYMBOL-      sed -i'.back' '/^#define.*\\\\$/{N;/^#define \\([A-Za-z0-9_]*\\) *\\\\\\n *\\1/d;}' src/include/openssl/*.h+      # To avoid symbol conflict with OpenSSL, gRPC needs to rename all the BoringSSL symbols with a +      # prefix. This is done with BoringSSL's BORINGSSL_PREFIX mechanism+      # (https://github.com/google/boringssl/blob/75148d7abf12bdd1797fec3c5da9a21963703516/BUILDING.md#building-with-prefixed-symbols).+      # The required prefix header file boringssl_prefix_symbols.h is not part of BoringSSL repo at+      # this moment. It has to be generated by BoringSSL's users and be injected to BoringSSL build.+      # gRPC generates this file in script /tools/distrib/upgrade_boringssl_objc.sh. This script+      # outputs a gzip+base64 encoded version of boringssl_prefix_symbols.h because of Cocoapods'+      # limit on the 'prepare_command' field length. The encoded header is generated from+      # /src/boringssl/boringssl_prefix_symbols.h. Here we decode the content and inject the header to+      # the correct location in BoringSSL.+      base64 -D <<EOF | gunzip > src/include/openssl/boringssl_prefix_symbols.h+        ${prefix_gz_b64}","nit: I looked at the generated file and it looks like it generates one extremely long line with base64.It would be nicer if you wrapped the base64 string at 80 characters, which should still be parseable by the base64 command, but the generated file would look that ugly?",
31627465,nanahpang,https://api.github.com/repos/grpc/grpc/pulls/22280,411549348,2020-04-20T17:13:24Z,src/core/ext/filters/client_channel/xds/xds_client.cc,"@@ -896,15 +897,22 @@ void XdsClient::ChannelState::AdsCallState::AcceptLdsUpdate(   }   if (GRPC_TRACE_FLAG_ENABLED(grpc_xds_client_trace)) {     gpr_log(GPR_INFO,-            ""[xds_client %p] LDS update received: route_config_name=%s, ""-            ""cluster_name=%s"",+            ""[xds_client %p] LDS update received: route_config_name=%s"",             xds_client(),             (!lds_update->route_config_name.empty()                  ? lds_update->route_config_name.c_str()-                 : ""<inlined>""),-            (lds_update->rds_update.has_value()-                 ? lds_update->rds_update->cluster_name.c_str()-                 : ""<to be obtained via RDS>""));+                 : ""<inlined>""));+    if (lds_update->rds_update.has_value()) {+      gpr_log(GPR_INFO, ""  RouteConfiguration contains %lu routes"",+              lds_update->rds_update.value().routes.size());","The line 907 and 970 has a format error shows up during the import and break many tests:format specifies type 'unsigned long' but the argument has type 'std::__1::vector<grpc_core::XdsApi::RdsRoute, std::__1::allocator<grpc_core::XdsApi::RdsRoute> >::size_type' (aka 'unsigned int') ",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/21194,411551469,2020-04-20T17:16:29Z,templates/gRPC-Core.podspec.template,"@@ -192,12 +192,12 @@       ss.header_mappings_dir = '.'       ss.libraries = 'z'       ss.dependency ""#{s.name}/Interface"", version-      ss.dependency 'BoringSSL-GRPC', '0.0.7'+      ss.dependency 'BoringSSL-GRPC', '0.0.8'       abseil_version = '1.20200225.0'       % for abseil_spec in grpc_abseil_specs:       ss.dependency '${abseil_spec}', abseil_version       % endfor-      ss.compiler_flags = '-DGRPC_SHADOW_BORINGSSL_SYMBOLS'+      ss.compiler_flags = '-DBORINGSSL_PREFIX=GRPC'","Yes I did check the symbols are prefixed, and correctly used by gRPC. The `boringssl_prefix_symbols.h` is used [in BoringSSL headers](https://github.com/google/boringssl/blob/538a124d7073eba59e8841a0b88bc38aa1fe95aa/include/openssl/base.h#L78-L80). We just need to define the `BORINGSSL_PREFIX` macro when building gRPC-Core and BoringSSL-GRPC.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22713,411565192,2020-04-20T17:37:37Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -84,7 +84,11 @@ class ClientCallDetails(     wait_for_ready: Optional[bool]  -class UnaryUnaryClientInterceptor(metaclass=ABCMeta):+class ClientInterceptor(metaclass=ABCMeta):+    """"""Base class used for all Aio Client Interceptor classes""""""",This base class it's helpful 👍  Should we expose it as a public API?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22713,411573827,2020-04-20T17:50:31Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -117,8 +121,40 @@ async def intercept_unary_unary(         """"""  -class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):-    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.+class UnaryStreamClientInterceptor(ClientInterceptor, metaclass=ABCMeta):+    """"""Affords intercepting unary-stream invocations.""""""++    @abstractmethod+    async def intercept_unary_stream(self, continuation: Callable[[+            ClientCallDetails, RequestType, AsyncIterable[ResponseType]+    ], UnaryStreamCall], client_call_details: ClientCallDetails,+                                     request: RequestType) -> UnaryStreamCall:+        """"""Intercepts a unary-stream invocation asynchronously.++        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the+            actual RPC on the underlying Channel. It is the interceptor's+            responsibility to call it if it decides to move the RPC forward.+            The interceptor can use+            `call = await continuation(client_call_details, request, response_iterator))`+            to continue with the RPC. `continuation` returns the call to the+            RPC.","Optional: Should we mention the expectation of arguments for `continuation`? A clumsy way can be, ...the next interceptor in chain or invoking the actual RPC on the underlying Channel **with given parameters**.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22713,411583627,2020-04-20T18:05:43Z,src/python/grpcio_tests/tests_aio/unit/client_unary_stream_interceptor_test.py,"@@ -0,0 +1,404 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest+import datetime++import grpc++from grpc.experimental import aio+from tests_aio.unit._constants import UNREACHABLE_TARGET+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc++_SHORT_TIMEOUT_S = datetime.timedelta(seconds=1).total_seconds()++_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+_NUM_STREAM_RESPONSES = 5+_REQUEST_PAYLOAD_SIZE = 7+_RESPONSE_PAYLOAD_SIZE = 7+_RESPONSE_INTERVAL_US = int(_SHORT_TIMEOUT_S * 1000 * 1000)+++class _ResponseIterator:++    def __init__(self, response_iterator):+        self._response_cnt = 0+        self._response_iterator = response_iterator++    async def _forward_responses(self):+        async for response in self._response_iterator:+            self._response_cnt += 1+            yield response++    def __aiter__(self):+        return self._forward_responses()++    @property+    def response_cnt(self):+        return self._response_cnt+++def _inject_callbacks(call):+    first_callback_ran = asyncio.Event()++    def first_callback(call):+        # Validate that all resopnses have been received+        # and the call is an end state.+        assert call.done()+        first_callback_ran.set()++    second_callback_ran = asyncio.Event()++    def second_callback(call):+        # Validate that all resopnses have been received+        # and the call is an end state.+        assert call.done()+        second_callback_ran.set()++    call.add_done_callback(first_callback)+    call.add_done_callback(second_callback)++    async def validation():+        await asyncio.wait_for(+            asyncio.gather(first_callback_ran.wait(),+                           second_callback_ran.wait()),+            test_constants.SHORT_TIMEOUT)++    return validation()+++class _UnaryStreamInterceptorEmpty(aio.UnaryStreamClientInterceptor):++    async def intercept_unary_stream(self, continuation, client_call_details,+                                     request):+        return await continuation(client_call_details, request)+++class _UnaryStreamInterceptorWith_ResponseIterator(+        aio.UnaryStreamClientInterceptor):++    def __init__(self):+        self.response_iterator = None++    async def intercept_unary_stream(self, continuation, client_call_details,+                                     request):+        call = await continuation(client_call_details, request)+        self.response_iterator = _ResponseIterator(call)+        return self.response_iterator+++class TestUnaryStreamClientInterceptor(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_intercepts(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):+                interceptor = interceptor_class()++                request = messages_pb2.StreamingOutputCallRequest()+                for _ in range(_NUM_STREAM_RESPONSES):+                    request.response_parameters.append(+                        messages_pb2.ResponseParameters(+                            size=_RESPONSE_PAYLOAD_SIZE))++                channel = aio.insecure_channel(self._server_target,+                                               interceptors=[interceptor])+                stub = test_pb2_grpc.TestServiceStub(channel)+                call = stub.StreamingOutputCall(request)++                await call.wait_for_connection()++                response_cnt = 0+                async for response in call:+                    response_cnt += 1+                    self.assertIs(type(response),+                                  messages_pb2.StreamingOutputCallResponse)+                    self.assertEqual(_RESPONSE_PAYLOAD_SIZE,+                                     len(response.payload.body))++                self.assertTrue(response_cnt, _NUM_STREAM_RESPONSES)+                self.assertEqual(await call.code(), grpc.StatusCode.OK)+                self.assertEqual(await call.initial_metadata(), ())+                self.assertEqual(await call.trailing_metadata(), ())+                self.assertEqual(await call.details(), '')+                self.assertEqual(await call.debug_error_string(), '')+                self.assertEqual(call.cancel(), False)+                self.assertEqual(call.cancelled(), False)+                self.assertEqual(call.done(), True)++                if interceptor_class == _UnaryStreamInterceptorWith_ResponseIterator:+                    self.assertTrue(interceptor.response_iterator.response_cnt,+                                    _NUM_STREAM_RESPONSES)++                await channel.close()++    async def test_add_done_callback(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):+                interceptor = interceptor_class()++                request = messages_pb2.StreamingOutputCallRequest()+                for _ in range(_NUM_STREAM_RESPONSES):+                    request.response_parameters.append(+                        messages_pb2.ResponseParameters(+                            size=_RESPONSE_PAYLOAD_SIZE))++                channel = aio.insecure_channel(self._server_target,+                                               interceptors=[interceptor])+                stub = test_pb2_grpc.TestServiceStub(channel)+                call = stub.StreamingOutputCall(request)++                validation = _inject_callbacks(call)++                async for response in call:+                    pass++                await validation++                await channel.close()++    async def test_add_done_callback_after_connection(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):+                interceptor = interceptor_class()++                request = messages_pb2.StreamingOutputCallRequest()+                for _ in range(_NUM_STREAM_RESPONSES):+                    request.response_parameters.append(+                        messages_pb2.ResponseParameters(+                            size=_RESPONSE_PAYLOAD_SIZE))++                channel = aio.insecure_channel(self._server_target,+                                               interceptors=[interceptor])+                stub = test_pb2_grpc.TestServiceStub(channel)+                call = stub.StreamingOutputCall(request)++                # This ensures that the callbacks will be registered+                # with the intercepted call rather than saving in the+                # pending state list.+                await call.wait_for_connection()++                validation = _inject_callbacks(call)++                async for response in call:+                    pass++                await validation++                await channel.close()++    async def test_response_iterator_using_read(self):+        interceptor = _UnaryStreamInterceptorWith_ResponseIterator()++        channel = aio.insecure_channel(self._server_target,+                                       interceptors=[interceptor])+        stub = test_pb2_grpc.TestServiceStub(channel)++        request = messages_pb2.StreamingOutputCallRequest()+        for _ in range(_NUM_STREAM_RESPONSES):+            request.response_parameters.append(+                messages_pb2.ResponseParameters(size=_RESPONSE_PAYLOAD_SIZE))++        call = stub.StreamingOutputCall(request)++        response_cnt = 0+        for response in range(_NUM_STREAM_RESPONSES):+            response = await call.read()+            response_cnt += 1+            self.assertIs(type(response),+                          messages_pb2.StreamingOutputCallResponse)+            self.assertEqual(_RESPONSE_PAYLOAD_SIZE, len(response.payload.body))++        self.assertTrue(response_cnt, _NUM_STREAM_RESPONSES)+        self.assertTrue(interceptor.response_iterator.response_cnt,+                        _NUM_STREAM_RESPONSES)+        self.assertEqual(await call.code(), grpc.StatusCode.OK)++        await channel.close()++    async def test_mulitple_interceptors_response_iterator(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):++                interceptors = [interceptor_class(), interceptor_class()]++                channel = aio.insecure_channel(self._server_target,+                                               interceptors=interceptors)+                stub = test_pb2_grpc.TestServiceStub(channel)++                request = messages_pb2.StreamingOutputCallRequest()+                for _ in range(_NUM_STREAM_RESPONSES):+                    request.response_parameters.append(+                        messages_pb2.ResponseParameters(+                            size=_RESPONSE_PAYLOAD_SIZE))++                call = stub.StreamingOutputCall(request)++                response_cnt = 0+                async for response in call:+                    response_cnt += 1+                    self.assertIs(type(response),+                                  messages_pb2.StreamingOutputCallResponse)+                    self.assertEqual(_RESPONSE_PAYLOAD_SIZE,+                                     len(response.payload.body))++                self.assertTrue(response_cnt, _NUM_STREAM_RESPONSES)+                self.assertEqual(await call.code(), grpc.StatusCode.OK)++                await channel.close()++    async def test_intercepts_response_iterator_rpc_error(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):++                channel = aio.insecure_channel(+                    UNREACHABLE_TARGET, interceptors=[interceptor_class()])+                request = messages_pb2.StreamingOutputCallRequest()+                stub = test_pb2_grpc.TestServiceStub(channel)+                call = stub.StreamingOutputCall(request)++                with self.assertRaises(aio.AioRpcError) as exception_context:+                    async for response in call:+                        pass++                self.assertEqual(grpc.StatusCode.UNAVAILABLE,+                                 exception_context.exception.code())++                self.assertTrue(call.done())+                self.assertEqual(grpc.StatusCode.UNAVAILABLE, await call.code())+                await channel.close()++    async def test_cancel_before_rpc(self):++        interceptor_reached = asyncio.Event()+        wait_for_ever = self.loop.create_future()++        class Interceptor(aio.UnaryStreamClientInterceptor):++            async def intercept_unary_stream(self, continuation,+                                             client_call_details, request):+                interceptor_reached.set()+                await wait_for_ever++        channel = aio.insecure_channel(UNREACHABLE_TARGET,+                                       interceptors=[Interceptor()])+        request = messages_pb2.StreamingOutputCallRequest()+        stub = test_pb2_grpc.TestServiceStub(channel)+        call = stub.StreamingOutputCall(request)++        self.assertFalse(call.cancelled())+        self.assertFalse(call.done())++        await interceptor_reached.wait()+        self.assertTrue(call.cancel())++        with self.assertRaises(asyncio.CancelledError):+            async for response in call:+                pass++        self.assertTrue(call.cancelled())+        self.assertTrue(call.done())+        self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+        self.assertEqual(await call.details(),+                         _LOCAL_CANCEL_DETAILS_EXPECTATION)+        self.assertEqual(await call.initial_metadata(), None)+        self.assertEqual(await call.trailing_metadata(), None)+        await channel.close()++    async def test_cancel_after_rpc(self):++        interceptor_reached = asyncio.Event()+        wait_for_ever = self.loop.create_future()++        class Interceptor(aio.UnaryStreamClientInterceptor):++            async def intercept_unary_stream(self, continuation,+                                             client_call_details, request):+                call = await continuation(client_call_details, request)+                interceptor_reached.set()+                await wait_for_ever++        channel = aio.insecure_channel(UNREACHABLE_TARGET,+                                       interceptors=[Interceptor()])+        request = messages_pb2.StreamingOutputCallRequest()+        stub = test_pb2_grpc.TestServiceStub(channel)+        call = stub.StreamingOutputCall(request)++        self.assertFalse(call.cancelled())+        self.assertFalse(call.done())++        await interceptor_reached.wait()+        self.assertTrue(call.cancel())++        with self.assertRaises(asyncio.CancelledError):+            async for response in call:+                pass++        self.assertTrue(call.cancelled())+        self.assertTrue(call.done())+        self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+        self.assertEqual(await call.details(),+                         _LOCAL_CANCEL_DETAILS_EXPECTATION)+        self.assertEqual(await call.initial_metadata(), None)+        self.assertEqual(await call.trailing_metadata(), None)+        await channel.close()++    async def test_cancel_consuming_response_iterator(self):+        request = messages_pb2.StreamingOutputCallRequest()+        for _ in range(_NUM_STREAM_RESPONSES):+            request.response_parameters.append(+                messages_pb2.ResponseParameters(+                    size=_RESPONSE_PAYLOAD_SIZE,+                    interval_us=_RESPONSE_INTERVAL_US))++        channel = aio.insecure_channel(+            self._server_target,+            interceptors=[_UnaryStreamInterceptorWith_ResponseIterator()])+        stub = test_pb2_grpc.TestServiceStub(channel)+        call = stub.StreamingOutputCall(request)++        with self.assertRaises(asyncio.CancelledError):+            async for response in call:+                call.cancel()++        self.assertTrue(call.cancelled())+        self.assertTrue(call.done())+        self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+        self.assertEqual(await call.details(),+                         _LOCAL_CANCEL_DETAILS_EXPECTATION)+        await channel.close()",Can we add test cases1. The interceptor cancels the entire call within `intercept_unary_stream` (to ensure the call functionality is working);2. An exception is raised within `intercept_unary_stream` (so we fix the error path behavior).,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22678,411587801,2020-04-20T18:12:34Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -325,14 +308,177 @@ async def debug_error_string(self) -> Optional[str]:          return await call.debug_error_string() +    async def wait_for_connection(self) -> None:+        call = await self._interceptors_task+        return await call.wait_for_connection()+++class InterceptedUnaryUnaryCall(InterceptedCall, _base_call.UnaryUnaryCall):+    """"""Used for running a `UnaryUnaryCall` wrapped by interceptors.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel++    # pylint: disable=too-many-arguments+    def __init__(self, interceptors: Sequence[UnaryUnaryClientInterceptor],+                 request: RequestType, timeout: Optional[float],+                 metadata: MetadataType,+                 credentials: Optional[grpc.CallCredentials],+                 wait_for_ready: Optional[bool], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction,+                 loop: asyncio.AbstractEventLoop) -> None:+        self._loop = loop+        self._channel = channel+        interceptors_task = loop.create_task(+            self._invoke(interceptors, method, timeout, metadata,+                         credentials, wait_for_ready, request,+                         request_serializer, response_deserializer)+        )+        super().__init__(interceptors_task)++    # pylint: disable=too-many-arguments+    async def _invoke(self,+                      interceptors: Sequence[UnaryUnaryClientInterceptor],+                      method: bytes, timeout: Optional[float],+                      metadata: Optional[MetadataType],+                      credentials: Optional[grpc.CallCredentials],+                      wait_for_ready: Optional[bool], request: RequestType,+                      request_serializer: SerializingFunction,+                      response_deserializer: DeserializingFunction+                     ) -> UnaryUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType) -> _base_call.UnaryUnaryCall:++            interceptor = next(interceptors, None)++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)++                call_or_response = await interceptor.intercept_unary_unary(+                    continuation, client_call_details, request)++                if isinstance(call_or_response, _base_call.UnaryUnaryCall):+                    return call_or_response+                else:+                    return UnaryUnaryCallResponse(call_or_response)++            else:+                return UnaryUnaryCall(+                    request, _timeout_to_deadline(client_call_details.timeout),+                    client_call_details.metadata,+                    client_call_details.credentials,+                    client_call_details.wait_for_ready, self._channel,+                    client_call_details.method, request_serializer,+                    response_deserializer, self._loop)++        client_call_details = ClientCallDetails(method, timeout, metadata,+                                                credentials, wait_for_ready)+        return await _run_interceptor(iter(interceptors), client_call_details,+                                      request)+     def __await__(self):         call = yield from self._interceptors_task.__await__()         response = yield from call.__await__()         return response -    async def wait_for_connection(self) -> None:++class InterceptedUnaryStreamCall(InterceptedCall, _base_call.UnaryStreamCall):+    """"""Used for running a `UnaryStreamCall` wrapped by interceptors.""""""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _response_aiter: AsyncIterable[ResponseType]++    # pylint: disable=too-many-arguments+    def __init__(self, interceptors: Sequence[UnaryStreamClientInterceptor],+                 request: RequestType, timeout: Optional[float],+                 metadata: MetadataType,+                 credentials: Optional[grpc.CallCredentials],+                 wait_for_ready: Optional[bool], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction,+                 loop: asyncio.AbstractEventLoop) -> None:+        self._loop = loop+        self._channel = channel+        self._response_aiter = self._wait_for_interceptor_task_response_iterator()+        interceptors_task = loop.create_task(+            self._invoke(interceptors, method, timeout, metadata,+                         credentials, wait_for_ready, request,+                         request_serializer, response_deserializer)+        )+        super().__init__(interceptors_task)++    # pylint: disable=too-many-arguments+    async def _invoke(self,+                      interceptors: Sequence[UnaryUnaryClientInterceptor],+                      method: bytes, timeout: Optional[float],+                      metadata: Optional[MetadataType],+                      credentials: Optional[grpc.CallCredentials],+                      wait_for_ready: Optional[bool], request: RequestType,+                      request_serializer: SerializingFunction,+                      response_deserializer: DeserializingFunction+                     ) -> UnaryStreamCall:+        """"""Run the RPC call wrapped in interceptors""""""++        last_returned_call_from_interceptors = [None]++        async def _run_interceptor(+                interceptors: Iterator[UnaryStreamClientInterceptor],+                client_call_details: ClientCallDetails,+                request: RequestType,+               ) -> _base_call.UnaryUnaryCall:++            interceptor = next(interceptors, None)++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)++                call_or_response_iterator = await interceptor.intercept_unary_stream(+                    continuation, client_call_details, request)++                if call_or_response_iterator is last_returned_call_from_interceptors[0]:+                    return call_or_response_iterator+                else:+                    last_returned_call_from_interceptors[0] = UnaryStreamCallResponseIterator(+                        last_returned_call_from_interceptors[0],+                        call_or_response_iterator+                    )+                    return last_returned_call_from_interceptors[0]+            else:+                last_returned_call_from_interceptors[0] = UnaryStreamCall(+                    request, _timeout_to_deadline(client_call_details.timeout),+                    client_call_details.metadata,+                    client_call_details.credentials,+                    client_call_details.wait_for_ready, self._channel,+                    client_call_details.method, request_serializer,+                    response_deserializer, self._loop)++                return last_returned_call_from_interceptors[0]++        client_call_details = ClientCallDetails(method, timeout, metadata,+                                                credentials, wait_for_ready)+        return await _run_interceptor(iter(interceptors), client_call_details,+                                      request)++    async def _wait_for_interceptor_task_response_iterator(self) -> ResponseType:         call = await self._interceptors_task-        return await call.wait_for_connection()+        async for response in call:+            yield response++    def __aiter__(self) -> AsyncIterable[ResponseType]:+        return self._response_aiter++    async def read(self) -> ResponseType:+        return await self._response_aiter.asend(None)","Sorry for the late reply. Last time our discussion around this topic ends with a conclusion that if we allow using both APIs, users might abuse them and writing racy code. Still, I think as a library we should prevent this behavior if possible.On the other hand, the prohibition should be per-call-instance, so users should allow to use `read` for the real `UnaryStreamCall` and use either `__aiter__` or `read` on `InterceptedUnaryStreamCall`. Since there is a layer of abstraction, both `__aiter__` and `read` of  `InterceptedUnaryStreamCall` are actually calling `read` on `UnaryStreamCall`, it doesn't violate the API style mixing prohibition.WDYT?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22713,411620337,2020-04-20T19:05:18Z,src/python/grpcio_tests/tests_aio/unit/client_unary_stream_interceptor_test.py,"@@ -0,0 +1,404 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest+import datetime++import grpc++from grpc.experimental import aio+from tests_aio.unit._constants import UNREACHABLE_TARGET+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc++_SHORT_TIMEOUT_S = datetime.timedelta(seconds=1).total_seconds()++_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+_NUM_STREAM_RESPONSES = 5+_REQUEST_PAYLOAD_SIZE = 7+_RESPONSE_PAYLOAD_SIZE = 7+_RESPONSE_INTERVAL_US = int(_SHORT_TIMEOUT_S * 1000 * 1000)+++class _ResponseIterator:++    def __init__(self, response_iterator):+        self._response_cnt = 0+        self._response_iterator = response_iterator++    async def _forward_responses(self):+        async for response in self._response_iterator:+            self._response_cnt += 1+            yield response++    def __aiter__(self):+        return self._forward_responses()++    @property+    def response_cnt(self):+        return self._response_cnt+++def _inject_callbacks(call):+    first_callback_ran = asyncio.Event()++    def first_callback(call):+        # Validate that all resopnses have been received+        # and the call is an end state.+        assert call.done()+        first_callback_ran.set()++    second_callback_ran = asyncio.Event()++    def second_callback(call):+        # Validate that all resopnses have been received+        # and the call is an end state.+        assert call.done()+        second_callback_ran.set()++    call.add_done_callback(first_callback)+    call.add_done_callback(second_callback)++    async def validation():+        await asyncio.wait_for(+            asyncio.gather(first_callback_ran.wait(),+                           second_callback_ran.wait()),+            test_constants.SHORT_TIMEOUT)++    return validation()+++class _UnaryStreamInterceptorEmpty(aio.UnaryStreamClientInterceptor):++    async def intercept_unary_stream(self, continuation, client_call_details,+                                     request):+        return await continuation(client_call_details, request)+++class _UnaryStreamInterceptorWith_ResponseIterator(+        aio.UnaryStreamClientInterceptor):++    def __init__(self):+        self.response_iterator = None++    async def intercept_unary_stream(self, continuation, client_call_details,+                                     request):+        call = await continuation(client_call_details, request)+        self.response_iterator = _ResponseIterator(call)+        return self.response_iterator+++class TestUnaryStreamClientInterceptor(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_intercepts(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):+                interceptor = interceptor_class()++                request = messages_pb2.StreamingOutputCallRequest()+                for _ in range(_NUM_STREAM_RESPONSES):",Nit: You can use `extend` instead of `append`:```pythonreqest.response_parameters.extend(  [messages_pb2.ResponseParameters(size=_RESPONSE_PAYLOAD_SIZE)] * _NUM_STREAM_RESPONSES)```,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22713,411622468,2020-04-20T19:08:52Z,src/python/grpcio_tests/tests_aio/unit/client_unary_stream_interceptor_test.py,"@@ -0,0 +1,404 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest+import datetime++import grpc++from grpc.experimental import aio+from tests_aio.unit._constants import UNREACHABLE_TARGET+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc++_SHORT_TIMEOUT_S = datetime.timedelta(seconds=1).total_seconds()++_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+_NUM_STREAM_RESPONSES = 5+_REQUEST_PAYLOAD_SIZE = 7+_RESPONSE_PAYLOAD_SIZE = 7+_RESPONSE_INTERVAL_US = int(_SHORT_TIMEOUT_S * 1000 * 1000)+++class _ResponseIterator:++    def __init__(self, response_iterator):+        self._response_cnt = 0+        self._response_iterator = response_iterator++    async def _forward_responses(self):+        async for response in self._response_iterator:+            self._response_cnt += 1+            yield response++    def __aiter__(self):+        return self._forward_responses()++    @property+    def response_cnt(self):+        return self._response_cnt+++def _inject_callbacks(call):+    first_callback_ran = asyncio.Event()++    def first_callback(call):+        # Validate that all resopnses have been received+        # and the call is an end state.+        assert call.done()+        first_callback_ran.set()++    second_callback_ran = asyncio.Event()++    def second_callback(call):+        # Validate that all resopnses have been received+        # and the call is an end state.+        assert call.done()+        second_callback_ran.set()++    call.add_done_callback(first_callback)+    call.add_done_callback(second_callback)++    async def validation():+        await asyncio.wait_for(+            asyncio.gather(first_callback_ran.wait(),+                           second_callback_ran.wait()),+            test_constants.SHORT_TIMEOUT)++    return validation()+++class _UnaryStreamInterceptorEmpty(aio.UnaryStreamClientInterceptor):++    async def intercept_unary_stream(self, continuation, client_call_details,+                                     request):+        return await continuation(client_call_details, request)+++class _UnaryStreamInterceptorWith_ResponseIterator(+        aio.UnaryStreamClientInterceptor):++    def __init__(self):+        self.response_iterator = None++    async def intercept_unary_stream(self, continuation, client_call_details,+                                     request):+        call = await continuation(client_call_details, request)+        self.response_iterator = _ResponseIterator(call)+        return self.response_iterator+++class TestUnaryStreamClientInterceptor(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_intercepts(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):+                interceptor = interceptor_class()++                request = messages_pb2.StreamingOutputCallRequest()+                for _ in range(_NUM_STREAM_RESPONSES):+                    request.response_parameters.append(+                        messages_pb2.ResponseParameters(+                            size=_RESPONSE_PAYLOAD_SIZE))++                channel = aio.insecure_channel(self._server_target,+                                               interceptors=[interceptor])+                stub = test_pb2_grpc.TestServiceStub(channel)+                call = stub.StreamingOutputCall(request)++                await call.wait_for_connection()++                response_cnt = 0+                async for response in call:+                    response_cnt += 1+                    self.assertIs(type(response),+                                  messages_pb2.StreamingOutputCallResponse)+                    self.assertEqual(_RESPONSE_PAYLOAD_SIZE,+                                     len(response.payload.body))++                self.assertTrue(response_cnt, _NUM_STREAM_RESPONSES)+                self.assertEqual(await call.code(), grpc.StatusCode.OK)+                self.assertEqual(await call.initial_metadata(), ())+                self.assertEqual(await call.trailing_metadata(), ())+                self.assertEqual(await call.details(), '')+                self.assertEqual(await call.debug_error_string(), '')+                self.assertEqual(call.cancel(), False)+                self.assertEqual(call.cancelled(), False)+                self.assertEqual(call.done(), True)++                if interceptor_class == _UnaryStreamInterceptorWith_ResponseIterator:","Nit: This check probably belongs in `_UnaryStreamInterceptorWith_ResponseIterator`. Maybe add an `assertInFinalState(self, test: unittest.TestCase)` method to your test interceptors? And then just call that?",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22713,411635261,2020-04-20T19:30:39Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -193,12 +202,13 @@ def __call__(self, class Channel(_base_channel.Channel):     _loop: asyncio.AbstractEventLoop     _channel: cygrpc.AioChannel-    _unary_unary_interceptors: Optional[Sequence[UnaryUnaryClientInterceptor]]+    _unary_unary_interceptors: Sequence[UnaryUnaryClientInterceptor]",Nit: Looks like we can change this annotation to `List[UnaryUnaryClientInterceptor]` since the member is guaranteed to always be a list.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22713,411636589,2020-04-20T19:33:04Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -210,22 +220,32 @@ def __init__(self, target: str, options: ChannelArgumentType,           interceptors: An optional list of interceptors that would be used for             intercepting any RPC executed with that channel.         """"""-        if interceptors is None:-            self._unary_unary_interceptors = None-        else:-            self._unary_unary_interceptors = list(-                filter(-                    lambda interceptor: isinstance(interceptor,-                                                   UnaryUnaryClientInterceptor),-                    interceptors))+        self._unary_unary_interceptors = []+        self._unary_stream_interceptors = []++        if interceptors:+            attrs_and_interceptor_classes = [+                (self._unary_unary_interceptors, UnaryUnaryClientInterceptor),+                (self._unary_stream_interceptors, UnaryStreamClientInterceptor)+            ]++            # pylint: disable=cell-var-from-loop+            for attr, interceptor_class in attrs_and_interceptor_classes:+                attr.extend(",Stylistic nit: I think this would be more readable as a list comprehension.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22713,411637333,2020-04-20T19:34:17Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -210,22 +220,32 @@ def __init__(self, target: str, options: ChannelArgumentType,           interceptors: An optional list of interceptors that would be used for             intercepting any RPC executed with that channel.         """"""-        if interceptors is None:-            self._unary_unary_interceptors = None-        else:-            self._unary_unary_interceptors = list(-                filter(-                    lambda interceptor: isinstance(interceptor,-                                                   UnaryUnaryClientInterceptor),-                    interceptors))+        self._unary_unary_interceptors = []+        self._unary_stream_interceptors = []++        if interceptors:+            attrs_and_interceptor_classes = [","Nit: This should be immutable. A tuple is probably a better option. If you're willing to use `getattr`, we can even lift this to a class-level constant.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22713,411637890,2020-04-20T19:35:19Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -210,22 +220,32 @@ def __init__(self, target: str, options: ChannelArgumentType,           interceptors: An optional list of interceptors that would be used for             intercepting any RPC executed with that channel.         """"""-        if interceptors is None:-            self._unary_unary_interceptors = None-        else:-            self._unary_unary_interceptors = list(-                filter(-                    lambda interceptor: isinstance(interceptor,-                                                   UnaryUnaryClientInterceptor),-                    interceptors))+        self._unary_unary_interceptors = []+        self._unary_stream_interceptors = []++        if interceptors:+            attrs_and_interceptor_classes = [+                (self._unary_unary_interceptors, UnaryUnaryClientInterceptor),+                (self._unary_stream_interceptors, UnaryStreamClientInterceptor)+            ]++            # pylint: disable=cell-var-from-loop+            for attr, interceptor_class in attrs_and_interceptor_classes:+                attr.extend(+                    list(+                        filter(+                            lambda interceptor: isinstance(+                                interceptor, interceptor_class), interceptors)))              invalid_interceptors = set(interceptors) - set(-                self._unary_unary_interceptors)+                self._unary_unary_interceptors) - set(+                    self._unary_stream_interceptors)              if invalid_interceptors:                 raise ValueError(                     ""Interceptor must be ""+\-                    ""UnaryUnaryClientInterceptors, the following are invalid: {}""\+                    ""UnaryUnaryClientInterceptors or ""+\+                    ""UnaryStreamClientInterceptors the following are invalid: {}""\","Stylistic: ""the"" starts a new sentence. Period and capitalization.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22713,411644022,2020-04-20T19:45:24Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -133,103 +169,67 @@ class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):     intercepted call, being at the same time the same call returned to the     interceptors. -    For most of the methods, like `initial_metadata()` the caller does not need-    to wait until the interceptors task is finished, once the RPC is done the-    caller will have the freedom for accessing to the results.--    For the `__await__` method is it is proxied to the intercepted call only when-    the interceptor task is finished.+    As a base class for all of the interceptors implements the logic around+    final status, metadata and cancellation.     """""" -    _loop: asyncio.AbstractEventLoop-    _channel: cygrpc.AioChannel-    _cancelled_before_rpc: bool-    _intercepted_call: Optional[_base_call.UnaryUnaryCall]-    _intercepted_call_created: asyncio.Event     _interceptors_task: asyncio.Task     _pending_add_done_callbacks: Sequence[DoneCallbackType] -    # pylint: disable=too-many-arguments-    def __init__(self, interceptors: Sequence[UnaryUnaryClientInterceptor],-                 request: RequestType, timeout: Optional[float],-                 metadata: MetadataType,-                 credentials: Optional[grpc.CallCredentials],-                 wait_for_ready: Optional[bool], channel: cygrpc.AioChannel,-                 method: bytes, request_serializer: SerializingFunction,-                 response_deserializer: DeserializingFunction,-                 loop: asyncio.AbstractEventLoop) -> None:-        self._channel = channel-        self._loop = loop-        self._interceptors_task = loop.create_task(-            self._invoke(interceptors, method, timeout, metadata, credentials,-                         wait_for_ready, request, request_serializer,-                         response_deserializer))+    def __init__(self, interceptors_task: asyncio.Task) -> None:+        self._interceptors_task = interceptors_task         self._pending_add_done_callbacks = []         self._interceptors_task.add_done_callback(-            self._fire_pending_add_done_callbacks)+            self._fire_or_add_pending_add_done_callbacks)      def __del__(self):         self.cancel() -    # pylint: disable=too-many-arguments-    async def _invoke(self, interceptors: Sequence[UnaryUnaryClientInterceptor],-                      method: bytes, timeout: Optional[float],-                      metadata: Optional[MetadataType],-                      credentials: Optional[grpc.CallCredentials],-                      wait_for_ready: Optional[bool], request: RequestType,-                      request_serializer: SerializingFunction,-                      response_deserializer: DeserializingFunction-                     ) -> UnaryUnaryCall:-        """"""Run the RPC call wrapped in interceptors""""""--        async def _run_interceptor(-                interceptors: Iterator[UnaryUnaryClientInterceptor],-                client_call_details: ClientCallDetails,-                request: RequestType) -> _base_call.UnaryUnaryCall:--            interceptor = next(interceptors, None)--            if interceptor:-                continuation = functools.partial(_run_interceptor, interceptors)+    def _fire_or_add_pending_add_done_callbacks(self,+                                                interceptors_task: asyncio.Task+                                               ) -> None: -                call_or_response = await interceptor.intercept_unary_unary(-                    continuation, client_call_details, request)--                if isinstance(call_or_response, _base_call.UnaryUnaryCall):-                    return call_or_response-                else:-                    return UnaryUnaryCallResponse(call_or_response)+        if not self._pending_add_done_callbacks:+            return -            else:-                return UnaryUnaryCall(-                    request, _timeout_to_deadline(client_call_details.timeout),-                    client_call_details.metadata,-                    client_call_details.credentials,-                    client_call_details.wait_for_ready, self._channel,-                    client_call_details.method, request_serializer,-                    response_deserializer, self._loop)+        fire = False -        client_call_details = ClientCallDetails(method, timeout, metadata,-                                                credentials, wait_for_ready)-        return await _run_interceptor(iter(interceptors), client_call_details,-                                      request)+        try:+            call = interceptors_task.result()+            if call.done():+                fire = True+        except (AioRpcError, asyncio.CancelledError):+            fire = True -    def _fire_pending_add_done_callbacks(self,-                                         unused_task: asyncio.Task) -> None:         for callback in self._pending_add_done_callbacks:-            callback(self)+            if fire:","The value of `fire` doesn't ever change inside of this loop, right? So we should be able to pull the conditional outside of the loop and get a performance boost.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22713,411644928,2020-04-20T19:46:52Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -133,103 +169,67 @@ class InterceptedUnaryUnaryCall(_base_call.UnaryUnaryCall):     intercepted call, being at the same time the same call returned to the     interceptors. -    For most of the methods, like `initial_metadata()` the caller does not need-    to wait until the interceptors task is finished, once the RPC is done the-    caller will have the freedom for accessing to the results.--    For the `__await__` method is it is proxied to the intercepted call only when-    the interceptor task is finished.+    As a base class for all of the interceptors implements the logic around+    final status, metadata and cancellation.     """""" -    _loop: asyncio.AbstractEventLoop-    _channel: cygrpc.AioChannel-    _cancelled_before_rpc: bool-    _intercepted_call: Optional[_base_call.UnaryUnaryCall]-    _intercepted_call_created: asyncio.Event     _interceptors_task: asyncio.Task     _pending_add_done_callbacks: Sequence[DoneCallbackType] -    # pylint: disable=too-many-arguments-    def __init__(self, interceptors: Sequence[UnaryUnaryClientInterceptor],-                 request: RequestType, timeout: Optional[float],-                 metadata: MetadataType,-                 credentials: Optional[grpc.CallCredentials],-                 wait_for_ready: Optional[bool], channel: cygrpc.AioChannel,-                 method: bytes, request_serializer: SerializingFunction,-                 response_deserializer: DeserializingFunction,-                 loop: asyncio.AbstractEventLoop) -> None:-        self._channel = channel-        self._loop = loop-        self._interceptors_task = loop.create_task(-            self._invoke(interceptors, method, timeout, metadata, credentials,-                         wait_for_ready, request, request_serializer,-                         response_deserializer))+    def __init__(self, interceptors_task: asyncio.Task) -> None:+        self._interceptors_task = interceptors_task         self._pending_add_done_callbacks = []         self._interceptors_task.add_done_callback(-            self._fire_pending_add_done_callbacks)+            self._fire_or_add_pending_add_done_callbacks)      def __del__(self):         self.cancel() -    # pylint: disable=too-many-arguments-    async def _invoke(self, interceptors: Sequence[UnaryUnaryClientInterceptor],-                      method: bytes, timeout: Optional[float],-                      metadata: Optional[MetadataType],-                      credentials: Optional[grpc.CallCredentials],-                      wait_for_ready: Optional[bool], request: RequestType,-                      request_serializer: SerializingFunction,-                      response_deserializer: DeserializingFunction-                     ) -> UnaryUnaryCall:-        """"""Run the RPC call wrapped in interceptors""""""--        async def _run_interceptor(-                interceptors: Iterator[UnaryUnaryClientInterceptor],-                client_call_details: ClientCallDetails,-                request: RequestType) -> _base_call.UnaryUnaryCall:--            interceptor = next(interceptors, None)--            if interceptor:-                continuation = functools.partial(_run_interceptor, interceptors)+    def _fire_or_add_pending_add_done_callbacks(self,+                                                interceptors_task: asyncio.Task+                                               ) -> None: -                call_or_response = await interceptor.intercept_unary_unary(-                    continuation, client_call_details, request)--                if isinstance(call_or_response, _base_call.UnaryUnaryCall):-                    return call_or_response-                else:-                    return UnaryUnaryCallResponse(call_or_response)+        if not self._pending_add_done_callbacks:+            return -            else:-                return UnaryUnaryCall(-                    request, _timeout_to_deadline(client_call_details.timeout),-                    client_call_details.metadata,-                    client_call_details.credentials,-                    client_call_details.wait_for_ready, self._channel,-                    client_call_details.method, request_serializer,-                    response_deserializer, self._loop)+        fire = False","Stylistic: It wasn't clear to me what `fire` meant on the first read through. It's not common to use a verb in the imperative form as a variable name. It looks like what this really signifies is whether the call has already completed, so maybe `call_completed` as an alternative name?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22713,411671637,2020-04-20T20:31:23Z,src/python/grpcio_tests/tests_aio/unit/client_unary_stream_interceptor_test.py,"@@ -0,0 +1,404 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest+import datetime++import grpc++from grpc.experimental import aio+from tests_aio.unit._constants import UNREACHABLE_TARGET+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc++_SHORT_TIMEOUT_S = datetime.timedelta(seconds=1).total_seconds()++_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+_NUM_STREAM_RESPONSES = 5+_REQUEST_PAYLOAD_SIZE = 7+_RESPONSE_PAYLOAD_SIZE = 7+_RESPONSE_INTERVAL_US = int(_SHORT_TIMEOUT_S * 1000 * 1000)+++class _ResponseIterator:++    def __init__(self, response_iterator):+        self._response_cnt = 0+        self._response_iterator = response_iterator++    async def _forward_responses(self):+        async for response in self._response_iterator:+            self._response_cnt += 1+            yield response++    def __aiter__(self):+        return self._forward_responses()++    @property+    def response_cnt(self):+        return self._response_cnt+++def _inject_callbacks(call):+    first_callback_ran = asyncio.Event()++    def first_callback(call):+        # Validate that all resopnses have been received+        # and the call is an end state.+        assert call.done()+        first_callback_ran.set()++    second_callback_ran = asyncio.Event()++    def second_callback(call):+        # Validate that all resopnses have been received+        # and the call is an end state.+        assert call.done()+        second_callback_ran.set()++    call.add_done_callback(first_callback)+    call.add_done_callback(second_callback)++    async def validation():+        await asyncio.wait_for(+            asyncio.gather(first_callback_ran.wait(),+                           second_callback_ran.wait()),+            test_constants.SHORT_TIMEOUT)++    return validation()+++class _UnaryStreamInterceptorEmpty(aio.UnaryStreamClientInterceptor):++    async def intercept_unary_stream(self, continuation, client_call_details,+                                     request):+        return await continuation(client_call_details, request)+++class _UnaryStreamInterceptorWith_ResponseIterator(+        aio.UnaryStreamClientInterceptor):++    def __init__(self):+        self.response_iterator = None++    async def intercept_unary_stream(self, continuation, client_call_details,+                                     request):+        call = await continuation(client_call_details, request)+        self.response_iterator = _ResponseIterator(call)+        return self.response_iterator+++class TestUnaryStreamClientInterceptor(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_intercepts(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):+                interceptor = interceptor_class()++                request = messages_pb2.StreamingOutputCallRequest()+                for _ in range(_NUM_STREAM_RESPONSES):+                    request.response_parameters.append(+                        messages_pb2.ResponseParameters(+                            size=_RESPONSE_PAYLOAD_SIZE))++                channel = aio.insecure_channel(self._server_target,+                                               interceptors=[interceptor])+                stub = test_pb2_grpc.TestServiceStub(channel)+                call = stub.StreamingOutputCall(request)++                await call.wait_for_connection()++                response_cnt = 0+                async for response in call:+                    response_cnt += 1+                    self.assertIs(type(response),+                                  messages_pb2.StreamingOutputCallResponse)+                    self.assertEqual(_RESPONSE_PAYLOAD_SIZE,+                                     len(response.payload.body))++                self.assertTrue(response_cnt, _NUM_STREAM_RESPONSES)+                self.assertEqual(await call.code(), grpc.StatusCode.OK)+                self.assertEqual(await call.initial_metadata(), ())+                self.assertEqual(await call.trailing_metadata(), ())+                self.assertEqual(await call.details(), '')+                self.assertEqual(await call.debug_error_string(), '')+                self.assertEqual(call.cancel(), False)+                self.assertEqual(call.cancelled(), False)+                self.assertEqual(call.done(), True)++                if interceptor_class == _UnaryStreamInterceptorWith_ResponseIterator:+                    self.assertTrue(interceptor.response_iterator.response_cnt,+                                    _NUM_STREAM_RESPONSES)++                await channel.close()++    async def test_add_done_callback(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):+                interceptor = interceptor_class()++                request = messages_pb2.StreamingOutputCallRequest()+                for _ in range(_NUM_STREAM_RESPONSES):+                    request.response_parameters.append(+                        messages_pb2.ResponseParameters(+                            size=_RESPONSE_PAYLOAD_SIZE))++                channel = aio.insecure_channel(self._server_target,+                                               interceptors=[interceptor])+                stub = test_pb2_grpc.TestServiceStub(channel)+                call = stub.StreamingOutputCall(request)++                validation = _inject_callbacks(call)++                async for response in call:+                    pass++                await validation++                await channel.close()++    async def test_add_done_callback_after_connection(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):+                interceptor = interceptor_class()++                request = messages_pb2.StreamingOutputCallRequest()+                for _ in range(_NUM_STREAM_RESPONSES):+                    request.response_parameters.append(+                        messages_pb2.ResponseParameters(+                            size=_RESPONSE_PAYLOAD_SIZE))++                channel = aio.insecure_channel(self._server_target,+                                               interceptors=[interceptor])+                stub = test_pb2_grpc.TestServiceStub(channel)+                call = stub.StreamingOutputCall(request)++                # This ensures that the callbacks will be registered+                # with the intercepted call rather than saving in the+                # pending state list.+                await call.wait_for_connection()++                validation = _inject_callbacks(call)++                async for response in call:+                    pass++                await validation++                await channel.close()++    async def test_response_iterator_using_read(self):+        interceptor = _UnaryStreamInterceptorWith_ResponseIterator()++        channel = aio.insecure_channel(self._server_target,+                                       interceptors=[interceptor])+        stub = test_pb2_grpc.TestServiceStub(channel)++        request = messages_pb2.StreamingOutputCallRequest()+        for _ in range(_NUM_STREAM_RESPONSES):+            request.response_parameters.append(+                messages_pb2.ResponseParameters(size=_RESPONSE_PAYLOAD_SIZE))++        call = stub.StreamingOutputCall(request)++        response_cnt = 0+        for response in range(_NUM_STREAM_RESPONSES):+            response = await call.read()+            response_cnt += 1+            self.assertIs(type(response),+                          messages_pb2.StreamingOutputCallResponse)+            self.assertEqual(_RESPONSE_PAYLOAD_SIZE, len(response.payload.body))++        self.assertTrue(response_cnt, _NUM_STREAM_RESPONSES)+        self.assertTrue(interceptor.response_iterator.response_cnt,+                        _NUM_STREAM_RESPONSES)+        self.assertEqual(await call.code(), grpc.StatusCode.OK)++        await channel.close()++    async def test_mulitple_interceptors_response_iterator(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):++                interceptors = [interceptor_class(), interceptor_class()]++                channel = aio.insecure_channel(self._server_target,+                                               interceptors=interceptors)+                stub = test_pb2_grpc.TestServiceStub(channel)++                request = messages_pb2.StreamingOutputCallRequest()+                for _ in range(_NUM_STREAM_RESPONSES):+                    request.response_parameters.append(+                        messages_pb2.ResponseParameters(+                            size=_RESPONSE_PAYLOAD_SIZE))++                call = stub.StreamingOutputCall(request)++                response_cnt = 0+                async for response in call:+                    response_cnt += 1+                    self.assertIs(type(response),+                                  messages_pb2.StreamingOutputCallResponse)+                    self.assertEqual(_RESPONSE_PAYLOAD_SIZE,+                                     len(response.payload.body))++                self.assertTrue(response_cnt, _NUM_STREAM_RESPONSES)+                self.assertEqual(await call.code(), grpc.StatusCode.OK)++                await channel.close()++    async def test_intercepts_response_iterator_rpc_error(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):++                channel = aio.insecure_channel(+                    UNREACHABLE_TARGET, interceptors=[interceptor_class()])+                request = messages_pb2.StreamingOutputCallRequest()+                stub = test_pb2_grpc.TestServiceStub(channel)+                call = stub.StreamingOutputCall(request)++                with self.assertRaises(aio.AioRpcError) as exception_context:+                    async for response in call:+                        pass++                self.assertEqual(grpc.StatusCode.UNAVAILABLE,+                                 exception_context.exception.code())++                self.assertTrue(call.done())+                self.assertEqual(grpc.StatusCode.UNAVAILABLE, await call.code())+                await channel.close()++    async def test_cancel_before_rpc(self):++        interceptor_reached = asyncio.Event()+        wait_for_ever = self.loop.create_future()++        class Interceptor(aio.UnaryStreamClientInterceptor):++            async def intercept_unary_stream(self, continuation,+                                             client_call_details, request):+                interceptor_reached.set()+                await wait_for_ever++        channel = aio.insecure_channel(UNREACHABLE_TARGET,+                                       interceptors=[Interceptor()])+        request = messages_pb2.StreamingOutputCallRequest()+        stub = test_pb2_grpc.TestServiceStub(channel)+        call = stub.StreamingOutputCall(request)++        self.assertFalse(call.cancelled())+        self.assertFalse(call.done())++        await interceptor_reached.wait()+        self.assertTrue(call.cancel())++        with self.assertRaises(asyncio.CancelledError):+            async for response in call:+                pass++        self.assertTrue(call.cancelled())+        self.assertTrue(call.done())+        self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+        self.assertEqual(await call.details(),+                         _LOCAL_CANCEL_DETAILS_EXPECTATION)+        self.assertEqual(await call.initial_metadata(), None)+        self.assertEqual(await call.trailing_metadata(), None)+        await channel.close()++    async def test_cancel_after_rpc(self):++        interceptor_reached = asyncio.Event()+        wait_for_ever = self.loop.create_future()++        class Interceptor(aio.UnaryStreamClientInterceptor):++            async def intercept_unary_stream(self, continuation,+                                             client_call_details, request):+                call = await continuation(client_call_details, request)+                interceptor_reached.set()+                await wait_for_ever++        channel = aio.insecure_channel(UNREACHABLE_TARGET,+                                       interceptors=[Interceptor()])+        request = messages_pb2.StreamingOutputCallRequest()+        stub = test_pb2_grpc.TestServiceStub(channel)+        call = stub.StreamingOutputCall(request)++        self.assertFalse(call.cancelled())+        self.assertFalse(call.done())++        await interceptor_reached.wait()+        self.assertTrue(call.cancel())++        with self.assertRaises(asyncio.CancelledError):+            async for response in call:+                pass++        self.assertTrue(call.cancelled())+        self.assertTrue(call.done())+        self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+        self.assertEqual(await call.details(),+                         _LOCAL_CANCEL_DETAILS_EXPECTATION)+        self.assertEqual(await call.initial_metadata(), None)+        self.assertEqual(await call.trailing_metadata(), None)+        await channel.close()++    async def test_cancel_consuming_response_iterator(self):+        request = messages_pb2.StreamingOutputCallRequest()+        for _ in range(_NUM_STREAM_RESPONSES):+            request.response_parameters.append(+                messages_pb2.ResponseParameters(+                    size=_RESPONSE_PAYLOAD_SIZE,+                    interval_us=_RESPONSE_INTERVAL_US))++        channel = aio.insecure_channel(+            self._server_target,+            interceptors=[_UnaryStreamInterceptorWith_ResponseIterator()])+        stub = test_pb2_grpc.TestServiceStub(channel)+        call = stub.StreamingOutputCall(request)++        with self.assertRaises(asyncio.CancelledError):+            async for response in call:+                call.cancel()++        self.assertTrue(call.cancelled())+        self.assertTrue(call.done())+        self.assertEqual(await call.code(), grpc.StatusCode.CANCELLED)+        self.assertEqual(await call.details(),+                         _LOCAL_CANCEL_DETAILS_EXPECTATION)+        await channel.close()",I mean make sure we don't change the error path behavior. It is defined by us. I made a bad word choice.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22713,411709406,2020-04-20T21:37:24Z,src/python/grpcio_tests/tests_aio/unit/client_unary_stream_interceptor_test.py,"@@ -0,0 +1,404 @@+# Copyright 2019 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest+import datetime++import grpc++from grpc.experimental import aio+from tests_aio.unit._constants import UNREACHABLE_TARGET+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc++_SHORT_TIMEOUT_S = datetime.timedelta(seconds=1).total_seconds()++_LOCAL_CANCEL_DETAILS_EXPECTATION = 'Locally cancelled by application!'+_NUM_STREAM_RESPONSES = 5+_REQUEST_PAYLOAD_SIZE = 7+_RESPONSE_PAYLOAD_SIZE = 7+_RESPONSE_INTERVAL_US = int(_SHORT_TIMEOUT_S * 1000 * 1000)+++class _ResponseIterator:++    def __init__(self, response_iterator):+        self._response_cnt = 0+        self._response_iterator = response_iterator++    async def _forward_responses(self):+        async for response in self._response_iterator:+            self._response_cnt += 1+            yield response++    def __aiter__(self):+        return self._forward_responses()++    @property+    def response_cnt(self):+        return self._response_cnt+++def _inject_callbacks(call):+    first_callback_ran = asyncio.Event()++    def first_callback(call):+        # Validate that all resopnses have been received+        # and the call is an end state.+        assert call.done()+        first_callback_ran.set()++    second_callback_ran = asyncio.Event()++    def second_callback(call):+        # Validate that all resopnses have been received+        # and the call is an end state.+        assert call.done()+        second_callback_ran.set()++    call.add_done_callback(first_callback)+    call.add_done_callback(second_callback)++    async def validation():+        await asyncio.wait_for(+            asyncio.gather(first_callback_ran.wait(),+                           second_callback_ran.wait()),+            test_constants.SHORT_TIMEOUT)++    return validation()+++class _UnaryStreamInterceptorEmpty(aio.UnaryStreamClientInterceptor):++    async def intercept_unary_stream(self, continuation, client_call_details,+                                     request):+        return await continuation(client_call_details, request)+++class _UnaryStreamInterceptorWith_ResponseIterator(+        aio.UnaryStreamClientInterceptor):++    def __init__(self):+        self.response_iterator = None++    async def intercept_unary_stream(self, continuation, client_call_details,+                                     request):+        call = await continuation(client_call_details, request)+        self.response_iterator = _ResponseIterator(call)+        return self.response_iterator+++class TestUnaryStreamClientInterceptor(AioTestBase):++    async def setUp(self):+        self._server_target, self._server = await start_test_server()++    async def tearDown(self):+        await self._server.stop(None)++    async def test_intercepts(self):+        for interceptor_class in (_UnaryStreamInterceptorEmpty,+                                  _UnaryStreamInterceptorWith_ResponseIterator):++            with self.subTest(name=interceptor_class):+                interceptor = interceptor_class()++                request = messages_pb2.StreamingOutputCallRequest()+                for _ in range(_NUM_STREAM_RESPONSES):+                    request.response_parameters.append(+                        messages_pb2.ResponseParameters(+                            size=_RESPONSE_PAYLOAD_SIZE))++                channel = aio.insecure_channel(self._server_target,+                                               interceptors=[interceptor])+                stub = test_pb2_grpc.TestServiceStub(channel)+                call = stub.StreamingOutputCall(request)++                await call.wait_for_connection()++                response_cnt = 0+                async for response in call:+                    response_cnt += 1+                    self.assertIs(type(response),+                                  messages_pb2.StreamingOutputCallResponse)+                    self.assertEqual(_RESPONSE_PAYLOAD_SIZE,+                                     len(response.payload.body))++                self.assertTrue(response_cnt, _NUM_STREAM_RESPONSES)+                self.assertEqual(await call.code(), grpc.StatusCode.OK)+                self.assertEqual(await call.initial_metadata(), ())+                self.assertEqual(await call.trailing_metadata(), ())+                self.assertEqual(await call.details(), '')+                self.assertEqual(await call.debug_error_string(), '')+                self.assertEqual(call.cancel(), False)+                self.assertEqual(call.cancelled(), False)+                self.assertEqual(call.done(), True)++                if interceptor_class == _UnaryStreamInterceptorWith_ResponseIterator:","This test case is parameterized on the `interceptor_class`, so ideally, it shouldn't have any specialized logic for each one because it violates information hiding.It would basically be```pythonclass _UnaryStreamInterceptorWith_ResponseIterator:  ...  def assert_in_final_state(self, test: unittest.TestCase):    test.assertEqual(_NUM_STREAM_RESPONSES, interceptor.response_iterator.response_cnt)...class _UnaryStreamInterceptorEmpty:  ...  def assert_in_final_state(self, test):    pass...def test_intercepts(self):  with self.subtest(...):    ...    interceptor.assert_in_final_state(self)```I marked this one a nit because I don't think it's a blocker for merge, but if we continue adding special checks for each of the interceptor types, it kind of defeats the point of having a parameterized `subtest`.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22713,411712155,2020-04-20T21:42:36Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -598,4 +598,6 @@ async def wait_for_connection(self) -> None:         return await self._call.wait_for_connection()      async def read(self) -> ResponseType:-        return await self._call.read()+        # Behind the scenes everyting goes through the+        # async iterator. So this path should not be reached.+        raise Exception()","Since an exception being raised here would indicate an error internal to the library, we should probably raise an `AssertionError`. Also, we probably want to move (something like) that comment to the error string so the user has a little more information to go on should they ever trigger this codepath somehow.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22717,411955246,2020-04-21T07:50:58Z,tools/internal_ci/linux/grpc_reverse_test_python.sh,"@@ -0,0 +1,39 @@+# Copyright 2020 The gRPC Authors","Why is this called  ""reverse_test""? I don't know what it means and it's likely that other won't know either.  Looks like this test builds grpc python packages, installs them and then tries to run firestore unit tests?Questions:- do we really need to run this on PRs? That means extra test load and latency. It seems that once we get the test green, we can only run it on master to make sure we haven't broken firestore (but it seems we can only break them by pushing a bad release)- look like this is basically a kind of a distrib test - we build python package and then try to use it (with firestore in this case). So perhaps we should just add this test as a python distrib test (by adding the job to distribtest_targets.py) or at least classify it like one (Having many uncategorized test jobs makes it hard to monitor test health and to maintain stuff, because everything is ""custom"").",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22715,412306731,2020-04-21T16:29:01Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -1811,7 +1811,8 @@ TEST_P(BasicTest, BackendsRestart) {   CheckRpcSendFailure();   // Restart all backends.  RPCs should start succeeding again.   StartAllBackends();-  CheckRpcSendOk(1, RpcOptions().set_timeout_ms(2000).set_wait_for_ready(true));","I assume that this RPC was failing because it was hitting its deadline?  If so, let's just increase the deadline here.If the RPC was failing with something other than DEADLINE_EXCEEDED, then there's a real bug here that we need to find and fix, because that should never happen for a `wait_for_ready` RPC in this scenario.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22738,413319831,2020-04-22T20:46:42Z,doc/python/sphinx/glossary.rst,"@@ -27,3 +27,17 @@ Glossary     the returned object doesn't have restrictions (i.e. ``None`` allowed). The     deserializer is invoked with inbound message bytes on both the server side     and the client-side.++  wait_for_ready+    If an RPC is issued but the channel is in TRANSIENT_FAILURE or SHUTDOWN+    states, the RPC is unable to be transmitted promptly. By default, gRPC+    implementations SHOULD fail such RPCs immediately. This is known as ""fail","It's kind of unconventional to describe what implementations *should* do, rather than what *this* implementation actually does. I would get rid of all of the usages of ""SHOULD"".I'd recommend that this sentence be changed to ""By default, the gRPC library will fail such RPCs immediately.""",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22738,413322781,2020-04-22T20:49:51Z,doc/python/sphinx/glossary.rst,"@@ -27,3 +27,17 @@ Glossary     the returned object doesn't have restrictions (i.e. ``None`` allowed). The     deserializer is invoked with inbound message bytes on both the server side     and the client-side.++  wait_for_ready+    If an RPC is issued but the channel is in TRANSIENT_FAILURE or SHUTDOWN+    states, the RPC is unable to be transmitted promptly. By default, gRPC","Suggestion: Prefer active voice to passive: ""the library cannot transmit the RPC at that moment.""",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22738,413325784,2020-04-22T20:54:43Z,doc/python/sphinx/glossary.rst,"@@ -27,3 +27,17 @@ Glossary     the returned object doesn't have restrictions (i.e. ``None`` allowed). The     deserializer is invoked with inbound message bytes on both the server side     and the client-side.++  wait_for_ready+    If an RPC is issued but the channel is in TRANSIENT_FAILURE or SHUTDOWN+    states, the RPC is unable to be transmitted promptly. By default, gRPC+    implementations SHOULD fail such RPCs immediately. This is known as ""fail+    fast,"" but the usage of the term is historical. RPCs SHOULD NOT fail as a+    result of the channel being in other states (CONNECTING, READY, or IDLE).++    gRPC implementations MAY provide a per-RPC option to not fail RPCs as a","Suggestion to remove the RFC lingo: ""When the `wait_for_ready` option is specified, the library will queue RPCs until the channel is READY. Any submitted RPCs may still fail before the READY state is reached for other reasons, e.g. the client channel has been shut down or the RPC's deadline has been reached.""",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22715,413933320,2020-04-23T16:14:28Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -1808,7 +1808,7 @@ TEST_P(BasicTest, BackendsRestart) {   WaitForAllBackends();   // Stop backends.  RPCs should fail.   ShutdownAllBackends();-  CheckRpcSendFailure();+  CheckRpcSendFailure(num_backends_);","Please add a comment here indicating that we are sending multiple failed requests instead of just one to ensure that the client notices that all backends are down before we restart them.  If we didn't do this, then a single RPC could fail here due to the race condition between the LB pick and the GOAWAY from the chosen backend being shut down, which would not actually prove that the client noticed that all of the backends are down.  Then, when we send another request below (which we expect to succeed), if the callbacks happen in the wrong order, the same race condition could happen again due to the client not yet having noticed that the backends were all down.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22745,413982515,2020-04-23T17:22:21Z,doc/python/sphinx/glossary.rst,"@@ -39,3 +39,11 @@ Glossary     until the channel is READY. Any submitted RPCs may still fail before the     READY state is reached for other reasons, e.g., the client channel has been     shut down or the RPC's deadline has been reached.++  channel_arguments","Add ""contain experimental API"" to the text. To me, if we say it is both unstable and advanced, it sounds like in a really bad shape. WDYT?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22745,414058070,2020-04-23T19:16:37Z,doc/python/sphinx/glossary.rst,"@@ -39,3 +39,11 @@ Glossary     until the channel is READY. Any submitted RPCs may still fail before the     READY state is reached for other reasons, e.g., the client channel has been     shut down or the RPC's deadline has been reached.++  channel_arguments","Unfortunately, this is a bit of a mess today.Historically, we've not done a great job of setting expectations here.  We've added a lot of channel args that were intended to be experimental but not actually marked as such.  So some of them are fairly stable and reliable, and others are less so, and there's no way for users to tell the difference, which sucks.To make matters worse, even for the stable options, I don't think we ever made a conscious design decision about how to expose these options in wrapped languages.  I don't think we ever intended the literal option strings to become part of the wrapped language APIs, but that's where we've wound up, because we never established any other design principle.Ideally, I would like to see us move to a place where the wrapped language APIs provide a different (cleaner) API for specifying the options that we do consider stable and reliable.  Direct access to the channel args API should be used only as an escape hatch for the experimental args, and those should never be considered stable.I suspect that in order to get from here to there, we will need a fair amount of work.  We will need to go through the current set of channel args, decide which ones are stable and reliable, agree on what semantics we want to use to expose those options in wrapped languages, and then implement those semantics in each language.  Probably not going to happen overnight, but if you feel like driving this forward, it would be a big win.Sorry I don't have a better response for you.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22745,414136035,2020-04-23T21:27:22Z,doc/python/sphinx/glossary.rst,"@@ -39,3 +39,11 @@ Glossary     until the channel is READY. Any submitted RPCs may still fail before the     READY state is reached for other reasons, e.g., the client channel has been     shut down or the RPC's deadline has been reached.++  channel_arguments","Updated our promise to:```Channel arguments are meant for advanced usages and contain experimental API (some may not labeled as experimental).```@markdroth Thanks for sharing the context around the channel arguments. Given the unstable nature of channel arguments, we do want to warn our users. (I checked the history of the `grpc_types.h`, and we do update the key name from time to time.)Standardize the channel arguments is possible. For Java & Golang, they provide some channel arguments via surface API. We should be able to unify a stable set of channel arguments that recommend all wrapper languages to expose as public API. I don't mind add this to my todo list.This PR still provide value to Python users, if they do want advanced features. Do you think this version is better? @gnossen ",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22758,414176656,2020-04-23T22:53:43Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi,"@@ -80,6 +81,8 @@ cdef class PollerCompletionQueue(BaseCompletionQueue):         self._loop.remove_reader(self._read_socket)         # TODO(https://github.com/grpc/grpc/issues/22365) perform graceful shutdown         grpc_completion_queue_shutdown(self._cq)+        while not self._shutdown:+            self._poller_thread.join(timeout=_POLL_AWAKE_INTERVAL_S)","~I'm assuming you added the timeout for signal handling? If that's the case, [I think `cpython` already takes care of this for you.](https://github.com/python/cpython/blob/master/Modules/_threadmodule.c#L71) No need to set a timeout.~*Edit:* I spoke too soon. Looks like [this call](https://github.com/python/cpython/blob/master/Modules/_threadmodule.c#L63) actually will block indefinitely without processing signals.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22758,414180244,2020-04-23T23:02:13Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi,"@@ -80,6 +81,8 @@ cdef class PollerCompletionQueue(BaseCompletionQueue):         self._loop.remove_reader(self._read_socket)         # TODO(https://github.com/grpc/grpc/issues/22365) perform graceful shutdown         grpc_completion_queue_shutdown(self._cq)+        while not self._shutdown:+            self._poller_thread.join(timeout=_POLL_AWAKE_INTERVAL_S)","Still, it allows signal to interrupt the call, I think your statement is correct (see https://github.com/python/cpython/blob/master/Modules/_threadmodule.c#L67).",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22770,414764629,2020-04-24T18:04:37Z,src/python/grpcio_status/grpc_status/_async.py,"@@ -13,14 +13,12 @@ # limitations under the License. """"""Reference implementation for status mapping in gRPC Python."""""" -from grpc.experimental import aio- from google.rpc import status_pb2  from ._common import code_to_grpc_status_code, GRPC_DETAILS_METADATA_KEY  -async def from_call(call: aio.Call):+async def from_call(call):","I'm not sure I understand the circular dependency here. `grpc.experimental.aio` doesn't import `grpc_status`, does it?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22717,414831219,2020-04-24T20:04:18Z,tools/internal_ci/linux/grpc_reverse_test_python.sh,"@@ -0,0 +1,39 @@+# Copyright 2020 The gRPC Authors","> do we really need to run this on PRs?You are right. We don't need to run this on PRs. So, I updated it internally to twice a day.> look like this is basically a kind of a distrib testI wanted to run it as distrib test, and I found distrib tests focus on testing platform compatibility. There are two blockers: 1) we need to install all Python version in all platform (currently, we only test 2.7); 2) running this dependency test is time consuming (might slow down our release process).",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/22774,414947282,2020-04-25T01:36:13Z,src/php/ext/grpc/channel.c,"@@ -586,6 +586,13 @@ void php_grpc_delete_persistent_list_entry(char *key, php_grpc_int key_len   gpr_mu_unlock(&global_persistent_list_mu); } +// Clean all channels in the persistent list+// Called at post fork+void php_grpc_clean_persistent_list(TSRMLS_D) {","Instead of refactoring out this `php_grpc_clean_persistent_list` function in `channel.c`, perhaps we can limit our changes to only `php_grpc.c`, and refactor out these 4 lines instead https://github.com/grpc/grpc/blob/master/src/php/ext/grpc/php_grpc.c#L435-L438, and call them in those 2 separate places, but within `php_grpc.c`",X
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/22774,414948368,2020-04-25T01:41:59Z,src/php/ext/grpc/php_grpc.c,"@@ -206,9 +206,8 @@ void postfork_child() {   grpc_init();   grpc_php_init_completion_queue(TSRMLS_C); -  // re-create grpc_channel and point wrapped to it-  // unlock wrapped grpc channel mutex-  restart_channels();+  // clean all channels in the persistent list+  php_grpc_clean_persistent_list(TSRMLS_C);","Instead of doing this here, perhaps we should do it slightly earlier in L193: https://github.com/grpc/grpc/blob/master/src/php/ext/grpc/php_grpc.c#L193Also, we might be missing a `release_persistent_locks()` call in `postfork_child()`. We call `acquire_persistent_locks()` at `prefork_parent()`, and then `release_persistent_locks()` in `postfork_parent()`, but never in `postfork_child()`. Calling `zend_hash_clean()` without `release_persistent_locks()` will result in a hang. So we probably should do it around L193 too. So I think the sequence should be```... in postfork_child()...destroy_grpc_channels();release_persistent_locks();// OR a function to abstract out 2 callers within php_grpc.czend_hash_clean(&grpc_persistent_list);zend_hash_destroy(&grpc_persistent_list);zend_hash_clean(&grpc_target_upper_bound_map);zend_hash_destroy(&grpc_target_upper_bound_map);grpc_php_shutdown_completion_queue()```",X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22789,415976994,2020-04-27T16:47:00Z,test/core/fling/BUILD,"@@ -37,6 +37,11 @@ grpc_cc_binary(     name = ""fling_server"",     testonly = 1,     srcs = [""server.cc""],+    data = [+        ""//src/core/tsi/test_creds:ca.pem"",+        ""//src/core/tsi/test_creds:server1.key"",+        ""//src/core/tsi/test_creds:server1.pem"",+    ],","Thanks for pointing this out! I refactored the code and it passed all the tests. Does this look good to you?If path like this way doesn't meet the standard, we might need to refactor many of the previous clean-up PRs  since we merged a bunch of PRs with similar changes ... Can I know what might be the possible fix on this?",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/22801,416142877,2020-04-27T20:56:58Z,test/cpp/end2end/client_lb_end2end_test.cc,"@@ -240,6 +241,7 @@ class ClientLbEnd2endTest : public ::testing::Test {     servers_.clear();     creds_.reset();     grpc_shutdown_blocking();+    grpc_wait_for_async_shutdown();","I can look into that, but I'm not sure if that matters a lot though. It looks like there's not a way for a user or a test to ""wait for grpc clean-up done"" in gRPC core's semantics. I think we need to either add a function to provide this semantic (as in the commit above) or change the test to not depend on this semantic.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/22801,416146300,2020-04-27T21:02:19Z,test/cpp/end2end/client_lb_end2end_test.cc,"@@ -240,6 +241,7 @@ class ClientLbEnd2endTest : public ::testing::Test {     servers_.clear();     creds_.reset();     grpc_shutdown_blocking();+    grpc_wait_for_async_shutdown();","So far the pending async shutdown I see is always in [`destroy_channel()`](https://github.com/grpc/grpc/blob/23c8cfcfdaeb1191eeb0e0823b1badb6be83d815/src/core/lib/surface/channel.cc#L520). Typically the stack looks like this:``` grpc_shutdown destroy_channel() exec_ctx_run() grpc_core::ExecCtx::Flush() grpc_core::Executor::RunClosures() grpc_core::Executor::ThreadMain() grpc_core::(anonymous namespace)::ThreadInternalsPosix::ThreadInternalsPosix()::{lambda()#1}::operator()() grpc_core::(anonymous namespace)::ThreadInternalsPosix::ThreadInternalsPosix()::{lambda()#1}::__invoke() start_thread```Probably due to a call reference that is not removed in the test's main thread (which kind of makes sense to me, but I'm not familiar with it enough to say if anything is broken or not).",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/22801,416175525,2020-04-27T21:53:26Z,test/cpp/end2end/client_lb_end2end_test.cc,"@@ -240,6 +241,7 @@ class ClientLbEnd2endTest : public ::testing::Test {     servers_.clear();     creds_.reset();     grpc_shutdown_blocking();+    grpc_wait_for_async_shutdown();",Looks like `destroy_channel()` can be called in an executor: https://github.com/grpc/grpc/blob/master/src/core/lib/transport/transport.cc#L53. That can explain why `grpc_shutdown` is not called immediately after clearing up things on the user side.,
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/22801,416213443,2020-04-27T23:17:33Z,test/cpp/end2end/client_lb_end2end_test.cc,"@@ -227,21 +226,17 @@ class ClientLbEnd2endTest : public ::testing::Test {     // Workaround Apple CFStream bug     gpr_setenv(""grpc_cfstream"", ""0""); #endif+    grpc_init();",It sounds like it should go to the SetupTestCase of the actual test class rather than the base class?,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22743,416576010,2020-04-28T12:35:31Z,src/csharp/Grpc.Core.Api/Metadata.cs,"@@ -343,13 +349,14 @@ public byte[] ValueBytes              /// <summary>             /// Gets the string value of this metadata entry.+            /// If the metadata entry is binary then an exception is thrown.             /// </summary>             public string Value             {                 get                 {                     GrpcPreconditions.CheckState(!IsBinary, ""Cannot access string value of a binary metadata entry"");-                    return value ?? EncodingASCII.GetString(valueBytes);","why this change?at the same time, we're keeping similar logic in ValueBytes, so I'd like to understand the reasoning behind it.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22628,416743761,2020-04-28T16:14:32Z,src/csharp/BUILD-INTEGRATION.md,"@@ -105,6 +105,17 @@ button](https://stackoverflow.com/a/9770061). Click on it, and choose ""Add as link"". If you do not select this option, Visual Studio will copy files to the project directory instead. +#### My .proto files have same filename in different folders++By default Grpc.Tools compiles all .proto files into `obj` directory, flattening relative paths. For proto files with duplicated names it will cause following error `NETSDK1022 Duplicate 'Compile' items were included. [...]`. If you want to keep relative paths in your `obj` folder, define `OutputDir=""$(Protobuf_OutputPath)%(RelativeDir)""`.++```xml+  <ItemGroup>+    <Protobuf Include=""Protos/v2/http.proto"" OutputDir=""$(Protobuf_OutputPath)%(RelativeDir)""  />","Btw, there's also a %(Directory) variable that behaves slightly differently, but might be useful for out-of-project fileshttps://docs.microsoft.com/en-us/visualstudio/msbuild/msbuild-well-known-item-metadata?view=vs-2019",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22628,416755868,2020-04-28T16:31:13Z,src/csharp/BUILD-INTEGRATION.md,"@@ -105,6 +105,17 @@ button](https://stackoverflow.com/a/9770061). Click on it, and choose ""Add as link"". If you do not select this option, Visual Studio will copy files to the project directory instead. +#### My .proto files have same filename in different folders++By default Grpc.Tools compiles all .proto files into `obj` directory, flattening relative paths. For proto files with duplicated names it will cause following error `NETSDK1022 Duplicate 'Compile' items were included. [...]`. If you want to keep relative paths in your `obj` folder, define `OutputDir=""$(Protobuf_OutputPath)%(RelativeDir)""`.++```xml+  <ItemGroup>+    <Protobuf Include=""Protos/v2/http.proto"" OutputDir=""$(Protobuf_OutputPath)%(RelativeDir)""  />","Using `<Protobuf Include=""../../../protos/bar/helloworld.proto"" Link=""someprefix/helloworld.proto"" OutputDir=""$(Protobuf_OutputPath)%(Link)"" />` also kind of works except the `Link` metadata will be expanded to `someprefix/helloworld.proto`, so the generated code will go under `someprefix/helloworld.proto/Helloworld.cs` (dirty but works).",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22800,416854698,2020-04-28T19:06:24Z,bazel/python_rules.bzl,"@@ -166,6 +166,8 @@ def _generate_pb2_grpc_src_impl(context):      imports = []     if out_dir.import_path:+        #TODO: I believe this can be deleted, the rule requires a py_proto_library, and that rule will",I believe you're right. You can go ahead and remove this conditional.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22808,417268586,2020-04-29T12:15:28Z,test/core/end2end/tests/keepalive_timeout.cc,"@@ -276,7 +276,7 @@ static void test_read_delays_keepalive(grpc_end2end_test_config config) {   grpc_slice response_payload_slice =       grpc_slice_from_copied_string(""hello you""); -  gpr_timespec deadline = five_seconds_from_now();+  gpr_timespec deadline = n_seconds_from_now(60);",what gives us the confidence that the increased timeout is going to help?why was 60s chosen (it seems like quite a bit of increase from 5s)? The increase should be based on some data as evidence otherwise we're just blindly increasing our test latency without necessarily seeing improvements in flakiness rate in return.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22818,417461323,2020-04-29T16:47:57Z,test/cpp/microbenchmarks/bm_opencensus_plugin.cc,"@@ -28,6 +28,7 @@ #include ""opencensus/stats/stats.h"" #include ""src/cpp/ext/filters/census/grpc_plugin.h"" #include ""src/proto/grpc/testing/echo.grpc.pb.h""+#include ""test/core/util/test_config.h""",this belongs to the other PR?,
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/22808,417564825,2020-04-29T19:40:41Z,test/core/end2end/tests/keepalive_timeout.cc,"@@ -276,7 +276,7 @@ static void test_read_delays_keepalive(grpc_end2end_test_config config) {   grpc_slice response_payload_slice =       grpc_slice_from_copied_string(""hello you""); -  gpr_timespec deadline = five_seconds_from_now();+  gpr_timespec deadline = n_seconds_from_now(60);","The test is no longer failing on master, so not making this change should also be fine. The ones that did fail in the past were because of the call deadline firing too soon, and this test does not really test the call's deadline.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22812,417626622,2020-04-29T21:35:34Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/common.pyx.pxi,"@@ -112,3 +112,57 @@ def schedule_coro_threadsafe(object coro, object loop):             )         else:             raise+++def async_generator_to_generator(object agen, object loop):+    """"""Converts an async generator into generator.""""""+    try:+        while True:+            future = asyncio.run_coroutine_threadsafe(+                agen.__anext__(),+                loop+            )+            response = future.result()+            if response is EOF:+                break+            else:+                yield response+    except StopAsyncIteration:+        # If StopAsyncIteration is raised, end this generator.+        pass+++async def generator_to_async_generator(object gen, object loop, object thread_pool):+    """"""Converts a generator into async generator.++    The generator might block, so we need to delegate the iteration to thread+    pool. Also, we can't simply delegate __next__ to the thread pool, otherwise+    we will see following error:++        TypeError: StopIteration interacts badly with generators and cannot be+            raised into a Future+    """"""+    queue = asyncio.Queue(loop=loop)++    def yield_to_queue():+        try:+            for item in gen:+                # For an infinite sized queue, the put_nowait should always success+                loop.call_soon_threadsafe(queue.put_nowait, item)","Question, what's the difference in the proposal and by just creating a list in memory with all of the items that the generator can provide? I would say, unless Im missing something, that there are no significant differences.So I would say that using an infinite queue and using the `putt_nowait` we are not doing any kind of cooperation.Should we try to achieve some kind of cooperation? so only producing messages if and only if messages are being consumed, which if I'm not wrong would be the behavior of two chained iterators.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22812,417631486,2020-04-29T21:45:45Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -254,11 +318,27 @@ async def _finish_handler_with_unary_response(RPCState rpc_state,     stream-unary handlers.     """"""     # Executes application logic-    -    cdef object response_message = await unary_handler(-        request,-        servicer_context,-    )+    cdef object response_message+    cdef _SyncServicerContext sync_servicer_context++    if _is_async_handler(unary_handler):",Is it possible to make this conditional a preprocessing step? I suspect that would give us a performance boost.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22821,417643841,2020-04-29T22:13:06Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -154,6 +161,42 @@ async def intercept_unary_stream(         """"""  +class StreamUnaryClientInterceptor(ClientInterceptor, metaclass=ABCMeta):+    """"""Affords intercepting stream-unary invocations.""""""++    @abstractmethod+    async def intercept_stream_unary(+            self,+            continuation: Callable[[ClientCallDetails, RequestType],+                                   UnaryStreamCall],+            client_call_details: ClientCallDetails,+            request_iterator: RequestIterableType,+    ) -> StreamUnaryCall:+        """"""Intercepts a stream-unary invocation asynchronously.++        Args:+          continuation: A coroutine that proceeds with the invocation by+            executing the next interceptor in chain or invoking the",Nit: s/in chain/in the chain/,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22812,417647432,2020-04-29T22:22:26Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/common.pyx.pxi,"@@ -112,3 +112,57 @@ def schedule_coro_threadsafe(object coro, object loop):             )         else:             raise+++def async_generator_to_generator(object agen, object loop):+    """"""Converts an async generator into generator.""""""+    try:+        while True:+            future = asyncio.run_coroutine_threadsafe(+                agen.__anext__(),+                loop+            )+            response = future.result()+            if response is EOF:+                break+            else:+                yield response+    except StopAsyncIteration:+        # If StopAsyncIteration is raised, end this generator.+        pass+++async def generator_to_async_generator(object gen, object loop, object thread_pool):+    """"""Converts a generator into async generator.++    The generator might block, so we need to delegate the iteration to thread+    pool. Also, we can't simply delegate __next__ to the thread pool, otherwise+    we will see following error:++        TypeError: StopIteration interacts badly with generators and cannot be+            raised into a Future+    """"""+    queue = asyncio.Queue(loop=loop)++    def yield_to_queue():+        try:+            for item in gen:+                # For an infinite sized queue, the put_nowait should always success+                loop.call_soon_threadsafe(queue.put_nowait, item)","Iterators might block between each yield. For example, imagine there is chat application using streaming RPC to communicate. The client might send arbitrary number of messages to the backend. If we try to convert the generator to list, it deadlocks.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22821,417649628,2020-04-29T22:28:06Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -504,6 +551,149 @@ def time_remaining(self) -> Optional[float]:         raise NotImplementedError()  +class InterceptedStreamUnaryCall(_InterceptedUnaryResponseMixin,+                                 InterceptedCall, _base_call.StreamUnaryCall):+    """"""Used for running a `StreamUnaryCall` wrapped by interceptors.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _write_to_iterator_async_gen: Optional[AsyncIterable[RequestType]]+    _write_to_iterator_queue: Optional[asyncio.Queue]++    _FINISH_ITERATOR_SENTINEL = tuple()++    # pylint: disable=too-many-arguments+    def __init__(self, interceptors: Sequence[StreamUnaryClientInterceptor],+                 request_iterator: Optional[RequestIterableType],+                 timeout: Optional[float], metadata: MetadataType,+                 credentials: Optional[grpc.CallCredentials],+                 wait_for_ready: Optional[bool], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction,+                 loop: asyncio.AbstractEventLoop) -> None:+        self._loop = loop+        self._channel = channel+        if not request_iterator:",Nit: This should probably be `if request_iterator is None`. It's technically possible to pass a falsey object that is nonetheless a valid iterator.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22812,417652077,2020-04-29T22:34:43Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -254,11 +318,27 @@ async def _finish_handler_with_unary_response(RPCState rpc_state,     stream-unary handlers.     """"""     # Executes application logic-    -    cdef object response_message = await unary_handler(-        request,-        servicer_context,-    )+    cdef object response_message+    cdef _SyncServicerContext sync_servicer_context++    if _is_async_handler(unary_handler):","In the interceptor PR, mmx mentioned we can register handlers to methods in Core (see [Core API](https://github.com/grpc/grpc/blob/master/include/grpc/grpc.h#L387).This is boosting the type check speed, but I wasn't sure if we can leverage API above. We can do it in a separate PR. Created: [Aio] Preprocess the categorization of method handlers #22824",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22628,417896097,2020-04-30T10:00:07Z,src/csharp/BUILD-INTEGRATION.md,"@@ -105,6 +105,17 @@ button](https://stackoverflow.com/a/9770061). Click on it, and choose ""Add as link"". If you do not select this option, Visual Studio will copy files to the project directory instead. +#### My .proto files have same filename in different folders++By default Grpc.Tools compiles all .proto files into `obj` directory, flattening relative paths. For proto files with duplicated names it will cause following error `NETSDK1022 Duplicate 'Compile' items were included. [...]`. If you want to keep relative paths in your `obj` folder, define `OutputDir=""$(Protobuf_OutputPath)%(RelativeDir)""`.++```xml+  <ItemGroup>+    <Protobuf Include=""Protos/v2/http.proto"" OutputDir=""$(Protobuf_OutputPath)%(RelativeDir)""  />","- It least on linux %(Directory) gave me the `OutputPath/the/full/path/without/the/root/Helloworld.cs` (I tried on linux), which worked but could cause problems with too long file names.- It think any directory structure that works and still allows some reasonable way of seeing which .proto file corresponds to which .cs file would work. (actually I think our best way forward to solving this problem once and for all is what is proposed here: https://github.com/grpc/grpc/issues/17672#issuecomment-621152800)",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22828,418089732,2020-04-30T15:19:08Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -1468,6 +1473,26 @@ class XdsEnd2endTest : public ::testing::TestWithParam<TestType> {     }   } +  void InvalidUpdateNackCheck(const RouteConfiguration& route_config,","If you make the changes I suggest below, then this function will probably no longer be needed, because the amount of code left here will be quite small.  And the tests will be clearer with this part of the logic done in-line in each test.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22821,418175883,2020-04-30T17:34:16Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -167,10 +169,17 @@ def __call__(self,          deadline = _timeout_to_deadline(timeout) -        call = StreamUnaryCall(request_iterator, deadline, metadata,-                               credentials, wait_for_ready, self._channel,-                               self._method, self._request_serializer,-                               self._response_deserializer, self._loop)+        if not self._interceptors:+            call = StreamUnaryCall(request_iterator, deadline, metadata,+                                   credentials, wait_for_ready, self._channel,+                                   self._method, self._request_serializer,+                                   self._response_deserializer, self._loop)","optional: At some point, we might want to reduce the number of arguments (this one has 10) with some data structure.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22821,418178752,2020-04-30T17:39:14Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -238,13 +251,15 @@ def __init__(self, target: str, options: ChannelArgumentType,              invalid_interceptors = set(interceptors) - set(                 self._unary_unary_interceptors) - set(-                    self._unary_stream_interceptors)+                    self._unary_stream_interceptors) - set(+                        self._stream_unary_interceptors)              if invalid_interceptors:                 raise ValueError(                     ""Interceptor must be ""+\                     ""UnaryUnaryClientInterceptors or ""+\-                    ""UnaryStreamClientInterceptors. The following are invalid: {}""\+                    ""UnaryUnaryClientInterceptors or ""+\+                    ""StreamUnaryClientInterceptors. The following are invalid: {}""\","nit: We can programmatically get the name of these three classes.```pythontuple(interceptor_class.__name__ for _, interceptor_class in attrs_and_interceptor_classes)```",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22821,418183521,2020-04-30T17:47:21Z,src/python/grpcio_tests/tests_aio/unit/client_stream_unary_interceptor_test.py,"@@ -0,0 +1,531 @@+# Copyright 2020 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest+import datetime++import grpc++from grpc.experimental import aio+from tests_aio.unit._constants import UNREACHABLE_TARGET+from tests_aio.unit._common import inject_callbacks+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc++_SHORT_TIMEOUT_S = 1.0++_NUM_STREAM_REQUESTS = 5+_REQUEST_PAYLOAD_SIZE = 7+_RESPONSE_INTERVAL_US = int(_SHORT_TIMEOUT_S * 1000 * 1000)+++class _CountingRequestIterator:++    def __init__(self, request_iterator):+        self.request_cnt = 0+        self._request_iterator = request_iterator++    async def _forward_requests(self):+        async for request in self._request_iterator:+            self.request_cnt += 1+            yield request++    def __aiter__(self):+        return self._forward_requests()+++class _StreamUnaryInterceptorEmpty(aio.StreamUnaryClientInterceptor):++    async def intercept_stream_unary(self, continuation, client_call_details,+                                     request_iterator):+        return await continuation(client_call_details, request_iterator)++    def assert_in_final_state(self, test: unittest.TestCase):+        pass+++class _StreamUnaryInterceptorWithRequestIterator(+        aio.StreamUnaryClientInterceptor):++    async def intercept_stream_unary(self, continuation, client_call_details,+                                     request_iterator):+        self.request_iterator = _CountingRequestIterator(request_iterator)+        call = await continuation(client_call_details, self.request_iterator)","Is `write` allowed within the interceptors?With mixing-api-style prohibited, it might be hard to do so since the interceptors are designed to pass a request iterator. Adding support for `write` here will further complex the interceptor logic. Maybe we should document that for client-side streaming RPCs `write` is not available in interceptors?",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22821,418264089,2020-04-30T20:17:43Z,src/python/grpcio_tests/tests_aio/unit/client_stream_unary_interceptor_test.py,"@@ -0,0 +1,531 @@+# Copyright 2020 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest+import datetime++import grpc++from grpc.experimental import aio+from tests_aio.unit._constants import UNREACHABLE_TARGET+from tests_aio.unit._common import inject_callbacks+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc++_SHORT_TIMEOUT_S = 1.0++_NUM_STREAM_REQUESTS = 5+_REQUEST_PAYLOAD_SIZE = 7+_RESPONSE_INTERVAL_US = int(_SHORT_TIMEOUT_S * 1000 * 1000)+++class _CountingRequestIterator:++    def __init__(self, request_iterator):+        self.request_cnt = 0+        self._request_iterator = request_iterator++    async def _forward_requests(self):+        async for request in self._request_iterator:+            self.request_cnt += 1+            yield request++    def __aiter__(self):+        return self._forward_requests()+++class _StreamUnaryInterceptorEmpty(aio.StreamUnaryClientInterceptor):++    async def intercept_stream_unary(self, continuation, client_call_details,+                                     request_iterator):+        return await continuation(client_call_details, request_iterator)++    def assert_in_final_state(self, test: unittest.TestCase):+        pass+++class _StreamUnaryInterceptorWithRequestIterator(+        aio.StreamUnaryClientInterceptor):++    async def intercept_stream_unary(self, continuation, client_call_details,+                                     request_iterator):+        self.request_iterator = _CountingRequestIterator(request_iterator)+        call = await continuation(client_call_details, self.request_iterator)","`write` should not be used within the interceptors, but is perfectly usable from the caller even thought the `write` is being sent through an iterator behind the scenes.There are some use cases within the iterator that they might have undesirable behaviors or at least unknown behaviors if they are not used in the right way.That's the case of the `write`, if the interceptor that has access o the full `call` object uses the `write` method will have as a side effect what you are saying unless the interceptor takes the responsibility to send everything that comes from the interceptor to `/dev/null`, so technically could be used.The same will happen with the `__await__`, if the caller did not provide an iterator would never have the chance to write to the RPC since the interceptor would be blocking the call because of the `__await__`.I can try to add some documentation, but TBH having the feeling that the support of the `write` (and the `read` as well`) is adding more friction and complexity in our implementation.and most likely `__await__` neither.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22821,418338709,2020-04-30T23:08:57Z,src/python/grpcio_tests/tests_aio/unit/client_stream_unary_interceptor_test.py,"@@ -0,0 +1,531 @@+# Copyright 2020 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest+import datetime++import grpc++from grpc.experimental import aio+from tests_aio.unit._constants import UNREACHABLE_TARGET+from tests_aio.unit._common import inject_callbacks+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc++_SHORT_TIMEOUT_S = 1.0++_NUM_STREAM_REQUESTS = 5+_REQUEST_PAYLOAD_SIZE = 7+_RESPONSE_INTERVAL_US = int(_SHORT_TIMEOUT_S * 1000 * 1000)+++class _CountingRequestIterator:++    def __init__(self, request_iterator):+        self.request_cnt = 0+        self._request_iterator = request_iterator++    async def _forward_requests(self):+        async for request in self._request_iterator:+            self.request_cnt += 1+            yield request++    def __aiter__(self):+        return self._forward_requests()+++class _StreamUnaryInterceptorEmpty(aio.StreamUnaryClientInterceptor):++    async def intercept_stream_unary(self, continuation, client_call_details,+                                     request_iterator):+        return await continuation(client_call_details, request_iterator)++    def assert_in_final_state(self, test: unittest.TestCase):+        pass+++class _StreamUnaryInterceptorWithRequestIterator(+        aio.StreamUnaryClientInterceptor):++    async def intercept_stream_unary(self, continuation, client_call_details,+                                     request_iterator):+        self.request_iterator = _CountingRequestIterator(request_iterator)+        call = await continuation(client_call_details, self.request_iterator)","I agree about the complexity. Given current design, I don't have better ways to support `write` in interceptors.There is one alternative I want to discuss, what if we don't link request iterator and `write` for users? If they want to intercept invocations of `write`, they have to update their interceptor implementation. For example, they could return a customized version of `Call` object that handles request iterator and `write` separately.This means writing interceptors for streaming RPCs are slightly harder, but can reduce a large amount of logic needed in our implementation.WDYT? @pfreixes What's your opinion about this trade-off between usability and complexity? @gnossen ",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22821,418517424,2020-05-01T12:13:28Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -504,6 +551,149 @@ def time_remaining(self) -> Optional[float]:         raise NotImplementedError()  +class InterceptedStreamUnaryCall(_InterceptedUnaryResponseMixin,+                                 InterceptedCall, _base_call.StreamUnaryCall):+    """"""Used for running a `StreamUnaryCall` wrapped by interceptors.++    For the `__await__` method is it is proxied to the intercepted call only when+    the interceptor task is finished.+    """"""++    _loop: asyncio.AbstractEventLoop+    _channel: cygrpc.AioChannel+    _write_to_iterator_async_gen: Optional[AsyncIterable[RequestType]]+    _write_to_iterator_queue: Optional[asyncio.Queue]++    _FINISH_ITERATOR_SENTINEL = tuple()++    # pylint: disable=too-many-arguments+    def __init__(self, interceptors: Sequence[StreamUnaryClientInterceptor],+                 request_iterator: Optional[RequestIterableType],+                 timeout: Optional[float], metadata: MetadataType,+                 credentials: Optional[grpc.CallCredentials],+                 wait_for_ready: Optional[bool], channel: cygrpc.AioChannel,+                 method: bytes, request_serializer: SerializingFunction,+                 response_deserializer: DeserializingFunction,+                 loop: asyncio.AbstractEventLoop) -> None:+        self._loop = loop+        self._channel = channel+        if not request_iterator:+            # We provide our own request iterator which is a proxy+            # of the futures writes that will be done by the caller.+            self._write_to_iterator_queue = asyncio.Queue(maxsize=1)+            self._write_to_iterator_async_gen = self._proxies_writes_as_a_request_iteerator(+            )+            request_iterator = self._write_to_iterator_async_gen+        else:+            self._write_to_iterator_queue = None++        interceptors_task = loop.create_task(+            self._invoke(interceptors, method, timeout, metadata, credentials,+                         wait_for_ready, request_iterator, request_serializer,+                         response_deserializer))+        super().__init__(interceptors_task)++    # pylint: disable=too-many-arguments+    async def _invoke(+            self, interceptors: Sequence[StreamUnaryClientInterceptor],+            method: bytes, timeout: Optional[float],+            metadata: Optional[MetadataType],+            credentials: Optional[grpc.CallCredentials],+            wait_for_ready: Optional[bool],+            request_iterator: RequestIterableType,+            request_serializer: SerializingFunction,+            response_deserializer: DeserializingFunction) -> StreamUnaryCall:+        """"""Run the RPC call wrapped in interceptors""""""++        async def _run_interceptor(+                interceptors: Iterator[UnaryUnaryClientInterceptor],+                client_call_details: ClientCallDetails,+                request_iterator: RequestIterableType+        ) -> _base_call.StreamUnaryCall:++            interceptor = next(interceptors, None)++            if interceptor:+                continuation = functools.partial(_run_interceptor, interceptors)++                return await interceptor.intercept_stream_unary(+                    continuation, client_call_details, request_iterator)+            else:","If we allow the interceptor to return a finished call, the user would need to consider to implement a defensive pattern every time that the `write` is used, for example:```pythoncal = StreamingUnaryCall()try:    for message in messages:        await call.write(message)except InvalidStateError:    # Is this because an error or because there is a response?    passtry:    response = await callexcept (AioRpcError, asyncio.CancelledError):    # there was an error```Also, Im wondering how the interceptor would now need to be returned if the interceptor does not have any chance of consuming the messages? should be the response to something that is built based on what has been observed in the message requests?Having the feeling that we would need to have a very good example of implementing what you are saying, should we wait till the community askes for this?",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/22821,418521153,2020-05-01T12:27:51Z,src/python/grpcio_tests/tests_aio/unit/client_stream_unary_interceptor_test.py,"@@ -0,0 +1,531 @@+# Copyright 2020 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest+import datetime++import grpc++from grpc.experimental import aio+from tests_aio.unit._constants import UNREACHABLE_TARGET+from tests_aio.unit._common import inject_callbacks+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc++_SHORT_TIMEOUT_S = 1.0++_NUM_STREAM_REQUESTS = 5+_REQUEST_PAYLOAD_SIZE = 7+_RESPONSE_INTERVAL_US = int(_SHORT_TIMEOUT_S * 1000 * 1000)+++class _CountingRequestIterator:++    def __init__(self, request_iterator):+        self.request_cnt = 0+        self._request_iterator = request_iterator++    async def _forward_requests(self):+        async for request in self._request_iterator:+            self.request_cnt += 1+            yield request++    def __aiter__(self):+        return self._forward_requests()+++class _StreamUnaryInterceptorEmpty(aio.StreamUnaryClientInterceptor):++    async def intercept_stream_unary(self, continuation, client_call_details,+                                     request_iterator):+        return await continuation(client_call_details, request_iterator)++    def assert_in_final_state(self, test: unittest.TestCase):+        pass+++class _StreamUnaryInterceptorWithRequestIterator(+        aio.StreamUnaryClientInterceptor):++    async def intercept_stream_unary(self, continuation, client_call_details,+                                     request_iterator):+        self.request_iterator = _CountingRequestIterator(request_iterator)+        call = await continuation(client_call_details, self.request_iterator)","> There is one alternative I want to discuss, what if we don't link request iterator and write for users? If we don't do that we will ask the interceptors to spread the logic in two different places, for when the user is using an iterator and when the user is using the write. With the current solution, the interceptor can provide a single point of logic.In any case, your point is about the use case of the `write` within the interceptors which technically would be possible but would compromise the whole request iterator chain, since an interceptor using the chain will automatically circumvent that chain and will be writing directly to the call.For that, we can either stress that out in the documentation, or we could basically return a narrowed version of the Call object with the `write` method removed.WDYT?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22821,418705018,2020-05-01T19:41:56Z,src/python/grpcio_tests/tests_aio/unit/client_stream_unary_interceptor_test.py,"@@ -0,0 +1,531 @@+# Copyright 2020 The gRPC Authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import asyncio+import logging+import unittest+import datetime++import grpc++from grpc.experimental import aio+from tests_aio.unit._constants import UNREACHABLE_TARGET+from tests_aio.unit._common import inject_callbacks+from tests_aio.unit._test_server import start_test_server+from tests_aio.unit._test_base import AioTestBase+from tests.unit.framework.common import test_constants+from src.proto.grpc.testing import messages_pb2, test_pb2_grpc++_SHORT_TIMEOUT_S = 1.0++_NUM_STREAM_REQUESTS = 5+_REQUEST_PAYLOAD_SIZE = 7+_RESPONSE_INTERVAL_US = int(_SHORT_TIMEOUT_S * 1000 * 1000)+++class _CountingRequestIterator:++    def __init__(self, request_iterator):+        self.request_cnt = 0+        self._request_iterator = request_iterator++    async def _forward_requests(self):+        async for request in self._request_iterator:+            self.request_cnt += 1+            yield request++    def __aiter__(self):+        return self._forward_requests()+++class _StreamUnaryInterceptorEmpty(aio.StreamUnaryClientInterceptor):++    async def intercept_stream_unary(self, continuation, client_call_details,+                                     request_iterator):+        return await continuation(client_call_details, request_iterator)++    def assert_in_final_state(self, test: unittest.TestCase):+        pass+++class _StreamUnaryInterceptorWithRequestIterator(+        aio.StreamUnaryClientInterceptor):++    async def intercept_stream_unary(self, continuation, client_call_details,+                                     request_iterator):+        self.request_iterator = _CountingRequestIterator(request_iterator)+        call = await continuation(client_call_details, self.request_iterator)","As you described, that solution requires duplication in users' interceptors. If the benefit is unclear, I think we should stick to this PR's design and add some documentation (or hide the `write` method). Documentation might be slightly better, since the `UsageError` already disabled using `write`.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22857,419774216,2020-05-04T22:50:11Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -962,7 +960,7 @@ class AdsServiceImpl : public AggregatedDiscoveryService::Service,   //   yet been destroyed by UnsetResource()).   // - There is at least one subscription for the resource.   ResourceMap resource_map_;-};+};  // namespace","This looks incorrect.  This is the end of a class, not the end of a namespace.",
940619,Falco20019,https://api.github.com/repos/grpc/grpc/pulls/22869,420054179,2020-05-05T11:59:06Z,src/csharp/Grpc.Tools/build/_protobuf/Google.Protobuf.Tools.targets,"@@ -252,6 +252,7 @@     <!-- Ensure output directories. -->     <MakeDir Directories=""%(_Protobuf_OutOfDateProto.OutputDir)"" />     <MakeDir Directories=""%(_Protobuf_OutOfDateProto.GrpcOutputDir)"" />+    <MakeDir Directories=""%(Protobuf_ExpectedOutputs.RelativeDir)"" />","I was trying to find out how we can make sure, the generated paths exist. I looked into the MSBuild code of `ProtobufCompilerOutputs` and saw, that the information about the path is already implicitly available in `PossibleOutputs` which gets assigned to `$(Protobuf_ExpectedOutputs)`. These are already prepared to be located in `%(_Protobuf_OutOfDateProto.OutputDir)` (as this will be the input to `ProtobufCompilerOutputs` a couple lines later) and contain the generated directory hash.`%(Protobuf_ExpectedOutputs.RelativeDir)` therefore is the union of all paths that will be generated (independent of what really is part of `_Protobuf_OutOfDateProto`, but as all those paths are necessary already for the first creation, i found that ignorable also for big sets of protos).We might be able to improve this by restrict it like `Condition = "" '%(Source)' != '' and '@(Protobuf_ExpectedOutputs)' == '' ""` (as done with the definition of `_Protobuf_OutOfDateProto` in `_Protobuf_GatherStaleSimple`) but I'm not too deep into MSBuild to know if this does what I think it does. As `%(Protobuf_ExpectedOutputs.Source)` is pointing to the proto of the file this expected output is related to, this might work.",
940619,Falco20019,https://api.github.com/repos/grpc/grpc/pulls/22869,420111531,2020-05-05T13:32:56Z,src/csharp/Grpc.Tools/ProtoCompile.cs,"@@ -433,6 +439,18 @@ protected override string GenerateResponseFileCommands()             return cmd.ToString();         } +        // If possible, disambiguate output dir by adding a hash of the proto file's path+        static string MaybeEnhanceOutputDir(string outputDir, ITaskItem[] protobufs)",I can add an option if you like. Any preferred naming for the option?Is files in the project root really a thing? I thought there is always at least a protos folder. Because then file.proto and protos/file.proto would also already be a thing to consider with the hash as difference.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22869,420219784,2020-05-05T15:56:37Z,src/csharp/Grpc.Tools/ProtoCompile.cs,"@@ -433,6 +439,18 @@ protected override string GenerateResponseFileCommands()             return cmd.ToString();         } +        // If possible, disambiguate output dir by adding a hash of the proto file's path+        static string MaybeEnhanceOutputDir(string outputDir, ITaskItem[] protobufs)","Let's add the option to make it possible to disable the hashed paths when needed (= which is basically fallback to the old behavior)Ad naming: how about `Protobuf_AvoidGeneratedFileCollisions`, `Protobuf_DisambiguateGenerateFiles` or sth in that sense?The option can be on by default (using hashes seems like a good default behavior) and one could disable it if needed.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22667,420327784,2020-05-05T18:42:45Z,src/python/grpcio_tests/tests_aio/channelz/channelz_servicer_test.py,"@@ -0,0 +1,327 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests of grpc_channelz.v1.channelz.""""""++import unittest+import logging+import asyncio++import grpc+from grpc.experimental import aio++from grpc_channelz.v1 import channelz+from grpc_channelz.v1 import channelz_pb2+from grpc_channelz.v1 import channelz_pb2_grpc++from tests.unit.framework.common import test_constants+from tests_aio.unit._test_base import AioTestBase++aio.shutdown_grpc_aio()++_SUCCESSFUL_UNARY_UNARY = '/test/SuccessfulUnaryUnary'+_FAILED_UNARY_UNARY = '/test/FailedUnaryUnary'+_SUCCESSFUL_STREAM_STREAM = '/test/SuccessfulStreamStream'++_REQUEST = b'\x00\x00\x00'+_RESPONSE = b'\x01\x01\x01'++_DISABLE_REUSE_PORT = (('grpc.so_reuseport', 0),)+_ENABLE_CHANNELZ = (('grpc.enable_channelz', 1),)+_DISABLE_CHANNELZ = (('grpc.enable_channelz', 0),)+++async def _successful_unary_unary(request, servicer_context):+    return _RESPONSE+++async def _failed_unary_unary(request, servicer_context):+    servicer_context.set_code(grpc.StatusCode.INTERNAL)+    servicer_context.set_details(""Channelz Test Intended Failure"")+++async def _successful_stream_stream(request_iterator, servicer_context):+    async for _ in request_iterator:+        yield _RESPONSE+++class _GenericHandler(grpc.GenericRpcHandler):++    def service(self, handler_call_details):+        if handler_call_details.method == _SUCCESSFUL_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_successful_unary_unary)+        elif handler_call_details.method == _FAILED_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_failed_unary_unary)+        elif handler_call_details.method == _SUCCESSFUL_STREAM_STREAM:+            return grpc.stream_stream_rpc_method_handler(+                _successful_stream_stream)+        else:+            return None+++class _ChannelServerPair(object):++    async def start(self):+        # Server will enable channelz service+        self.server = aio.server(options=_DISABLE_REUSE_PORT + _ENABLE_CHANNELZ)+        port = self.server.add_insecure_port('[::]:0')+        self.server.add_generic_rpc_handlers((_GenericHandler(),))+        await self.server.start()++        # Channel will enable channelz service...+        self.channel = aio.insecure_channel('localhost:%d' % port,+                                            options=_ENABLE_CHANNELZ)+++# Stores channel-server pairs globally, since the memory deallocation is+# non-deterministic in both Core and Python with multiple threads. The+# destroyed Channelz node might still present. So, as a work around, this+# test doesn't close channel-server-pairs between cases.+_pairs = []+++async def _generate_channel_server_pairs(n):+    """"""Creates channel-server pairs globally, returns their indexes.""""""+    new_pairs = [_ChannelServerPair() for i in range(n)]+    for pair in new_pairs:+        await pair.start()+    _pairs.extend(new_pairs)+    return list(range(len(_pairs) - n, len(_pairs)))+++class ChannelzServicerTest(AioTestBase):++    async def setUp(self):+        self._pairs = []+        # This server is for Channelz info fetching only+        # It self should not enable Channelz+        self._server = aio.server(options=_DISABLE_REUSE_PORT ++                                  _DISABLE_CHANNELZ)+        port = self._server.add_insecure_port('[::]:0')+        channelz.add_channelz_servicer(self._server)+        await self._server.start()++        # This channel is used to fetch Channelz info only+        # Channelz should not be enabled+        self._channel = aio.insecure_channel('localhost:%d' % port,+                                             options=_DISABLE_CHANNELZ)+        self._channelz_stub = channelz_pb2_grpc.ChannelzStub(self._channel)++    async def tearDown(self):+        await self._server.stop(None)+        await self._channel.close()++    async def _send_successful_unary_unary(self, idx):+        call = _pairs[idx].channel.unary_unary(_SUCCESSFUL_UNARY_UNARY)(+            _REQUEST)+        self.assertEqual(grpc.StatusCode.OK, await call.code())++    async def _send_failed_unary_unary(self, idx):+        try:+            await _pairs[idx].channel.unary_unary(_FAILED_UNARY_UNARY)(_REQUEST)+        except grpc.RpcError:+            return+        else:+            self.fail(""This call supposed to fail"")++    async def _send_successful_stream_stream(self, idx):+        call = _pairs[idx].channel.stream_stream(_SUCCESSFUL_STREAM_STREAM)(+            iter([_REQUEST] * test_constants.STREAM_LENGTH))+        cnt = 0+        async for _ in call:+            cnt += 1+        self.assertEqual(cnt, test_constants.STREAM_LENGTH)++    async def _get_channel_id(self, idx):+        """"""Channel id may not be consecutive""""""+        resp = await self._channelz_stub.GetTopChannels(","Nit: Do we have an API guarantee that ID allocation in core will be monotonically increasing over time? If not, this test could break when/if core changes how channelz works. It's not very likely at this point, but it's something to be aware of.A method that should work regardless of the allocation algorithm in core would be: - start with no connected channels - connect a channel - get the list of channels - identify the new channel ID as the channel that you just added - store the fact that this ID corresponds to an existing channel - repeat",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22667,420330176,2020-05-05T18:46:45Z,src/python/grpcio_tests/tests_aio/channelz/channelz_servicer_test.py,"@@ -0,0 +1,327 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests of grpc_channelz.v1.channelz.""""""++import unittest+import logging+import asyncio++import grpc+from grpc.experimental import aio++from grpc_channelz.v1 import channelz+from grpc_channelz.v1 import channelz_pb2+from grpc_channelz.v1 import channelz_pb2_grpc++from tests.unit.framework.common import test_constants+from tests_aio.unit._test_base import AioTestBase++aio.shutdown_grpc_aio()++_SUCCESSFUL_UNARY_UNARY = '/test/SuccessfulUnaryUnary'+_FAILED_UNARY_UNARY = '/test/FailedUnaryUnary'+_SUCCESSFUL_STREAM_STREAM = '/test/SuccessfulStreamStream'++_REQUEST = b'\x00\x00\x00'+_RESPONSE = b'\x01\x01\x01'++_DISABLE_REUSE_PORT = (('grpc.so_reuseport', 0),)+_ENABLE_CHANNELZ = (('grpc.enable_channelz', 1),)+_DISABLE_CHANNELZ = (('grpc.enable_channelz', 0),)+++async def _successful_unary_unary(request, servicer_context):+    return _RESPONSE+++async def _failed_unary_unary(request, servicer_context):+    servicer_context.set_code(grpc.StatusCode.INTERNAL)+    servicer_context.set_details(""Channelz Test Intended Failure"")+++async def _successful_stream_stream(request_iterator, servicer_context):+    async for _ in request_iterator:+        yield _RESPONSE+++class _GenericHandler(grpc.GenericRpcHandler):++    def service(self, handler_call_details):+        if handler_call_details.method == _SUCCESSFUL_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_successful_unary_unary)+        elif handler_call_details.method == _FAILED_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_failed_unary_unary)+        elif handler_call_details.method == _SUCCESSFUL_STREAM_STREAM:+            return grpc.stream_stream_rpc_method_handler(+                _successful_stream_stream)+        else:+            return None+++class _ChannelServerPair(object):++    async def start(self):+        # Server will enable channelz service+        self.server = aio.server(options=_DISABLE_REUSE_PORT + _ENABLE_CHANNELZ)+        port = self.server.add_insecure_port('[::]:0')+        self.server.add_generic_rpc_handlers((_GenericHandler(),))+        await self.server.start()++        # Channel will enable channelz service...+        self.channel = aio.insecure_channel('localhost:%d' % port,+                                            options=_ENABLE_CHANNELZ)+++# Stores channel-server pairs globally, since the memory deallocation is+# non-deterministic in both Core and Python with multiple threads. The+# destroyed Channelz node might still present. So, as a work around, this+# test doesn't close channel-server-pairs between cases.+_pairs = []+++async def _generate_channel_server_pairs(n):+    """"""Creates channel-server pairs globally, returns their indexes.""""""+    new_pairs = [_ChannelServerPair() for i in range(n)]+    for pair in new_pairs:+        await pair.start()+    _pairs.extend(new_pairs)+    return list(range(len(_pairs) - n, len(_pairs)))+++class ChannelzServicerTest(AioTestBase):++    async def setUp(self):+        self._pairs = []+        # This server is for Channelz info fetching only+        # It self should not enable Channelz+        self._server = aio.server(options=_DISABLE_REUSE_PORT ++                                  _DISABLE_CHANNELZ)+        port = self._server.add_insecure_port('[::]:0')+        channelz.add_channelz_servicer(self._server)+        await self._server.start()++        # This channel is used to fetch Channelz info only+        # Channelz should not be enabled+        self._channel = aio.insecure_channel('localhost:%d' % port,+                                             options=_DISABLE_CHANNELZ)+        self._channelz_stub = channelz_pb2_grpc.ChannelzStub(self._channel)++    async def tearDown(self):+        await self._server.stop(None)+        await self._channel.close()++    async def _send_successful_unary_unary(self, idx):+        call = _pairs[idx].channel.unary_unary(_SUCCESSFUL_UNARY_UNARY)(+            _REQUEST)+        self.assertEqual(grpc.StatusCode.OK, await call.code())++    async def _send_failed_unary_unary(self, idx):+        try:+            await _pairs[idx].channel.unary_unary(_FAILED_UNARY_UNARY)(_REQUEST)+        except grpc.RpcError:+            return+        else:+            self.fail(""This call supposed to fail"")++    async def _send_successful_stream_stream(self, idx):+        call = _pairs[idx].channel.stream_stream(_SUCCESSFUL_STREAM_STREAM)(+            iter([_REQUEST] * test_constants.STREAM_LENGTH))+        cnt = 0+        async for _ in call:+            cnt += 1+        self.assertEqual(cnt, test_constants.STREAM_LENGTH)++    async def _get_channel_id(self, idx):+        """"""Channel id may not be consecutive""""""+        resp = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=0))+        self.assertGreater(len(resp.channel), idx)+        return resp.channel[idx].ref.channel_id++    async def _get_server_by_id(self, idx):+        """"""Server id may not be consecutive""""""+        resp = await self._channelz_stub.GetServers(+            channelz_pb2.GetServersRequest(start_server_id=0))+        return resp.server[idx]++    async def test_get_top_channels_basic(self):+        before = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=0))+        await _generate_channel_server_pairs(1)+        after = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=0))+        self.assertEqual(len(after.channel) - len(before.channel), 1)+        self.assertEqual(after.end, True)++    async def test_get_top_channels_high_start_id(self):+        await _generate_channel_server_pairs(1)+        resp = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=10000))+        self.assertEqual(len(resp.channel), 0)+        self.assertEqual(resp.end, True)++    async def test_successful_request(self):+        idx = await _generate_channel_server_pairs(1)",Nit: Index is singular. It's surprising that this is actually a list. `indices`?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22667,420334838,2020-05-05T18:54:54Z,src/python/grpcio_tests/tests_aio/channelz/channelz_servicer_test.py,"@@ -0,0 +1,327 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests of grpc_channelz.v1.channelz.""""""++import unittest+import logging+import asyncio++import grpc+from grpc.experimental import aio++from grpc_channelz.v1 import channelz+from grpc_channelz.v1 import channelz_pb2+from grpc_channelz.v1 import channelz_pb2_grpc++from tests.unit.framework.common import test_constants+from tests_aio.unit._test_base import AioTestBase++aio.shutdown_grpc_aio()++_SUCCESSFUL_UNARY_UNARY = '/test/SuccessfulUnaryUnary'+_FAILED_UNARY_UNARY = '/test/FailedUnaryUnary'+_SUCCESSFUL_STREAM_STREAM = '/test/SuccessfulStreamStream'++_REQUEST = b'\x00\x00\x00'+_RESPONSE = b'\x01\x01\x01'++_DISABLE_REUSE_PORT = (('grpc.so_reuseport', 0),)+_ENABLE_CHANNELZ = (('grpc.enable_channelz', 1),)+_DISABLE_CHANNELZ = (('grpc.enable_channelz', 0),)+++async def _successful_unary_unary(request, servicer_context):+    return _RESPONSE+++async def _failed_unary_unary(request, servicer_context):+    servicer_context.set_code(grpc.StatusCode.INTERNAL)+    servicer_context.set_details(""Channelz Test Intended Failure"")+++async def _successful_stream_stream(request_iterator, servicer_context):+    async for _ in request_iterator:+        yield _RESPONSE+++class _GenericHandler(grpc.GenericRpcHandler):++    def service(self, handler_call_details):+        if handler_call_details.method == _SUCCESSFUL_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_successful_unary_unary)+        elif handler_call_details.method == _FAILED_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_failed_unary_unary)+        elif handler_call_details.method == _SUCCESSFUL_STREAM_STREAM:+            return grpc.stream_stream_rpc_method_handler(+                _successful_stream_stream)+        else:+            return None+++class _ChannelServerPair(object):++    async def start(self):+        # Server will enable channelz service+        self.server = aio.server(options=_DISABLE_REUSE_PORT + _ENABLE_CHANNELZ)+        port = self.server.add_insecure_port('[::]:0')+        self.server.add_generic_rpc_handlers((_GenericHandler(),))+        await self.server.start()++        # Channel will enable channelz service...+        self.channel = aio.insecure_channel('localhost:%d' % port,+                                            options=_ENABLE_CHANNELZ)+++# Stores channel-server pairs globally, since the memory deallocation is+# non-deterministic in both Core and Python with multiple threads. The+# destroyed Channelz node might still present. So, as a work around, this+# test doesn't close channel-server-pairs between cases.+_pairs = []+++async def _generate_channel_server_pairs(n):+    """"""Creates channel-server pairs globally, returns their indexes.""""""+    new_pairs = [_ChannelServerPair() for i in range(n)]+    for pair in new_pairs:+        await pair.start()+    _pairs.extend(new_pairs)+    return list(range(len(_pairs) - n, len(_pairs)))+++class ChannelzServicerTest(AioTestBase):++    async def setUp(self):+        self._pairs = []+        # This server is for Channelz info fetching only+        # It self should not enable Channelz+        self._server = aio.server(options=_DISABLE_REUSE_PORT ++                                  _DISABLE_CHANNELZ)+        port = self._server.add_insecure_port('[::]:0')+        channelz.add_channelz_servicer(self._server)+        await self._server.start()++        # This channel is used to fetch Channelz info only+        # Channelz should not be enabled+        self._channel = aio.insecure_channel('localhost:%d' % port,+                                             options=_DISABLE_CHANNELZ)+        self._channelz_stub = channelz_pb2_grpc.ChannelzStub(self._channel)++    async def tearDown(self):+        await self._server.stop(None)+        await self._channel.close()++    async def _send_successful_unary_unary(self, idx):+        call = _pairs[idx].channel.unary_unary(_SUCCESSFUL_UNARY_UNARY)(+            _REQUEST)+        self.assertEqual(grpc.StatusCode.OK, await call.code())++    async def _send_failed_unary_unary(self, idx):+        try:+            await _pairs[idx].channel.unary_unary(_FAILED_UNARY_UNARY)(_REQUEST)+        except grpc.RpcError:+            return+        else:+            self.fail(""This call supposed to fail"")++    async def _send_successful_stream_stream(self, idx):+        call = _pairs[idx].channel.stream_stream(_SUCCESSFUL_STREAM_STREAM)(+            iter([_REQUEST] * test_constants.STREAM_LENGTH))+        cnt = 0+        async for _ in call:+            cnt += 1+        self.assertEqual(cnt, test_constants.STREAM_LENGTH)++    async def _get_channel_id(self, idx):+        """"""Channel id may not be consecutive""""""+        resp = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=0))+        self.assertGreater(len(resp.channel), idx)+        return resp.channel[idx].ref.channel_id++    async def _get_server_by_id(self, idx):+        """"""Server id may not be consecutive""""""+        resp = await self._channelz_stub.GetServers(+            channelz_pb2.GetServersRequest(start_server_id=0))+        return resp.server[idx]++    async def test_get_top_channels_basic(self):+        before = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=0))+        await _generate_channel_server_pairs(1)+        after = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=0))+        self.assertEqual(len(after.channel) - len(before.channel), 1)+        self.assertEqual(after.end, True)++    async def test_get_top_channels_high_start_id(self):+        await _generate_channel_server_pairs(1)+        resp = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=10000))+        self.assertEqual(len(resp.channel), 0)+        self.assertEqual(resp.end, True)++    async def test_successful_request(self):+        idx = await _generate_channel_server_pairs(1)+        await self._send_successful_unary_unary(idx[0])+        resp = await self._channelz_stub.GetChannel(+            channelz_pb2.GetChannelRequest(+                channel_id=await self._get_channel_id(idx[0])))+        self.assertEqual(resp.channel.data.calls_started, 1)+        self.assertEqual(resp.channel.data.calls_succeeded, 1)+        self.assertEqual(resp.channel.data.calls_failed, 0)++    async def test_failed_request(self):+        idx = await _generate_channel_server_pairs(1)+        await self._send_failed_unary_unary(idx[0])+        resp = await self._channelz_stub.GetChannel(+            channelz_pb2.GetChannelRequest(+                channel_id=await self._get_channel_id(idx[0])))+        self.assertEqual(resp.channel.data.calls_started, 1)+        self.assertEqual(resp.channel.data.calls_succeeded, 0)+        self.assertEqual(resp.channel.data.calls_failed, 1)++    async def test_many_requests(self):+        idx = await _generate_channel_server_pairs(1)+        k_success = 7","Nit: It's more conventional in Python to use all caps for constants. Also, I just found out that in Python 3.8+, you can use [`typing.Final`](https://docs.python.org/3/library/typing.html#typing.Final)",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22667,420336938,2020-05-05T18:58:40Z,src/python/grpcio_tests/tests_aio/channelz/channelz_servicer_test.py,"@@ -0,0 +1,327 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests of grpc_channelz.v1.channelz.""""""++import unittest+import logging+import asyncio++import grpc+from grpc.experimental import aio++from grpc_channelz.v1 import channelz+from grpc_channelz.v1 import channelz_pb2+from grpc_channelz.v1 import channelz_pb2_grpc++from tests.unit.framework.common import test_constants+from tests_aio.unit._test_base import AioTestBase++aio.shutdown_grpc_aio()++_SUCCESSFUL_UNARY_UNARY = '/test/SuccessfulUnaryUnary'+_FAILED_UNARY_UNARY = '/test/FailedUnaryUnary'+_SUCCESSFUL_STREAM_STREAM = '/test/SuccessfulStreamStream'++_REQUEST = b'\x00\x00\x00'+_RESPONSE = b'\x01\x01\x01'++_DISABLE_REUSE_PORT = (('grpc.so_reuseport', 0),)+_ENABLE_CHANNELZ = (('grpc.enable_channelz', 1),)+_DISABLE_CHANNELZ = (('grpc.enable_channelz', 0),)+++async def _successful_unary_unary(request, servicer_context):+    return _RESPONSE+++async def _failed_unary_unary(request, servicer_context):+    servicer_context.set_code(grpc.StatusCode.INTERNAL)+    servicer_context.set_details(""Channelz Test Intended Failure"")+++async def _successful_stream_stream(request_iterator, servicer_context):+    async for _ in request_iterator:+        yield _RESPONSE+++class _GenericHandler(grpc.GenericRpcHandler):++    def service(self, handler_call_details):+        if handler_call_details.method == _SUCCESSFUL_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_successful_unary_unary)+        elif handler_call_details.method == _FAILED_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_failed_unary_unary)+        elif handler_call_details.method == _SUCCESSFUL_STREAM_STREAM:+            return grpc.stream_stream_rpc_method_handler(+                _successful_stream_stream)+        else:+            return None+++class _ChannelServerPair(object):++    async def start(self):+        # Server will enable channelz service+        self.server = aio.server(options=_DISABLE_REUSE_PORT + _ENABLE_CHANNELZ)+        port = self.server.add_insecure_port('[::]:0')+        self.server.add_generic_rpc_handlers((_GenericHandler(),))+        await self.server.start()++        # Channel will enable channelz service...+        self.channel = aio.insecure_channel('localhost:%d' % port,+                                            options=_ENABLE_CHANNELZ)+++# Stores channel-server pairs globally, since the memory deallocation is+# non-deterministic in both Core and Python with multiple threads. The+# destroyed Channelz node might still present. So, as a work around, this+# test doesn't close channel-server-pairs between cases.+_pairs = []+++async def _generate_channel_server_pairs(n):+    """"""Creates channel-server pairs globally, returns their indexes.""""""+    new_pairs = [_ChannelServerPair() for i in range(n)]+    for pair in new_pairs:+        await pair.start()+    _pairs.extend(new_pairs)+    return list(range(len(_pairs) - n, len(_pairs)))+++class ChannelzServicerTest(AioTestBase):++    async def setUp(self):+        self._pairs = []+        # This server is for Channelz info fetching only+        # It self should not enable Channelz+        self._server = aio.server(options=_DISABLE_REUSE_PORT ++                                  _DISABLE_CHANNELZ)+        port = self._server.add_insecure_port('[::]:0')+        channelz.add_channelz_servicer(self._server)+        await self._server.start()++        # This channel is used to fetch Channelz info only+        # Channelz should not be enabled+        self._channel = aio.insecure_channel('localhost:%d' % port,+                                             options=_DISABLE_CHANNELZ)+        self._channelz_stub = channelz_pb2_grpc.ChannelzStub(self._channel)++    async def tearDown(self):+        await self._server.stop(None)+        await self._channel.close()++    async def _send_successful_unary_unary(self, idx):+        call = _pairs[idx].channel.unary_unary(_SUCCESSFUL_UNARY_UNARY)(+            _REQUEST)+        self.assertEqual(grpc.StatusCode.OK, await call.code())++    async def _send_failed_unary_unary(self, idx):+        try:+            await _pairs[idx].channel.unary_unary(_FAILED_UNARY_UNARY)(_REQUEST)+        except grpc.RpcError:+            return+        else:+            self.fail(""This call supposed to fail"")++    async def _send_successful_stream_stream(self, idx):+        call = _pairs[idx].channel.stream_stream(_SUCCESSFUL_STREAM_STREAM)(+            iter([_REQUEST] * test_constants.STREAM_LENGTH))+        cnt = 0+        async for _ in call:+            cnt += 1+        self.assertEqual(cnt, test_constants.STREAM_LENGTH)++    async def _get_channel_id(self, idx):+        """"""Channel id may not be consecutive""""""+        resp = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=0))+        self.assertGreater(len(resp.channel), idx)+        return resp.channel[idx].ref.channel_id++    async def _get_server_by_id(self, idx):+        """"""Server id may not be consecutive""""""+        resp = await self._channelz_stub.GetServers(+            channelz_pb2.GetServersRequest(start_server_id=0))+        return resp.server[idx]++    async def test_get_top_channels_basic(self):+        before = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=0))+        await _generate_channel_server_pairs(1)+        after = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=0))+        self.assertEqual(len(after.channel) - len(before.channel), 1)+        self.assertEqual(after.end, True)++    async def test_get_top_channels_high_start_id(self):+        await _generate_channel_server_pairs(1)+        resp = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=10000))+        self.assertEqual(len(resp.channel), 0)+        self.assertEqual(resp.end, True)++    async def test_successful_request(self):+        idx = await _generate_channel_server_pairs(1)+        await self._send_successful_unary_unary(idx[0])+        resp = await self._channelz_stub.GetChannel(+            channelz_pb2.GetChannelRequest(+                channel_id=await self._get_channel_id(idx[0])))+        self.assertEqual(resp.channel.data.calls_started, 1)+        self.assertEqual(resp.channel.data.calls_succeeded, 1)+        self.assertEqual(resp.channel.data.calls_failed, 0)++    async def test_failed_request(self):+        idx = await _generate_channel_server_pairs(1)+        await self._send_failed_unary_unary(idx[0])+        resp = await self._channelz_stub.GetChannel(+            channelz_pb2.GetChannelRequest(+                channel_id=await self._get_channel_id(idx[0])))+        self.assertEqual(resp.channel.data.calls_started, 1)+        self.assertEqual(resp.channel.data.calls_succeeded, 0)+        self.assertEqual(resp.channel.data.calls_failed, 1)++    async def test_many_requests(self):+        idx = await _generate_channel_server_pairs(1)+        k_success = 7+        k_failed = 9+        for i in range(k_success):+            await self._send_successful_unary_unary(idx[0])+        for i in range(k_failed):+            await self._send_failed_unary_unary(idx[0])+        resp = await self._channelz_stub.GetChannel(+            channelz_pb2.GetChannelRequest(+                channel_id=await self._get_channel_id(idx[0])))+        self.assertEqual(resp.channel.data.calls_started, k_success + k_failed)+        self.assertEqual(resp.channel.data.calls_succeeded, k_success)+        self.assertEqual(resp.channel.data.calls_failed, k_failed)++    async def test_server_call(self):+        idx = await _generate_channel_server_pairs(1)+        k_success = 23+        k_failed = 29+        for i in range(k_success):+            await self._send_successful_unary_unary(idx[0])+        for i in range(k_failed):+            await self._send_failed_unary_unary(idx[0])++        resp = await self._get_server_by_id(idx[0])+        self.assertEqual(resp.data.calls_started, k_success + k_failed)+        self.assertEqual(resp.data.calls_succeeded, k_success)+        self.assertEqual(resp.data.calls_failed, k_failed)++    async def test_streaming_rpc(self):+        idx = await _generate_channel_server_pairs(1)+        # In C++, the argument for _send_successful_stream_stream is message length.+        # Here the argument is still channel idx, to be consistent with the other two.+        await self._send_successful_stream_stream(idx[0])++        gc_resp = await self._channelz_stub.GetChannel(+            channelz_pb2.GetChannelRequest(+                channel_id=await self._get_channel_id(idx[0])))+        self.assertEqual(gc_resp.channel.data.calls_started, 1)+        self.assertEqual(gc_resp.channel.data.calls_succeeded, 1)+        self.assertEqual(gc_resp.channel.data.calls_failed, 0)+        # Subchannel exists+        self.assertGreater(len(gc_resp.channel.subchannel_ref), 0)++        gsc_resp = await self._channelz_stub.GetSubchannel(+            channelz_pb2.GetSubchannelRequest(+                subchannel_id=gc_resp.channel.subchannel_ref[0].subchannel_id))+        self.assertEqual(gsc_resp.subchannel.data.calls_started, 1)+        self.assertEqual(gsc_resp.subchannel.data.calls_succeeded, 1)+        self.assertEqual(gsc_resp.subchannel.data.calls_failed, 0)+        # Socket exists+        self.assertEqual(len(gsc_resp.subchannel.socket_ref), 1)++        gs_resp = await self._channelz_stub.GetSocket(+            channelz_pb2.GetSocketRequest(+                socket_id=gsc_resp.subchannel.socket_ref[0].socket_id))+        self.assertEqual(gs_resp.socket.data.streams_started, 1)+        self.assertEqual(gs_resp.socket.data.streams_succeeded, 1)+        self.assertEqual(gs_resp.socket.data.streams_failed, 0)+        self.assertEqual(gs_resp.socket.data.messages_sent,+                         test_constants.STREAM_LENGTH)+        self.assertEqual(gs_resp.socket.data.messages_received,+                         test_constants.STREAM_LENGTH)++    async def test_server_sockets(self):+        idx = await _generate_channel_server_pairs(1)+        await self._send_successful_unary_unary(idx[0])+        await self._send_failed_unary_unary(idx[0])++        resp = await self._get_server_by_id(idx[0])+        self.assertEqual(resp.data.calls_started, 2)+        self.assertEqual(resp.data.calls_succeeded, 1)+        self.assertEqual(resp.data.calls_failed, 1)++        gss_resp = await self._channelz_stub.GetServerSockets(+            channelz_pb2.GetServerSocketsRequest(server_id=resp.ref.server_id,+                                                 start_socket_id=0))+        # If the RPC call failed, it will raise a grpc.RpcError+        # So, if there is no exception raised, considered pass++    async def test_server_listen_sockets(self):+        idx = await _generate_channel_server_pairs(1)++        resp = await self._get_server_by_id(idx[0])+        self.assertEqual(len(resp.listen_socket), 1)++        gs_resp = await self._channelz_stub.GetSocket(+            channelz_pb2.GetSocketRequest(+                socket_id=resp.listen_socket[0].socket_id))+        # If the RPC call failed, it will raise a grpc.RpcError+        # So, if there is no exception raised, considered pass++    async def test_invalid_query_get_server(self):+        with self.assertRaises(aio.AioRpcError) as exception_context:+            await self._channelz_stub.GetServer(+                channelz_pb2.GetServerRequest(server_id=10000))+        self.assertEqual(grpc.StatusCode.NOT_FOUND,+                         exception_context.exception.code())++    async def test_invalid_query_get_channel(self):+        with self.assertRaises(aio.AioRpcError) as exception_context:+            await self._channelz_stub.GetChannel(+                channelz_pb2.GetChannelRequest(channel_id=10000))","Nit: Does the ID allocation algorithm guarantee that we won't get `10000` assigned to us? This could be a small percentage flake. As long as the flake rate is low enough, I'm fine with this as it is though.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22860,420340093,2020-05-05T19:04:18Z,src/python/grpcio_tests/tests/unit/_metadata_flags_test.py,"@@ -101,8 +102,9 @@ def service(self, handler_call_details):  def create_dummy_channel():     """"""Creating dummy channels is a workaround for retries""""""-    with bound_socket() as (host, port):-        return grpc.insecure_channel('{}:{}'.format(host, port))+    host, port, sock = get_socket()+    sock.close()","Without `SO_REUSEPORT`, there's a race condition here, right? The thread gets preempted between `sock.close()` and `grpc.insecure_channel`, and then the kernel allocates `port` to some other process on the system. Then `grpc.insecure_channel` fails to bind. I'm not sure what the failure rate should be though. Can you run with `--runs_per_test=10000` to find out whether or not this is worth worrying about?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22860,420342512,2020-05-05T19:08:27Z,src/python/grpcio_tests/tests/unit/_metadata_flags_test.py,"@@ -203,51 +205,56 @@ def test_call_wait_for_ready_enabled(self):         #   main thread. So, it need another method to store the         #   exceptions and raise them again in main thread.         unhandled_exceptions = queue.Queue()-        with bound_socket(listen=False) as (host, port):-            addr = '{}:{}'.format(host, port)-            wg = test_common.WaitGroup(len(_ALL_CALL_CASES)) -            def wait_for_transient_failure(channel_connectivity):-                if channel_connectivity == grpc.ChannelConnectivity.TRANSIENT_FAILURE:+        # We just need an unused TCP port+        host, port, sock = get_socket()+        sock.close()",Same comment as above. It looks like there's a race condition here.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22667,420378419,2020-05-05T20:14:30Z,src/python/grpcio_channelz/grpc_channelz/v1/_async.py,"@@ -0,0 +1,66 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""AsyncIO version of Channelz servicer.""""""++from grpc.experimental import aio++import grpc_channelz.v1.channelz_pb2 as _channelz_pb2+import grpc_channelz.v1.channelz_pb2_grpc as _channelz_pb2_grpc+from grpc_channelz.v1._servicer import ChannelzServicer as _SyncChannelzServicer+++class ChannelzServicer(_channelz_pb2_grpc.ChannelzServicer):+    """"""AsyncIO servicer for handling RPCs for service statuses.""""""++    @staticmethod+    async def GetTopChannels(request: _channelz_pb2.GetTopChannelsRequest,+                             context: aio.ServicerContext+                            ) -> _channelz_pb2.GetTopChannelsResponse:+        return _SyncChannelzServicer.GetTopChannels(request, context)","Underlying the servicer methods, the Core is merely collecting stats from Channelz Node objects. So, it should not block. The only blocking case is Channelz's metric collecting deadlocks, but that can't be solved by the Cython wrapper.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22667,420383169,2020-05-05T20:23:33Z,src/python/grpcio_tests/tests_aio/channelz/channelz_servicer_test.py,"@@ -0,0 +1,327 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests of grpc_channelz.v1.channelz.""""""++import unittest+import logging+import asyncio++import grpc+from grpc.experimental import aio++from grpc_channelz.v1 import channelz+from grpc_channelz.v1 import channelz_pb2+from grpc_channelz.v1 import channelz_pb2_grpc++from tests.unit.framework.common import test_constants+from tests_aio.unit._test_base import AioTestBase++aio.shutdown_grpc_aio()++_SUCCESSFUL_UNARY_UNARY = '/test/SuccessfulUnaryUnary'+_FAILED_UNARY_UNARY = '/test/FailedUnaryUnary'+_SUCCESSFUL_STREAM_STREAM = '/test/SuccessfulStreamStream'++_REQUEST = b'\x00\x00\x00'+_RESPONSE = b'\x01\x01\x01'++_DISABLE_REUSE_PORT = (('grpc.so_reuseport', 0),)+_ENABLE_CHANNELZ = (('grpc.enable_channelz', 1),)+_DISABLE_CHANNELZ = (('grpc.enable_channelz', 0),)+++async def _successful_unary_unary(request, servicer_context):+    return _RESPONSE+++async def _failed_unary_unary(request, servicer_context):+    servicer_context.set_code(grpc.StatusCode.INTERNAL)+    servicer_context.set_details(""Channelz Test Intended Failure"")+++async def _successful_stream_stream(request_iterator, servicer_context):+    async for _ in request_iterator:+        yield _RESPONSE+++class _GenericHandler(grpc.GenericRpcHandler):++    def service(self, handler_call_details):+        if handler_call_details.method == _SUCCESSFUL_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_successful_unary_unary)+        elif handler_call_details.method == _FAILED_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_failed_unary_unary)+        elif handler_call_details.method == _SUCCESSFUL_STREAM_STREAM:+            return grpc.stream_stream_rpc_method_handler(+                _successful_stream_stream)+        else:+            return None+++class _ChannelServerPair(object):++    async def start(self):+        # Server will enable channelz service+        self.server = aio.server(options=_DISABLE_REUSE_PORT + _ENABLE_CHANNELZ)+        port = self.server.add_insecure_port('[::]:0')+        self.server.add_generic_rpc_handlers((_GenericHandler(),))+        await self.server.start()++        # Channel will enable channelz service...+        self.channel = aio.insecure_channel('localhost:%d' % port,+                                            options=_ENABLE_CHANNELZ)+++# Stores channel-server pairs globally, since the memory deallocation is","This is bugging me for a long time. If we close the servers and channels, there is a chance of Channelz Node being destroyed or labeled as destroyed for a while. This test was written in a way depending on the order of Channelz stats message. It was quite flaky if Channelz not returning deterministic response.You remind me there is another solution, which is trying to find the Channelz ref id for each Python channel or server object. I will experiment with it.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22667,420384348,2020-05-05T20:25:44Z,src/python/grpcio_tests/tests_aio/channelz/channelz_servicer_test.py,"@@ -0,0 +1,327 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests of grpc_channelz.v1.channelz.""""""++import unittest+import logging+import asyncio++import grpc+from grpc.experimental import aio++from grpc_channelz.v1 import channelz+from grpc_channelz.v1 import channelz_pb2+from grpc_channelz.v1 import channelz_pb2_grpc++from tests.unit.framework.common import test_constants+from tests_aio.unit._test_base import AioTestBase++aio.shutdown_grpc_aio()++_SUCCESSFUL_UNARY_UNARY = '/test/SuccessfulUnaryUnary'+_FAILED_UNARY_UNARY = '/test/FailedUnaryUnary'+_SUCCESSFUL_STREAM_STREAM = '/test/SuccessfulStreamStream'++_REQUEST = b'\x00\x00\x00'+_RESPONSE = b'\x01\x01\x01'++_DISABLE_REUSE_PORT = (('grpc.so_reuseport', 0),)+_ENABLE_CHANNELZ = (('grpc.enable_channelz', 1),)+_DISABLE_CHANNELZ = (('grpc.enable_channelz', 0),)+++async def _successful_unary_unary(request, servicer_context):+    return _RESPONSE+++async def _failed_unary_unary(request, servicer_context):+    servicer_context.set_code(grpc.StatusCode.INTERNAL)+    servicer_context.set_details(""Channelz Test Intended Failure"")+++async def _successful_stream_stream(request_iterator, servicer_context):+    async for _ in request_iterator:+        yield _RESPONSE+++class _GenericHandler(grpc.GenericRpcHandler):++    def service(self, handler_call_details):+        if handler_call_details.method == _SUCCESSFUL_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_successful_unary_unary)+        elif handler_call_details.method == _FAILED_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_failed_unary_unary)+        elif handler_call_details.method == _SUCCESSFUL_STREAM_STREAM:+            return grpc.stream_stream_rpc_method_handler(+                _successful_stream_stream)+        else:+            return None+++class _ChannelServerPair(object):++    async def start(self):+        # Server will enable channelz service+        self.server = aio.server(options=_DISABLE_REUSE_PORT + _ENABLE_CHANNELZ)+        port = self.server.add_insecure_port('[::]:0')+        self.server.add_generic_rpc_handlers((_GenericHandler(),))+        await self.server.start()++        # Channel will enable channelz service...+        self.channel = aio.insecure_channel('localhost:%d' % port,+                                            options=_ENABLE_CHANNELZ)+++# Stores channel-server pairs globally, since the memory deallocation is+# non-deterministic in both Core and Python with multiple threads. The+# destroyed Channelz node might still present. So, as a work around, this+# test doesn't close channel-server-pairs between cases.+_pairs = []+++async def _generate_channel_server_pairs(n):+    """"""Creates channel-server pairs globally, returns their indexes.""""""+    new_pairs = [_ChannelServerPair() for i in range(n)]+    for pair in new_pairs:+        await pair.start()+    _pairs.extend(new_pairs)+    return list(range(len(_pairs) - n, len(_pairs)))+++class ChannelzServicerTest(AioTestBase):++    async def setUp(self):+        self._pairs = []+        # This server is for Channelz info fetching only+        # It self should not enable Channelz+        self._server = aio.server(options=_DISABLE_REUSE_PORT ++                                  _DISABLE_CHANNELZ)+        port = self._server.add_insecure_port('[::]:0')+        channelz.add_channelz_servicer(self._server)+        await self._server.start()++        # This channel is used to fetch Channelz info only+        # Channelz should not be enabled+        self._channel = aio.insecure_channel('localhost:%d' % port,+                                             options=_DISABLE_CHANNELZ)+        self._channelz_stub = channelz_pb2_grpc.ChannelzStub(self._channel)++    async def tearDown(self):+        await self._server.stop(None)+        await self._channel.close()++    async def _send_successful_unary_unary(self, idx):+        call = _pairs[idx].channel.unary_unary(_SUCCESSFUL_UNARY_UNARY)(+            _REQUEST)+        self.assertEqual(grpc.StatusCode.OK, await call.code())++    async def _send_failed_unary_unary(self, idx):+        try:+            await _pairs[idx].channel.unary_unary(_FAILED_UNARY_UNARY)(_REQUEST)+        except grpc.RpcError:+            return+        else:+            self.fail(""This call supposed to fail"")++    async def _send_successful_stream_stream(self, idx):+        call = _pairs[idx].channel.stream_stream(_SUCCESSFUL_STREAM_STREAM)(+            iter([_REQUEST] * test_constants.STREAM_LENGTH))+        cnt = 0+        async for _ in call:+            cnt += 1+        self.assertEqual(cnt, test_constants.STREAM_LENGTH)++    async def _get_channel_id(self, idx):+        """"""Channel id may not be consecutive""""""+        resp = await self._channelz_stub.GetTopChannels(",I'm thinking about similar algorithm. The id is increasing over time. We might be able to find the ground truth TCP port from Channelz response to map Channelz object and Python object.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22667,420385260,2020-05-05T20:27:26Z,src/python/grpcio_tests/tests_aio/channelz/channelz_servicer_test.py,"@@ -0,0 +1,327 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests of grpc_channelz.v1.channelz.""""""++import unittest+import logging+import asyncio++import grpc+from grpc.experimental import aio++from grpc_channelz.v1 import channelz+from grpc_channelz.v1 import channelz_pb2+from grpc_channelz.v1 import channelz_pb2_grpc++from tests.unit.framework.common import test_constants+from tests_aio.unit._test_base import AioTestBase++aio.shutdown_grpc_aio()++_SUCCESSFUL_UNARY_UNARY = '/test/SuccessfulUnaryUnary'+_FAILED_UNARY_UNARY = '/test/FailedUnaryUnary'+_SUCCESSFUL_STREAM_STREAM = '/test/SuccessfulStreamStream'++_REQUEST = b'\x00\x00\x00'+_RESPONSE = b'\x01\x01\x01'++_DISABLE_REUSE_PORT = (('grpc.so_reuseport', 0),)+_ENABLE_CHANNELZ = (('grpc.enable_channelz', 1),)+_DISABLE_CHANNELZ = (('grpc.enable_channelz', 0),)+++async def _successful_unary_unary(request, servicer_context):+    return _RESPONSE+++async def _failed_unary_unary(request, servicer_context):+    servicer_context.set_code(grpc.StatusCode.INTERNAL)+    servicer_context.set_details(""Channelz Test Intended Failure"")+++async def _successful_stream_stream(request_iterator, servicer_context):+    async for _ in request_iterator:+        yield _RESPONSE+++class _GenericHandler(grpc.GenericRpcHandler):++    def service(self, handler_call_details):+        if handler_call_details.method == _SUCCESSFUL_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_successful_unary_unary)+        elif handler_call_details.method == _FAILED_UNARY_UNARY:+            return grpc.unary_unary_rpc_method_handler(_failed_unary_unary)+        elif handler_call_details.method == _SUCCESSFUL_STREAM_STREAM:+            return grpc.stream_stream_rpc_method_handler(+                _successful_stream_stream)+        else:+            return None+++class _ChannelServerPair(object):++    async def start(self):+        # Server will enable channelz service+        self.server = aio.server(options=_DISABLE_REUSE_PORT + _ENABLE_CHANNELZ)+        port = self.server.add_insecure_port('[::]:0')+        self.server.add_generic_rpc_handlers((_GenericHandler(),))+        await self.server.start()++        # Channel will enable channelz service...+        self.channel = aio.insecure_channel('localhost:%d' % port,+                                            options=_ENABLE_CHANNELZ)+++# Stores channel-server pairs globally, since the memory deallocation is+# non-deterministic in both Core and Python with multiple threads. The+# destroyed Channelz node might still present. So, as a work around, this+# test doesn't close channel-server-pairs between cases.+_pairs = []+++async def _generate_channel_server_pairs(n):+    """"""Creates channel-server pairs globally, returns their indexes.""""""+    new_pairs = [_ChannelServerPair() for i in range(n)]+    for pair in new_pairs:+        await pair.start()+    _pairs.extend(new_pairs)+    return list(range(len(_pairs) - n, len(_pairs)))+++class ChannelzServicerTest(AioTestBase):++    async def setUp(self):+        self._pairs = []+        # This server is for Channelz info fetching only+        # It self should not enable Channelz+        self._server = aio.server(options=_DISABLE_REUSE_PORT ++                                  _DISABLE_CHANNELZ)+        port = self._server.add_insecure_port('[::]:0')+        channelz.add_channelz_servicer(self._server)+        await self._server.start()++        # This channel is used to fetch Channelz info only+        # Channelz should not be enabled+        self._channel = aio.insecure_channel('localhost:%d' % port,+                                             options=_DISABLE_CHANNELZ)+        self._channelz_stub = channelz_pb2_grpc.ChannelzStub(self._channel)++    async def tearDown(self):+        await self._server.stop(None)+        await self._channel.close()++    async def _send_successful_unary_unary(self, idx):+        call = _pairs[idx].channel.unary_unary(_SUCCESSFUL_UNARY_UNARY)(+            _REQUEST)+        self.assertEqual(grpc.StatusCode.OK, await call.code())++    async def _send_failed_unary_unary(self, idx):+        try:+            await _pairs[idx].channel.unary_unary(_FAILED_UNARY_UNARY)(_REQUEST)+        except grpc.RpcError:+            return+        else:+            self.fail(""This call supposed to fail"")++    async def _send_successful_stream_stream(self, idx):+        call = _pairs[idx].channel.stream_stream(_SUCCESSFUL_STREAM_STREAM)(+            iter([_REQUEST] * test_constants.STREAM_LENGTH))+        cnt = 0+        async for _ in call:+            cnt += 1+        self.assertEqual(cnt, test_constants.STREAM_LENGTH)++    async def _get_channel_id(self, idx):+        """"""Channel id may not be consecutive""""""+        resp = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=0))+        self.assertGreater(len(resp.channel), idx)+        return resp.channel[idx].ref.channel_id++    async def _get_server_by_id(self, idx):+        """"""Server id may not be consecutive""""""+        resp = await self._channelz_stub.GetServers(+            channelz_pb2.GetServersRequest(start_server_id=0))+        return resp.server[idx]++    async def test_get_top_channels_basic(self):+        before = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=0))+        await _generate_channel_server_pairs(1)+        after = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=0))+        self.assertEqual(len(after.channel) - len(before.channel), 1)+        self.assertEqual(after.end, True)++    async def test_get_top_channels_high_start_id(self):+        await _generate_channel_server_pairs(1)+        resp = await self._channelz_stub.GetTopChannels(+            channelz_pb2.GetTopChannelsRequest(start_channel_id=10000))+        self.assertEqual(len(resp.channel), 0)+        self.assertEqual(resp.end, True)++    async def test_successful_request(self):+        idx = await _generate_channel_server_pairs(1)+        await self._send_successful_unary_unary(idx[0])+        resp = await self._channelz_stub.GetChannel(+            channelz_pb2.GetChannelRequest(+                channel_id=await self._get_channel_id(idx[0])))+        self.assertEqual(resp.channel.data.calls_started, 1)+        self.assertEqual(resp.channel.data.calls_succeeded, 1)+        self.assertEqual(resp.channel.data.calls_failed, 0)++    async def test_failed_request(self):+        idx = await _generate_channel_server_pairs(1)+        await self._send_failed_unary_unary(idx[0])+        resp = await self._channelz_stub.GetChannel(+            channelz_pb2.GetChannelRequest(+                channel_id=await self._get_channel_id(idx[0])))+        self.assertEqual(resp.channel.data.calls_started, 1)+        self.assertEqual(resp.channel.data.calls_succeeded, 0)+        self.assertEqual(resp.channel.data.calls_failed, 1)++    async def test_many_requests(self):+        idx = await _generate_channel_server_pairs(1)+        k_success = 7","TIL! I hope this variable can be different across different test cases, so the stats won't accidentally be the same.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22667,420417749,2020-05-05T21:29:09Z,src/python/grpcio_channelz/grpc_channelz/v1/_async.py,"@@ -0,0 +1,66 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""AsyncIO version of Channelz servicer.""""""++from grpc.experimental import aio++import grpc_channelz.v1.channelz_pb2 as _channelz_pb2+import grpc_channelz.v1.channelz_pb2_grpc as _channelz_pb2_grpc+from grpc_channelz.v1._servicer import ChannelzServicer as _SyncChannelzServicer+++class ChannelzServicer(_channelz_pb2_grpc.ChannelzServicer):+    """"""AsyncIO servicer for handling RPCs for service statuses.""""""++    @staticmethod+    async def GetTopChannels(request: _channelz_pb2.GetTopChannelsRequest,+                             context: aio.ServicerContext+                            ) -> _channelz_pb2.GetTopChannelsResponse:+        return _SyncChannelzServicer.GetTopChannels(request, context)","To clarify, the definition I'm using for ""blocking"" here is not ""may not terminate for an arbitrarily long period of time"". What I mean by ""blocking"" is ""may take longer than some (unspecified) upper time bound."" I would argue that this definition is more useful for us since `time.sleep(100)` doesn't meet the first definition of ""blocking"" but definitely ruins the responsiveness of your `asyncio` application.That's not quite all it does. It gets the stats from core (which could itself constitute a blocking operation) and then [converts the JSON to a serialized protocol buffer message](https://github.com/grpc/grpc/blob/da428e90203d9e58f260833c6b725d3ba6463df4/src/python/grpcio_channelz/grpc_channelz/v1/_servicer.py#L31). For large json objects, this could take a while to complete, satisfying the (somewhat fuzzy) definition of ""blocking operation."" Again though, I think this is a problem that isn't high-value enough to invest the time to solve.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22865,420447688,2020-05-05T22:37:08Z,src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc,"@@ -86,8 +88,33 @@ typedef struct grpc_ares_hostbyname_request {   uint16_t port;   /** is it a grpclb address */   bool is_balancer;+  /** for logging and errors: the query type (""A"" or ""AAAA"") */+  grpc_core::UniquePtr<char> qtype; } grpc_ares_hostbyname_request; +static void grpc_ares_request_ref_locked(grpc_ares_request* r);+static void grpc_ares_request_unref_locked(grpc_ares_request* r);++class GrpcAresQuery {","I don't understand the purpose of this class.  It seems to be a simple wrapper of `grpc_ares_request`, but it doesn't seem to provide any functionality that isn't already present in the latter.  Why is this wrapper being added?",X
940619,Falco20019,https://api.github.com/repos/grpc/grpc/pulls/22869,420589573,2020-05-06T07:19:42Z,src/csharp/Grpc.Tools/ProtoCompile.cs,"@@ -433,6 +439,18 @@ protected override string GenerateResponseFileCommands()             return cmd.ToString();         } +        // If possible, disambiguate output dir by adding a hash of the proto file's path+        static string MaybeEnhanceOutputDir(string outputDir, ITaskItem[] protobufs)","I just saw that in `CppGeneratorServices`, we already use `GetRelativeDir` to generate intermediate paths instead of hashes. `protoc` seems to generate the folders by default for C++.Do we want to use hashes for C# or do we also want to just create the folders using the `GetRelativeDir` approach? Right now `MaybeEnhanceOutputDir` in `ProtoCompile` is always applied, also for C++. We would need to disable that based on `Generator`.Technically, we could do it either way. But using the relative dir also in C# would make both generators more similar. And C++ would also already have the problem of the paths getting long. So having this behavior also in C# wouldn't create a gap.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22869,421483218,2020-05-07T12:58:39Z,src/csharp/Grpc.Tools/DepFileUtil.cs,"@@ -269,5 +301,51 @@ static int FindLineSeparator(string line)                 return new string[0];             }         }++        // Calculate part of proto path relative to root. Protoc is very picky+        // about them matching exactly, so can be we. Expect root be exact prefix+        // to proto, minus some slash normalization.+        internal static string GetRelativeDir(string root, string proto, TaskLoggingHelper log)","It doesn't seem that there are any tests for this logic so it's hard for me to see what is this method supposed to do and how exactly it fits into the bigger picture.What is the benefit of having this method over the previous state? It's fine to introduce more logic in case where it makes sense (in this case I don't quite understand what the goal is), but otherwise I'd like to keep things as simple as possible.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22869,421488697,2020-05-07T13:07:23Z,src/csharp/Grpc.Tools/ProtoCompilerOutputs.cs,"@@ -68,8 +76,13 @@ public override bool Execute()             // Get language-specific possible output. The generator expects certain             // metadata be set on the proto item.             var possible = new List<ITaskItem>();+            var patched = new List<ITaskItem>();             foreach (var proto in Protobuf)             {+                // This operates on a local copy and has no effect on the MSBuild instance!","Just saw this comment.  It seems to contradict the doc comment of the method: `// Update OutputDir and GrpcOutputDir for the item and all subsequent        // targets using this item. This should only be done if the real        // output directories for protoc should be modified.`If we're actually only modifying the output dir values in this instance, there should be a less confusing way of doing this (store in a different variable / metadata?)",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22869,421491556,2020-05-07T13:11:46Z,src/csharp/Grpc.Tools/ProtoCompilerOutputs.cs,"@@ -68,8 +76,13 @@ public override bool Execute()             // Get language-specific possible output. The generator expects certain             // metadata be set on the proto item.             var possible = new List<ITaskItem>();+            var patched = new List<ITaskItem>();             foreach (var proto in Protobuf)             {+                // This operates on a local copy and has no effect on the MSBuild instance!+                generator.PatchOutputDirectory(proto);+                patched.Add(proto);","this seems odd. You're taking items from `Protobuf` and you're modifying them (without making a defensive copy), so once done, both Protobuf and PatchedProtobuf will contain the same instances of ITaskItem, which is very confusing (and the items in Protobuf will all be patched, which is counterintuitive given the naming).",X
940619,Falco20019,https://api.github.com/repos/grpc/grpc/pulls/22869,421563515,2020-05-07T14:46:31Z,src/csharp/Grpc.Tools/GeneratorServices.cs,"@@ -55,31 +54,58 @@ protected bool GrpcOutputPossible(ITaskItem proto)                 && !gsm.EqualNoCase(""false"");         } -        public abstract string[] GetPossibleOutputs(ITaskItem proto);+        // Update OutputDir and GrpcOutputDir for the item and all subsequent+        // targets using this item. This should only be done if the real+        // output directories for protoc should be modified.+        public virtual void PatchOutputDirectory(ITaskItem protoItem)+        {+            // Nothing to do+        }++        public abstract string[] GetPossibleOutputs(ITaskItem protoItem);     };      // C# generator services.     internal class CSharpGeneratorServices : GeneratorServices     {         public CSharpGeneratorServices(TaskLoggingHelper log) : base(log) { } +        public override void PatchOutputDirectory(ITaskItem protoItem)+        {+            string root = protoItem.GetMetadata(Metadata.ProtoRoot);+            string proto = protoItem.ItemSpec;+            string relative = DepFileUtil.GetRelativeDir(root, proto, Log);++            string outdir = protoItem.GetMetadata(Metadata.OutputDir);+            string pathStem = Path.Combine(outdir, relative);+            protoItem.SetMetadata(Metadata.OutputDir, pathStem);++            // Override outdir if GrpcOutputDir present, default to proto output.+            string grpcdir = protoItem.GetMetadata(Metadata.GrpcOutputDir);+            if (grpcdir != """")+            {+                pathStem = Path.Combine(grpcdir, relative);+            }+            protoItem.SetMetadata(Metadata.GrpcOutputDir, pathStem);","This will be used to update `Protobuf_Compile` for the rest of the build and it's documented in the `targets` file to make it easier to follow. This allows us to streamline the build. We could also add a separate metadata instead of overwriting it. But then, the original `OutputDir` would not be used anywhere in the `targets`.For C++, this is skipped as protoc is generating the sub-folders according to this logic already. For C#, we would adjust the `OutputDir` to point to the relative path as C++ and other languages would do it.```Identity: protos/sub/folder/foo.protoProtoRoot: protosOutputDir (before patch): obj/Debug/netstandard2.1OutputDir (after patch): obj/Debug/netstandard2.1/sub/folder```Therefore the plugin will generate the files to `obj/Debug/netstandard2.1/sub/folder` as if the user had manually set `OutputDir = ""$(Protobuf_OutputPath)%(RelativeDir)""` but this also works for protos not on the drive since `ProtoRoot` is used and not the project location. This just replicates the C++ behavior.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22865,421576082,2020-05-07T15:03:12Z,src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc,"@@ -86,8 +88,33 @@ typedef struct grpc_ares_hostbyname_request {   uint16_t port;   /** is it a grpclb address */   bool is_balancer;+  /** for logging and errors: the query type (""A"" or ""AAAA"") */+  grpc_core::UniquePtr<char> qtype; } grpc_ares_hostbyname_request; +static void grpc_ares_request_ref_locked(grpc_ares_request* r);+static void grpc_ares_request_unref_locked(grpc_ares_request* r);++class GrpcAresQuery {","Okay, I guess this is fine for now.  I would like to find a cleaner way to do this that does not require allocating so many different data structures, but I guess we can optimize that as part of the larger C++ conversion.  For now, please add a TODO about this.",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/22802,421755533,2020-05-07T19:53:30Z,src/core/lib/iomgr/tcp_client_posix.cc,"@@ -275,6 +285,24 @@ grpc_error* grpc_tcp_client_prepare_fd(const grpc_channel_args* channel_args,   return GRPC_ERROR_NONE; } +/* Fills in dest with a new string containing the human readable+ * source address of fd. If source address extraction fails for any reason,+ * this then fills in dest with ""source_address_extraction_failed"". */+static void extract_source_ip(char** dest, int fd) {","following C++ TOTW 176 - Prefer return values to output parameters,this should be -static char * <func_name> (int fd)",
43831800,sheenaqotj,https://api.github.com/repos/grpc/grpc/pulls/22837,421796682,2020-05-07T21:11:05Z,test/cpp/end2end/client_callback_end2end_test.cc,"@@ -517,13 +524,19 @@ TEST_P(ClientCallbackEnd2endTest, SequentialGenericRpcs) { TEST_P(ClientCallbackEnd2endTest, SequentialGenericRpcsAsBidi) {   MAYBE_SKIP_TEST;   ResetStub();-  SendGenericEchoAsBidi(10, 1);+  SendGenericEchoAsBidi(10, 1, /*do_writes_done=*/true);","Is it useful to do combos of writesdone and not for all of these tests, either through parameterized testing or by just sending two RPCs each time?",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/22891,421844674,2020-05-07T23:09:25Z,src/csharp/Grpc.Core/Internal/BatchContextSafeHandle.cs,"@@ -92,7 +92,8 @@ public ClientSideStatus GetReceivedStatusOnClient()             UIntPtr detailsLength;             IntPtr detailsPtr = Native.grpcsharp_batch_context_recv_status_on_client_details(this, out detailsLength);             string details = MarshalUtils.PtrToStringUTF8(detailsPtr, (int)detailsLength.ToUInt32());-            var status = new Status(Native.grpcsharp_batch_context_recv_status_on_client_status(this), details);+            string error = Marshal.PtrToStringAnsi(Native.grpcsharp_batch_context_recv_status_on_client_error_string(this));",naming consistency nit: suggest calling this: `debugErrorString`,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/22863,421958738,2020-05-08T06:11:19Z,src/core/tsi/ssl_transport_security.cc,"@@ -377,7 +374,7 @@ static tsi_result add_subject_alt_names_properties_to_peer(       result = tsi_construct_string_peer_property(           TSI_X509_SUBJECT_ALTERNATIVE_NAME_PEER_PROPERTY,           reinterpret_cast<const char*>(name), static_cast<size_t>(name_size),-          &peer->properties[peer->property_count++]);+          &peer->properties[(*current_insert_index)++]);","Yeah that's another option. I choose the other way because we have a `if (result != TSI_OK) break;` check at the end of this big if-else block. If we pile the two `tsi_construct_string_peer_property` at the same block, we might want to do an additional tsi_result check in between, and free the memory if necessary.I don't have a strong preference of the two ways, and the newly pushed commit is the new version I described above. Let me know if it works:)",
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/22887,422359932,2020-05-08T20:34:11Z,test/cpp/interop/interop_client.cc,"@@ -1065,36 +1067,122 @@ bool InteropClient::DoCustomMetadata() {   return true; } -bool InteropClient::DoRpcSoakTest(int32_t soak_iterations) {-  gpr_log(GPR_DEBUG, ""Sending %d RPCs..."", soak_iterations);-  GPR_ASSERT(soak_iterations > 0);+std::tuple<bool, grpc_millis, std::string>+InteropClient::PerformOneSoakTestIteration(+    const bool reset_channel,+    const int64_t max_acceptable_per_iteration_latency_ms) {+  gpr_timespec start = gpr_now(GPR_CLOCK_MONOTONIC);   SimpleRequest request;   SimpleResponse response;+  // Don't set the deadline on the RPC, and instead just+  // record how long the RPC took and compare. This makes+  // debugging easier when looking at failure results.+  ClientContext context;+  InteropClientContextInspector inspector(context);+  request.set_response_size(kLargeResponseSize);+  grpc::string payload(kLargeRequestSize, '\0');+  request.mutable_payload()->set_body(payload.c_str(), kLargeRequestSize);+  if (reset_channel) {+    serviceStub_.ResetChannel();+  }+  Status s = serviceStub_.Get()->UnaryCall(&context, request, &response);+  gpr_timespec now = gpr_now(GPR_CLOCK_MONOTONIC);+  grpc_millis elapsed_ms =+      grpc_timespec_to_millis_round_down(gpr_time_sub(now, start));+  if (!s.ok()) {+    return std::make_tuple(false, elapsed_ms, context.debug_error_string());+  } else if (elapsed_ms > max_acceptable_per_iteration_latency_ms) {+    char* out;+    GPR_ASSERT(gpr_asprintf(+                   &out, ""%ld ms exceeds max acceptable latency: %ld ms."",+                   elapsed_ms, max_acceptable_per_iteration_latency_ms) != -1);+    std::string debug_string(out);+    gpr_free(out);+    return std::make_tuple(false, elapsed_ms, debug_string);+  } else {+    return std::make_tuple(true, elapsed_ms, """");+  }+}++void InteropClient::PerformSoakTest(+    const bool reset_channel_per_iteration, const int32_t soak_iterations,+    const int32_t max_failures,+    const int64_t max_acceptable_per_iteration_latency_ms) {+  std::vector<std::tuple<bool, grpc_millis, std::string>> results;+  std::vector<grpc_millis> latencies_ms;   for (int i = 0; i < soak_iterations; ++i) {-    if (!PerformLargeUnary(&request, &response)) {-      gpr_log(GPR_ERROR, ""rpc_soak test failed on iteration %d"", i);-      return false;+    auto result = PerformOneSoakTestIteration(+        reset_channel_per_iteration, max_acceptable_per_iteration_latency_ms);+    results.push_back(result);+    latencies_ms.push_back(std::get<1>(result));+  }+  int total_failures = 0;+  for (size_t i = 0; i < results.size(); i++) {+    bool success = std::get<0>(results[i]);+    grpc_millis elapsed_ms = std::get<1>(results[i]);+    std::string debug_string = std::get<2>(results[i]);+    if (!success) {+      gpr_log(GPR_DEBUG, ""soak iteration:%ld elapsed_ms:%ld failed: %s"", i,+              elapsed_ms, debug_string.c_str());+      total_failures++;+    } else {+      gpr_log(GPR_DEBUG, ""soak iteration:%ld elapsed_ms:%ld succeeded"", i,+              elapsed_ms);     }   }+  std::sort(latencies_ms.begin(), latencies_ms.end());+  grpc_millis latency_ms_median = latencies_ms.size() >= 2+                                      ? latencies_ms[latencies_ms.size() / 2]+                                      : latencies_ms.back();+  grpc_millis latency_ms_90th =+      latencies_ms.size() >= 10 ? latencies_ms[(latencies_ms.size() / 10) * 9]","Note there is a histogram class in core and also in cpp qps test (built on top of that). Maybe you would like to use either class. For this hand-rolled version, it looks like the 90th is not calculated correctly because `size()` will return an integer. So you at least need something like `static_cast<size_t>(latencies_ms.size() / 10.0 * 9)`",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22855,422398903,2020-05-08T22:11:02Z,src/python/grpcio_tests/tests_py3_only/unit/_leak_test.py,"@@ -0,0 +1,93 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""A smoke test for memory leaks.""""""++import logging+import os+import resource+import sys+import unittest+from concurrent.futures import ThreadPoolExecutor++import grpc++_TEST_METHOD = '/test/Test'+_REQUEST = b'\x23\x33'+_LARGE_NUM_OF_ITERATIONS = 5000++# If MAX_RSS inflated more than this size, the test is failed.+_FAIL_THRESHOLD = 25 * 1024 * 1024  #  25 MiB+++def _get_max_rss():+    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss+++def _pretty_print_bytes(x):+    if x > 1024 * 1024 * 1024:+        return ""%.2f GiB"" % (x / 1024.0 / 1024 / 1024)+    elif x > 1024 * 1024:+        return ""%.2f MiB"" % (x / 1024.0 / 1024)+    elif x > 1024:+        return ""%.2f KiB"" % (x / 1024.0)+    else:+        return ""%d B"" % x+++class _GenericHandler(grpc.GenericRpcHandler):++    def service(self, handler_call_details):+        if handler_call_details.method == _TEST_METHOD:+            return grpc.unary_unary_rpc_method_handler(lambda x, _: x)+++def _start_a_test_server():+    server = grpc.server(ThreadPoolExecutor(max_workers=1),+                         options=(('grpc.so_reuseport', 0),))+    server.add_generic_rpc_handlers((_GenericHandler(),))+    port = server.add_insecure_port('localhost:0')+    server.start()+    return 'localhost:%d' % port, server+++def _perform_an_rpc(address):+    channel = grpc.insecure_channel(address)+    multicallable = channel.unary_unary(_TEST_METHOD)+    response = multicallable(_REQUEST)+    assert _REQUEST == response+++class TestLeak(unittest.TestCase):++    def test_leak_with_single_shot_rpcs(self):+        address, server = _start_a_test_server()++        # Records memory before experiment.+        before = _get_max_rss()++        # Amplifies the leak.+        for n in range(_LARGE_NUM_OF_ITERATIONS):","If you make this `while True` and check the RSS on every iteration, will it ever fail? Is this platform-dependent? What is the flake rate for this test as written?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22855,422413669,2020-05-08T23:06:48Z,src/python/grpcio_tests/tests_py3_only/unit/_leak_test.py,"@@ -0,0 +1,93 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""A smoke test for memory leaks.""""""++import logging+import os+import resource+import sys+import unittest+from concurrent.futures import ThreadPoolExecutor++import grpc++_TEST_METHOD = '/test/Test'+_REQUEST = b'\x23\x33'+_LARGE_NUM_OF_ITERATIONS = 5000++# If MAX_RSS inflated more than this size, the test is failed.+_FAIL_THRESHOLD = 25 * 1024 * 1024  #  25 MiB+++def _get_max_rss():+    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss+++def _pretty_print_bytes(x):+    if x > 1024 * 1024 * 1024:+        return ""%.2f GiB"" % (x / 1024.0 / 1024 / 1024)+    elif x > 1024 * 1024:+        return ""%.2f MiB"" % (x / 1024.0 / 1024)+    elif x > 1024:+        return ""%.2f KiB"" % (x / 1024.0)+    else:+        return ""%d B"" % x+++class _GenericHandler(grpc.GenericRpcHandler):++    def service(self, handler_call_details):+        if handler_call_details.method == _TEST_METHOD:+            return grpc.unary_unary_rpc_method_handler(lambda x, _: x)+++def _start_a_test_server():+    server = grpc.server(ThreadPoolExecutor(max_workers=1),+                         options=(('grpc.so_reuseport', 0),))+    server.add_generic_rpc_handlers((_GenericHandler(),))+    port = server.add_insecure_port('localhost:0')+    server.start()+    return 'localhost:%d' % port, server+++def _perform_an_rpc(address):+    channel = grpc.insecure_channel(address)+    multicallable = channel.unary_unary(_TEST_METHOD)+    response = multicallable(_REQUEST)+    assert _REQUEST == response+++class TestLeak(unittest.TestCase):++    def test_leak_with_single_shot_rpcs(self):+        address, server = _start_a_test_server()++        # Records memory before experiment.+        before = _get_max_rss()++        # Amplifies the leak.+        for n in range(_LARGE_NUM_OF_ITERATIONS):","The memory increase stops around 5k iterations (increased hundreds KB), and stays there for minutes. So, I think it should be safe.This leak is not platform-dependent as far as I know.Flake rate < 0.1%, passed 1k runs with 0 failure.```//src/python/grpcio_tests/tests_py3_only/unit:_leak_test                 PASSED in 10.6s  Stats over 1000 runs: max = 10.6s, min = 9.4s, avg = 10.3s, dev = 0.2s```",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22344,423158951,2020-05-11T16:19:41Z,src/cpp/README.md,"@@ -9,6 +9,23 @@ This section describes how to add gRPC as a dependency to your C++ project. In the C++ world, there's no universally accepted standard for managing project dependencies. Therefore, gRPC supports several major build systems, which should satisfy most users. +## Supported Platforms++* Supported: These platforms are officially supported. We test our code on these+platform and have automated continuous integration tests for them.++* Best Effort: We do not have continous integration tests for these, but we are fairly confident that gRPC C++ would work on them.+We will make our best effort to support them, and we welcome patches for such platforms, but we might need to declare bankruptcy on some issues.++| Operating System | Architectures | Versions | Support Level |+|------------------|---------------|----------|---------------|+| Linux            | x86, x64      | clang 3.4+, GCC 4.9+ | Supported |","Linux distribution does matter. E.g. we have a bunch of open issues around Alpine Linux support and we're being unclear about whether that's supported or not (Alpine is pretty widely used in Kubernetes deployments, but it uses a different libc library).I think we fully support Debian, Ubuntu and CentOS / RedHat (for all of them we support the stable release + newer releases). For Alpine our support is ""best effort"". ",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22344,423160261,2020-05-11T16:21:47Z,src/cpp/README.md,"@@ -9,6 +9,23 @@ This section describes how to add gRPC as a dependency to your C++ project. In the C++ world, there's no universally accepted standard for managing project dependencies. Therefore, gRPC supports several major build systems, which should satisfy most users. +## Supported Platforms++* Supported: These platforms are officially supported. We test our code on these+platform and have automated continuous integration tests for them.++* Best Effort: We do not have continous integration tests for these, but we are fairly confident that gRPC C++ would work on them.+We will make our best effort to support them, and we welcome patches for such platforms, but we might need to declare bankruptcy on some issues.++| Operating System | Architectures | Versions | Support Level |+|------------------|---------------|----------|---------------|+| Linux            | x86, x64      | clang 3.4+, GCC 4.9+ | Supported |+| Windows 7+       | x86, x64      | Visual Studio 2015+  | Supported |","Windows 7 is actually end of life since January 2020, so we might as well drop its official support from the list. Windows 10+ is enough.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22922,423255649,2020-05-11T19:04:43Z,test/core/surface/sequential_connectivity_test.cc,"@@ -133,7 +134,18 @@ static void insecure_test_add_port(grpc_server* server, const char* addr) { }  static grpc_channel* insecure_test_create_channel(const char* addr) {-  return grpc_insecure_channel_create(addr, nullptr, nullptr);+  grpc_arg arg = {GRPC_ARG_INTEGER,",Please use `grpc_channel_arg_integer_create()` here:https://github.com/grpc/grpc/blob/4383f8dc38d69bdcd0b4c750f8cea360f1145a31/src/core/lib/channel/channel_args.h#L113,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22922,423256695,2020-05-11T19:06:48Z,test/core/surface/sequential_connectivity_test.cc,"@@ -170,12 +182,16 @@ static grpc_channel* secure_test_create_channel(const char* addr) {   grpc_channel_credentials* ssl_creds =       grpc_ssl_credentials_create(test_root_cert, nullptr, nullptr, nullptr);   grpc_slice_unref(ca_slice);-  grpc_arg ssl_name_override = {-      GRPC_ARG_STRING,-      const_cast<char*>(GRPC_SSL_TARGET_NAME_OVERRIDE_ARG),-      {const_cast<char*>(""foo.test.google.fr"")}};+  const int kNumArgs = 2;+  grpc_arg args[kNumArgs] = {+      {GRPC_ARG_STRING,+       const_cast<char*>(GRPC_SSL_TARGET_NAME_OVERRIDE_ARG),+       {const_cast<char*>(""foo.test.google.fr"")}},+      {GRPC_ARG_INTEGER,",Please use `grpc_channel_arg_integer_create()` here:https://github.com/grpc/grpc/blob/4383f8dc38d69bdcd0b4c750f8cea360f1145a31/src/core/lib/channel/channel_args.h#L113,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22922,423256882,2020-05-11T19:07:09Z,test/core/surface/sequential_connectivity_test.cc,"@@ -170,12 +182,16 @@ static grpc_channel* secure_test_create_channel(const char* addr) {   grpc_channel_credentials* ssl_creds =       grpc_ssl_credentials_create(test_root_cert, nullptr, nullptr, nullptr);   grpc_slice_unref(ca_slice);-  grpc_arg ssl_name_override = {-      GRPC_ARG_STRING,-      const_cast<char*>(GRPC_SSL_TARGET_NAME_OVERRIDE_ARG),-      {const_cast<char*>(""foo.test.google.fr"")}};+  const int kNumArgs = 2;+  grpc_arg args[kNumArgs] = {+      {GRPC_ARG_STRING,",Please use `grpc_channel_arg_string_create()` here:https://github.com/grpc/grpc/blob/4383f8dc38d69bdcd0b4c750f8cea360f1145a31/src/core/lib/channel/channel_args.h#L112,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22917,423764776,2020-05-12T14:10:23Z,bazel/grpc_deps.bzl,"@@ -212,11 +212,11 @@ def grpc_deps():         http_archive(             name = ""com_github_cares_cares"",             build_file = ""@com_github_grpc_grpc//third_party:cares/cares.BUILD"",-            sha256 = ""e8c2751ddc70fed9dc6f999acd92e232d5846f009ee1674f8aee81f19b2b915a"",-            strip_prefix = ""c-ares-e982924acee7f7313b4baa4ee5ec000c5e373c30"",+            sha256 = ""6fc3e03427d2237e28b97cb3c0ac7106f2a3ac72fbe8698363cff5877faeb83a"",+            strip_prefix = ""c-ares-077a587dccbe2f0d8a1987fbd3525333705c2249"",             urls = [-                ""https://storage.googleapis.com/grpc-bazel-mirror/github.com/c-ares/c-ares/archive/e982924acee7f7313b4baa4ee5ec000c5e373c30.tar.gz"",-                ""https://github.com/c-ares/c-ares/archive/e982924acee7f7313b4baa4ee5ec000c5e373c30.tar.gz"",+                ""https://storage.googleapis.com/grpc-bazel-mirror/github.com/c-ares/c-ares/archive/077a587dccbe2f0d8a1987fbd3525333705c2249.tar.gz"",",@apolcyn  let's make sure the mirror is updated via https://github.com/grpc/grpc/blob/master/bazel/update_mirror.sh before merging.,
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/22929,423946487,2020-05-12T18:30:02Z,src/core/lib/compression/message_compress.cc,"@@ -34,7 +34,7 @@ static int zlib_body(z_stream* zs, grpc_slice_buffer* input,                      grpc_slice_buffer* output,                      int (*flate)(z_stream* zs, int flush)) {-  int r;+  int r = Z_OK;","thinking again, Z_STREAM_END should also be fine, since the compression/decompression semantics are in our control, and it would may be fine to call it a success on an empty message",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/22929,423947482,2020-05-12T18:31:43Z,test/core/compression/message_compress_test.cc,"@@ -210,12 +210,10 @@ static void test_bad_decompression_data_missing_trailer(void) {   grpc_core::ExecCtx exec_ctx;   /* compress it */   grpc_msg_compress(GRPC_MESSAGE_COMPRESS_GZIP, &input, &corrupted);-  /* corrupt the output by smashing the CRC */-  GPR_ASSERT(corrupted.count > 1);-  GPR_ASSERT(GRPC_SLICE_LENGTH(corrupted.slices[1]) > 8);+  GPR_ASSERT(GRPC_SLICE_LENGTH(corrupted.slices[corrupted.count - 1]) > 8);+  /* Remove the footer by manipulating the slice length */","Hmm it's definitely better, but what if there are 3 slices or the length of the second slice is less than 8? Does core have any function to cut the tail of a slice buffer in a sophisticated way? Or alternatively, if you have reason to believe that the compressed slice buffer will always contain exactly 2 slices and the second slice will always be longer than 8, we can put the reason as comment here and move on.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22925,424004570,2020-05-12T20:12:03Z,src/python/grpcio/grpc/experimental/__init__.py,"@@ -78,11 +79,37 @@ def _wrapper(*args, **kwargs):     return _wrapper  +def wrap_server_method_handler(wrapper, handler):+    """"""Wraps the server method handler function.++    The server implementation requires all server handlers being wrapped as+    RpcMethodHandler objects. This helper function ease the pain of writing+    server handler wrappers.+    """"""+    if not handler:+        return None++    if not handler.request_streaming:","If we want to be compatible with ProtoBuf generated code, we need to comply with existing design of the RpcMethodHandler class. I think it only needs 4 fields instead of 8: arity, behavior, serializer, deserializer.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22925,424035146,2020-05-12T21:09:26Z,src/python/grpcio_tests/tests_aio/unit/server_interceptor_test.py,"@@ -162,6 +180,138 @@ async def test_apply_different_interceptors_by_metadata(self):                 'log2:intercept_service',             ], record) +    async def test_response_caching(self):+        # Prepares a preset value to help testing+        cache_store = {+            42:+                messages_pb2.SimpleResponse(payload=messages_pb2.Payload(+                    body=b'\x42'))+        }++        async def intercept_and_cache(+                continuation: Callable[[grpc.HandlerCallDetails], Awaitable[+                    grpc.RpcMethodHandler]],+                handler_call_details: grpc.HandlerCallDetails+        ) -> grpc.RpcMethodHandler:+            # Get the actual handler+            handler = await continuation(handler_call_details)++            def wrapper(behavior: Callable[+                [messages_pb2.SimpleRequest, aio.+                 ServerInterceptor], messages_pb2.SimpleResponse]):++                @functools.wraps(behavior)+                async def wrapper(request: messages_pb2.SimpleRequest,+                                  context: aio.ServicerContext+                                 ) -> messages_pb2.SimpleResponse:+                    if request.response_size not in cache_store:+                        cache_store[request.response_size] = await behavior(+                            request, context)+                    return cache_store[request.response_size]++                return wrapper++            return wrap_server_method_handler(wrapper, handler)++        # Constructs a server with the cache interceptor+        server, stub = await _create_server_stub_pair(+            _GenericInterceptor(intercept_and_cache))++        # Tests if the cache store is used+        response = await stub.UnaryCall(+            messages_pb2.SimpleRequest(response_size=42))+        self.assertEqual(1, len(cache_store[42].payload.body))+        self.assertEqual(cache_store[42], response)++        # Tests response can be cached+        response = await stub.UnaryCall(+            messages_pb2.SimpleRequest(response_size=1337))+        self.assertEqual(1337, len(cache_store[1337].payload.body))+        self.assertEqual(cache_store[1337], response)+        response = await stub.UnaryCall(+            messages_pb2.SimpleRequest(response_size=1337))+        self.assertEqual(cache_store[1337], response)++    async def test_interceptor_unary_stream(self):+        record = []+        server, stub = await _create_server_stub_pair(+            _LoggingInterceptor('log_unary_stream', record))++        # Prepares the request+        request = messages_pb2.StreamingOutputCallRequest()+        for _ in range(_NUM_STREAM_RESPONSES):+            request.response_parameters.append(+                messages_pb2.ResponseParameters(size=_RESPONSE_PAYLOAD_SIZE,))++        # Tests if the cache store is used+        call = stub.StreamingOutputCall(request)++        # Ensures the RPC goes fine+        async for response in call:+            self.assertEqual(_RESPONSE_PAYLOAD_SIZE, len(response.payload.body))+        self.assertEqual(await call.code(), grpc.StatusCode.OK)++        self.assertSequenceEqual([+            'log_unary_stream:intercept_service',","Intercepting streaming request or response is quite difficult with this design. The interceptor author would need to duplicate the interface of ServicerContext, and synchronize it with a queue. Link to a TODO issue https://github.com/grpc/grpc/issues/18191 requesting for a complete server interceptor.This PR proves server interceptors can intercept RPCs and act upon metadata, or unary requests. This is as-good-as existing stack, we can open another PR for possible ways to achieve functionalities better than existing stack.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/22704,424706706,2020-05-13T20:18:50Z,src/csharp/Grpc.IntegrationTesting/XdsInteropClient.cs,"@@ -0,0 +1,287 @@+#region Copyright notice and license++// Copyright 2020 The gRPC Authors+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#endregion++using System;+using System.Collections.Generic;+using System.Threading;+using System.Threading.Tasks;++using CommandLine;+using Grpc.Core;+using Grpc.Core.Logging;+using Grpc.Core.Internal;+using Grpc.Testing;++namespace Grpc.IntegrationTesting+{+    public class XdsInteropClient+    {+        internal class ClientOptions+        {+            [Option(""num_channels"", Default = 1)]+            public int NumChannels { get; set; }++            [Option(""qps"", Default = 1)]+            public int Qps { get; set; }++            [Option(""server"", Default = ""localhost:8080"")]+            public string Server { get; set; }++            [Option(""stats_port"", Default = 8081)]+            public int StatsPort { get; set; }++            [Option(""rpc_timeout_sec"", Default = 30)]+            public int RpcTimeoutSec { get; set; }++            [Option(""print_response"", Default = false)]+            public bool PrintResponse { get; set; }+        }++        ClientOptions options;++        StatsWatcher statsWatcher = new StatsWatcher();++        // make watcher accessible by tests+        internal StatsWatcher StatsWatcher => statsWatcher;++        internal XdsInteropClient(ClientOptions options)+        {+            this.options = options;+        }++        public static void Run(string[] args)+        {+            GrpcEnvironment.SetLogger(new ConsoleLogger());+            var parserResult = Parser.Default.ParseArguments<ClientOptions>(args)+                .WithNotParsed(errors => Environment.Exit(1))+                .WithParsed(options =>+                {+                    var xdsInteropClient = new XdsInteropClient(options);+                    xdsInteropClient.RunAsync().Wait();+                });+        }++        private async Task RunAsync()+        {+            var server = new Server+            {+                Services = { LoadBalancerStatsService.BindService(new LoadBalancerStatsServiceImpl(statsWatcher)) }+            };++            string host = ""0.0.0.0"";+            server.Ports.Add(host, options.StatsPort, ServerCredentials.Insecure);+            Console.WriteLine($""Running server on {host}:{options.StatsPort}"");+            server.Start();++            var cancellationTokenSource = new CancellationTokenSource();+            await RunChannelsAsync(cancellationTokenSource.Token);++            await server.ShutdownAsync();+        }++        // method made internal to make it runnable by tests+        internal async Task RunChannelsAsync(CancellationToken cancellationToken)+        {+            var channelTasks = new List<Task>();+            for (int channelId = 0; channelId < options.NumChannels; channelId++)+            {+                var channelTask = RunSingleChannelAsync(channelId, cancellationToken);+                channelTasks.Add(channelTask);+            }++            for (int channelId = 0; channelId < options.NumChannels; channelId++)+            {+                await channelTasks[channelId];+            }+        }++        private async Task RunSingleChannelAsync(int channelId, CancellationToken cancellationToken)+        {+            Console.WriteLine($""Starting channel {channelId}"");+            var channel = new Channel(options.Server, ChannelCredentials.Insecure);+            var client = new TestService.TestServiceClient(channel);","looks like we are creating channels in a way that will facilitate subchannel sharing - I'm not sure this is what we want though?Especially as `options.NumChannels` becomes large, the behavior and performance of this test with and without subchannel sharing might vary somewhat dramatically (especially e.g. if the backends are setting `htttp2 max concurrent streams`).Suggest either disabling subchannel sharing (using this arg:https://github.com/grpc/grpc/blob/2d63189237a87a1402517ab0f5ac306c924072e6/test/core/tsi/alts/handshaker/alts_concurrent_connectivity_test.cc#L87), or being explicit about whether or not this test uses subchannel sharing by adding one more option to the test runner to control whether or not we will use subchannel sharing.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/22704,424739964,2020-05-13T21:24:45Z,src/csharp/Grpc.IntegrationTesting/XdsInteropClient.cs,"@@ -0,0 +1,287 @@+#region Copyright notice and license++// Copyright 2020 The gRPC Authors+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#endregion++using System;+using System.Collections.Generic;+using System.Threading;+using System.Threading.Tasks;++using CommandLine;+using Grpc.Core;+using Grpc.Core.Logging;+using Grpc.Core.Internal;+using Grpc.Testing;++namespace Grpc.IntegrationTesting+{+    public class XdsInteropClient+    {+        internal class ClientOptions+        {+            [Option(""num_channels"", Default = 1)]+            public int NumChannels { get; set; }++            [Option(""qps"", Default = 1)]+            public int Qps { get; set; }++            [Option(""server"", Default = ""localhost:8080"")]+            public string Server { get; set; }++            [Option(""stats_port"", Default = 8081)]+            public int StatsPort { get; set; }++            [Option(""rpc_timeout_sec"", Default = 30)]+            public int RpcTimeoutSec { get; set; }++            [Option(""print_response"", Default = false)]+            public bool PrintResponse { get; set; }+        }++        ClientOptions options;++        StatsWatcher statsWatcher = new StatsWatcher();++        // make watcher accessible by tests+        internal StatsWatcher StatsWatcher => statsWatcher;++        internal XdsInteropClient(ClientOptions options)+        {+            this.options = options;+        }++        public static void Run(string[] args)+        {+            GrpcEnvironment.SetLogger(new ConsoleLogger());+            var parserResult = Parser.Default.ParseArguments<ClientOptions>(args)+                .WithNotParsed(errors => Environment.Exit(1))+                .WithParsed(options =>+                {+                    var xdsInteropClient = new XdsInteropClient(options);+                    xdsInteropClient.RunAsync().Wait();+                });+        }++        private async Task RunAsync()+        {+            var server = new Server+            {+                Services = { LoadBalancerStatsService.BindService(new LoadBalancerStatsServiceImpl(statsWatcher)) }+            };++            string host = ""0.0.0.0"";+            server.Ports.Add(host, options.StatsPort, ServerCredentials.Insecure);+            Console.WriteLine($""Running server on {host}:{options.StatsPort}"");+            server.Start();++            var cancellationTokenSource = new CancellationTokenSource();+            await RunChannelsAsync(cancellationTokenSource.Token);++            await server.ShutdownAsync();+        }++        // method made internal to make it runnable by tests+        internal async Task RunChannelsAsync(CancellationToken cancellationToken)+        {+            var channelTasks = new List<Task>();+            for (int channelId = 0; channelId < options.NumChannels; channelId++)+            {+                var channelTask = RunSingleChannelAsync(channelId, cancellationToken);+                channelTasks.Add(channelTask);+            }++            for (int channelId = 0; channelId < options.NumChannels; channelId++)+            {+                await channelTasks[channelId];+            }+        }++        private async Task RunSingleChannelAsync(int channelId, CancellationToken cancellationToken)+        {+            Console.WriteLine($""Starting channel {channelId}"");+            var channel = new Channel(options.Server, ChannelCredentials.Insecure);+            var client = new TestService.TestServiceClient(channel);++            var inflightTasks = new List<Task>();+            int millisPerQuery = (int)(1000.0 / options.Qps);  // qps value is per-channel+            while (!cancellationToken.IsCancellationRequested)+            {+                inflightTasks.Add(RunSingleRpcAsync(client, cancellationToken));++                await CleanupCompletedTasksAsync(inflightTasks);++                Console.WriteLine($""Currently {inflightTasks.Count} in-flight RPCs"");+                await Task.Delay(millisPerQuery);  // not accurate, but good enough for low QPS.+            }++            Console.WriteLine($""Shutting down channel {channelId}"");+            await channel.ShutdownAsync();+            Console.WriteLine($""Channel shutdown {channelId}"");+        }++        private async Task RunSingleRpcAsync(TestService.TestServiceClient client, CancellationToken cancellationToken)+        {+            long rpcId = statsWatcher.RpcIdGenerator.Increment();+            try+            {+                Console.WriteLine($""Starting RPC {rpcId}."");+                var response = await client.UnaryCallAsync(new SimpleRequest(),+                    new CallOptions(cancellationToken: cancellationToken, deadline: DateTime.UtcNow.AddSeconds(options.RpcTimeoutSec)));+                +                statsWatcher.OnRpcComplete(rpcId, response.Hostname);+                if (options.PrintResponse)+                {+                    Console.WriteLine($""Got response {response}"");+                }+                Console.WriteLine($""RPC {rpcId} succeeded "");+            }+            catch (RpcException ex)+            {+                statsWatcher.OnRpcComplete(rpcId, null);+                Console.WriteLine($""RPC {rpcId} failed: {ex}"");+            }+        }++        private async Task CleanupCompletedTasksAsync(List<Task> tasks)+        {+            var toRemove = new List<Task>();+            foreach (var task in tasks)+            {+                if (task.IsCompleted)+                {+                    // awaiting tasks that have already completed should be instantaneous+                    await task;+                }+                toRemove.Add(task);+            }+            foreach (var task in toRemove)+            {+                tasks.Remove(task);+            }+        }+    }++    internal class StatsWatcher+    {+        private readonly object myLock = new object();+        private readonly AtomicCounter rpcIdGenerator = new AtomicCounter(0);++        private long? firstAcceptedRpcId;+        private int numRpcsWanted;+        private int rpcsCompleted;+        private int rpcsNoHostname;+        private Dictionary<string, int> rpcsByHostname;++        public AtomicCounter RpcIdGenerator => rpcIdGenerator;++        public StatsWatcher()+        {+            Reset();+        }++        public void OnRpcComplete(long rpcId, string responseHostname)+        {+            lock (myLock)+            {+                if (!firstAcceptedRpcId.HasValue || rpcId < firstAcceptedRpcId || rpcId >= firstAcceptedRpcId + numRpcsWanted)+                {+                    return;+                }++                if (string.IsNullOrEmpty(responseHostname))+                {+                    rpcsNoHostname ++;+                }+                else +                {+                    if (!rpcsByHostname.ContainsKey(responseHostname))+                    {+                        rpcsByHostname[responseHostname] = 0;+                    }+                    rpcsByHostname[responseHostname] += 1;+                }+                rpcsCompleted += 1;++                if (rpcsCompleted >= numRpcsWanted)+                {+                    Monitor.Pulse(myLock);+                }+            }+        }++        public void Reset()+        {+            lock (myLock)+            {+                firstAcceptedRpcId = null;+                numRpcsWanted = 0;+                rpcsCompleted = 0;+                rpcsNoHostname = 0;+                rpcsByHostname = new Dictionary<string, int>();+            }+        }++        public LoadBalancerStatsResponse WaitForRpcStatsResponse(int rpcsWanted, int timeoutSec)+        {+            lock (myLock)+            {+                if (firstAcceptedRpcId.HasValue)+                {+                    throw new InvalidOperationException(""StateWatcher is already collecting stats."");","It looks like this is set up to only accept one GetClientStats RPC call at a time (not at all familiar with C# so I might be misunderstanding). It seems like it would be simpler and more flexible in the future to instantiate a StatsWatcher per incoming request, rather than having a single StatsWatcher for the entire service that must be reset between GetClientStats RPCs.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22704,424979376,2020-05-14T08:58:23Z,src/csharp/Grpc.IntegrationTesting/XdsInteropClient.cs,"@@ -0,0 +1,287 @@+#region Copyright notice and license++// Copyright 2020 The gRPC Authors+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#endregion++using System;+using System.Collections.Generic;+using System.Threading;+using System.Threading.Tasks;++using CommandLine;+using Grpc.Core;+using Grpc.Core.Logging;+using Grpc.Core.Internal;+using Grpc.Testing;++namespace Grpc.IntegrationTesting+{+    public class XdsInteropClient+    {+        internal class ClientOptions+        {+            [Option(""num_channels"", Default = 1)]+            public int NumChannels { get; set; }++            [Option(""qps"", Default = 1)]+            public int Qps { get; set; }++            [Option(""server"", Default = ""localhost:8080"")]+            public string Server { get; set; }++            [Option(""stats_port"", Default = 8081)]+            public int StatsPort { get; set; }++            [Option(""rpc_timeout_sec"", Default = 30)]+            public int RpcTimeoutSec { get; set; }++            [Option(""print_response"", Default = false)]+            public bool PrintResponse { get; set; }+        }++        ClientOptions options;++        StatsWatcher statsWatcher = new StatsWatcher();++        // make watcher accessible by tests+        internal StatsWatcher StatsWatcher => statsWatcher;++        internal XdsInteropClient(ClientOptions options)+        {+            this.options = options;+        }++        public static void Run(string[] args)+        {+            GrpcEnvironment.SetLogger(new ConsoleLogger());+            var parserResult = Parser.Default.ParseArguments<ClientOptions>(args)+                .WithNotParsed(errors => Environment.Exit(1))+                .WithParsed(options =>+                {+                    var xdsInteropClient = new XdsInteropClient(options);+                    xdsInteropClient.RunAsync().Wait();+                });+        }++        private async Task RunAsync()+        {+            var server = new Server+            {+                Services = { LoadBalancerStatsService.BindService(new LoadBalancerStatsServiceImpl(statsWatcher)) }+            };++            string host = ""0.0.0.0"";+            server.Ports.Add(host, options.StatsPort, ServerCredentials.Insecure);+            Console.WriteLine($""Running server on {host}:{options.StatsPort}"");+            server.Start();++            var cancellationTokenSource = new CancellationTokenSource();+            await RunChannelsAsync(cancellationTokenSource.Token);++            await server.ShutdownAsync();+        }++        // method made internal to make it runnable by tests+        internal async Task RunChannelsAsync(CancellationToken cancellationToken)+        {+            var channelTasks = new List<Task>();+            for (int channelId = 0; channelId < options.NumChannels; channelId++)+            {+                var channelTask = RunSingleChannelAsync(channelId, cancellationToken);+                channelTasks.Add(channelTask);+            }++            for (int channelId = 0; channelId < options.NumChannels; channelId++)+            {+                await channelTasks[channelId];+            }+        }++        private async Task RunSingleChannelAsync(int channelId, CancellationToken cancellationToken)+        {+            Console.WriteLine($""Starting channel {channelId}"");+            var channel = new Channel(options.Server, ChannelCredentials.Insecure);+            var client = new TestService.TestServiceClient(channel);++            var inflightTasks = new List<Task>();","It should stay outside the loop, it maintains a list of calls still inflight. the CleanupCompletedTasksAsync(inflightTasks); only cleans up calls that have already finished, call still inflight will be cleaned up later (and therefore the list needs to survive a single iteration).Added a comment.",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/22950,425540498,2020-05-15T03:04:41Z,src/ruby/pb/test/xds_client.rb,"@@ -0,0 +1,193 @@+#!/usr/bin/env ruby++# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This is the xDS interop test Ruby client. This is meant to be run by+# the run_xds_tests.py test runner.+#+# Usage: $ tools/run_tests/run_xds_tests.py --test_case=... ...+#    --client_cmd=""path/to/xds_client.rb --server=<hostname> \+#                                        --stats_port=<port> \+#                                        --qps=<qps>""++# These lines are required for the generated files to load grpc+this_dir = File.expand_path(File.dirname(__FILE__))+lib_dir = File.join(File.dirname(File.dirname(this_dir)), 'lib')+pb_dir = File.dirname(this_dir)+$LOAD_PATH.unshift(lib_dir) unless $LOAD_PATH.include?(lib_dir)+$LOAD_PATH.unshift(pb_dir) unless $LOAD_PATH.include?(pb_dir)++require 'optparse'+require 'logger'++require_relative '../../lib/grpc'+require 'google/protobuf'++require_relative '../src/proto/grpc/testing/empty_pb'+require_relative '../src/proto/grpc/testing/messages_pb'+require_relative '../src/proto/grpc/testing/test_services_pb'++# Some global variables to be shared by server and client",Not sure if this is OK - or should I make some sort of class to wrap these?,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/22950,426089753,2020-05-15T23:40:50Z,src/ruby/pb/test/xds_client.rb,"@@ -0,0 +1,193 @@+#!/usr/bin/env ruby++# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This is the xDS interop test Ruby client. This is meant to be run by+# the run_xds_tests.py test runner.+#+# Usage: $ tools/run_tests/run_xds_tests.py --test_case=... ...+#    --client_cmd=""path/to/xds_client.rb --server=<hostname> \+#                                        --stats_port=<port> \+#                                        --qps=<qps>""++# These lines are required for the generated files to load grpc+this_dir = File.expand_path(File.dirname(__FILE__))+lib_dir = File.join(File.dirname(File.dirname(this_dir)), 'lib')+pb_dir = File.dirname(this_dir)+$LOAD_PATH.unshift(lib_dir) unless $LOAD_PATH.include?(lib_dir)+$LOAD_PATH.unshift(pb_dir) unless $LOAD_PATH.include?(pb_dir)++require 'optparse'+require 'logger'++require_relative '../../lib/grpc'+require 'google/protobuf'++require_relative '../src/proto/grpc/testing/empty_pb'+require_relative '../src/proto/grpc/testing/messages_pb'+require_relative '../src/proto/grpc/testing/test_services_pb'++# Some global variables to be shared by server and client+$watchers = Array.new+$watchers_mutex = Mutex.new+$current_request_id = 0;++# RubyLogger defines a logger for gRPC based on the standard ruby logger.+module RubyLogger+  def logger+    LOGGER+  end++  LOGGER = Logger.new(STDOUT)+  LOGGER.level = Logger::INFO+end++# GRPC is the general RPC module+module GRPC+  # Inject the noop #logger if no module-level logger method has been injected.+  extend RubyLogger+end++# creates a test stub+def create_stub(opts)+  address = ""#{opts.server}""++  GRPC.logger.info(""... connecting insecurely to #{address}"")+  Grpc::Testing::TestService::Stub.new(+    address,+    :this_channel_is_insecure,+  )+end++# This implements LoadBalancerStatsService required by the test runner+class TestTarget < Grpc::Testing::LoadBalancerStatsService::Service+  include Grpc::Testing++  def get_client_stats(req, _call)+    start_id = $current_request_id + 1+    end_id = start_id + req['num_rpcs'] # watch the next X rpcs+    start_time = Time.now.to_f+    finish_time = start_time + req['timeout_sec']+    index = 0+    $watchers_mutex.synchronize do+      index = $watchers.length+      # Add a watcher for this range of id's+      $watchers << {+        ""start_id"" => start_id,+        ""end_id"" => end_id,+        ""rpcs_by_peer"" => Hash.new(0),+        ""rpcs_needed"" => req['num_rpcs'],+        ""no_remote_peer"" => 0+      }+    end+    # wait until all RPCs are done, or we are past timeout+    while ($watchers[index]['rpcs_needed'] > 0) &&+          (Time.now.to_f < finish_time)+      sleep(0.1)+    end+    LoadBalancerStatsResponse.new(+      rpcs_by_peer: $watchers[index]['rpcs_by_peer'],+      num_failures: $watchers[index]['no_remote_peer'] ++      $watchers[index]['rpcs_needed']+    );+  end+end++# send 1 rpc every 1/qps second+def run_test_loop(stub, duration_per_query)+  include Grpc::Testing+  req = SimpleRequest.new()++  while true+    start = Time.now.to_f+    begin+      resp = stub.unary_call(req)+      remote_peer = resp.hostname+    rescue GRPC::Unavailable => e","Is it correct to treat ""unavailable"" errors specially? I don't see anywhere in [the spec](https://github.com/grpc/grpc/blob/master/doc/xds-test-descriptions.md) that we should treat ""unavailable"" errors differently from other types of errors.Following the spec on the client, I think we need to add the `fail_on_failed_rpcs` command arg to this client, and then tweak this logic to look as follows:```begin  resp = stub.unary_call(req)  remote_peer = resp.hostnamerescue GRPC::BadStatus => e  remote_peer = """"  GRPC.logger.info(""ruby xds: rpc failed:|#{e.message}|, this may or may not be expected"")  if fail_on_failed_rpcs    raise e  endend```",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/22950,426090489,2020-05-15T23:44:57Z,src/ruby/pb/test/xds_client.rb,"@@ -0,0 +1,193 @@+#!/usr/bin/env ruby++# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This is the xDS interop test Ruby client. This is meant to be run by+# the run_xds_tests.py test runner.+#+# Usage: $ tools/run_tests/run_xds_tests.py --test_case=... ...+#    --client_cmd=""path/to/xds_client.rb --server=<hostname> \+#                                        --stats_port=<port> \+#                                        --qps=<qps>""++# These lines are required for the generated files to load grpc+this_dir = File.expand_path(File.dirname(__FILE__))+lib_dir = File.join(File.dirname(File.dirname(this_dir)), 'lib')+pb_dir = File.dirname(this_dir)+$LOAD_PATH.unshift(lib_dir) unless $LOAD_PATH.include?(lib_dir)+$LOAD_PATH.unshift(pb_dir) unless $LOAD_PATH.include?(pb_dir)++require 'optparse'+require 'logger'++require_relative '../../lib/grpc'+require 'google/protobuf'++require_relative '../src/proto/grpc/testing/empty_pb'+require_relative '../src/proto/grpc/testing/messages_pb'+require_relative '../src/proto/grpc/testing/test_services_pb'++# Some global variables to be shared by server and client+$watchers = Array.new+$watchers_mutex = Mutex.new+$current_request_id = 0;++# RubyLogger defines a logger for gRPC based on the standard ruby logger.+module RubyLogger+  def logger+    LOGGER+  end++  LOGGER = Logger.new(STDOUT)+  LOGGER.level = Logger::INFO+end++# GRPC is the general RPC module+module GRPC+  # Inject the noop #logger if no module-level logger method has been injected.+  extend RubyLogger+end++# creates a test stub+def create_stub(opts)+  address = ""#{opts.server}""++  GRPC.logger.info(""... connecting insecurely to #{address}"")+  Grpc::Testing::TestService::Stub.new(+    address,+    :this_channel_is_insecure,+  )+end++# This implements LoadBalancerStatsService required by the test runner+class TestTarget < Grpc::Testing::LoadBalancerStatsService::Service+  include Grpc::Testing++  def get_client_stats(req, _call)+    start_id = $current_request_id + 1+    end_id = start_id + req['num_rpcs'] # watch the next X rpcs+    start_time = Time.now.to_f+    finish_time = start_time + req['timeout_sec']+    index = 0+    $watchers_mutex.synchronize do+      index = $watchers.length+      # Add a watcher for this range of id's+      $watchers << {+        ""start_id"" => start_id,+        ""end_id"" => end_id,+        ""rpcs_by_peer"" => Hash.new(0),+        ""rpcs_needed"" => req['num_rpcs'],+        ""no_remote_peer"" => 0+      }+    end+    # wait until all RPCs are done, or we are past timeout+    while ($watchers[index]['rpcs_needed'] > 0) &&+          (Time.now.to_f < finish_time)+      sleep(0.1)+    end+    LoadBalancerStatsResponse.new(+      rpcs_by_peer: $watchers[index]['rpcs_by_peer'],+      num_failures: $watchers[index]['no_remote_peer'] ++      $watchers[index]['rpcs_needed']+    );+  end+end++# send 1 rpc every 1/qps second+def run_test_loop(stub, duration_per_query)",naming nit: for clarity can we rename this to `target_seconds_between_rpcs`,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/22950,426092617,2020-05-15T23:57:19Z,src/ruby/pb/test/xds_client.rb,"@@ -0,0 +1,193 @@+#!/usr/bin/env ruby++# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This is the xDS interop test Ruby client. This is meant to be run by+# the run_xds_tests.py test runner.+#+# Usage: $ tools/run_tests/run_xds_tests.py --test_case=... ...+#    --client_cmd=""path/to/xds_client.rb --server=<hostname> \+#                                        --stats_port=<port> \+#                                        --qps=<qps>""++# These lines are required for the generated files to load grpc+this_dir = File.expand_path(File.dirname(__FILE__))+lib_dir = File.join(File.dirname(File.dirname(this_dir)), 'lib')+pb_dir = File.dirname(this_dir)+$LOAD_PATH.unshift(lib_dir) unless $LOAD_PATH.include?(lib_dir)+$LOAD_PATH.unshift(pb_dir) unless $LOAD_PATH.include?(pb_dir)++require 'optparse'+require 'logger'++require_relative '../../lib/grpc'+require 'google/protobuf'++require_relative '../src/proto/grpc/testing/empty_pb'+require_relative '../src/proto/grpc/testing/messages_pb'+require_relative '../src/proto/grpc/testing/test_services_pb'++# Some global variables to be shared by server and client+$watchers = Array.new+$watchers_mutex = Mutex.new+$current_request_id = 0;++# RubyLogger defines a logger for gRPC based on the standard ruby logger.+module RubyLogger+  def logger+    LOGGER+  end++  LOGGER = Logger.new(STDOUT)+  LOGGER.level = Logger::INFO+end++# GRPC is the general RPC module+module GRPC+  # Inject the noop #logger if no module-level logger method has been injected.+  extend RubyLogger+end++# creates a test stub+def create_stub(opts)+  address = ""#{opts.server}""++  GRPC.logger.info(""... connecting insecurely to #{address}"")+  Grpc::Testing::TestService::Stub.new(+    address,+    :this_channel_is_insecure,+  )+end++# This implements LoadBalancerStatsService required by the test runner+class TestTarget < Grpc::Testing::LoadBalancerStatsService::Service+  include Grpc::Testing++  def get_client_stats(req, _call)+    start_id = $current_request_id + 1+    end_id = start_id + req['num_rpcs'] # watch the next X rpcs+    start_time = Time.now.to_f+    finish_time = start_time + req['timeout_sec']+    index = 0+    $watchers_mutex.synchronize do+      index = $watchers.length+      # Add a watcher for this range of id's+      $watchers << {+        ""start_id"" => start_id,+        ""end_id"" => end_id,+        ""rpcs_by_peer"" => Hash.new(0),+        ""rpcs_needed"" => req['num_rpcs'],+        ""no_remote_peer"" => 0+      }+    end+    # wait until all RPCs are done, or we are past timeout+    while ($watchers[index]['rpcs_needed'] > 0) &&+          (Time.now.to_f < finish_time)+      sleep(0.1)+    end+    LoadBalancerStatsResponse.new(+      rpcs_by_peer: $watchers[index]['rpcs_by_peer'],+      num_failures: $watchers[index]['no_remote_peer'] ++      $watchers[index]['rpcs_needed']+    );+  end+end++# send 1 rpc every 1/qps second+def run_test_loop(stub, duration_per_query)+  include Grpc::Testing+  req = SimpleRequest.new()++  while true+    start = Time.now.to_f","nit: can we move the QPS pacing logic all up to the point just before we make the RPC?Also, can we get use the monotonic clock rather than realtime?E.g. something like:```target_next_start = Process.clock_gettime(Process::CLOCK_MONOTONIC)while true  now = Process.clock_gettime(Process::CLOCK_MONOTONIC)  sleep_seconds = target_next_start - now  if sleep_seconds < 0    GRPC.logger.info(""ruby xds: warning, rpc takes too long to finish. "" \                       ""If you consistently see this, the qps is too high."")  else    sleep(sleep_seconds)  end  target_next_start = now + target_seconds_between_rpcs  begin    ....```",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/22950,426097641,2020-05-16T00:31:47Z,src/ruby/pb/test/xds_client.rb,"@@ -0,0 +1,193 @@+#!/usr/bin/env ruby++# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This is the xDS interop test Ruby client. This is meant to be run by+# the run_xds_tests.py test runner.+#+# Usage: $ tools/run_tests/run_xds_tests.py --test_case=... ...+#    --client_cmd=""path/to/xds_client.rb --server=<hostname> \+#                                        --stats_port=<port> \+#                                        --qps=<qps>""++# These lines are required for the generated files to load grpc+this_dir = File.expand_path(File.dirname(__FILE__))+lib_dir = File.join(File.dirname(File.dirname(this_dir)), 'lib')+pb_dir = File.dirname(this_dir)+$LOAD_PATH.unshift(lib_dir) unless $LOAD_PATH.include?(lib_dir)+$LOAD_PATH.unshift(pb_dir) unless $LOAD_PATH.include?(pb_dir)++require 'optparse'+require 'logger'++require_relative '../../lib/grpc'+require 'google/protobuf'++require_relative '../src/proto/grpc/testing/empty_pb'+require_relative '../src/proto/grpc/testing/messages_pb'+require_relative '../src/proto/grpc/testing/test_services_pb'++# Some global variables to be shared by server and client+$watchers = Array.new+$watchers_mutex = Mutex.new+$current_request_id = 0;++# RubyLogger defines a logger for gRPC based on the standard ruby logger.+module RubyLogger+  def logger+    LOGGER+  end++  LOGGER = Logger.new(STDOUT)+  LOGGER.level = Logger::INFO+end++# GRPC is the general RPC module+module GRPC+  # Inject the noop #logger if no module-level logger method has been injected.+  extend RubyLogger+end++# creates a test stub+def create_stub(opts)+  address = ""#{opts.server}""++  GRPC.logger.info(""... connecting insecurely to #{address}"")+  Grpc::Testing::TestService::Stub.new(+    address,+    :this_channel_is_insecure,+  )+end++# This implements LoadBalancerStatsService required by the test runner+class TestTarget < Grpc::Testing::LoadBalancerStatsService::Service+  include Grpc::Testing++  def get_client_stats(req, _call)+    start_id = $current_request_id + 1+    end_id = start_id + req['num_rpcs'] # watch the next X rpcs+    start_time = Time.now.to_f+    finish_time = start_time + req['timeout_sec']+    index = 0+    $watchers_mutex.synchronize do+      index = $watchers.length+      # Add a watcher for this range of id's+      $watchers << {+        ""start_id"" => start_id,+        ""end_id"" => end_id,+        ""rpcs_by_peer"" => Hash.new(0),+        ""rpcs_needed"" => req['num_rpcs'],+        ""no_remote_peer"" => 0+      }+    end+    # wait until all RPCs are done, or we are past timeout+    while ($watchers[index]['rpcs_needed'] > 0) &&","we can use a `ConditionVariable` to replace the sleeps here.Let's add a new global, `$watchers_cv = ConditionVariable.new`Then do something like this (this also removes `watcher` after it's no longer used)```$watchers_mutex.synchronize do  watcher = { ... }  $watchers << watcher  seconds_remaining = Process.clock_gettime(Process::CLOCK_MONOTONIC) - finish_time  while watcher['rpcs_needed'] > 0 && seconds_remaining > 0    $watchers_cv.wait($watchers_mutex, seconds_remaining)    seconds_remaining = Process.clock_gettime(Process::CLOCK_MONOTONIC) - finish_time  end  $watchers.delete_at($watchers.index watcher)endLoadBalancerStatsResponse.new(...)```",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/22950,426099724,2020-05-16T00:47:52Z,src/ruby/pb/test/xds_client.rb,"@@ -0,0 +1,193 @@+#!/usr/bin/env ruby++# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This is the xDS interop test Ruby client. This is meant to be run by+# the run_xds_tests.py test runner.+#+# Usage: $ tools/run_tests/run_xds_tests.py --test_case=... ...+#    --client_cmd=""path/to/xds_client.rb --server=<hostname> \+#                                        --stats_port=<port> \+#                                        --qps=<qps>""++# These lines are required for the generated files to load grpc+this_dir = File.expand_path(File.dirname(__FILE__))+lib_dir = File.join(File.dirname(File.dirname(this_dir)), 'lib')+pb_dir = File.dirname(this_dir)+$LOAD_PATH.unshift(lib_dir) unless $LOAD_PATH.include?(lib_dir)+$LOAD_PATH.unshift(pb_dir) unless $LOAD_PATH.include?(pb_dir)++require 'optparse'+require 'logger'++require_relative '../../lib/grpc'+require 'google/protobuf'++require_relative '../src/proto/grpc/testing/empty_pb'+require_relative '../src/proto/grpc/testing/messages_pb'+require_relative '../src/proto/grpc/testing/test_services_pb'++# Some global variables to be shared by server and client+$watchers = Array.new+$watchers_mutex = Mutex.new+$current_request_id = 0;++# RubyLogger defines a logger for gRPC based on the standard ruby logger.+module RubyLogger+  def logger+    LOGGER+  end++  LOGGER = Logger.new(STDOUT)+  LOGGER.level = Logger::INFO+end++# GRPC is the general RPC module+module GRPC+  # Inject the noop #logger if no module-level logger method has been injected.+  extend RubyLogger+end++# creates a test stub+def create_stub(opts)+  address = ""#{opts.server}""++  GRPC.logger.info(""... connecting insecurely to #{address}"")+  Grpc::Testing::TestService::Stub.new(+    address,+    :this_channel_is_insecure,+  )+end++# This implements LoadBalancerStatsService required by the test runner+class TestTarget < Grpc::Testing::LoadBalancerStatsService::Service+  include Grpc::Testing++  def get_client_stats(req, _call)+    start_id = $current_request_id + 1+    end_id = start_id + req['num_rpcs'] # watch the next X rpcs+    start_time = Time.now.to_f+    finish_time = start_time + req['timeout_sec']+    index = 0+    $watchers_mutex.synchronize do+      index = $watchers.length+      # Add a watcher for this range of id's+      $watchers << {+        ""start_id"" => start_id,+        ""end_id"" => end_id,+        ""rpcs_by_peer"" => Hash.new(0),+        ""rpcs_needed"" => req['num_rpcs'],+        ""no_remote_peer"" => 0+      }+    end+    # wait until all RPCs are done, or we are past timeout+    while ($watchers[index]['rpcs_needed'] > 0) &&+          (Time.now.to_f < finish_time)+      sleep(0.1)+    end+    LoadBalancerStatsResponse.new(+      rpcs_by_peer: $watchers[index]['rpcs_by_peer'],+      num_failures: $watchers[index]['no_remote_peer'] ++      $watchers[index]['rpcs_needed']+    );+  end+end++# send 1 rpc every 1/qps second+def run_test_loop(stub, duration_per_query)+  include Grpc::Testing+  req = SimpleRequest.new()++  while true+    start = Time.now.to_f+    begin+      resp = stub.unary_call(req)+      remote_peer = resp.hostname+    rescue GRPC::Unavailable => e+      remote_peer = """"+    end+    finish = Time.now.to_f+    delay = duration_per_query - (finish - start)+    if delay > 0+      sleep(delay)+    else+      GRPC.logger.info(""ruby xds: warning, rpc takes too long to finish. "" \+                       ""If you consistently see this, the qps is too high."")+    end+    $current_request_id += 1+    $watchers_mutex.synchronize do+      $watchers.each do |watcher|+        if (watcher['start_id'] <= $current_request_id) &&","nit: if we remove `watchers` from the `$watchers` list after we're done with them, then I think there's no need to have the `end_id` field within watcher objects at all.I think we can delete this conditional logic then and unconditionally run the body of this if statement",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/22704,426877784,2020-05-18T20:33:01Z,src/csharp/Grpc.IntegrationTesting/XdsInteropClient.cs,"@@ -0,0 +1,300 @@+#region Copyright notice and license++// Copyright 2020 The gRPC Authors+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#endregion++using System;+using System.Collections.Generic;+using System.Diagnostics;+using System.Threading;+using System.Threading.Tasks;++using CommandLine;+using Grpc.Core;+using Grpc.Core.Logging;+using Grpc.Core.Internal;+using Grpc.Testing;++namespace Grpc.IntegrationTesting+{+    public class XdsInteropClient+    {+        internal class ClientOptions+        {+            [Option(""num_channels"", Default = 1)]+            public int NumChannels { get; set; }++            [Option(""qps"", Default = 1)]++            // The desired QPS per channel.+            public int Qps { get; set; }++            [Option(""server"", Default = ""localhost:8080"")]+            public string Server { get; set; }++            [Option(""stats_port"", Default = 8081)]+            public int StatsPort { get; set; }++            [Option(""rpc_timeout_sec"", Default = 30)]+            public int RpcTimeoutSec { get; set; }++            [Option(""print_response"", Default = false)]+            public bool PrintResponse { get; set; }+        }++        ClientOptions options;++        StatsWatcher statsWatcher = new StatsWatcher();++        // make watcher accessible by tests+        internal StatsWatcher StatsWatcher => statsWatcher;++        internal XdsInteropClient(ClientOptions options)+        {+            this.options = options;+        }++        public static void Run(string[] args)+        {+            GrpcEnvironment.SetLogger(new ConsoleLogger());+            var parserResult = Parser.Default.ParseArguments<ClientOptions>(args)+                .WithNotParsed(errors => Environment.Exit(1))+                .WithParsed(options =>+                {+                    var xdsInteropClient = new XdsInteropClient(options);+                    xdsInteropClient.RunAsync().Wait();+                });+        }++        private async Task RunAsync()+        {+            var server = new Server+            {+                Services = { LoadBalancerStatsService.BindService(new LoadBalancerStatsServiceImpl(statsWatcher)) }+            };++            string host = ""0.0.0.0"";+            server.Ports.Add(host, options.StatsPort, ServerCredentials.Insecure);+            Console.WriteLine($""Running server on {host}:{options.StatsPort}"");+            server.Start();++            var cancellationTokenSource = new CancellationTokenSource();+            await RunChannelsAsync(cancellationTokenSource.Token);++            await server.ShutdownAsync();+        }++        // method made internal to make it runnable by tests+        internal async Task RunChannelsAsync(CancellationToken cancellationToken)+        {+            var channelTasks = new List<Task>();+            for (int channelId = 0; channelId < options.NumChannels; channelId++)+            {+                var channelTask = RunSingleChannelAsync(channelId, cancellationToken);+                channelTasks.Add(channelTask);+            }++            for (int channelId = 0; channelId < options.NumChannels; channelId++)+            {+                await channelTasks[channelId];+            }+        }++        private async Task RunSingleChannelAsync(int channelId, CancellationToken cancellationToken)+        {+            Console.WriteLine($""Starting channel {channelId}"");+            var channel = new Channel(options.Server, ChannelCredentials.Insecure);+            var client = new TestService.TestServiceClient(channel);++            var inflightTasks = new List<Task>();+            long rpcsStarted = 0;+            var stopwatch = Stopwatch.StartNew();+            while (!cancellationToken.IsCancellationRequested)+            {+                inflightTasks.Add(RunSingleRpcAsync(client, cancellationToken));+                rpcsStarted++;++                // only cleanup calls that have already completed, calls that are still inflight will be cleaned up later.+                await CleanupCompletedTasksAsync(inflightTasks);++                Console.WriteLine($""Currently {inflightTasks.Count} in-flight RPCs"");++                // if needed, wait a bit before we start the next RPC.+                int nextDueInMillis = (int) Math.Max(0, (1000 * rpcsStarted / options.Qps) - stopwatch.ElapsedMilliseconds);+                if (nextDueInMillis > 0)+                {+                    await Task.Delay(nextDueInMillis);+                }+            }+            stopwatch.Stop();++            Console.WriteLine($""Shutting down channel {channelId}"");+            await channel.ShutdownAsync();+            Console.WriteLine($""Channel shutdown {channelId}"");+        }++        private async Task RunSingleRpcAsync(TestService.TestServiceClient client, CancellationToken cancellationToken)+        {+            long rpcId = statsWatcher.RpcIdGenerator.Increment();+            try+            {+                Console.WriteLine($""Starting RPC {rpcId}."");+                var response = await client.UnaryCallAsync(new SimpleRequest(),+                    new CallOptions(cancellationToken: cancellationToken, deadline: DateTime.UtcNow.AddSeconds(options.RpcTimeoutSec)));+                +                statsWatcher.OnRpcComplete(rpcId, response.Hostname);+                if (options.PrintResponse)+                {+                    Console.WriteLine($""Got response {response}"");+                }+                Console.WriteLine($""RPC {rpcId} succeeded "");+            }+            catch (RpcException ex)+            {","in the [spec](https://github.com/grpc/grpc/blob/master/doc/xds-test-descriptions.md) there's an option to ""fail on failed RPCs"". But AFAICS we're not consuming that flag here.Should the code in the `catch` block here be checking that flag, and raising the error up, if set true?If this isn't important for now, but planned for later, then maybe just add a TODO?",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/22950,427041617,2020-05-19T05:40:05Z,src/ruby/pb/test/xds_client.rb,"@@ -0,0 +1,193 @@+#!/usr/bin/env ruby++# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This is the xDS interop test Ruby client. This is meant to be run by+# the run_xds_tests.py test runner.+#+# Usage: $ tools/run_tests/run_xds_tests.py --test_case=... ...+#    --client_cmd=""path/to/xds_client.rb --server=<hostname> \+#                                        --stats_port=<port> \+#                                        --qps=<qps>""++# These lines are required for the generated files to load grpc+this_dir = File.expand_path(File.dirname(__FILE__))+lib_dir = File.join(File.dirname(File.dirname(this_dir)), 'lib')+pb_dir = File.dirname(this_dir)+$LOAD_PATH.unshift(lib_dir) unless $LOAD_PATH.include?(lib_dir)+$LOAD_PATH.unshift(pb_dir) unless $LOAD_PATH.include?(pb_dir)++require 'optparse'+require 'logger'++require_relative '../../lib/grpc'+require 'google/protobuf'++require_relative '../src/proto/grpc/testing/empty_pb'+require_relative '../src/proto/grpc/testing/messages_pb'+require_relative '../src/proto/grpc/testing/test_services_pb'++# Some global variables to be shared by server and client+$watchers = Array.new+$watchers_mutex = Mutex.new+$current_request_id = 0;++# RubyLogger defines a logger for gRPC based on the standard ruby logger.+module RubyLogger+  def logger+    LOGGER+  end++  LOGGER = Logger.new(STDOUT)+  LOGGER.level = Logger::INFO+end++# GRPC is the general RPC module+module GRPC+  # Inject the noop #logger if no module-level logger method has been injected.+  extend RubyLogger+end++# creates a test stub+def create_stub(opts)+  address = ""#{opts.server}""++  GRPC.logger.info(""... connecting insecurely to #{address}"")+  Grpc::Testing::TestService::Stub.new(+    address,+    :this_channel_is_insecure,+  )+end++# This implements LoadBalancerStatsService required by the test runner+class TestTarget < Grpc::Testing::LoadBalancerStatsService::Service+  include Grpc::Testing++  def get_client_stats(req, _call)+    start_id = $current_request_id + 1+    end_id = start_id + req['num_rpcs'] # watch the next X rpcs+    start_time = Time.now.to_f+    finish_time = start_time + req['timeout_sec']+    index = 0+    $watchers_mutex.synchronize do+      index = $watchers.length+      # Add a watcher for this range of id's+      $watchers << {+        ""start_id"" => start_id,+        ""end_id"" => end_id,+        ""rpcs_by_peer"" => Hash.new(0),+        ""rpcs_needed"" => req['num_rpcs'],+        ""no_remote_peer"" => 0+      }+    end+    # wait until all RPCs are done, or we are past timeout+    while ($watchers[index]['rpcs_needed'] > 0) &&+          (Time.now.to_f < finish_time)+      sleep(0.1)+    end+    LoadBalancerStatsResponse.new(+      rpcs_by_peer: $watchers[index]['rpcs_by_peer'],+      num_failures: $watchers[index]['no_remote_peer'] ++      $watchers[index]['rpcs_needed']+    );+  end+end++# send 1 rpc every 1/qps second+def run_test_loop(stub, duration_per_query)+  include Grpc::Testing+  req = SimpleRequest.new()++  while true+    start = Time.now.to_f+    begin+      resp = stub.unary_call(req)+      remote_peer = resp.hostname+    rescue GRPC::Unavailable => e+      remote_peer = """"+    end+    finish = Time.now.to_f+    delay = duration_per_query - (finish - start)+    if delay > 0+      sleep(delay)+    else+      GRPC.logger.info(""ruby xds: warning, rpc takes too long to finish. "" \+                       ""If you consistently see this, the qps is too high."")+    end+    $current_request_id += 1+    $watchers_mutex.synchronize do+      $watchers.each do |watcher|+        if (watcher['start_id'] <= $current_request_id) &&","Yes this works. Basically the idea is that when we add a `watcher`, we are interested in the _next_ `num_rpcs`. So each subsequent completed rpcs will just decrement every `watcher`'s `num_rpcs` in the list by 1. When a `watcher` reaches `num_rpcs = 0` (or timed out), it's removed from the `watchers` list. So it can be reasoned that any successful rpc is useful to _every_ `watcher` currently still in the list.",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/22950,427056731,2020-05-19T06:24:31Z,src/ruby/pb/test/xds_client.rb,"@@ -0,0 +1,213 @@+#!/usr/bin/env ruby++# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This is the xDS interop test Ruby client. This is meant to be run by+# the run_xds_tests.py test runner.+#+# Usage: $ tools/run_tests/run_xds_tests.py --test_case=... ...+#    --client_cmd=""path/to/xds_client.rb --server=<hostname> \+#                                        --stats_port=<port> \+#                                        --qps=<qps>""++# These lines are required for the generated files to load grpc+this_dir = File.expand_path(File.dirname(__FILE__))+lib_dir = File.join(File.dirname(File.dirname(this_dir)), 'lib')+pb_dir = File.dirname(this_dir)+$LOAD_PATH.unshift(lib_dir) unless $LOAD_PATH.include?(lib_dir)+$LOAD_PATH.unshift(pb_dir) unless $LOAD_PATH.include?(pb_dir)++require 'optparse'+require 'logger'++require_relative '../../lib/grpc'+require 'google/protobuf'++require_relative '../src/proto/grpc/testing/empty_pb'+require_relative '../src/proto/grpc/testing/messages_pb'+require_relative '../src/proto/grpc/testing/test_services_pb'++# Some global variables to be shared by server and client+$watchers = Array.new+$watchers_mutex = Mutex.new+$watchers_cv = ConditionVariable.new+$shutdown = false++# RubyLogger defines a logger for gRPC based on the standard ruby logger.+module RubyLogger+  def logger+    LOGGER+  end++  LOGGER = Logger.new(STDOUT)+  LOGGER.level = Logger::INFO+end++# GRPC is the general RPC module+module GRPC+  # Inject the noop #logger if no module-level logger method has been injected.+  extend RubyLogger+end++# creates a test stub+def create_stub(opts)+  address = ""#{opts.server}""++  GRPC.logger.info(""... connecting insecurely to #{address}"")+  Grpc::Testing::TestService::Stub.new(+    address,+    :this_channel_is_insecure,+  )+end++# This implements LoadBalancerStatsService required by the test runner+class TestTarget < Grpc::Testing::LoadBalancerStatsService::Service+  include Grpc::Testing++  def get_client_stats(req, _call)+    finish_time = Process.clock_gettime(Process::CLOCK_MONOTONIC) ++                  req['timeout_sec']+    watcher = {}+    $watchers_mutex.synchronize do+      watcher = {+        ""rpcs_by_peer"" => Hash.new(0),+        ""rpcs_needed"" => req['num_rpcs'],+        ""no_remote_peer"" => 0+      }+      $watchers << watcher+      seconds_remaining = finish_time -+                          Process.clock_gettime(Process::CLOCK_MONOTONIC)+      while watcher['rpcs_needed'] > 0 && seconds_remaining > 0+        $watchers_cv.wait($watchers_mutex, seconds_remaining)+        seconds_remaining = finish_time -+                            Process.clock_gettime(Process::CLOCK_MONOTONIC)+      end+      $watchers.delete_at($watchers.index(watcher))+    end+    LoadBalancerStatsResponse.new(+      rpcs_by_peer: watcher['rpcs_by_peer'],+      num_failures: watcher['no_remote_peer'] + watcher['rpcs_needed']","What if `timeout_sec` is hit, but not all the rpcs are sent? I thought that was the intent that those should be counted as `num_failures`. I copied that logic from the c++ xds interop client here: https://github.com/grpc/grpc/blob/master/test/cpp/interop/xds_interop_client.cc#L107. But it would be good if Eric can confirm. ",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/22948,427152666,2020-05-19T09:14:53Z,src/core/lib/surface/channel.cc,"@@ -143,10 +144,15 @@ static grpc_core::UniquePtr<char> get_default_authority(     } else if (0 == strcmp(input_args->args[i].key,                            GRPC_SSL_TARGET_NAME_OVERRIDE_ARG)) {       ssl_override = grpc_channel_arg_get_string(&input_args->args[i]);+    } else if (0 == strcmp(input_args->args[i].key,+                           GRPC_ALTS_TARGET_NAME_OVERRIDE_ARG)) {","Done, I moved both the SSL and ALTS override codes into the channel_credentials [update_arguments](https://github.com/grpc/grpc/blob/83dd71dfe58253b4ceb3cf394a939f70adc7ea95/src/core/lib/security/credentials/credentials.h#L128) hook.This then orphaned this function in `src/core/lib/surface/channel.cc`.",X
2030323,JunTaoLuo,https://api.github.com/repos/grpc/grpc/pulls/22895,427499074,2020-05-19T18:06:04Z,src/csharp/Grpc.Tools/build/_protobuf/Google.Protobuf.Tools.targets,"@@ -115,27 +116,33 @@           DependsOnTargets=""Protobuf_Compile""           Condition="" '$(Language)' == 'C#' "" /> -  <Target Name=""_Protobuf_SelectFiles"">+  <Target Name=""_Protobuf_SetProtoRoot"">     <!-- Guess .proto root for the files. Whenever the root is set for a file explicitly,          leave it as is. Otherwise, for files under the project directory, set the root          to ""."" for the project's directory, as it is the current when compiling; for the-         files outside of project directory, use each .proto file's directory as the root. -->+         files outside of project directory, use each .proto file's directory as the root +		 or Protobuf_ProtoRoot if set. -->     <FindUnderPath Path=""$(MSBuildProjectDirectory)""                    Files=""@(Protobuf->WithMetadataValue('ProtoRoot',''))"">       <Output TaskParameter=""InPath"" ItemName=""_Protobuf_NoRootInProject""/>       <Output TaskParameter=""OutOfPath"" ItemName=""_Protobuf_NoRootElsewhere""/>     </FindUnderPath>+	+    <ItemGroup>+      <Protobuf>+		<!-- In-project files will have ProtoRoot='.'. -->+        <ProtoRoot Condition="" '%(ProtoRoot)' == '' and '@(_Protobuf_NoRootInProject)' != '' "">.</ProtoRoot>","@jtattermusch In MSBuild, you can modify or update the metadata of the ItemGroup with an entry under the ItemGroup node. So in this case, it seems like the logic wants to set the `ProtoRoot` metadata of `Protobuf` items if it's not already set based on `_Protobuf_NoRootInProject` and `_Protobuf_NoRootElsewhere `. However I'm not sure if it would work correctly.  `_Protobuf_NoRootInProject` and `_Protobuf_NoRootElsewhere ` are ItemGroups so '@(_Protobuf_NoRootElsewhere)' != '' would evaluate to true as long as those items contain a single element. However, the logic doesn't take into account whether the `Protobuf`item belongs to one group or the other.For example, currently, if a.proto was placed in `_Protobuf_NoRootInProject` and b.proto was placed in  `_Protobuf_NoRootElsewhere` both would have their ProtoRoot set to `%(RelativeDir)` since both conditions are true and the second metadata update overrides the first.",
940619,Falco20019,https://api.github.com/repos/grpc/grpc/pulls/22895,427535019,2020-05-19T19:06:40Z,src/csharp/Grpc.Tools/build/_protobuf/Google.Protobuf.Tools.targets,"@@ -115,27 +116,33 @@           DependsOnTargets=""Protobuf_Compile""           Condition="" '$(Language)' == 'C#' "" /> -  <Target Name=""_Protobuf_SelectFiles"">+  <Target Name=""_Protobuf_SetProtoRoot"">     <!-- Guess .proto root for the files. Whenever the root is set for a file explicitly,          leave it as is. Otherwise, for files under the project directory, set the root          to ""."" for the project's directory, as it is the current when compiling; for the-         files outside of project directory, use each .proto file's directory as the root. -->+         files outside of project directory, use each .proto file's directory as the root +		 or Protobuf_ProtoRoot if set. -->     <FindUnderPath Path=""$(MSBuildProjectDirectory)""                    Files=""@(Protobuf->WithMetadataValue('ProtoRoot',''))"">       <Output TaskParameter=""InPath"" ItemName=""_Protobuf_NoRootInProject""/>       <Output TaskParameter=""OutOfPath"" ItemName=""_Protobuf_NoRootElsewhere""/>     </FindUnderPath>+	+    <ItemGroup>+      <Protobuf>+		<!-- In-project files will have ProtoRoot='.'. -->+        <ProtoRoot Condition="" '%(ProtoRoot)' == '' and '@(_Protobuf_NoRootInProject)' != '' "">.</ProtoRoot>","Are you sure? I‘m not too deep into MSBuild, but based on what I saw in the use cases, it seems that `<MetaName Condition="" '@(GroupName)' != '' "">` in a item groups metadata condition checks if `%(Identity)` is part of `GroupName` and returns the value. I will try to verify the behavior. Thanks for the input!",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/22986,427673731,2020-05-20T00:23:41Z,examples/php/composer.json,"@@ -7,7 +7,9 @@   },   ""autoload"": {     ""psr-4"": {-      """": ""route_guide/""+      """": ""route_guide/"",","Can you see if you can fix this line as well? Instead of a wildcard (everything) class matching to the `route_guide/` directory, can you explicitly mark them as `""Routeguide\\"": ""route_guide/""`? The code is here: https://github.com/grpc/grpc/blob/master/examples/php/route_guide/route_guide_client.phpBut in doing so, can you please make sure https://grpc.io/docs/tutorials/basic/php/ is still runnable correctly? Thanks.",
49655798,jeffreyqw,https://api.github.com/repos/grpc/grpc/pulls/22986,427743747,2020-05-20T05:07:17Z,examples/php/composer.json,"@@ -7,7 +7,9 @@   },   ""autoload"": {     ""psr-4"": {-      """": ""route_guide/""+      """": ""route_guide/"",+      ""GPBMetadata\\"": ""GPBMetadata/"",","I got class 'GPBMetadata\Helloworld' not found, if remove this line",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23003,428189455,2020-05-20T17:35:20Z,src/csharp/Grpc.Core/Internal/AsyncCall.cs,"@@ -78,7 +78,12 @@ public TResponse UnaryCall(TRequest msg)             var profiler = Profilers.ForCurrentThread();              using (profiler.NewScope(""AsyncCall.UnaryCall""))-            using (CompletionQueueSafeHandle cq = CompletionQueueSafeHandle.CreateSync())++            // Create a pluckable completion queue for the call. Avoid creating a completion queue when we know the channel has already+            // been shutdown. In such case, the call will fail with ObjectDisposedException immediately anyway and creating / destroying+            // a completion queue would lead to crash if this was the last channel in the application (and thus GrpcEnvironment has been shutdown).+            // See https://github.com/grpc/grpc/issues/19090+            using (CompletionQueueSafeHandle cq = details.Channel.Handle.IsClosed ?  null : CompletionQueueSafeHandle.CreateSync())","this seems racey, because the channel could still be destroyed from another thread, after creating the cq here, but before the cq destructor runs, right?Can we fix this instead by having not only the channel but also the completion queue also take a ""ref"" to the gRPC library? E.g. add a `GrpcEnvironment.GrpcNativeInit();` and `GrpcEnvironment.GrpcNativeShutdown` pair to `CompletionQueueSafeHandle`'s constructor and ReleaseHandle?",
49655798,jeffreyqw,https://api.github.com/repos/grpc/grpc/pulls/22986,428326583,2020-05-20T21:47:22Z,examples/php/composer.json,"@@ -7,7 +7,9 @@   },   ""autoload"": {     ""psr-4"": {-      """": ""route_guide/""+      """": ""route_guide/"",","I have verified, it's working fine. still need to keep ""GPBMetadata"" line.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23007,428341155,2020-05-20T22:21:56Z,src/python/grpcio_tests/tests_aio/unit/channel_ready_test.py,"@@ -32,8 +32,8 @@ class TestChannelReady(AioTestBase):      async def setUp(self):         address, self._port, self._socket = get_socket(listen=False)-        self._channel = aio.insecure_channel(f""{address}:{self._port}"")         self._socket.close()+        self._channel = aio.insecure_channel(f""{address}:{self._port}"")","I'm not sure, but the chance of kernel preempt between the socket close and channel creation should be much smaller than 10% (current flake rate). For the metadata flag test, the socket close introduces a flake rate around 0.045%. This PR selects a less worse way of flake.BTW, what do you think could be the culprit of this flake?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23007,428346388,2020-05-20T22:35:26Z,src/python/grpcio_tests/tests_aio/unit/channel_ready_test.py,"@@ -32,8 +32,8 @@ class TestChannelReady(AioTestBase):      async def setUp(self):         address, self._port, self._socket = get_socket(listen=False)-        self._channel = aio.insecure_channel(f""{address}:{self._port}"")         self._socket.close()+        self._channel = aio.insecure_channel(f""{address}:{self._port}"")","Ah. I didn't have the full context for this test before. So it looks like the channel is somehow reaching a ready state when connecting to a socket with no process on the other end. That's very odd.Since we're binding with SO_REUSEPORT, I suppose it's possible that some other process on the test runner machine also bound to that port as a gRPC server. I once ran into that issue where I would occasionally connect to a gRPC service on the bazel daemon. Maybe you could just disable `SO_REUSEPORT` and listen on the socket? Then 1. The channel should connect and never come ready, as intended. 2. No other process should be able to listen on that port.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22936,429737587,2020-05-25T05:31:52Z,BUILD,"@@ -301,6 +301,7 @@ grpc_cc_library(     language = ""c++"",     public_hdrs = GPR_PUBLIC_HDRS,     standalone = True,+    visibility = [""//visibility:public""],","how were the list of ""public"" libraries established? Did you just hand-picked those? How can we make sure the list is complete and there's nothing missing?AFAIK not all the public libraries are actually in the /BUILD file, there are also some public items in some of the subdirectories.",
940619,Falco20019,https://api.github.com/repos/grpc/grpc/pulls/22895,429748061,2020-05-25T06:13:21Z,src/csharp/Grpc.Tools/build/_protobuf/Google.Protobuf.Tools.targets,"@@ -115,27 +116,33 @@           DependsOnTargets=""Protobuf_Compile""           Condition="" '$(Language)' == 'C#' "" /> -  <Target Name=""_Protobuf_SelectFiles"">+  <Target Name=""_Protobuf_SetProtoRoot"">     <!-- Guess .proto root for the files. Whenever the root is set for a file explicitly,          leave it as is. Otherwise, for files under the project directory, set the root          to ""."" for the project's directory, as it is the current when compiling; for the-         files outside of project directory, use each .proto file's directory as the root. -->+         files outside of project directory, use each .proto file's directory as the root +		 or Protobuf_ProtoRoot if set. -->     <FindUnderPath Path=""$(MSBuildProjectDirectory)""                    Files=""@(Protobuf->WithMetadataValue('ProtoRoot',''))"">       <Output TaskParameter=""InPath"" ItemName=""_Protobuf_NoRootInProject""/>       <Output TaskParameter=""OutOfPath"" ItemName=""_Protobuf_NoRootElsewhere""/>     </FindUnderPath>+	+    <ItemGroup>+      <Protobuf>+		<!-- In-project files will have ProtoRoot='.'. -->+        <ProtoRoot Condition="" '%(ProtoRoot)' == '' and '@(_Protobuf_NoRootInProject)' != '' "">.</ProtoRoot>","@JunTaoLuo I found out why my test was working. You were right, it is overwritten. But for `protos\config.proto` the `%(RelativeDir)` was `.`.I will update the target file to use the original logic but with a separate item group. I only wanted to avoid having a duplicate of the group until necessary.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23045,430594734,2020-05-26T17:46:25Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -209,10 +205,13 @@ def time_remaining(self) -> Optional[float]:         return self._cython_call.time_remaining()      async def initial_metadata(self) -> MetadataType:-        return await self._cython_call.initial_metadata()+        raw_metadata_tuple = await self._cython_call.initial_metadata()+        return Metadata(*(raw_metadata_tuple or ()))",Should we make `Metadata` take in tuple directly? We could perform some type checking in its `__init__` function. WDYT?EDIT: Richard's comment about adding a method is cleaner.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23045,430595789,2020-05-26T17:48:12Z,src/python/grpcio/grpc/experimental/aio/_typing.py,"@@ -13,17 +13,18 @@ # limitations under the License. """"""Common types for gRPC Async API"""""" -from typing import (Any, AnyStr, AsyncIterable, Callable, Iterable, Sequence,-                    Tuple, TypeVar, Union)+from typing import (Any, AsyncIterable, Callable, Iterable, Sequence, Tuple,+                    TypeVar, Union)  from grpc._cython.cygrpc import EOF+from ._metadata import Metadata, MetadataKey, MetadataValue  RequestType = TypeVar('RequestType') ResponseType = TypeVar('ResponseType') SerializingFunction = Callable[[Any], bytes] DeserializingFunction = Callable[[bytes], Any]-MetadatumType = Tuple[str, AnyStr]-MetadataType = Sequence[MetadatumType]+MetadatumType = Tuple[MetadataKey, MetadataValue]",`MetadataKey` is pointing to `Text` which is used for Python 2/3 compatibility. Shall we change it to `str` to be more accurate in Python 3?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23045,430596438,2020-05-26T17:49:15Z,src/python/grpcio/grpc/experimental/aio/_typing.py,"@@ -13,17 +13,18 @@ # limitations under the License. """"""Common types for gRPC Async API"""""" -from typing import (Any, AnyStr, AsyncIterable, Callable, Iterable, Sequence,-                    Tuple, TypeVar, Union)+from typing import (Any, AsyncIterable, Callable, Iterable, Sequence, Tuple,+                    TypeVar, Union)  from grpc._cython.cygrpc import EOF+from ._metadata import Metadata, MetadataKey, MetadataValue  RequestType = TypeVar('RequestType') ResponseType = TypeVar('ResponseType') SerializingFunction = Callable[[Any], bytes] DeserializingFunction = Callable[[bytes], Any]-MetadatumType = Tuple[str, AnyStr]-MetadataType = Sequence[MetadatumType]+MetadatumType = Tuple[MetadataKey, MetadataValue]+MetadataType = Metadata","Should we keep the backward compatibility? Like `Union[Sequence[MetadatumType], Metadata]`.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23045,430598500,2020-05-26T17:52:52Z,src/python/grpcio_tests/tests_aio/unit/metadata_test.py,"@@ -56,20 +59,12 @@     ),     (         TypeError,-        (({}, {}),),+        ((None, {}),),",The invalid metadata test is testing if our library is resistant against badly formed metadata. Maybe we should test both cases?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23045,430599119,2020-05-26T17:53:55Z,src/python/grpcio_tests/tests_aio/unit/metadata_test.py,"@@ -105,13 +100,13 @@ async def _test_client_to_server(request, context):     async def _test_server_to_client(request, context):         assert _REQUEST == request         await context.send_initial_metadata(-            _INITIAL_METADATA_FROM_SERVER_TO_CLIENT)+            tuple(_INITIAL_METADATA_FROM_SERVER_TO_CLIENT))",Maybe we can perform the type conversion inside `send_initial_metadata`? It should better reflect the fact that the `Metadata` type is supported officially.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23045,430600086,2020-05-26T17:55:27Z,src/python/grpcio_tests/tests_aio/unit/metadata_test.py,"@@ -212,8 +208,8 @@ async def test_trailing_metadata(self):      async def test_from_client_to_server_with_list(self):         multicallable = self._client.unary_unary(_TEST_CLIENT_TO_SERVER)-        call = multicallable(-            _REQUEST, metadata=list(_INITIAL_METADATA_FROM_CLIENT_TO_SERVER))+        call = multicallable(_REQUEST,+                             metadata=_INITIAL_METADATA_FROM_CLIENT_TO_SERVER)","This test is explicitly guarding that we accept lists. Unfortunately, one of our big users depends on this behavior due to previous unclear definition of ""sequence"".",
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/23009,430657936,2020-05-26T19:34:53Z,tools/run_tests/run_xds_tests.py,"@@ -499,6 +535,93 @@ def test_secondary_locality_gets_requests_on_primary_failure(         patch_backend_instances(gcp, backend_service, [primary_instance_group])  +def test_traffic_splitting(gcp, original_backend_service, instance_group,+                           alternate_backend_service, same_zone_instance_group):+    # This test start with all traffic going to original_backend_service. Then+    # it updates URL-map to set default action to traffic splitting between+    # original and alternate. It waits for all backends in both services to+    # receive traffic, then verifies that weights are expected.+    logger.info('Running test_traffic_splitting')++    logger.info('waiting for original to become healthy')+    wait_for_healthy_backends(gcp, original_backend_service, instance_group)++    patch_backend_instances(gcp, alternate_backend_service,+                            [same_zone_instance_group])+    logger.info('waiting for alternate to become healthy')+    wait_for_healthy_backends(gcp, alternate_backend_service,+                              same_zone_instance_group)++    original_backend_instances = get_instance_names(gcp, instance_group)+    logger.info('original backends instances: %s', original_backend_instances)++    alternate_backend_instances = get_instance_names(gcp,+                                                     same_zone_instance_group)+    logger.info('alternate backends instances: %s', alternate_backend_instances)++    # Start with all traffic going to original_backend_service.+    logger.info('waiting for traffic to all go to original')+    wait_until_all_rpcs_go_to_given_backends(original_backend_instances,+                                             _WAIT_FOR_STATS_SEC)++    try:+        # Path urlmap, change route action to traffic splitting between original+        # and alternate.+        logger.info('patching url map with traffic splitting')+        expected_service_percentage = [20, 80]+        patch_url_map_weighted_backend_services(+            gcp, {+                original_backend_service: expected_service_percentage[0],+                alternate_backend_service: expected_service_percentage[1],+            })+        expected_instance_percentage = [+            expected_service_percentage[0] * 1.0 /+            len(original_backend_instances)+        ] * len(original_backend_instances) + [+            expected_service_percentage[1] * 1.0 /+            len(alternate_backend_instances)+        ] * len(alternate_backend_instances)++        # Wait for traffic to go to both services.+        logger.info(+            'waiting for traffic to go to all backends (including alternate)')+        wait_until_all_rpcs_go_to_given_backends(+            original_backend_instances + alternate_backend_instances,+            _WAIT_FOR_STATS_SEC)++        # Verify that weights between two services are expected.+        retry_count = 3","Changed to 10 tries, that's about 100 seconds. Would it be long enough?",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/23045,430675984,2020-05-26T20:03:48Z,src/python/grpcio/grpc/experimental/aio/_typing.py,"@@ -13,17 +13,18 @@ # limitations under the License. """"""Common types for gRPC Async API"""""" -from typing import (Any, AnyStr, AsyncIterable, Callable, Iterable, Sequence,-                    Tuple, TypeVar, Union)+from typing import (Any, AsyncIterable, Callable, Iterable, Sequence, Tuple,+                    TypeVar, Union)  from grpc._cython.cygrpc import EOF+from ._metadata import Metadata, MetadataKey, MetadataValue  RequestType = TypeVar('RequestType') ResponseType = TypeVar('ResponseType') SerializingFunction = Callable[[Any], bytes] DeserializingFunction = Callable[[bytes], Any]-MetadatumType = Tuple[str, AnyStr]-MetadataType = Sequence[MetadatumType]+MetadatumType = Tuple[MetadataKey, MetadataValue]+MetadataType = Metadata","Maybe I'm missing something, but if I'm not wrong the `MetadataType` is only used internally or for returning a metadata object to the user, never for getting data from the user. If this is the case we wouldn't need to keep backward compatibility, indeed we could remove `MetadataType` and just annotate the attributes with a simple `Metadata`.am I wrong?",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/23045,430681823,2020-05-26T20:15:11Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -88,8 +84,8 @@ def __init__(self,         super().__init__(self)         self._code = code         self._details = details-        self._initial_metadata = initial_metadata-        self._trailing_metadata = trailing_metadata+        self._initial_metadata = Metadata(*(initial_metadata or ()))+        self._trailing_metadata = Metadata(*(trailing_metadata or ()))","If Im not wrong most of the signatures that we have for returning metadata are `Optional[MetadataType]` [1] , meaning that if there is no metadata a `None` is returned. If this is the case every time that we build a metadata object we should do that if and only if there are values, having something like this:```pythonself._trailing_metadata = Metadata(*initial_metadata) if initial_metadata is not None else None```Also, note that maybe for avoiding the expansion - which might have a little impact - we could change the contractor of the `Metadata` for taking one parameter that would be the sequence.[1] https://github.com/grpc/grpc/pull/23045/files#diff-802047f7d320089cb1d9c7de77c9cae5R107",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/23045,430683184,2020-05-26T20:17:54Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -209,10 +205,13 @@ def time_remaining(self) -> Optional[float]:         return self._cython_call.time_remaining()      async def initial_metadata(self) -> MetadataType:-        return await self._cython_call.initial_metadata()+        raw_metadata_tuple = await self._cython_call.initial_metadata()+        return Metadata(*(raw_metadata_tuple or ()))","+1And we should revisit the return type, either go for `Optional[MetadatType]` or `MetadataType` but not both of them mixed.For keeping backward compatibility as much as possible we would need to go for `Optional[MetadatType]`.",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/23045,430685750,2020-05-26T20:22:46Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -89,6 +89,19 @@ def __init__(         self._response_deserializer = response_deserializer         self._interceptors = interceptors +    @staticmethod+    def _init_metadata(metadata: Optional[Metadata] = None,+                       compression: Optional[grpc.Compression] = None+                      ) -> Metadata:+        """"""Based on the provided values for <metadata> or <compression> initialise the final+        metadata, as it should be used for the current call.+        """"""+        metadata = metadata or Metadata()+        if compression:+            metadata = Metadata(+                *_compression.augment_metadata(metadata, compression))","Most likely Im missing something, but I would say that `augment_metadata` is only dealing with tuples, not `Metadata` objects. If the first parameter is None, automatically this function will receive as a first parameter an empty `Metadata`.Should we just first make the augmentation - new tuple created - and then, later on, create the metadata object?am I wrong?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23045,430703401,2020-05-26T20:57:12Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -209,10 +205,13 @@ def time_remaining(self) -> Optional[float]:         return self._cython_call.time_remaining()      async def initial_metadata(self) -> MetadataType:-        return await self._cython_call.initial_metadata()+        raw_metadata_tuple = await self._cython_call.initial_metadata()+        return Metadata(*(raw_metadata_tuple or ()))","Alternatively, you could add a staticmethod `Metadata.from_tuple`. Then you wouldn't incur the cost of runtime type checks.",
2030323,JunTaoLuo,https://api.github.com/repos/grpc/grpc/pulls/22895,430708258,2020-05-26T21:07:20Z,src/csharp/Grpc.Tools/build/_protobuf/Google.Protobuf.Tools.targets,"@@ -346,15 +356,15 @@        Protobuf_ExpectedOutputs with all possible output. An option is to include        all existing outputs using Include with a wildcard, if you know where to look. -       Note this is like Protobuf_PrepareCompile, but uses @(Protobuf) regardless+       Note this is like Protobuf_PrepareCompile, but uses @(Protobuf_Rooted) regardless        of the Compile metadata, to remove all possible outputs. Plugins should err        on the side of overextending the Protobuf_ExpectedOutputs here.         All ExpectedOutputs will be removed. -->-  <Target Name=""Protobuf_PrepareClean"" Condition="" '@(Protobuf)' != '' "">+  <Target Name=""Protobuf_PrepareClean"" Condition="" '@(Protobuf_Rooted)' != '' "">","Same here, I think it would make sense for Protobuf_PrepareClean to depend on _Protobuf_SetProtoRoot, especially since this target isn't prefixed with _ which generally signals that it could be called independently.",
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/23045,431076496,2020-05-27T12:25:31Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -88,8 +84,8 @@ def __init__(self,         super().__init__(self)         self._code = code         self._details = details-        self._initial_metadata = initial_metadata-        self._trailing_metadata = trailing_metadata+        self._initial_metadata = Metadata(*(initial_metadata or ()))+        self._trailing_metadata = Metadata(*(trailing_metadata or ()))","In the cases where it was returning ``None`` it's better to now return an empty metadata object now that we have the more specific type, as that will allow us to treat all calls to these methods the same (without exceptions of the kind ``if metadata is not None: ... ``), so I'd replace the ``Optional[MetadataType]`` by ``Metadata``. WDYT?",
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/23045,431088105,2020-05-27T12:44:53Z,src/python/grpcio/grpc/experimental/aio/_channel.py,"@@ -89,6 +89,19 @@ def __init__(         self._response_deserializer = response_deserializer         self._interceptors = interceptors +    @staticmethod+    def _init_metadata(metadata: Optional[Metadata] = None,+                       compression: Optional[grpc.Compression] = None+                      ) -> Metadata:+        """"""Based on the provided values for <metadata> or <compression> initialise the final+        metadata, as it should be used for the current call.+        """"""+        metadata = metadata or Metadata()+        if compression:+            metadata = Metadata(+                *_compression.augment_metadata(metadata, compression))","Yes, but ``augment_metadata``, internally does ``tuple(metadata)``, which works for the metadata object, so that remains as before.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23045,431299215,2020-05-27T16:57:47Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -88,8 +84,8 @@ def __init__(self,         super().__init__(self)         self._code = code         self._details = details-        self._initial_metadata = initial_metadata-        self._trailing_metadata = trailing_metadata+        self._initial_metadata = Metadata(*(initial_metadata or ()))+        self._trailing_metadata = Metadata(*(trailing_metadata or ()))",Agree. Deterministic `Metadata` is better.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22985,431339384,2020-05-27T17:59:51Z,src/compiler/python_generator.cc,"@@ -777,7 +777,8 @@ pair<bool, grpc::string> PrivateGenerator::GetGrpcServices() {     if (generate_in_pb2_grpc) {       out->Print(           ""# Generated by the gRPC Python protocol compiler plugin. ""-          ""DO NOT EDIT!\n"");+          ""DO NOT EDIT!\n\""\""\""""+          ""Protocol buffer generated client and server classes.\""\""\""\n"");","The current wording could be interpreted as ""Classes generated *by* protocol buffers."" Perhaps ""Client and server classes corresponding to protobuf-defined services.""?",
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/23045,431661695,2020-05-28T08:18:29Z,src/python/grpcio/grpc/experimental/aio/_typing.py,"@@ -13,17 +13,18 @@ # limitations under the License. """"""Common types for gRPC Async API"""""" -from typing import (Any, AnyStr, AsyncIterable, Callable, Iterable, Sequence,-                    Tuple, TypeVar, Union)+from typing import (Any, AsyncIterable, Callable, Iterable, Sequence, Tuple,+                    TypeVar, Union)  from grpc._cython.cygrpc import EOF+from ._metadata import Metadata, MetadataKey, MetadataValue  RequestType = TypeVar('RequestType') ResponseType = TypeVar('ResponseType') SerializingFunction = Callable[[Any], bytes] DeserializingFunction = Callable[[bytes], Any]-MetadatumType = Tuple[str, AnyStr]-MetadataType = Sequence[MetadatumType]+MetadatumType = Tuple[MetadataKey, MetadataValue]+MetadataType = Metadata","@pfreixes I agree, I'm replacing the ``MetadataType`` by ``Metadata``",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/23020,432127099,2020-05-28T21:13:35Z,src/php/lib/Grpc/AbstractCall.php,"@@ -141,12 +134,7 @@ protected function _deserializeResponse($value)         if (is_array($this->deserialize)) {             list($className, $deserializeFunc) = $this->deserialize;             $obj = new $className();-            if (method_exists($obj, $deserializeFunc)) {-                $obj->$deserializeFunc($value);-            } else {-                $obj->mergeFromString($value);-            }-+            $obj->mergeFromString($value);             return $obj;         } ","Can you also remove the next 2 lines ```        // Protobuf-PHP implementation        return call_user_func($this->deserialize, $value);```Then this is parallel to the work you did up in `_serializeMessage`.Then you can also remove the `if (is_array($this->deserialize)) {` wrap.So the entire `_deserializeResponse` function should be something like```    protected function _deserializeResponse($value)    {        if ($value === null) {            return;        }        list($className, $deserializeFunc) = $this->deserialize;        $obj = new $className();        $obj->mergeFromString($value);        return $obj;    }```",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/22985,432193635,2020-05-29T00:24:26Z,src/compiler/python_generator.cc,"@@ -777,7 +777,8 @@ pair<bool, grpc::string> PrivateGenerator::GetGrpcServices() {     if (generate_in_pb2_grpc) {       out->Print(           ""# Generated by the gRPC Python protocol compiler plugin. ""-          ""DO NOT EDIT!\n"");+          ""DO NOT EDIT!\n\""\""\""""+          ""Protocol buffer generated client and server classes.\""\""\""\n"");","As for your other PR, things are a bit trickier on the `_pb2.py` side. The contents of those files can't be summed up as crisply as ""classes"". Maybe just ""Generated protocol buffer code.""",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/23091,432696591,2020-05-29T19:40:23Z,tools/run_tests/run_xds_tests.py,"@@ -569,6 +569,8 @@ def test_traffic_splitting(gcp, original_backend_service, instance_group,     try:         # Patch urlmap, change route action to traffic splitting between         # original and alternate.+        logger.info('disabling validateForProxy in target proxy')+        set_validate_for_proxyless(gcp, False)","If possible, can you also add a note about why this is necessary for the test to pass, and when/what conditions would enable removing this line in the future?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23094,432744289,2020-05-29T21:30:12Z,tools/distrib/python/grpc_prefixed/generate.py,"@@ -0,0 +1,138 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Generates grpc-prefixed packages using template renderer.++To use this script, please use 3.7+ interpreter. This script is work-directory+agnostic. A quick executable command:++    python3 tools/distrib/python/grpc_prefixed/generate.py+""""""++import dataclasses+import logging+import os+import shutil+import subprocess+import sys++import jinja2++WORK_PATH = os.path.realpath(os.path.dirname(__file__))+LICENSE = os.path.join(WORK_PATH, '../../../../LICENSE')+BUILD_PATH = os.path.join(WORK_PATH, 'build')+DIST_PATH = os.path.join(WORK_PATH, 'dist')++env = jinja2.Environment(+    loader=jinja2.FileSystemLoader(os.path.join(WORK_PATH, 'templates')))++LOGGER = logging.getLogger(__name__)+++@dataclasses.dataclass+class PackageMeta:+    """"""Meta-info of a PyPI package.""""""+    name: str+    name_long: str+    destination_package: str+    version: str = '1.0.0'+++def clean() -> None:+    try:+        shutil.rmtree(BUILD_PATH)+    except FileNotFoundError:+        pass++    try:+        shutil.rmtree(DIST_PATH)+    except FileNotFoundError:+        pass+++def generate_package(meta: PackageMeta) -> None:+    # Makes package directory+    package_path = os.path.join(BUILD_PATH, meta.name)+    os.makedirs(package_path, exist_ok=True)++    # Copy license+    shutil.copyfile(LICENSE, os.path.join(package_path, 'LICENSE'))++    # Generates source code+    for template_name in env.list_templates():+        template = env.get_template(template_name)+        with open(+                os.path.join(package_path,+                             template_name.replace('.template', '')), 'w') as f:+            f.write(template.render(dataclasses.asdict(meta)))++    # Creates wheel+    job = subprocess.Popen([+        sys.executable,+        os.path.join(package_path, 'setup.py'), 'sdist', '--dist-dir', DIST_PATH+    ],+                           cwd=package_path,+                           stdout=subprocess.PIPE,","If the subprocess creates too much output, this pipe could fill up and [deadlock the process](https://docs.python.org/3/library/subprocess.html#subprocess.Popen.wait).",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23094,432746699,2020-05-29T21:37:14Z,tools/distrib/python/grpc_prefixed/generate.py,"@@ -0,0 +1,138 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Generates grpc-prefixed packages using template renderer.++To use this script, please use 3.7+ interpreter. This script is work-directory","`dataclasses` is 3.7+ only. For Python lower than 3.7, we will got a `ModuleNotFoundError: No module named 'dataclasses'`.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23094,432755814,2020-05-29T21:52:11Z,tools/distrib/python/grpc_prefixed/generate.py,"@@ -0,0 +1,138 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Generates grpc-prefixed packages using template renderer.++To use this script, please use 3.7+ interpreter. This script is work-directory+agnostic. A quick executable command:++    python3 tools/distrib/python/grpc_prefixed/generate.py+""""""++import dataclasses+import logging+import os+import shutil+import subprocess+import sys++import jinja2++WORK_PATH = os.path.realpath(os.path.dirname(__file__))+LICENSE = os.path.join(WORK_PATH, '../../../../LICENSE')+BUILD_PATH = os.path.join(WORK_PATH, 'build')+DIST_PATH = os.path.join(WORK_PATH, 'dist')++env = jinja2.Environment(+    loader=jinja2.FileSystemLoader(os.path.join(WORK_PATH, 'templates')))++LOGGER = logging.getLogger(__name__)+++@dataclasses.dataclass+class PackageMeta:+    """"""Meta-info of a PyPI package.""""""+    name: str+    name_long: str+    destination_package: str+    version: str = '1.0.0'+++def clean() -> None:+    try:+        shutil.rmtree(BUILD_PATH)+    except FileNotFoundError:+        pass++    try:+        shutil.rmtree(DIST_PATH)+    except FileNotFoundError:+        pass+++def generate_package(meta: PackageMeta) -> None:+    # Makes package directory+    package_path = os.path.join(BUILD_PATH, meta.name)+    os.makedirs(package_path, exist_ok=True)++    # Copy license+    shutil.copyfile(LICENSE, os.path.join(package_path, 'LICENSE'))++    # Generates source code+    for template_name in env.list_templates():+        template = env.get_template(template_name)+        with open(+                os.path.join(package_path,+                             template_name.replace('.template', '')), 'w') as f:+            f.write(template.render(dataclasses.asdict(meta)))++    # Creates wheel+    job = subprocess.Popen([+        sys.executable,+        os.path.join(package_path, 'setup.py'), 'sdist', '--dist-dir', DIST_PATH+    ],+                           cwd=package_path,+                           stdout=subprocess.PIPE,","Actually, considering that this script is only going to be run once, it's not that big of a concern. LGTM.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23092,432782696,2020-05-29T23:36:27Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -388,6 +433,111 @@ def __await__(self):         return response  +class _InterceptedStreamResponseMixin:+    _response_aiter: AsyncIterable[ResponseType]++    def _init_stream_response_mixin(self) -> None:+        self._response_aiter = self._wait_for_interceptor_task_response_iterator(+        )","AsyncIO has a weird behavior that async generator are treated as coroutine. So, if there isn't a consumer for the `self._response_aiter`, it will log a warning saying that a coroutine is deallocated without `await`. A workaround is only instantiate the coroutine when `__aiter__` is called.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23092,432783667,2020-05-29T23:41:32Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -388,6 +433,111 @@ def __await__(self):         return response  +class _InterceptedStreamResponseMixin:+    _response_aiter: AsyncIterable[ResponseType]++    def _init_stream_response_mixin(self) -> None:+        self._response_aiter = self._wait_for_interceptor_task_response_iterator(+        )++    async def _wait_for_interceptor_task_response_iterator(self+                                                          ) -> ResponseType:+        call = await self._interceptors_task+        async for response in call:+            yield response++    def __aiter__(self) -> AsyncIterable[ResponseType]:+        return self._response_aiter++    async def read(self) -> ResponseType:+        return await self._response_aiter.asend(None)++    def time_remaining(self) -> Optional[float]:+        raise NotImplementedError()",This is duplicated with `InterceptedStreamStreamCall`.,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22987,433264036,2020-06-01T14:22:30Z,CMakeLists.txt,"@@ -3627,6 +3666,78 @@ if(gRPC_INSTALL)   ) endif() +endif()+if(gRPC_BUILD_TESTS)++add_library(re2",re2 has its own cmake that we should use. https://github.com/google/re2/blob/master/CMakeLists.txtSo seeing the re2 sources listed in our CMakeLists.txt  seems suspicious.,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22936,433316745,2020-06-01T15:42:02Z,BUILD,"@@ -301,6 +301,7 @@ grpc_cc_library(     language = ""c++"",     public_hdrs = GPR_PUBLIC_HDRS,     standalone = True,+    visibility = [""//visibility:public""],","I think we need to make sure that there's a clear mapping between `visibility = [""//visibility:public""]` and the `build` attribute in the script that generates build.yaml metadata from the bazel build:https://github.com/grpc/grpc/blob/a43082112f5a2e4ef475c8b9c364ca08349e201b/tools/buildgen/extract_metadata_from_bazel_xml.py#L575(there's clearly some relationship between the ""public"" targets and the ones marked as e.g. `build: all` and while doing this cleanup we should figure out what the source of truth is).",
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/23045,433679820,2020-06-02T07:39:59Z,src/python/grpcio/grpc/experimental/aio/_typing.py,"@@ -13,17 +13,18 @@ # limitations under the License. """"""Common types for gRPC Async API"""""" -from typing import (Any, AnyStr, AsyncIterable, Callable, Iterable, Sequence,-                    Tuple, TypeVar, Union)+from typing import (Any, AsyncIterable, Callable, Iterable, Sequence, Tuple,+                    TypeVar, Union)  from grpc._cython.cygrpc import EOF+from ._metadata import Metadata, MetadataKey, MetadataValue  RequestType = TypeVar('RequestType') ResponseType = TypeVar('ResponseType') SerializingFunction = Callable[[Any], bytes] DeserializingFunction = Callable[[bytes], Any]-MetadatumType = Tuple[str, AnyStr]-MetadataType = Sequence[MetadatumType]+MetadatumType = Tuple[MetadataKey, MetadataValue]","Done, now ``MetadataKey`` points to ``str``",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23109,433928041,2020-06-02T14:38:37Z,src/core/ext/filters/client_channel/xds/xds_client.cc,"@@ -812,12 +812,17 @@ void XdsClient::ChannelState::AdsCallState::SendMessageLocked(         GRPC_ERROR_REF(state.error), !sent_initial_message_);     state.subscribed_resources[xds_client()->server_name_]->Start(Ref());   } else if (type_url == XdsApi::kRdsTypeUrl) {-    resource_names.insert(xds_client()->lds_result_->route_config_name);+    std::string name = (xds_client()->lds_result_.has_value()+                            ? xds_client()->lds_result_->route_config_name+                            : xds_client()->server_name_);+    resource_names.insert(name);     request_payload_slice = xds_client()->api_.CreateRdsRequest(-        xds_client()->lds_result_->route_config_name, state.version,-        state.nonce, GRPC_ERROR_REF(state.error), !sent_initial_message_);-    state.subscribed_resources[xds_client()->lds_result_->route_config_name]-        ->Start(Ref());+        name, state.version, state.nonce, GRPC_ERROR_REF(state.error),+        !sent_initial_message_);+    auto resource_it = state.subscribed_resources.find(name);+    if (resource_it != state.subscribed_resources.end()) {","This also looks wrong to me.  If we don't actually have an entry in the subscribed resources list, why are we sending a request in the first place?Actually, I wonder if a better approach would be to construct the list of resource names from the list of subscribed resources, just like we do for CDS and EDS requests.  But we would need to be careful about the case where the list is empty -- if that happens on the very first request for a given resource type, we should not send the request at all, since that would be interpretted as a wildcard request.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23045,434052433,2020-06-02T17:33:37Z,src/python/grpcio/grpc/experimental/aio/_metadata.py,"@@ -37,6 +37,12 @@ def __init__(self, *args: Tuple[MetadataKey, MetadataValue]) -> None:         for md_key, md_value in args:             self.add(md_key, md_value) +    @classmethod+    def from_tuple(cls, raw_metadata: tuple):+        if raw_metadata:",What's the reasoning behind the conditional here? Shouldn't `return cls(*raw_metadata)` just work?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/23045,434132839,2020-06-02T19:41:49Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -58,22 +59,17 @@ class AioRpcError(grpc.RpcError):     determined. Hence, its methods no longer needs to be coroutines.     """""" -    # TODO(https://github.com/grpc/grpc/issues/20144) Metadata-    # type returned by `initial_metadata` and `trailing_metadata`-    # and also taken in the constructor needs to be revisit and make-    # it more specific.-     _code: grpc.StatusCode     _details: Optional[str]-    _initial_metadata: Optional[MetadataType]-    _trailing_metadata: Optional[MetadataType]+    _initial_metadata: Optional[Metadata]+    _trailing_metadata: Optional[Metadata]     _debug_error_string: Optional[str]      def __init__(self,                  code: grpc.StatusCode,                  details: Optional[str] = None,-                 initial_metadata: Optional[MetadataType] = None,-                 trailing_metadata: Optional[MetadataType] = None,+                 initial_metadata: Optional[Metadata] = None,+                 trailing_metadata: Optional[Metadata] = None,","Can be them set to `None`? If so the return type for the getter methods would need to be changed, for example [1], if not we should remove the `Optional` (the same for the class type definition)https://github.com/grpc/grpc/pull/23045/files#diff-802047f7d320089cb1d9c7de77c9cae5R107",
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/23045,434139525,2020-06-02T19:54:39Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -248,7 +248,7 @@ async def intercept_stream_stream(   class InterceptedCall:-    """"""Base implementation for all intecepted call arities.+    """"""Base implementation for all intercepted call arities.","Thanks for fixing the typo. In any case seems that within the `_interceptror.py` file there are some reference to the old Metadata type like this [1] which would need to be changed, the same for the `ClientCallDetaills` which is still using an `Optional[MetadataType]` should this be changed to `Metadata`? [2][1] https://github.com/grpc/grpc/pull/23045/files#diff-173e8bd5d2219ba109f86237680068a4R383[2] https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/experimental/aio/_interceptor.py#L85",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23045,434222989,2020-06-02T23:08:35Z,src/python/grpcio_tests/tests_aio/unit/metadata_test.py,"@@ -56,20 +59,12 @@     ),     (         TypeError,-        (({}, {}),),+        ((None, {}),),","Please take a look at the corner cases here. If certain bad value crash the process, we should try to prevent it from happening.",X
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/23045,434383006,2020-06-03T08:04:50Z,src/python/grpcio/grpc/experimental/aio/_metadata.py,"@@ -37,6 +37,12 @@ def __init__(self, *args: Tuple[MetadataKey, MetadataValue]) -> None:         for md_key, md_value in args:             self.add(md_key, md_value) +    @classmethod+    def from_tuple(cls, raw_metadata: tuple):+        if raw_metadata:","Yes, but ``raw_metadata`` can be None.I'll change it to be explicit, like```pythonif raw_metadata is None: ...```",
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/23045,434520267,2020-06-03T12:13:02Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -58,22 +59,17 @@ class AioRpcError(grpc.RpcError):     determined. Hence, its methods no longer needs to be coroutines.     """""" -    # TODO(https://github.com/grpc/grpc/issues/20144) Metadata-    # type returned by `initial_metadata` and `trailing_metadata`-    # and also taken in the constructor needs to be revisit and make-    # it more specific.-     _code: grpc.StatusCode     _details: Optional[str]-    _initial_metadata: Optional[MetadataType]-    _trailing_metadata: Optional[MetadataType]+    _initial_metadata: Optional[Metadata]+    _trailing_metadata: Optional[Metadata]     _debug_error_string: Optional[str]      def __init__(self,                  code: grpc.StatusCode,                  details: Optional[str] = None,-                 initial_metadata: Optional[MetadataType] = None,-                 trailing_metadata: Optional[MetadataType] = None,+                 initial_metadata: Optional[Metadata] = None,+                 trailing_metadata: Optional[Metadata] = None,","Good point, I think we can remove the ``Optional[]`` because it's always a ``Metadata`` object, however, I wouldn't remove the default argument ``= None`` because that would imply changing the position of the parameters (they would have to go right after ``code``), and that could cause backward incompatibilities, and also confuse users.The proposal is then to remove the ``Optional[..]`` annotation but keep the default argument to ``None``. WDYT?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23115,434659493,2020-06-03T15:34:00Z,examples/python/xds/README.md,"@@ -3,11 +3,8 @@ gRPC Hostname Example  The hostname example is a Hello World server whose response includes its hostname. It also supports health and reflection services. This makes it a good-server to test infrastructure, like load balancing.--The example requires grpc to already be built. You are strongly encouraged-to check out a git release tag, since there will already be a build of gRPC-available.+server to test infrastructure, like load balancing .This example depends on a+gRPC version of 1.28.1 or newer.  ### Run the example ","The instructions in the ""Run the example section"" are a bit confusing.There are 3 numbered steps but after that there's a big section on  configuring the server and then running a client.It's not clear whether all the steps belong under section 3. Once done with the client, the docs talk about testing the server with grpcurl. Does that still belong to section 3? (probably not). I think the way the instructions are divided into sections needs some love.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22891,434699327,2020-06-03T16:28:34Z,src/csharp/Grpc.Core/Internal/DebugErrorException.cs,"@@ -0,0 +1,35 @@+#region Copyright notice and license++// Copyright 2019 The gRPC Authors+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#endregion++using System;+using System.Runtime.InteropServices;+using System.Threading;+using Grpc.Core.Utils;++namespace Grpc.Core.Internal+{+    /// <summary>+    /// Represents error details provides by C-core's debug_error_string+    /// </summary>+    internal class DebugErrorException : Exception",I think throwing a raw Exception is generally discouraged.The exception provides at least some classification of the problem so it doesn't do any harm to throw a type specific exception.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23003,434713269,2020-06-03T16:50:10Z,src/csharp/Grpc.Core/Internal/AsyncCall.cs,"@@ -78,7 +78,12 @@ public TResponse UnaryCall(TRequest msg)             var profiler = Profilers.ForCurrentThread();              using (profiler.NewScope(""AsyncCall.UnaryCall""))-            using (CompletionQueueSafeHandle cq = CompletionQueueSafeHandle.CreateSync())++            // Create a pluckable completion queue for the call. Avoid creating a completion queue when we know the channel has already+            // been shutdown. In such case, the call will fail with ObjectDisposedException immediately anyway and creating / destroying+            // a completion queue would lead to crash if this was the last channel in the application (and thus GrpcEnvironment has been shutdown).+            // See https://github.com/grpc/grpc/issues/19090+            using (CompletionQueueSafeHandle cq = details.Channel.Handle.IsClosed ?  null : CompletionQueueSafeHandle.CreateSync())","The problem with completion queue incrementing refcount on GrpcEnvironment is that it requires a global lock. Global lock is fine for creating channels/server because their creation/shutdown is heavyweight and quite rare.On the other hand, for sync unary calls, a new completion queue is created for each call, so the cost of acquiring a global lock seems prohibitive.I agree that the fix is not 100% but it would improve the current situation a lot.I can add a comment that the fix is known to be racy, but it's a big improvement and doesn't have an associated performance hit.Btw this fix protects against a situation that should not happen at all under normal circumstances (no point of starting a call after you've shutdown your channel). It's basically a protection against users shooting themselves in the foot when the write misbehaving code.WDYT?",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23003,434730448,2020-06-03T17:19:23Z,src/csharp/Grpc.Core/Internal/AsyncCall.cs,"@@ -78,7 +78,12 @@ public TResponse UnaryCall(TRequest msg)             var profiler = Profilers.ForCurrentThread();              using (profiler.NewScope(""AsyncCall.UnaryCall""))-            using (CompletionQueueSafeHandle cq = CompletionQueueSafeHandle.CreateSync())++            // Create a pluckable completion queue for the call. Avoid creating a completion queue when we know the channel has already+            // been shutdown. In such case, the call will fail with ObjectDisposedException immediately anyway and creating / destroying+            // a completion queue would lead to crash if this was the last channel in the application (and thus GrpcEnvironment has been shutdown).+            // See https://github.com/grpc/grpc/issues/19090+            using (CompletionQueueSafeHandle cq = details.Channel.Handle.IsClosed ?  null : CompletionQueueSafeHandle.CreateSync())","I could see how there would be performance benefits to not acquiring the lock, but my concerns are:a) I have doubts that the magnitude of the gain is worth it - this lock should be uncontended in the common case, unless people are doing lots of concurrent sync RPCs from different threads (in which case they should probably be using async API).b) Even though this should improve things a lot, the potential crash will be hard to debug if hit.c) Creating a new CQ, reffing the grpc library, and taking a global lock while doing so, are I believe exactly what the C++ sync API does (when it calls grpc_init and grpc_shutdown in the ctor and dtor of it's CQ object).",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23115,434761285,2020-06-03T18:12:38Z,examples/python/xds/README.md,"@@ -3,11 +3,8 @@ gRPC Hostname Example  The hostname example is a Hello World server whose response includes its hostname. It also supports health and reflection services. This makes it a good-server to test infrastructure, like load balancing.--The example requires grpc to already be built. You are strongly encouraged-to check out a git release tag, since there will already be a build of gRPC-available.+server to test infrastructure, like load balancing .This example depends on a+gRPC version of 1.28.1 or newer.  ### Run the example ",The numbered list wasn't supposed to serve as organization for the entire document. I've added sections with their own numbered lists to make things a bit more homogeneous. Hopefully this works a bit better for you. What do you think?,
1226121,pfreixes,https://api.github.com/repos/grpc/grpc/pulls/23045,434801448,2020-06-03T19:26:16Z,src/python/grpcio/grpc/experimental/aio/_interceptor.py,"@@ -248,7 +248,7 @@ async def intercept_stream_stream(   class InterceptedCall:-    """"""Base implementation for all intecepted call arities.+    """"""Base implementation for all intercepted call arities.","One last thing, if the input from the user hasn't  changed, so its still providing the metadata for te call using a tuple I wold not change either the `ClientCallDetails` since this is an interface used by the interceptors for mutating the metadata provided by the user.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23125,434905549,2020-06-03T23:08:10Z,examples/ruby/greeter_client.rb,"@@ -29,8 +29,12 @@ def main   user = ARGV.size > 0 ?  ARGV[0] : 'world'   hostname = ARGV.size > 1 ?  ARGV[1] : 'localhost:50051'   stub = Helloworld::Greeter::Stub.new(hostname, :this_channel_is_insecure)-  message = stub.say_hello(Helloworld::HelloRequest.new(name: user)).message-  p ""Greeting: #{message}""+  begin+    message = stub.say_hello(Helloworld::HelloRequest.new(name: user)).message+    p ""Greeting: #{message}""+  rescue GRPC::BadStatus => e+    abort ""ERROR: #{e.code}, #{e.details}""","nit: please print the exception's message, rather than individual fields.Otherwise, we're actually missing one field, `debug_error_string`, added in https://github.com/grpc/grpc/pull/22876, which is coming in next release",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/23104,435340750,2020-06-04T15:19:06Z,src/core/ext/transport/chttp2/server/chttp2_server.cc,"@@ -47,405 +49,429 @@ #include ""src/core/lib/surface/api_trace.h"" #include ""src/core/lib/surface/server.h"" -typedef struct {-  grpc_server* server;-  grpc_tcp_server* tcp_server;-  grpc_channel_args* args;-  gpr_mu mu;-  bool shutdown;-  grpc_closure tcp_server_shutdown_complete;-  grpc_closure* server_destroy_listener_done;-  grpc_core::HandshakeManager* pending_handshake_mgrs;-  grpc_core::RefCountedPtr<grpc_core::channelz::ListenSocketNode>-      channelz_listen_socket;-} server_state;--typedef struct {-  gpr_refcount refs;-  server_state* svr_state;-  grpc_pollset* accepting_pollset;-  grpc_tcp_server_acceptor* acceptor;-  grpc_core::RefCountedPtr<grpc_core::HandshakeManager> handshake_mgr;-  // State for enforcing handshake timeout on receiving HTTP/2 settings.-  grpc_chttp2_transport* transport;-  grpc_millis deadline;-  grpc_timer timer;-  grpc_closure on_timeout;-  grpc_closure on_receive_settings;-  grpc_pollset_set* interested_parties;-} server_connection_state;--static void server_connection_state_unref(-    server_connection_state* connection_state) {-  if (gpr_unref(&connection_state->refs)) {-    if (connection_state->transport != nullptr) {-      GRPC_CHTTP2_UNREF_TRANSPORT(connection_state->transport,-                                  ""receive settings timeout"");-    }-    grpc_pollset_set_del_pollset(connection_state->interested_parties,-                                 connection_state->accepting_pollset);-    grpc_pollset_set_destroy(connection_state->interested_parties);-    gpr_free(connection_state);+namespace grpc_core {+namespace {++class Chttp2ServerListener : public ServerListenerInterface {+ public:+  static grpc_error* Create(grpc_server* server, const char* addr,+                            grpc_channel_args* args, int* port_num);++  static grpc_error* CreateWithAcceptor(grpc_server* server, const char* name,+                                        grpc_channel_args* args);++  // Do not instantiate directly.  Use one of the factory methods above.+  Chttp2ServerListener(grpc_server* server, grpc_channel_args* args);+  ~Chttp2ServerListener();++  void SetOnDestroyDone(grpc_closure* on_destroy_done) override;++  void Start(grpc_server* server, grpc_pollset** pollsets, size_t npollsets)+      override;++  channelz::ListenSocketNode* channelz_listen_socket_node() const override {+    return channelz_listen_socket_.get();+  }++  void Orphan() override;++ private:+  class ConnectionState : public RefCounted<ConnectionState> {+   public:+    ConnectionState(Chttp2ServerListener* listener,+                    grpc_pollset* accepting_pollset,+                    grpc_tcp_server_acceptor* acceptor,+                    RefCountedPtr<HandshakeManager> handshake_mgr,+                    grpc_channel_args* args, grpc_endpoint* endpoint);++    ~ConnectionState();++   private:+    static void OnTimeout(void* arg, grpc_error* error);+    static void OnReceiveSettings(void* arg, grpc_error* error);+    static void OnHandshakeDone(void* arg, grpc_error* error);++    Chttp2ServerListener* listener_;+    grpc_pollset* accepting_pollset_;+    grpc_tcp_server_acceptor* acceptor_;+    RefCountedPtr<HandshakeManager> handshake_mgr_;+    // State for enforcing handshake timeout on receiving HTTP/2 settings.+    grpc_chttp2_transport* transport_ = nullptr;+    grpc_millis deadline_;+    grpc_timer timer_;+    grpc_closure on_timeout_;+    grpc_closure on_receive_settings_;+    grpc_pollset_set* interested_parties_;+  };++  static void OnAccept(void* arg, grpc_endpoint* tcp,+                       grpc_pollset* accepting_pollset,+                       grpc_tcp_server_acceptor* acceptor);++  RefCountedPtr<HandshakeManager> CreateHandshakeManager();++  static void TcpServerShutdownComplete(void* arg, grpc_error* error);++  static void DestroyListener(grpc_server* /*server*/, void* arg,+                              grpc_closure* destroy_done);++  grpc_server* server_;+  grpc_tcp_server* tcp_server_;+  grpc_channel_args* args_;+  Mutex mu_;+  bool shutdown_ = true;+  grpc_closure tcp_server_shutdown_complete_;+  grpc_closure* on_destroy_done_ = nullptr;+  HandshakeManager* pending_handshake_mgrs_ = nullptr;+  RefCountedPtr<channelz::ListenSocketNode> channelz_listen_socket_;+};++//+// Chttp2ServerListener::ConnectionState+//++grpc_millis GetConnectionDeadline(const grpc_channel_args* args) {+  int timeout_ms =+      grpc_channel_args_find_integer(args, GRPC_ARG_SERVER_HANDSHAKE_TIMEOUT_MS,+                                     {120 * GPR_MS_PER_SEC, 1, INT_MAX});+  return ExecCtx::Get()->Now() + timeout_ms;+}++Chttp2ServerListener::ConnectionState::ConnectionState(+    Chttp2ServerListener* listener, grpc_pollset* accepting_pollset,+    grpc_tcp_server_acceptor* acceptor,+    RefCountedPtr<HandshakeManager> handshake_mgr, grpc_channel_args* args,+    grpc_endpoint* endpoint)+    : listener_(listener),+      accepting_pollset_(accepting_pollset),+      acceptor_(acceptor),+      handshake_mgr_(std::move(handshake_mgr)),+      deadline_(GetConnectionDeadline(args)),+      interested_parties_(grpc_pollset_set_create()) {+  grpc_pollset_set_add_pollset(interested_parties_, accepting_pollset_);+  HandshakerRegistry::AddHandshakers(HANDSHAKER_SERVER, args,+                                     interested_parties_, handshake_mgr_.get());+  handshake_mgr_->DoHandshake(endpoint, args, deadline_, acceptor_,+                              OnHandshakeDone, this);+}++Chttp2ServerListener::ConnectionState::~ConnectionState() {+  if (transport_ != nullptr) {+    GRPC_CHTTP2_UNREF_TRANSPORT(transport_, ""receive settings timeout"");   }+  grpc_pollset_set_del_pollset(interested_parties_, accepting_pollset_);+  grpc_pollset_set_destroy(interested_parties_); } -static void on_timeout(void* arg, grpc_error* error) {-  server_connection_state* connection_state =-      static_cast<server_connection_state*>(arg);+void Chttp2ServerListener::ConnectionState::OnTimeout(void* arg,+                                                      grpc_error* error) {+  ConnectionState* self = static_cast<ConnectionState*>(arg);   // Note that we may be called with GRPC_ERROR_NONE when the timer fires   // or with an error indicating that the timer system is being shut down.   if (error != GRPC_ERROR_CANCELLED) {     grpc_transport_op* op = grpc_make_transport_op(nullptr);     op->disconnect_with_error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(         ""Did not receive HTTP/2 settings before handshake timeout"");-    grpc_transport_perform_op(&connection_state->transport->base, op);+    grpc_transport_perform_op(&self->transport_->base, op);   }-  server_connection_state_unref(connection_state);+  self->Unref(); } -static void on_receive_settings(void* arg, grpc_error* error) {-  server_connection_state* connection_state =-      static_cast<server_connection_state*>(arg);+void Chttp2ServerListener::ConnectionState::OnReceiveSettings(+    void* arg, grpc_error* error) {+  ConnectionState* self = static_cast<ConnectionState*>(arg);   if (error == GRPC_ERROR_NONE) {-    grpc_timer_cancel(&connection_state->timer);+    grpc_timer_cancel(&self->timer_);   }-  server_connection_state_unref(connection_state);+  self->Unref(); } -static void on_handshake_done(void* arg, grpc_error* error) {-  auto* args = static_cast<grpc_core::HandshakerArgs*>(arg);-  server_connection_state* connection_state =-      static_cast<server_connection_state*>(args->user_data);-  gpr_mu_lock(&connection_state->svr_state->mu);-  grpc_resource_user* resource_user = grpc_server_get_default_resource_user(-      connection_state->svr_state->server);-  if (error != GRPC_ERROR_NONE || connection_state->svr_state->shutdown) {-    const char* error_str = grpc_error_string(error);-    gpr_log(GPR_DEBUG, ""Handshaking failed: %s"", error_str);-    grpc_resource_user* resource_user = grpc_server_get_default_resource_user(-        connection_state->svr_state->server);-    if (resource_user != nullptr) {-      grpc_resource_user_free(resource_user, GRPC_RESOURCE_QUOTA_CHANNEL_SIZE);-    }-    if (error == GRPC_ERROR_NONE && args->endpoint != nullptr) {-      // We were shut down after handshaking completed successfully, so-      // destroy the endpoint here.-      // TODO(ctiller): It is currently necessary to shutdown endpoints-      // before destroying them, even if we know that there are no-      // pending read/write callbacks.  This should be fixed, at which-      // point this can be removed.-      grpc_endpoint_shutdown(args->endpoint, GRPC_ERROR_NONE);-      grpc_endpoint_destroy(args->endpoint);-      grpc_channel_args_destroy(args->args);-      grpc_slice_buffer_destroy_internal(args->read_buffer);-      gpr_free(args->read_buffer);-    }-  } else {-    // If the handshaking succeeded but there is no endpoint, then the-    // handshaker may have handed off the connection to some external-    // code, so we can just clean up here without creating a transport.-    if (args->endpoint != nullptr) {-      grpc_transport* transport = grpc_create_chttp2_transport(-          args->args, args->endpoint, false, resource_user);-      grpc_server_setup_transport(-          connection_state->svr_state->server, transport,-          connection_state->accepting_pollset, args->args,-          grpc_chttp2_transport_get_socket_node(transport), resource_user);-      // Use notify_on_receive_settings callback to enforce the-      // handshake deadline.-      connection_state->transport =-          reinterpret_cast<grpc_chttp2_transport*>(transport);-      gpr_ref(&connection_state->refs);-      GRPC_CLOSURE_INIT(&connection_state->on_receive_settings,-                        on_receive_settings, connection_state,-                        grpc_schedule_on_exec_ctx);-      grpc_chttp2_transport_start_reading(-          transport, args->read_buffer, &connection_state->on_receive_settings);-      grpc_channel_args_destroy(args->args);-      gpr_ref(&connection_state->refs);-      GRPC_CHTTP2_REF_TRANSPORT((grpc_chttp2_transport*)transport,-                                ""receive settings timeout"");-      GRPC_CLOSURE_INIT(&connection_state->on_timeout, on_timeout,-                        connection_state, grpc_schedule_on_exec_ctx);-      grpc_timer_init(&connection_state->timer, connection_state->deadline,-                      &connection_state->on_timeout);-    } else {+void Chttp2ServerListener::ConnectionState::OnHandshakeDone(void* arg,+                                                            grpc_error* error) {+  auto* args = static_cast<HandshakerArgs*>(arg);+  ConnectionState* self = static_cast<ConnectionState*>(args->user_data);+  {+    MutexLock lock(&self->listener_->mu_);+    grpc_resource_user* resource_user =+        grpc_server_get_default_resource_user(self->listener_->server_);+    if (error != GRPC_ERROR_NONE || self->listener_->shutdown_) {+      const char* error_str = grpc_error_string(error);+      gpr_log(GPR_DEBUG, ""Handshaking failed: %s"", error_str);+      grpc_resource_user* resource_user =+          grpc_server_get_default_resource_user(self->listener_->server_);       if (resource_user != nullptr) {         grpc_resource_user_free(resource_user,                                 GRPC_RESOURCE_QUOTA_CHANNEL_SIZE);       }+      if (error == GRPC_ERROR_NONE && args->endpoint != nullptr) {+        // We were shut down after handshaking completed successfully, so+        // destroy the endpoint here.+        // TODO(ctiller): It is currently necessary to shutdown endpoints+        // before destroying them, even if we know that there are no+        // pending read/write callbacks.  This should be fixed, at which+        // point this can be removed.+        grpc_endpoint_shutdown(args->endpoint, GRPC_ERROR_NONE);+        grpc_endpoint_destroy(args->endpoint);+        grpc_channel_args_destroy(args->args);+        grpc_slice_buffer_destroy_internal(args->read_buffer);+        gpr_free(args->read_buffer);+      }+    } else {+      // If the handshaking succeeded but there is no endpoint, then the+      // handshaker may have handed off the connection to some external+      // code, so we can just clean up here without creating a transport.+      if (args->endpoint != nullptr) {+        grpc_transport* transport = grpc_create_chttp2_transport(+            args->args, args->endpoint, false, resource_user);+        grpc_server_setup_transport(+            self->listener_->server_, transport, self->accepting_pollset_,+            args->args, grpc_chttp2_transport_get_socket_node(transport),+            resource_user);+        // Use notify_on_receive_settings callback to enforce the+        // handshake deadline.+        self->transport_ = reinterpret_cast<grpc_chttp2_transport*>(transport);+        self->Ref().release();  // Held by OnReceiveSettings().+        GRPC_CLOSURE_INIT(&self->on_receive_settings_, OnReceiveSettings, self,+                          grpc_schedule_on_exec_ctx);+        grpc_chttp2_transport_start_reading(transport, args->read_buffer,+                                            &self->on_receive_settings_);+        grpc_channel_args_destroy(args->args);+        self->Ref().release();  // Held by OnTimeout().+        GRPC_CHTTP2_REF_TRANSPORT((grpc_chttp2_transport*)transport,+                                  ""receive settings timeout"");+        GRPC_CLOSURE_INIT(&self->on_timeout_, OnTimeout, self,+                          grpc_schedule_on_exec_ctx);+        grpc_timer_init(&self->timer_, self->deadline_, &self->on_timeout_);+      } else {+        if (resource_user != nullptr) {+          grpc_resource_user_free(resource_user,+                                  GRPC_RESOURCE_QUOTA_CHANNEL_SIZE);+        }+      }     }+    self->handshake_mgr_->RemoveFromPendingMgrList(+        &self->listener_->pending_handshake_mgrs_);   }-  connection_state->handshake_mgr->RemoveFromPendingMgrList(-      &connection_state->svr_state->pending_handshake_mgrs);-  gpr_mu_unlock(&connection_state->svr_state->mu);-  connection_state->handshake_mgr.reset();-  gpr_free(connection_state->acceptor);-  grpc_tcp_server_unref(connection_state->svr_state->tcp_server);-  server_connection_state_unref(connection_state);+  self->handshake_mgr_.reset();+  gpr_free(self->acceptor_);+  grpc_tcp_server_unref(self->listener_->tcp_server_);+  self->Unref(); } -static void on_accept(void* arg, grpc_endpoint* tcp,-                      grpc_pollset* accepting_pollset,-                      grpc_tcp_server_acceptor* acceptor) {-  server_state* state = static_cast<server_state*>(arg);-  gpr_mu_lock(&state->mu);-  if (state->shutdown) {-    gpr_mu_unlock(&state->mu);-    grpc_endpoint_shutdown(tcp, GRPC_ERROR_NONE);-    grpc_endpoint_destroy(tcp);-    gpr_free(acceptor);-    return;+//+// Chttp2ServerListener+//++grpc_error* Chttp2ServerListener::Create(grpc_server* server, const char* addr,+                                         grpc_channel_args* args,+                                         int* port_num) {+  std::vector<grpc_error*> error_list;+  grpc_resolved_addresses* resolved = nullptr;+  Chttp2ServerListener* listener = nullptr;+  grpc_error* error = [&]() {","This is pretty large as a lambda, especially since it's just called with an empty argument; would this make more sense as a helper function?",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/23104,435350787,2020-06-04T15:32:47Z,src/core/ext/transport/chttp2/server/chttp2_server.cc,"@@ -47,405 +49,429 @@ #include ""src/core/lib/surface/api_trace.h"" #include ""src/core/lib/surface/server.h"" -typedef struct {-  grpc_server* server;-  grpc_tcp_server* tcp_server;-  grpc_channel_args* args;-  gpr_mu mu;-  bool shutdown;-  grpc_closure tcp_server_shutdown_complete;-  grpc_closure* server_destroy_listener_done;-  grpc_core::HandshakeManager* pending_handshake_mgrs;-  grpc_core::RefCountedPtr<grpc_core::channelz::ListenSocketNode>-      channelz_listen_socket;-} server_state;--typedef struct {-  gpr_refcount refs;-  server_state* svr_state;-  grpc_pollset* accepting_pollset;-  grpc_tcp_server_acceptor* acceptor;-  grpc_core::RefCountedPtr<grpc_core::HandshakeManager> handshake_mgr;-  // State for enforcing handshake timeout on receiving HTTP/2 settings.-  grpc_chttp2_transport* transport;-  grpc_millis deadline;-  grpc_timer timer;-  grpc_closure on_timeout;-  grpc_closure on_receive_settings;-  grpc_pollset_set* interested_parties;-} server_connection_state;--static void server_connection_state_unref(-    server_connection_state* connection_state) {-  if (gpr_unref(&connection_state->refs)) {-    if (connection_state->transport != nullptr) {-      GRPC_CHTTP2_UNREF_TRANSPORT(connection_state->transport,-                                  ""receive settings timeout"");-    }-    grpc_pollset_set_del_pollset(connection_state->interested_parties,-                                 connection_state->accepting_pollset);-    grpc_pollset_set_destroy(connection_state->interested_parties);-    gpr_free(connection_state);+namespace grpc_core {+namespace {++class Chttp2ServerListener : public ServerListenerInterface {+ public:+  static grpc_error* Create(grpc_server* server, const char* addr,+                            grpc_channel_args* args, int* port_num);++  static grpc_error* CreateWithAcceptor(grpc_server* server, const char* name,+                                        grpc_channel_args* args);++  // Do not instantiate directly.  Use one of the factory methods above.+  Chttp2ServerListener(grpc_server* server, grpc_channel_args* args);+  ~Chttp2ServerListener();++  void SetOnDestroyDone(grpc_closure* on_destroy_done) override;++  void Start(grpc_server* server, grpc_pollset** pollsets, size_t npollsets)+      override;++  channelz::ListenSocketNode* channelz_listen_socket_node() const override {+    return channelz_listen_socket_.get();+  }++  void Orphan() override;++ private:+  class ConnectionState : public RefCounted<ConnectionState> {+   public:+    ConnectionState(Chttp2ServerListener* listener,+                    grpc_pollset* accepting_pollset,+                    grpc_tcp_server_acceptor* acceptor,+                    RefCountedPtr<HandshakeManager> handshake_mgr,+                    grpc_channel_args* args, grpc_endpoint* endpoint);++    ~ConnectionState();++   private:+    static void OnTimeout(void* arg, grpc_error* error);+    static void OnReceiveSettings(void* arg, grpc_error* error);+    static void OnHandshakeDone(void* arg, grpc_error* error);++    Chttp2ServerListener* listener_;+    grpc_pollset* accepting_pollset_;+    grpc_tcp_server_acceptor* acceptor_;",```suggestion    Chttp2ServerListener* const listener_;    grpc_pollset* const accepting_pollset_;    grpc_tcp_server_acceptor* const acceptor_;```to emphasize a lack of change in these and that they don't need a default initializer,
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/23104,435362201,2020-06-04T15:45:53Z,src/core/ext/transport/chttp2/server/chttp2_server.cc,"@@ -47,405 +49,429 @@ #include ""src/core/lib/surface/api_trace.h"" #include ""src/core/lib/surface/server.h"" -typedef struct {-  grpc_server* server;-  grpc_tcp_server* tcp_server;-  grpc_channel_args* args;-  gpr_mu mu;-  bool shutdown;-  grpc_closure tcp_server_shutdown_complete;-  grpc_closure* server_destroy_listener_done;-  grpc_core::HandshakeManager* pending_handshake_mgrs;-  grpc_core::RefCountedPtr<grpc_core::channelz::ListenSocketNode>-      channelz_listen_socket;-} server_state;--typedef struct {-  gpr_refcount refs;-  server_state* svr_state;-  grpc_pollset* accepting_pollset;-  grpc_tcp_server_acceptor* acceptor;-  grpc_core::RefCountedPtr<grpc_core::HandshakeManager> handshake_mgr;-  // State for enforcing handshake timeout on receiving HTTP/2 settings.-  grpc_chttp2_transport* transport;-  grpc_millis deadline;-  grpc_timer timer;-  grpc_closure on_timeout;-  grpc_closure on_receive_settings;-  grpc_pollset_set* interested_parties;-} server_connection_state;--static void server_connection_state_unref(-    server_connection_state* connection_state) {-  if (gpr_unref(&connection_state->refs)) {-    if (connection_state->transport != nullptr) {-      GRPC_CHTTP2_UNREF_TRANSPORT(connection_state->transport,-                                  ""receive settings timeout"");-    }-    grpc_pollset_set_del_pollset(connection_state->interested_parties,-                                 connection_state->accepting_pollset);-    grpc_pollset_set_destroy(connection_state->interested_parties);-    gpr_free(connection_state);+namespace grpc_core {+namespace {++class Chttp2ServerListener : public ServerListenerInterface {+ public:+  static grpc_error* Create(grpc_server* server, const char* addr,+                            grpc_channel_args* args, int* port_num);++  static grpc_error* CreateWithAcceptor(grpc_server* server, const char* name,+                                        grpc_channel_args* args);++  // Do not instantiate directly.  Use one of the factory methods above.+  Chttp2ServerListener(grpc_server* server, grpc_channel_args* args);+  ~Chttp2ServerListener();++  void SetOnDestroyDone(grpc_closure* on_destroy_done) override;++  void Start(grpc_server* server, grpc_pollset** pollsets, size_t npollsets)+      override;++  channelz::ListenSocketNode* channelz_listen_socket_node() const override {+    return channelz_listen_socket_.get();+  }++  void Orphan() override;++ private:+  class ConnectionState : public RefCounted<ConnectionState> {+   public:+    ConnectionState(Chttp2ServerListener* listener,+                    grpc_pollset* accepting_pollset,+                    grpc_tcp_server_acceptor* acceptor,+                    RefCountedPtr<HandshakeManager> handshake_mgr,+                    grpc_channel_args* args, grpc_endpoint* endpoint);++    ~ConnectionState();++   private:+    static void OnTimeout(void* arg, grpc_error* error);+    static void OnReceiveSettings(void* arg, grpc_error* error);+    static void OnHandshakeDone(void* arg, grpc_error* error);++    Chttp2ServerListener* listener_;+    grpc_pollset* accepting_pollset_;+    grpc_tcp_server_acceptor* acceptor_;+    RefCountedPtr<HandshakeManager> handshake_mgr_;+    // State for enforcing handshake timeout on receiving HTTP/2 settings.+    grpc_chttp2_transport* transport_ = nullptr;+    grpc_millis deadline_;+    grpc_timer timer_;+    grpc_closure on_timeout_;+    grpc_closure on_receive_settings_;+    grpc_pollset_set* interested_parties_;+  };++  static void OnAccept(void* arg, grpc_endpoint* tcp,+                       grpc_pollset* accepting_pollset,+                       grpc_tcp_server_acceptor* acceptor);++  RefCountedPtr<HandshakeManager> CreateHandshakeManager();++  static void TcpServerShutdownComplete(void* arg, grpc_error* error);++  static void DestroyListener(grpc_server* /*server*/, void* arg,+                              grpc_closure* destroy_done);++  grpc_server* server_;+  grpc_tcp_server* tcp_server_;+  grpc_channel_args* args_;+  Mutex mu_;+  bool shutdown_ = true;+  grpc_closure tcp_server_shutdown_complete_;+  grpc_closure* on_destroy_done_ = nullptr;+  HandshakeManager* pending_handshake_mgrs_ = nullptr;+  RefCountedPtr<channelz::ListenSocketNode> channelz_listen_socket_;+};++//+// Chttp2ServerListener::ConnectionState+//++grpc_millis GetConnectionDeadline(const grpc_channel_args* args) {+  int timeout_ms =+      grpc_channel_args_find_integer(args, GRPC_ARG_SERVER_HANDSHAKE_TIMEOUT_MS,+                                     {120 * GPR_MS_PER_SEC, 1, INT_MAX});+  return ExecCtx::Get()->Now() + timeout_ms;+}++Chttp2ServerListener::ConnectionState::ConnectionState(+    Chttp2ServerListener* listener, grpc_pollset* accepting_pollset,+    grpc_tcp_server_acceptor* acceptor,+    RefCountedPtr<HandshakeManager> handshake_mgr, grpc_channel_args* args,+    grpc_endpoint* endpoint)+    : listener_(listener),+      accepting_pollset_(accepting_pollset),+      acceptor_(acceptor),+      handshake_mgr_(std::move(handshake_mgr)),+      deadline_(GetConnectionDeadline(args)),+      interested_parties_(grpc_pollset_set_create()) {+  grpc_pollset_set_add_pollset(interested_parties_, accepting_pollset_);+  HandshakerRegistry::AddHandshakers(HANDSHAKER_SERVER, args,+                                     interested_parties_, handshake_mgr_.get());+  handshake_mgr_->DoHandshake(endpoint, args, deadline_, acceptor_,+                              OnHandshakeDone, this);+}++Chttp2ServerListener::ConnectionState::~ConnectionState() {+  if (transport_ != nullptr) {+    GRPC_CHTTP2_UNREF_TRANSPORT(transport_, ""receive settings timeout"");   }+  grpc_pollset_set_del_pollset(interested_parties_, accepting_pollset_);+  grpc_pollset_set_destroy(interested_parties_); } -static void on_timeout(void* arg, grpc_error* error) {-  server_connection_state* connection_state =-      static_cast<server_connection_state*>(arg);+void Chttp2ServerListener::ConnectionState::OnTimeout(void* arg,+                                                      grpc_error* error) {+  ConnectionState* self = static_cast<ConnectionState*>(arg);   // Note that we may be called with GRPC_ERROR_NONE when the timer fires   // or with an error indicating that the timer system is being shut down.   if (error != GRPC_ERROR_CANCELLED) {     grpc_transport_op* op = grpc_make_transport_op(nullptr);     op->disconnect_with_error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(         ""Did not receive HTTP/2 settings before handshake timeout"");-    grpc_transport_perform_op(&connection_state->transport->base, op);+    grpc_transport_perform_op(&self->transport_->base, op);   }-  server_connection_state_unref(connection_state);+  self->Unref(); } -static void on_receive_settings(void* arg, grpc_error* error) {-  server_connection_state* connection_state =-      static_cast<server_connection_state*>(arg);+void Chttp2ServerListener::ConnectionState::OnReceiveSettings(+    void* arg, grpc_error* error) {+  ConnectionState* self = static_cast<ConnectionState*>(arg);   if (error == GRPC_ERROR_NONE) {-    grpc_timer_cancel(&connection_state->timer);+    grpc_timer_cancel(&self->timer_);   }-  server_connection_state_unref(connection_state);+  self->Unref(); } -static void on_handshake_done(void* arg, grpc_error* error) {-  auto* args = static_cast<grpc_core::HandshakerArgs*>(arg);-  server_connection_state* connection_state =-      static_cast<server_connection_state*>(args->user_data);-  gpr_mu_lock(&connection_state->svr_state->mu);-  grpc_resource_user* resource_user = grpc_server_get_default_resource_user(-      connection_state->svr_state->server);-  if (error != GRPC_ERROR_NONE || connection_state->svr_state->shutdown) {-    const char* error_str = grpc_error_string(error);-    gpr_log(GPR_DEBUG, ""Handshaking failed: %s"", error_str);-    grpc_resource_user* resource_user = grpc_server_get_default_resource_user(-        connection_state->svr_state->server);-    if (resource_user != nullptr) {-      grpc_resource_user_free(resource_user, GRPC_RESOURCE_QUOTA_CHANNEL_SIZE);-    }-    if (error == GRPC_ERROR_NONE && args->endpoint != nullptr) {-      // We were shut down after handshaking completed successfully, so-      // destroy the endpoint here.-      // TODO(ctiller): It is currently necessary to shutdown endpoints-      // before destroying them, even if we know that there are no-      // pending read/write callbacks.  This should be fixed, at which-      // point this can be removed.-      grpc_endpoint_shutdown(args->endpoint, GRPC_ERROR_NONE);-      grpc_endpoint_destroy(args->endpoint);-      grpc_channel_args_destroy(args->args);-      grpc_slice_buffer_destroy_internal(args->read_buffer);-      gpr_free(args->read_buffer);-    }-  } else {-    // If the handshaking succeeded but there is no endpoint, then the-    // handshaker may have handed off the connection to some external-    // code, so we can just clean up here without creating a transport.-    if (args->endpoint != nullptr) {-      grpc_transport* transport = grpc_create_chttp2_transport(-          args->args, args->endpoint, false, resource_user);-      grpc_server_setup_transport(-          connection_state->svr_state->server, transport,-          connection_state->accepting_pollset, args->args,-          grpc_chttp2_transport_get_socket_node(transport), resource_user);-      // Use notify_on_receive_settings callback to enforce the-      // handshake deadline.-      connection_state->transport =-          reinterpret_cast<grpc_chttp2_transport*>(transport);-      gpr_ref(&connection_state->refs);-      GRPC_CLOSURE_INIT(&connection_state->on_receive_settings,-                        on_receive_settings, connection_state,-                        grpc_schedule_on_exec_ctx);-      grpc_chttp2_transport_start_reading(-          transport, args->read_buffer, &connection_state->on_receive_settings);-      grpc_channel_args_destroy(args->args);-      gpr_ref(&connection_state->refs);-      GRPC_CHTTP2_REF_TRANSPORT((grpc_chttp2_transport*)transport,-                                ""receive settings timeout"");-      GRPC_CLOSURE_INIT(&connection_state->on_timeout, on_timeout,-                        connection_state, grpc_schedule_on_exec_ctx);-      grpc_timer_init(&connection_state->timer, connection_state->deadline,-                      &connection_state->on_timeout);-    } else {+void Chttp2ServerListener::ConnectionState::OnHandshakeDone(void* arg,+                                                            grpc_error* error) {+  auto* args = static_cast<HandshakerArgs*>(arg);+  ConnectionState* self = static_cast<ConnectionState*>(args->user_data);+  {+    MutexLock lock(&self->listener_->mu_);+    grpc_resource_user* resource_user =+        grpc_server_get_default_resource_user(self->listener_->server_);+    if (error != GRPC_ERROR_NONE || self->listener_->shutdown_) {+      const char* error_str = grpc_error_string(error);+      gpr_log(GPR_DEBUG, ""Handshaking failed: %s"", error_str);+      grpc_resource_user* resource_user =+          grpc_server_get_default_resource_user(self->listener_->server_);       if (resource_user != nullptr) {         grpc_resource_user_free(resource_user,                                 GRPC_RESOURCE_QUOTA_CHANNEL_SIZE);       }+      if (error == GRPC_ERROR_NONE && args->endpoint != nullptr) {+        // We were shut down after handshaking completed successfully, so+        // destroy the endpoint here.+        // TODO(ctiller): It is currently necessary to shutdown endpoints+        // before destroying them, even if we know that there are no+        // pending read/write callbacks.  This should be fixed, at which+        // point this can be removed.+        grpc_endpoint_shutdown(args->endpoint, GRPC_ERROR_NONE);+        grpc_endpoint_destroy(args->endpoint);+        grpc_channel_args_destroy(args->args);+        grpc_slice_buffer_destroy_internal(args->read_buffer);+        gpr_free(args->read_buffer);+      }+    } else {+      // If the handshaking succeeded but there is no endpoint, then the+      // handshaker may have handed off the connection to some external+      // code, so we can just clean up here without creating a transport.+      if (args->endpoint != nullptr) {+        grpc_transport* transport = grpc_create_chttp2_transport(+            args->args, args->endpoint, false, resource_user);+        grpc_server_setup_transport(+            self->listener_->server_, transport, self->accepting_pollset_,+            args->args, grpc_chttp2_transport_get_socket_node(transport),+            resource_user);+        // Use notify_on_receive_settings callback to enforce the+        // handshake deadline.+        self->transport_ = reinterpret_cast<grpc_chttp2_transport*>(transport);",To me reinterpret_cast is a code smell but I understand why it works here. Can I suggest a comment above saying that this reinterpret_cast is safe because grpc_chttp2_transport is currently a C-style extension of grpc_transport and thus morally equivalent to a static_cast to a derived class?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23104,435456294,2020-06-04T18:18:30Z,src/core/ext/transport/chttp2/server/chttp2_server.cc,"@@ -47,405 +49,429 @@ #include ""src/core/lib/surface/api_trace.h"" #include ""src/core/lib/surface/server.h"" -typedef struct {-  grpc_server* server;-  grpc_tcp_server* tcp_server;-  grpc_channel_args* args;-  gpr_mu mu;-  bool shutdown;-  grpc_closure tcp_server_shutdown_complete;-  grpc_closure* server_destroy_listener_done;-  grpc_core::HandshakeManager* pending_handshake_mgrs;-  grpc_core::RefCountedPtr<grpc_core::channelz::ListenSocketNode>-      channelz_listen_socket;-} server_state;--typedef struct {-  gpr_refcount refs;-  server_state* svr_state;-  grpc_pollset* accepting_pollset;-  grpc_tcp_server_acceptor* acceptor;-  grpc_core::RefCountedPtr<grpc_core::HandshakeManager> handshake_mgr;-  // State for enforcing handshake timeout on receiving HTTP/2 settings.-  grpc_chttp2_transport* transport;-  grpc_millis deadline;-  grpc_timer timer;-  grpc_closure on_timeout;-  grpc_closure on_receive_settings;-  grpc_pollset_set* interested_parties;-} server_connection_state;--static void server_connection_state_unref(-    server_connection_state* connection_state) {-  if (gpr_unref(&connection_state->refs)) {-    if (connection_state->transport != nullptr) {-      GRPC_CHTTP2_UNREF_TRANSPORT(connection_state->transport,-                                  ""receive settings timeout"");-    }-    grpc_pollset_set_del_pollset(connection_state->interested_parties,-                                 connection_state->accepting_pollset);-    grpc_pollset_set_destroy(connection_state->interested_parties);-    gpr_free(connection_state);+namespace grpc_core {+namespace {++class Chttp2ServerListener : public ServerListenerInterface {+ public:+  static grpc_error* Create(grpc_server* server, const char* addr,+                            grpc_channel_args* args, int* port_num);++  static grpc_error* CreateWithAcceptor(grpc_server* server, const char* name,+                                        grpc_channel_args* args);++  // Do not instantiate directly.  Use one of the factory methods above.+  Chttp2ServerListener(grpc_server* server, grpc_channel_args* args);+  ~Chttp2ServerListener();++  void SetOnDestroyDone(grpc_closure* on_destroy_done) override;++  void Start(grpc_server* server, grpc_pollset** pollsets, size_t npollsets)+      override;++  channelz::ListenSocketNode* channelz_listen_socket_node() const override {+    return channelz_listen_socket_.get();+  }++  void Orphan() override;++ private:+  class ConnectionState : public RefCounted<ConnectionState> {+   public:+    ConnectionState(Chttp2ServerListener* listener,+                    grpc_pollset* accepting_pollset,+                    grpc_tcp_server_acceptor* acceptor,+                    RefCountedPtr<HandshakeManager> handshake_mgr,+                    grpc_channel_args* args, grpc_endpoint* endpoint);++    ~ConnectionState();++   private:+    static void OnTimeout(void* arg, grpc_error* error);+    static void OnReceiveSettings(void* arg, grpc_error* error);+    static void OnHandshakeDone(void* arg, grpc_error* error);++    Chttp2ServerListener* listener_;+    grpc_pollset* accepting_pollset_;+    grpc_tcp_server_acceptor* acceptor_;+    RefCountedPtr<HandshakeManager> handshake_mgr_;+    // State for enforcing handshake timeout on receiving HTTP/2 settings.+    grpc_chttp2_transport* transport_ = nullptr;+    grpc_millis deadline_;+    grpc_timer timer_;+    grpc_closure on_timeout_;+    grpc_closure on_receive_settings_;+    grpc_pollset_set* interested_parties_;+  };++  static void OnAccept(void* arg, grpc_endpoint* tcp,+                       grpc_pollset* accepting_pollset,+                       grpc_tcp_server_acceptor* acceptor);++  RefCountedPtr<HandshakeManager> CreateHandshakeManager();++  static void TcpServerShutdownComplete(void* arg, grpc_error* error);++  static void DestroyListener(grpc_server* /*server*/, void* arg,+                              grpc_closure* destroy_done);++  grpc_server* server_;+  grpc_tcp_server* tcp_server_;+  grpc_channel_args* args_;+  Mutex mu_;+  bool shutdown_ = true;+  grpc_closure tcp_server_shutdown_complete_;+  grpc_closure* on_destroy_done_ = nullptr;+  HandshakeManager* pending_handshake_mgrs_ = nullptr;+  RefCountedPtr<channelz::ListenSocketNode> channelz_listen_socket_;+};++//+// Chttp2ServerListener::ConnectionState+//++grpc_millis GetConnectionDeadline(const grpc_channel_args* args) {+  int timeout_ms =+      grpc_channel_args_find_integer(args, GRPC_ARG_SERVER_HANDSHAKE_TIMEOUT_MS,+                                     {120 * GPR_MS_PER_SEC, 1, INT_MAX});+  return ExecCtx::Get()->Now() + timeout_ms;+}++Chttp2ServerListener::ConnectionState::ConnectionState(+    Chttp2ServerListener* listener, grpc_pollset* accepting_pollset,+    grpc_tcp_server_acceptor* acceptor,+    RefCountedPtr<HandshakeManager> handshake_mgr, grpc_channel_args* args,+    grpc_endpoint* endpoint)+    : listener_(listener),+      accepting_pollset_(accepting_pollset),+      acceptor_(acceptor),+      handshake_mgr_(std::move(handshake_mgr)),+      deadline_(GetConnectionDeadline(args)),+      interested_parties_(grpc_pollset_set_create()) {+  grpc_pollset_set_add_pollset(interested_parties_, accepting_pollset_);+  HandshakerRegistry::AddHandshakers(HANDSHAKER_SERVER, args,+                                     interested_parties_, handshake_mgr_.get());+  handshake_mgr_->DoHandshake(endpoint, args, deadline_, acceptor_,+                              OnHandshakeDone, this);+}++Chttp2ServerListener::ConnectionState::~ConnectionState() {+  if (transport_ != nullptr) {+    GRPC_CHTTP2_UNREF_TRANSPORT(transport_, ""receive settings timeout"");   }+  grpc_pollset_set_del_pollset(interested_parties_, accepting_pollset_);+  grpc_pollset_set_destroy(interested_parties_); } -static void on_timeout(void* arg, grpc_error* error) {-  server_connection_state* connection_state =-      static_cast<server_connection_state*>(arg);+void Chttp2ServerListener::ConnectionState::OnTimeout(void* arg,+                                                      grpc_error* error) {+  ConnectionState* self = static_cast<ConnectionState*>(arg);   // Note that we may be called with GRPC_ERROR_NONE when the timer fires   // or with an error indicating that the timer system is being shut down.   if (error != GRPC_ERROR_CANCELLED) {     grpc_transport_op* op = grpc_make_transport_op(nullptr);     op->disconnect_with_error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(         ""Did not receive HTTP/2 settings before handshake timeout"");-    grpc_transport_perform_op(&connection_state->transport->base, op);+    grpc_transport_perform_op(&self->transport_->base, op);   }-  server_connection_state_unref(connection_state);+  self->Unref(); } -static void on_receive_settings(void* arg, grpc_error* error) {-  server_connection_state* connection_state =-      static_cast<server_connection_state*>(arg);+void Chttp2ServerListener::ConnectionState::OnReceiveSettings(+    void* arg, grpc_error* error) {+  ConnectionState* self = static_cast<ConnectionState*>(arg);   if (error == GRPC_ERROR_NONE) {-    grpc_timer_cancel(&connection_state->timer);+    grpc_timer_cancel(&self->timer_);   }-  server_connection_state_unref(connection_state);+  self->Unref(); } -static void on_handshake_done(void* arg, grpc_error* error) {-  auto* args = static_cast<grpc_core::HandshakerArgs*>(arg);-  server_connection_state* connection_state =-      static_cast<server_connection_state*>(args->user_data);-  gpr_mu_lock(&connection_state->svr_state->mu);-  grpc_resource_user* resource_user = grpc_server_get_default_resource_user(-      connection_state->svr_state->server);-  if (error != GRPC_ERROR_NONE || connection_state->svr_state->shutdown) {-    const char* error_str = grpc_error_string(error);-    gpr_log(GPR_DEBUG, ""Handshaking failed: %s"", error_str);-    grpc_resource_user* resource_user = grpc_server_get_default_resource_user(-        connection_state->svr_state->server);-    if (resource_user != nullptr) {-      grpc_resource_user_free(resource_user, GRPC_RESOURCE_QUOTA_CHANNEL_SIZE);-    }-    if (error == GRPC_ERROR_NONE && args->endpoint != nullptr) {-      // We were shut down after handshaking completed successfully, so-      // destroy the endpoint here.-      // TODO(ctiller): It is currently necessary to shutdown endpoints-      // before destroying them, even if we know that there are no-      // pending read/write callbacks.  This should be fixed, at which-      // point this can be removed.-      grpc_endpoint_shutdown(args->endpoint, GRPC_ERROR_NONE);-      grpc_endpoint_destroy(args->endpoint);-      grpc_channel_args_destroy(args->args);-      grpc_slice_buffer_destroy_internal(args->read_buffer);-      gpr_free(args->read_buffer);-    }-  } else {-    // If the handshaking succeeded but there is no endpoint, then the-    // handshaker may have handed off the connection to some external-    // code, so we can just clean up here without creating a transport.-    if (args->endpoint != nullptr) {-      grpc_transport* transport = grpc_create_chttp2_transport(-          args->args, args->endpoint, false, resource_user);-      grpc_server_setup_transport(-          connection_state->svr_state->server, transport,-          connection_state->accepting_pollset, args->args,-          grpc_chttp2_transport_get_socket_node(transport), resource_user);-      // Use notify_on_receive_settings callback to enforce the-      // handshake deadline.-      connection_state->transport =-          reinterpret_cast<grpc_chttp2_transport*>(transport);-      gpr_ref(&connection_state->refs);-      GRPC_CLOSURE_INIT(&connection_state->on_receive_settings,-                        on_receive_settings, connection_state,-                        grpc_schedule_on_exec_ctx);-      grpc_chttp2_transport_start_reading(-          transport, args->read_buffer, &connection_state->on_receive_settings);-      grpc_channel_args_destroy(args->args);-      gpr_ref(&connection_state->refs);-      GRPC_CHTTP2_REF_TRANSPORT((grpc_chttp2_transport*)transport,-                                ""receive settings timeout"");-      GRPC_CLOSURE_INIT(&connection_state->on_timeout, on_timeout,-                        connection_state, grpc_schedule_on_exec_ctx);-      grpc_timer_init(&connection_state->timer, connection_state->deadline,-                      &connection_state->on_timeout);-    } else {+void Chttp2ServerListener::ConnectionState::OnHandshakeDone(void* arg,+                                                            grpc_error* error) {+  auto* args = static_cast<HandshakerArgs*>(arg);+  ConnectionState* self = static_cast<ConnectionState*>(args->user_data);+  {+    MutexLock lock(&self->listener_->mu_);+    grpc_resource_user* resource_user =+        grpc_server_get_default_resource_user(self->listener_->server_);+    if (error != GRPC_ERROR_NONE || self->listener_->shutdown_) {+      const char* error_str = grpc_error_string(error);+      gpr_log(GPR_DEBUG, ""Handshaking failed: %s"", error_str);+      grpc_resource_user* resource_user =+          grpc_server_get_default_resource_user(self->listener_->server_);       if (resource_user != nullptr) {         grpc_resource_user_free(resource_user,                                 GRPC_RESOURCE_QUOTA_CHANNEL_SIZE);       }+      if (error == GRPC_ERROR_NONE && args->endpoint != nullptr) {+        // We were shut down after handshaking completed successfully, so+        // destroy the endpoint here.+        // TODO(ctiller): It is currently necessary to shutdown endpoints+        // before destroying them, even if we know that there are no+        // pending read/write callbacks.  This should be fixed, at which+        // point this can be removed.+        grpc_endpoint_shutdown(args->endpoint, GRPC_ERROR_NONE);+        grpc_endpoint_destroy(args->endpoint);+        grpc_channel_args_destroy(args->args);+        grpc_slice_buffer_destroy_internal(args->read_buffer);+        gpr_free(args->read_buffer);+      }+    } else {+      // If the handshaking succeeded but there is no endpoint, then the+      // handshaker may have handed off the connection to some external+      // code, so we can just clean up here without creating a transport.+      if (args->endpoint != nullptr) {+        grpc_transport* transport = grpc_create_chttp2_transport(+            args->args, args->endpoint, false, resource_user);+        grpc_server_setup_transport(+            self->listener_->server_, transport, self->accepting_pollset_,+            args->args, grpc_chttp2_transport_get_socket_node(transport),+            resource_user);+        // Use notify_on_receive_settings callback to enforce the+        // handshake deadline.+        self->transport_ = reinterpret_cast<grpc_chttp2_transport*>(transport);+        self->Ref().release();  // Held by OnReceiveSettings().+        GRPC_CLOSURE_INIT(&self->on_receive_settings_, OnReceiveSettings, self,+                          grpc_schedule_on_exec_ctx);+        grpc_chttp2_transport_start_reading(transport, args->read_buffer,+                                            &self->on_receive_settings_);+        grpc_channel_args_destroy(args->args);+        self->Ref().release();  // Held by OnTimeout().+        GRPC_CHTTP2_REF_TRANSPORT((grpc_chttp2_transport*)transport,+                                  ""receive settings timeout"");+        GRPC_CLOSURE_INIT(&self->on_timeout_, OnTimeout, self,+                          grpc_schedule_on_exec_ctx);+        grpc_timer_init(&self->timer_, self->deadline_, &self->on_timeout_);+      } else {+        if (resource_user != nullptr) {+          grpc_resource_user_free(resource_user,+                                  GRPC_RESOURCE_QUOTA_CHANNEL_SIZE);+        }+      }     }+    self->handshake_mgr_->RemoveFromPendingMgrList(+        &self->listener_->pending_handshake_mgrs_);   }-  connection_state->handshake_mgr->RemoveFromPendingMgrList(-      &connection_state->svr_state->pending_handshake_mgrs);-  gpr_mu_unlock(&connection_state->svr_state->mu);-  connection_state->handshake_mgr.reset();-  gpr_free(connection_state->acceptor);-  grpc_tcp_server_unref(connection_state->svr_state->tcp_server);-  server_connection_state_unref(connection_state);+  self->handshake_mgr_.reset();+  gpr_free(self->acceptor_);+  grpc_tcp_server_unref(self->listener_->tcp_server_);+  self->Unref(); } -static void on_accept(void* arg, grpc_endpoint* tcp,-                      grpc_pollset* accepting_pollset,-                      grpc_tcp_server_acceptor* acceptor) {-  server_state* state = static_cast<server_state*>(arg);-  gpr_mu_lock(&state->mu);-  if (state->shutdown) {-    gpr_mu_unlock(&state->mu);-    grpc_endpoint_shutdown(tcp, GRPC_ERROR_NONE);-    grpc_endpoint_destroy(tcp);-    gpr_free(acceptor);-    return;+//+// Chttp2ServerListener+//++grpc_error* Chttp2ServerListener::Create(grpc_server* server, const char* addr,+                                         grpc_channel_args* args,+                                         int* port_num) {+  std::vector<grpc_error*> error_list;+  grpc_resolved_addresses* resolved = nullptr;+  Chttp2ServerListener* listener = nullptr;+  grpc_error* error = [&]() {","The only reason I moved this into a lambda is to eliminate the use of C-style goto.  The logic inside of the lambda is pretty tightly coupled to this method; it I made it a helper function, it would require access to literally every local variable defined here, so it doesn't seem worth doing that.Honestly, the real cause of the complexity here is that `grpc_tcp_server_create()` requires passing in the cleanup closure at construction time.  Since that closure is part of the listener, that means we need to allocate the listener before we know if the operation succeeds, which complicates the error-handling logic.  If it weren't for that, we could just use normal RAII techniques here.For now, I've added a comment explaining why this lambda is here.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23143,436057680,2020-06-05T17:21:11Z,tools/run_tests/run_xds_tests.py,"@@ -478,57 +478,77 @@ def test_round_robin(gcp, backend_service, instance_group):   def test_secondary_locality_gets_no_requests_on_partial_primary_failure(-        gcp, backend_service, primary_instance_group,-        secondary_zone_instance_group):+        gcp, backend_service, primary_instance_group, secondary_instance_group):     logger.info(         'Running test_secondary_locality_gets_no_requests_on_partial_primary_failure'     )     try:         patch_backend_instances(             gcp, backend_service,-            [primary_instance_group, secondary_zone_instance_group])+            [primary_instance_group, secondary_instance_group])         wait_for_healthy_backends(gcp, backend_service, primary_instance_group)         wait_for_healthy_backends(gcp, backend_service,-                                  secondary_zone_instance_group)-        primary_instance_names = get_instance_names(gcp, instance_group)-        secondary_instance_names = get_instance_names(-            gcp, secondary_zone_instance_group)+                                  secondary_instance_group)+        # Ensure the backend patch has propagated before verifying which IG is+        # primary. An unconditional wait is necessary since, prior to the patch,+        # traffic can only go to primary_instance_group regardless of TD's+        # assignment post-patch.+        time.sleep(_WAIT_FOR_VALID_CONFIG_SEC)",Is this really the only way? It seems like this could introduce a flake. What if it takes longer than this for the config to propagate?,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23145,436100500,2020-06-05T18:41:10Z,src/core/ext/transport/chttp2/transport/chttp2_transport.cc,"@@ -2390,6 +2398,7 @@ static void close_from_api(grpc_chttp2_transport* t, grpc_chttp2_stream* s,   grpc_slice_buffer_add(&t->qbuf, status_hdr);   grpc_slice_buffer_add(&t->qbuf, message_pfx);   grpc_slice_buffer_add(&t->qbuf, grpc_slice_ref_internal(slice));+  grpc_chttp2_reset_ping_clock(t);","As I understand the problem here was that the server thought that the client was violating GRPC_ARG_HTTP2_MIN_RECV_PING_INTERVAL_WITHOUT_DATA_MS ?If so I'm a little confused by the documentation around `GRPC_ARG_HTTP2_MIN_RECV_PING_INTERVAL_WITHOUT_DATA_MS`, that are described in https://github.com/grpc/grpc/blob/master/doc/keepalive.md.Mainly this wording: ""If there are no data frames being sent on the transport"", makes me think that the transport only cares <i>data frames</i>. Should the wording there be expanded beyond data frames to all frame types?",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/23145,436110948,2020-06-05T19:03:35Z,src/core/ext/transport/chttp2/transport/chttp2_transport.cc,"@@ -2390,6 +2398,7 @@ static void close_from_api(grpc_chttp2_transport* t, grpc_chttp2_stream* s,   grpc_slice_buffer_add(&t->qbuf, status_hdr);   grpc_slice_buffer_add(&t->qbuf, message_pfx);   grpc_slice_buffer_add(&t->qbuf, grpc_slice_ref_internal(slice));+  grpc_chttp2_reset_ping_clock(t);","You are right. (Minor correction - ""all frame types"" - ""data/header/window_update""). Created #23151",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23151,436113302,2020-06-05T19:09:01Z,doc/keepalive.md,"@@ -16,11 +16,11 @@ The above two channel arguments should be sufficient for most users, but the fol * **GRPC_ARG_KEEPALIVE_PERMIT_WITHOUT_CALLS**   * This channel argument if set to 1 (0 : false; 1 : true), allows keepalive pings to be sent even if there are no calls in flight.  * **GRPC_ARG_HTTP2_MAX_PINGS_WITHOUT_DATA**-  * This channel argument controls the maximum number of pings that can be sent when there is no other data (data frame or header frame) to be sent. GRPC Core will not continue sending pings if we run over the limit. Setting it to 0 allows sending pings without sending data.+  * This channel argument controls the maximum number of pings that can be sent when there is no data/header/window_update frame to be sent. GRPC Core will not continue sending pings if we run over the limit. Setting it to 0 allows sending pings without such a restriction.","Nit: to be pedantic is this strictly limited to `data/header/window_update` frames? Or are other h2 frame types, like `RST_STREAM` or settings, also considered?I.e., should `data/header/window_update` be changed to something like `http2 frames (data frames, header frames, window_update frames, etc.)` ?",
1331988,rmariano,https://api.github.com/repos/grpc/grpc/pulls/23045,436751112,2020-06-08T14:26:44Z,src/python/grpcio/grpc/experimental/aio/_call.py,"@@ -58,22 +59,17 @@ class AioRpcError(grpc.RpcError):     determined. Hence, its methods no longer needs to be coroutines.     """""" -    # TODO(https://github.com/grpc/grpc/issues/20144) Metadata-    # type returned by `initial_metadata` and `trailing_metadata`-    # and also taken in the constructor needs to be revisit and make-    # it more specific.-     _code: grpc.StatusCode     _details: Optional[str]-    _initial_metadata: Optional[MetadataType]-    _trailing_metadata: Optional[MetadataType]+    _initial_metadata: Optional[Metadata]+    _trailing_metadata: Optional[Metadata]     _debug_error_string: Optional[str]      def __init__(self,                  code: grpc.StatusCode,                  details: Optional[str] = None,-                 initial_metadata: Optional[MetadataType] = None,-                 trailing_metadata: Optional[MetadataType] = None,+                 initial_metadata: Optional[Metadata] = None,+                 trailing_metadata: Optional[Metadata] = None,","I'll refactor this, to make these parameters no longer optional.",
28123841,guantaol,https://api.github.com/repos/grpc/grpc/pulls/22030,437119788,2020-06-09T03:38:25Z,cmake/modules/Findlibuv.cmake,"@@ -0,0 +1,87 @@+find_package(PkgConfig)","Thanks for the suggestion! I created a new libuv find module based on https://github.com/Kitware/CMake/blob/master/Source/Modules/FindLibUV.cmake, and added a comment at the top of the file.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22030,437599121,2020-06-09T17:28:30Z,setup.py,"@@ -213,8 +229,62 @@ def check_linker_need_libatomic(): CYTHON_HELPER_C_FILES = ()  CORE_C_FILES = tuple(grpc_core_dependencies.CORE_SOURCE_FILES)+if ""linux"" in sys.platform:+    CORE_C_FILES = filter(lambda x: 'third_party/libuv/src/win' not in x,+                          CORE_C_FILES)+    # The following source files are under libuv/src/unix but only for Darwin.+    # They need to be excluded when running on Linux, and must keep synchronous+    # with darwin_uv_srcs in src/libuv/gen_build_yaml.py, except+    # src/unix/proctitle.c which exists in both darwin_uv_srcs and+    # linux_uv_srcs.+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/bsd-ifaddrs.c' not in x,+        CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/darwin.c' not in x, CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/darwin-proctitle.c' not in x,+        CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/fsevents.c' not in x,+        CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/kqueue.c' not in x, CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/random-getentropy.c' not in x,+        CORE_C_FILES)","How about:```pythonignore_core_c_files = []if blah:    ignore_core_c_files.append('third_party/libuv/src/unix/bsd-ifaddrs.c')else:    ignore_core_c_files.append(...)CORE_C_FILES = filter(lambda x: x not in ignore_core_c_files, CORE_C_FILES)```These bunch of filter-plus-not-in statements can be merged into one list... It's quite inefficient from algorithm perspective.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23163,437631937,2020-06-09T18:24:31Z,src/python/grpcio/grpc/_simple_stubs.py,"@@ -221,9 +238,10 @@ def unary_unary(         immediately if the connection is not ready at the time the RPC is         invoked, or if it should wait until the connection to the server         becomes ready. When using this option, the user will likely also want-        to set a timeout. Defaults to False.+        to set a timeout. Defaults to True.       timeout: An optional duration of time in seconds to allow for the RPC,-        after which an exception will be raised.+        after which an exception will be raised. If timeout is unspecified and+        wait_for_ready is True, defaults to one minute.","While I like the interface here, the only way to implement this hurts the overall readibility of the code (both the gRPC surface and generated code).```pythondef unary_unary(...,     wait_for_ready=Optional[bool],      **kwargs):  if ""timeout"" in kwargs:    do_one_thing()  else:    do_another()```I do agree with you that it's better to keep the semantics of the existing API where possible, but the required change in signature seems like a net negative.*Edit: Maybe I'm not understanding you properly. Both code lines in [this comment](https://github.com/grpc/grpc/pull/23163#discussion_r437616598) were intended to be part of the same example. Code on the left, with the resulting RPC configurations on the right. Let's talk about it face-to-face later today.*",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23163,437635829,2020-06-09T18:31:38Z,src/python/grpcio/grpc/_simple_stubs.py,"@@ -221,9 +238,10 @@ def unary_unary(         immediately if the connection is not ready at the time the RPC is         invoked, or if it should wait until the connection to the server         becomes ready. When using this option, the user will likely also want-        to set a timeout. Defaults to False.+        to set a timeout. Defaults to True.       timeout: An optional duration of time in seconds to allow for the RPC,-        after which an exception will be raised.+        after which an exception will be raised. If timeout is unspecified and+        wait_for_ready is True, defaults to one minute.","If we want to provide type annotation for our users, we will need to expose a `grpc.DEFAULT` (or something like this). Then, it is equivalent or slightly worse than exposing an extra `grpc.NoTimeout`. `grpc.NoTimeout` semantically suggests it is a timeout value, while `grpc.DEFAULT` is vague. Sure, we can discuss this later today.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23174,438234296,2020-06-10T15:57:41Z,src/csharp/Grpc.Tools.Tests/Grpc.Tools.Tests.csproj,"@@ -7,14 +7,13 @@    <Import Project=""..\Grpc.Core\SourceLink.csproj.include"" /> -  <PropertyGroup Condition="" '$(OS)' != 'Windows_NT' and '$(MSBuildRuntimeType)' == 'Core' "">-    <!-- Use Mono reference assemblies in SDK build: https://github.com/dotnet/sdk/issues/335.-         This is a different approach than used in Grpc.Core/Common.csproj.include because-         the workaround used there doesn't seem to be working for Microsoft.Build.* assemblies -->-    <FrameworkPathOverride Condition=""Exists('/usr/lib/mono/4.5-api')"">/usr/lib/mono/4.5-api</FrameworkPathOverride>-    <FrameworkPathOverride Condition=""Exists('/usr/local/lib/mono/4.5-api')"">/usr/local/lib/mono/4.5-api</FrameworkPathOverride>-    <FrameworkPathOverride Condition=""Exists('/Library/Frameworks/Mono.framework/Versions/Current/lib/mono/4.5-api')"">/Library/Frameworks/Mono.framework/Versions/Current/lib/mono/4.5-api</FrameworkPathOverride>-  </PropertyGroup>+  <!-- Needed for the net45 build to work on Unix. See https://github.com/dotnet/designs/pull/33 -->+  <ItemGroup>",note that this is the recommended way of making net45 build work with pure dotnet SDK.We already use it here (and Grpc.Tools and Grpc.Tools.Tests are the only remaining project that still use the old hack): https://github.com/grpc/grpc/blob/80e834abab5dff45e16e9a1e3b98f20eae5f91ad/src/csharp/Grpc.Core/Common.csproj.include#L22,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23163,438389717,2020-06-10T20:29:14Z,src/python/grpcio_tests/tests_py3_only/unit/_simple_stubs_test.py,"@@ -311,6 +332,83 @@ def test_insecure_sugar_mutually_exclusive(self):                     insecure=True,                     channel_credentials=grpc.local_channel_credentials()) +    def test_default_wait_for_ready(self):+        addr, port, sock = get_socket()+        sock.close()+        target = f'{addr}:{port}'+        channel = grpc._simple_stubs.ChannelCache.get().get_channel(+            target, (), None, True, None)+        rpc_finished_event = threading.Event()+        rpc_failed_event = threading.Event()+        server = None++        def _on_connectivity_changed(connectivity):+            nonlocal server+            if connectivity is grpc.ChannelConnectivity.TRANSIENT_FAILURE:+                self.assertFalse(rpc_finished_event.is_set())+                self.assertFalse(rpc_failed_event.is_set())+                server = test_common.test_server()+                server.add_insecure_port(target)+                server.add_generic_rpc_handlers((_GenericHandler(),))+                server.start()+                channel.unsubscribe(_on_connectivity_changed)+            elif connectivity in (grpc.ChannelConnectivity.IDLE,+                                  grpc.ChannelConnectivity.CONNECTING):+                pass+            else:+                self.fail(""Encountered unknown state."")++        channel.subscribe(_on_connectivity_changed)++        def _send_rpc():+            try:+                response = grpc.experimental.unary_unary(_REQUEST,+                                                         target,+                                                         _UNARY_UNARY,+                                                         timeout=None,+                                                         insecure=True)+                rpc_finished_event.set()+            except Exception as e:+                rpc_failed_event.set()++        t = threading.Thread(target=_send_rpc)+        t.start()+        t.join()+        self.assertFalse(rpc_failed_event.is_set())+        self.assertTrue(rpc_finished_event.is_set())+        if server is not None:+            server.stop(None)++    def assert_times_out(self, invocation_args):+        with _server(None) as port:+            target = f'localhost:{port}'+            with self.assertRaises(grpc.RpcError) as cm:+                response = grpc.experimental.unary_unary(_REQUEST,+                                                         target,+                                                         _BLACK_HOLE,+                                                         insecure=True,+                                                         **invocation_args)+            self.assertEqual(grpc.StatusCode.DEADLINE_EXCEEDED,+                             cm.exception.code())++    def test_default_timeout(self):+        not_present = object()+        wait_for_ready_values = [True, not_present]+        timeout_values = [0.5, not_present]+        cases = []+        for wait_for_ready in wait_for_ready_values:+            for timeout in timeout_values:",Ops. My bad.,
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/23165,438540429,2020-06-11T04:47:37Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1472,10 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    unsigned char* unused_bytes = nullptr;","I did not understand this part. So with ssl_bytes_remaining, another read from SSL is done. However, is it possible that received_bytes_size is larger than bytes_consumed? The old code seems to assume so and in the new code that part of data is just ignored?",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,438981264,2020-06-11T18:14:38Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1472,10 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    unsigned char* unused_bytes = nullptr;","The old code had the following bug in the implementation of `ssl_handshaker_process_bytes_from_peer`. It computed `bytes_consumed` as the number of bytes written to (the write BIO of) `impl->bio`. Those bytes are then moved to (the read BIO of) `impl->ssl` when `SSL_do_handshake` is called, so `bytes_consumed` will *always* be equal to `received_bytes_size`. This bug was never caught because the TLS 1.2 RFC explicitly prohibits any application data from being appended to a handshake message (i.e. there are never any unused bytes for TLS 1.2).The new code computes `unused_bytes_size` by reading from the correct BIO, namely (the read buffer of) `impl->ssl`. Does that make sense?Additionally (and this might be what you were asking), there *should* be a check that `unused_bytes_size <= received_bytes_size` after the call to `ssl_bytes_remaining`. I'll add this in a separate commit.",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/23178,439112356,2020-06-11T22:48:46Z,test/cpp/qps/worker.cc,"@@ -34,6 +34,9 @@ DEFINE_int32(driver_port, 0, ""Port for communication with driver""); DEFINE_int32(server_port, 0, ""Port for operation as a server""); DEFINE_string(credential_type, grpc::testing::kInsecureCredentialsType,               ""Credential type for communication with driver"");+DEFINE_string(+    driver_port_result_path, """",+    ""Path for the file to have a driver port. (Useful when driver_port is 0)"");","This comment should further say that this mechanism only works when the driver is on the same machine as the workers and instantiates the workers itself. This is the use case for the unit test versions of the QPS tests but not the actual benchmark versions. As an alternative, have you considered passing this information from the driver to the worker as a new RPC instead of through a file, since the workers are servers?",
10122250,yang-g,https://api.github.com/repos/grpc/grpc/pulls/23165,439135370,2020-06-12T00:07:40Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1480,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.+    unsigned char* unused_bytes = nullptr;+    size_t unused_bytes_size = 0;+    status = ssl_bytes_remaining(impl, &unused_bytes, &unused_bytes_size);+    if (status != TSI_OK) return status;+    if (unused_bytes_size > received_bytes_size) {+      gpr_log(GPR_INFO, ""More unused bytes than received bytes."");+      gpr_free(unused_bytes);+      return TSI_INTERNAL_ERROR;+    }     status = ssl_handshaker_result_create(impl, unused_bytes, unused_bytes_size,                                           handshaker_result);",It seems result_create used to take an unowned buffer and now you are passing in an owned buffer to it. Not sure whether you should release the unused_bytes after calling it.,
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,439187296,2020-06-12T03:28:06Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1480,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.+    unsigned char* unused_bytes = nullptr;+    size_t unused_bytes_size = 0;+    status = ssl_bytes_remaining(impl, &unused_bytes, &unused_bytes_size);+    if (status != TSI_OK) return status;+    if (unused_bytes_size > received_bytes_size) {+      gpr_log(GPR_INFO, ""More unused bytes than received bytes."");+      gpr_free(unused_bytes);+      return TSI_INTERNAL_ERROR;+    }     status = ssl_handshaker_result_create(impl, unused_bytes, unused_bytes_size,                                           handshaker_result);","Great catch. I've changed `ssl_handshaker_result_create` so that it takes ownership of `unused_bytes`, and it will be freed when `ssl_handshaker_result_destroy` is called. This also saves us from doing a copy in `ssl_handshaker_result_create`.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23200,439659926,2020-06-12T22:03:09Z,requirements.bazel.txt,"@@ -6,13 +6,11 @@ protobuf>=3.5.0.post1 six>=1.10 wheel>=0.29 futures>=2.2.0-google-auth>=1.17.1+google-auth>=1.17.2 oauth2client==4.1.0 requests>=2.14.2 urllib3>=1.23 chardet==3.0.4 certifi==2017.4.17 idna==2.7 googleapis-common-protos==1.5.5-# rsa 4.3 is the last version support Python 2-rsa==4.3","I'm pretty sure this will break. IIRC, `rules_python` requires you to identify versions for your entire dependency tree.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22030,439660278,2020-06-12T22:04:33Z,setup.py,"@@ -213,8 +229,62 @@ def check_linker_need_libatomic(): CYTHON_HELPER_C_FILES = ()  CORE_C_FILES = tuple(grpc_core_dependencies.CORE_SOURCE_FILES)+if ""linux"" in sys.platform:+    CORE_C_FILES = filter(lambda x: 'third_party/libuv/src/win' not in x,+                          CORE_C_FILES)+    # The following source files are under libuv/src/unix but only for Darwin.+    # They need to be excluded when running on Linux, and must keep synchronous+    # with darwin_uv_srcs in src/libuv/gen_build_yaml.py, except+    # src/unix/proctitle.c which exists in both darwin_uv_srcs and+    # linux_uv_srcs.+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/bsd-ifaddrs.c' not in x,+        CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/darwin.c' not in x, CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/darwin-proctitle.c' not in x,+        CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/fsevents.c' not in x,+        CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/kqueue.c' not in x, CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/random-getentropy.c' not in x,+        CORE_C_FILES)",Can you also apply the same pattern for all file-excluding logic? You can find them through the `CORE_C_FILES = filter(lambda x:` prefix.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23200,439660872,2020-06-12T22:06:45Z,requirements.bazel.txt,"@@ -6,13 +6,11 @@ protobuf>=3.5.0.post1 six>=1.10 wheel>=0.29 futures>=2.2.0-google-auth>=1.17.1+google-auth>=1.17.2 oauth2client==4.1.0 requests>=2.14.2 urllib3>=1.23 chardet==3.0.4 certifi==2017.4.17 idna==2.7 googleapis-common-protos==1.5.5-# rsa 4.3 is the last version support Python 2-rsa==4.3","I thought so, but `rsa` is a new dependency I manually added in https://github.com/grpc/grpc/pull/23197. If it wasn't breaking in the past, reverting should not break other stuff.However, Python tests are still failing. I'm not sure where the `rsa==4.0` slipped into our CI. ",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23200,439664962,2020-06-12T22:22:10Z,requirements.bazel.txt,"@@ -6,13 +6,11 @@ protobuf>=3.5.0.post1 six>=1.10 wheel>=0.29 futures>=2.2.0-google-auth>=1.17.1+google-auth>=1.17.2 oauth2client==4.1.0 requests>=2.14.2 urllib3>=1.23 chardet==3.0.4 certifi==2017.4.17 idna==2.7 googleapis-common-protos==1.5.5-# rsa 4.3 is the last version support Python 2-rsa==4.3",Huh. Maybe `google-auth` only has a soft dependency on `rsa`?**Edit**: That appears to be [precisely what's happening](https://github.com/googleapis/google-auth-library-python/blob/37141e4dffc2ba3f3f57c5914544fb8b9cf7d017/google/auth/crypt/rsa.py).,
28123841,guantaol,https://api.github.com/repos/grpc/grpc/pulls/22030,439670996,2020-06-12T22:48:24Z,setup.py,"@@ -213,8 +229,62 @@ def check_linker_need_libatomic(): CYTHON_HELPER_C_FILES = ()  CORE_C_FILES = tuple(grpc_core_dependencies.CORE_SOURCE_FILES)+if ""linux"" in sys.platform:+    CORE_C_FILES = filter(lambda x: 'third_party/libuv/src/win' not in x,+                          CORE_C_FILES)+    # The following source files are under libuv/src/unix but only for Darwin.+    # They need to be excluded when running on Linux, and must keep synchronous+    # with darwin_uv_srcs in src/libuv/gen_build_yaml.py, except+    # src/unix/proctitle.c which exists in both darwin_uv_srcs and+    # linux_uv_srcs.+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/bsd-ifaddrs.c' not in x,+        CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/darwin.c' not in x, CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/darwin-proctitle.c' not in x,+        CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/fsevents.c' not in x,+        CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/kqueue.c' not in x, CORE_C_FILES)+    CORE_C_FILES = filter(+        lambda x: 'third_party/libuv/src/unix/random-getentropy.c' not in x,+        CORE_C_FILES)","Others may not work with this pattern since they are paths to directories but not files, so they are checking whether the string is part of a file path, e.g.`CORE_C_FILES = filter(lambda x: 'third_party/cares' not in x, CORE_C_FILES)`",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/23165,439671464,2020-06-12T22:50:29Z,src/core/tsi/ssl_transport_security.cc,"@@ -1410,6 +1407,36 @@ static void ssl_handshaker_destroy(tsi_handshaker* self) {   gpr_free(impl); } +// Removes the bytes remaining in |impl->SSL|'s read BIO and writes them to+// |bytes_remaining|.+static tsi_result ssl_bytes_remaining(tsi_ssl_handshaker* impl,","Not sure why we need to read unused bytes again. In ssl_handshaker_next, we already have received_bytes. We just need to call `BIO_pending(SSL_get_rbio(impl->ssl))` to find out how many bytes have been consumed and how many have not, then we can use the received_bytes buffer as unused_bytes (which is received_bytes + bytes_consumed).",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,439672307,2020-06-12T22:54:51Z,src/core/tsi/ssl_transport_security.cc,"@@ -1410,6 +1407,36 @@ static void ssl_handshaker_destroy(tsi_handshaker* self) {   gpr_free(impl); } +// Removes the bytes remaining in |impl->SSL|'s read BIO and writes them to+// |bytes_remaining|.+static tsi_result ssl_bytes_remaining(tsi_ssl_handshaker* impl,","We have to remove the unused bytes from `impl->ssl`'s read BIO, otherwise it will try to decrypt the record twice. We could either:1. Get the length (using `BIO_pending`) and clear `impl->ssl`'s read BIO, and then use received bytes.2. Read the bytes from `impl->ssl` and set those to unused bytes.There's no difference in memory use between the 2 options because `received_bytes` used to be copied. I opted for the second option because I think it is conceptually cleaner.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/22984,439679138,2020-06-12T23:29:51Z,test/cpp/qps/driver.cc,"@@ -504,16 +508,19 @@ std::unique_ptr<ScenarioResult> RunScenario(       GPR_ASSERT(false);     }   }-  gpr_log(GPR_INFO, ""Finishing servers"");-  for (size_t i = 0; i < num_servers; i++) {-    auto server = &servers[i];-    if (!server->stream->Write(server_mark)) {-      gpr_log(GPR_ERROR, ""Couldn't write mark to server %zu"", i);-      GPR_ASSERT(false);-    }-    if (!server->stream->WritesDone()) {-      gpr_log(GPR_ERROR, ""Failed WritesDone for server %zu"", i);-      GPR_ASSERT(false);++  if (!client_finish_first) {+    gpr_log(GPR_INFO, ""Finishing servers"");",Sure I'm going to refactor this large function into many small functions.,X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/23178,439681263,2020-06-12T23:42:30Z,test/cpp/qps/worker.cc,"@@ -34,6 +34,9 @@ DEFINE_int32(driver_port, 0, ""Port for communication with driver""); DEFINE_int32(server_port, 0, ""Port for operation as a server""); DEFINE_string(credential_type, grpc::testing::kInsecureCredentialsType,               ""Credential type for communication with driver"");+DEFINE_string(+    driver_port_result_path, """",+    ""Path for the file to have a driver port. (Useful when driver_port is 0)"");","Practically this parameter is limited on the same machine but it's still possible if the path is accessible from both machines but I can add more context for that and I don't think I understand your suggestion, how do workers pass the info to the driver?",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/23165,439685819,2020-06-13T00:11:25Z,src/core/tsi/ssl_transport_security.cc,"@@ -1410,6 +1407,36 @@ static void ssl_handshaker_destroy(tsi_handshaker* self) {   gpr_free(impl); } +// Removes the bytes remaining in |impl->SSL|'s read BIO and writes them to+// |bytes_remaining|.+static tsi_result ssl_bytes_remaining(tsi_ssl_handshaker* impl,","I see. Thanks for explaining.impl->network_io already consumes and buffers the unused bytes. Later impl->network_io and impl->ssl are handed over from handshaker to frame protector. Maybe in unprotect(), we can first process the unused bytes from BIO, while gRPC stack treats unused bytes size = 0. so that we can avoid BIO_read again and set unused bytes.",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,439686462,2020-06-13T00:16:11Z,src/core/tsi/ssl_transport_security.cc,"@@ -1410,6 +1407,36 @@ static void ssl_handshaker_destroy(tsi_handshaker* self) {   gpr_free(impl); } +// Removes the bytes remaining in |impl->SSL|'s read BIO and writes them to+// |bytes_remaining|.+static tsi_result ssl_bytes_remaining(tsi_ssl_handshaker* impl,","This is what I tried originally but it created a problem (causing many tests to fail): if we set `unused_bytes_size` to zero, then the gRPC stack doesn't know it has any bytes waiting in the BIO that should be read. So, the server will just hang waiting for the client to send application data which the server already has, but doesn't know it has.If we wanted to leave the bytes in BIO, then we would need to plumb a new way to indicate to the gRPC stack that it has bytes waiting to be read. But this is precisely what the unused bytes are already used for. This is why I think we should stick with this strategy.",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,439687027,2020-06-13T00:20:37Z,src/core/tsi/ssl_transport_security.cc,"@@ -1410,6 +1407,36 @@ static void ssl_handshaker_destroy(tsi_handshaker* self) {   gpr_free(impl); } +// Removes the bytes remaining in |impl->SSL|'s read BIO and writes them to+// |bytes_remaining|.+static tsi_result ssl_bytes_remaining(tsi_ssl_handshaker* impl,+                                      unsigned char** bytes_remaining,+                                      size_t* bytes_remaining_size) {+  if (impl == nullptr || bytes_remaining == nullptr ||+      bytes_remaining_size == nullptr) {+    return TSI_INVALID_ARGUMENT;+  }+  // Atempt to read all of the bytes in SSL's read BIO. These bytes should+  // contain (encrypted) application data that was appended to a ClientFinished+  // or ServerFinished record.+  size_t bytes_in_ssl = BIO_pending(SSL_get_rbio(impl->ssl));","That's what I though originally too, but @agl explained to me that it should be the read BIO held by `impl->ssl`. The reason is that there are [2 BIO's created as a pair](https://github.com/grpc/grpc/blob/eff1579811248a8117d721f3d6c5d2c7c5f3416b/src/core/tsi/ssl_transport_security.cc#L1511), so gRPC should write to `impl->network_io` and these are read from the read BIO held by `impl->ssl`.",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/23165,439688216,2020-06-13T00:28:50Z,src/core/tsi/ssl_transport_security.cc,"@@ -1410,6 +1407,36 @@ static void ssl_handshaker_destroy(tsi_handshaker* self) {   gpr_free(impl); } +// Removes the bytes remaining in |impl->SSL|'s read BIO and writes them to+// |bytes_remaining|.+static tsi_result ssl_bytes_remaining(tsi_ssl_handshaker* impl,",Make sense. I agree that we should stick to unused bytes then.,
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/23143,439793550,2020-06-14T05:32:01Z,tools/run_tests/run_xds_tests.py,"@@ -478,57 +478,77 @@ def test_round_robin(gcp, backend_service, instance_group):   def test_secondary_locality_gets_no_requests_on_partial_primary_failure(-        gcp, backend_service, primary_instance_group,-        secondary_zone_instance_group):+        gcp, backend_service, primary_instance_group, secondary_instance_group):     logger.info(         'Running test_secondary_locality_gets_no_requests_on_partial_primary_failure'     )     try:         patch_backend_instances(             gcp, backend_service,-            [primary_instance_group, secondary_zone_instance_group])+            [primary_instance_group, secondary_instance_group])         wait_for_healthy_backends(gcp, backend_service, primary_instance_group)         wait_for_healthy_backends(gcp, backend_service,-                                  secondary_zone_instance_group)-        primary_instance_names = get_instance_names(gcp, instance_group)-        secondary_instance_names = get_instance_names(-            gcp, secondary_zone_instance_group)+                                  secondary_instance_group)+        # Ensure the backend patch has propagated before verifying which IG is+        # primary. An unconditional wait is necessary since, prior to the patch,+        # traffic can only go to primary_instance_group regardless of TD's+        # assignment post-patch.+        time.sleep(_WAIT_FOR_VALID_CONFIG_SEC)","~~So this check is essentially trying to prove that, when the config propagates, traffic will *not* go to `secondary_instance_group`. Since that definitely holds before propagation and should continue to hold afterwards, there's not really anything we can wait for here...I guess we could exit early in the (uncommon) case where traffic does start going to `secondary_instance_group` and save some of the unconditional 60 second wait. But the case that occurs 99+% of the time, with all traffic continuing to go to the expected primary locality, does as best as I can tell require this unconditional waiting.~~Hmm. Actually, maybe it would be preferable just to more-or-less wrap this entire test's asserts into a try-except block - so we begin with the assumption that the expected primary zone really is primary and, only if that fails (which will be very rare), fall back to check if the test would have passed if we swapped our primary/secondary expectation. This avoids the silly unconditional wait and keeps things ""speedy"" for the common case.I'll try to rework things and update this PR.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23213,440347705,2020-06-15T17:54:42Z,doc/grpc_xds_features.md,"@@ -0,0 +1,40 @@+# xDS Features in gRPC++This document lists the [xDS](https://github.com/envoyproxy/data-plane-api/tree/master/envoy/api/v2)+features supported in various gRPC language implementations and versions.++Note that a gRPC client will simply ignore the configuration of a feature it+does not support. The gRPC client does not generate a log+to indicate that some configuration was ignored. It is impractical to generate+a log and keep it up-to-date because xDS has a large number of APIs that gRPC+does not support and the APIs keep evolving too. We recommend reading the+[first gRFC](https://github.com/grpc/proposal/blob/master/A27-xds-global-load-balancing.md)+on xDS support in gRPC to understand the design philosophy.++The EDS policy will *not* support+[overprovisioning](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/load_balancing/overprovisioning),+which is different from Envoy.  Envoy takes the overprovisioning into+account in both [locality-weighted load balancing](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/load_balancing/locality_weight)+and [priority failover](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/load_balancing/priority),+but gRPC assumes that the xDS server will update it to redirect traffic+when this kind of graceful failover is needed.  gRPC will send the+[`envoy.lb.does_not_support_overprovisioning` client+feature](https://github.com/envoyproxy/envoy/pull/10136) to the xDS+server to tell the xDS server that it will not perform graceful failover;+xDS server implementations may use this to decide whether to perform+graceful failover themselves.++The EDS policy will not support per-endpoint stats; it will report only+per-locality stats.++An [`lb_endpoint`](https://github.com/envoyproxy/envoy/blob/master/api/envoy/api/v2/endpoint/endpoint_components.proto)",Suggest having this point to https://github.com/envoyproxy/envoy/blob/12a4bc430eaf440ceb0d11286cfbd4c16b79cdd1/api/envoy/api/v2/endpoint/endpoint_components.proto#L72.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23213,440348730,2020-06-15T17:56:26Z,doc/grpc_xds_features.md,"@@ -0,0 +1,40 @@+# xDS Features in gRPC++This document lists the [xDS](https://github.com/envoyproxy/data-plane-api/tree/master/envoy/api/v2)+features supported in various gRPC language implementations and versions.++Note that a gRPC client will simply ignore the configuration of a feature it+does not support. The gRPC client does not generate a log+to indicate that some configuration was ignored. It is impractical to generate+a log and keep it up-to-date because xDS has a large number of APIs that gRPC+does not support and the APIs keep evolving too. We recommend reading the+[first gRFC](https://github.com/grpc/proposal/blob/master/A27-xds-global-load-balancing.md)+on xDS support in gRPC to understand the design philosophy.++The EDS policy will *not* support+[overprovisioning](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/load_balancing/overprovisioning),+which is different from Envoy.  Envoy takes the overprovisioning into+account in both [locality-weighted load balancing](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/load_balancing/locality_weight)+and [priority failover](https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/load_balancing/priority),+but gRPC assumes that the xDS server will update it to redirect traffic+when this kind of graceful failover is needed.  gRPC will send the+[`envoy.lb.does_not_support_overprovisioning` client+feature](https://github.com/envoyproxy/envoy/pull/10136) to the xDS+server to tell the xDS server that it will not perform graceful failover;+xDS server implementations may use this to decide whether to perform+graceful failover themselves.++The EDS policy will not support per-endpoint stats; it will report only+per-locality stats.++An [`lb_endpoint`](https://github.com/envoyproxy/envoy/blob/master/api/envoy/api/v2/endpoint/endpoint_components.proto)+is ignored if the `health_status` is not HEALTHY or UNKNOWN.+The optional `load_balancing_weight` is always ignored.++Initially, only `google_default` channel creds will be supported+to authenticate with the xDS server.++Features | gRFCs  | [C++, Python,<br> Ruby, PHP, C#](https://github.com/grpc/grpc/releases) | [Java](https://github.com/grpc/grpc-java/releases) | [Go](https://github.com/grpc/grpc-go/releases)+---------|--------|--------------|------|------+**xDS Infrastructure:**<br>LDS->RDS->CDS->EDS flow,<br>ADS stream,<br>Client-side Load reporting via [LRS](https://github.com/envoyproxy/data-plane-api/blob/master/envoy/service/load_stats/v2/lrs.proto), | [A27](https://github.com/grpc/proposal/blob/master/A27-xds-global-load-balancing.md) | v1.30.0  | v1.30.0 | v1.30.0 |","The ""Client-side load reporting via LRS"" part probably belongs in the ""Load Balancing"" entry, not in the ""xDS Infrastructure"" entry.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/23181,440362163,2020-06-15T18:21:38Z,src/core/ext/transport/cronet/transport/cronet_transport.cc,"@@ -1260,17 +1269,10 @@ static enum e_op_result execute_stream_op(struct op_and_state* oas) {           oas->state.state_op_done[OP_RECV_MESSAGE] = true;            /* Extra read to trigger on_succeed */-          stream_state->rs.read_buffer = stream_state->rs.grpc_header_bytes;-          stream_state->rs.remaining_bytes = GRPC_HEADER_SIZE_IN_BYTES;-          stream_state->rs.received_bytes = 0;-          stream_state->rs.compressed = false;           stream_state->rs.length_field_received = false;","`state_op_done` is set when a gRPC message is completely read. When pre-reading the gRPC header (5 bytes), the field should be left untouched. `length_field_received` is set when the gRPC header is completely read, and reset when the message is completely read. They are not related to the ""read"" action; that's why I don't think they should not belong to the `do_an_extra_read` function.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/23181,440362933,2020-06-15T18:23:12Z,src/core/ext/transport/cronet/transport/cronet_transport.cc,"@@ -317,6 +317,17 @@ static void maybe_flush_read(stream_obj* s) {   } } +static void do_an_extra_read(stream_obj* s) {",I would suggest naming it `read_grpc_header` or something like that.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22987,441355761,2020-06-17T07:59:56Z,bazel/grpc_deps.bzl,"@@ -208,6 +213,17 @@ def grpc_deps():             ],         ) +    if ""com_github_google_re2"" not in native.existing_rules():+        http_archive(+            name = ""com_github_google_re2"",+            sha256 = ""77c34387d39f9d77c36821cd5207fe634cbdd9e67976ee75e882dd4c6195ec8b"",+            strip_prefix = ""re2-52b4b94b00f094d4a86c9b6bbb8276b21ec53505"",+            urls = [","make sure you're using a released version of re2.https://github.com/google/re2/releases - use a commit sha that corresponds to one of the released version (not a random commit on head). This is per https://github.com/grpc/grpc/blob/master/third_party/README.md#guidelines-on-updating-submodules (please read).Also, don't forget to run update_mirror.sh before submitting.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22987,441358362,2020-06-17T08:04:28Z,src/re2/gen_build_yaml.py,"@@ -0,0 +1,48 @@+#!/usr/bin/env python2.7++# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++from __future__ import print_function+import os+import sys+import glob+import yaml++os.chdir(os.path.dirname(sys.argv[0]) + '/../..')++out = {}++out['libs'] = [{+    'name':","adding the `re2` library this way works, but depending on how the corresponding bazel BUILD file works, you might be able to get away with just extracting the list of source files from bazel BUILD automatically.I think for the first version of the PR this is optional, but I wanted to mentions that there might be a better and cleaner way of doing this and it's something you should look into after this PR is in.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22030,441494589,2020-06-17T12:06:38Z,CMakeLists.txt,"@@ -3682,6 +3709,243 @@ if(gRPC_INSTALL)   ) endif() +if(gRPC_BUILD_TESTS)++add_library(uv_linux","this seems wrong, you should be using the libuv targets provided by libuv's CmakeLists.txt, not constructing the libraries here.Should you list the libuv targets here: https://github.com/grpc/grpc/blob/84cf6660147f91f616e886757cbaf7ccedbe01c5/templates/CMakeLists.txt.template#L458?",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22030,441502001,2020-06-17T12:20:48Z,src/libuv/gen_build_yaml.py,"@@ -0,0 +1,127 @@+#!/usr/bin/env python2.7++# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import os+import sys+import yaml+import glob++out = {}","needing to maintain this file is going to be a pain (especially because it hand lists the source files and there are 3 different libraries for 3 different systems (linux, darwin, windows).",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22030,441502727,2020-06-17T12:22:03Z,src/libuv/gen_build_yaml.py,"@@ -0,0 +1,127 @@+#!/usr/bin/env python2.7++# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import os+import sys+import yaml+import glob++out = {}",how does the selection between the 3 flavors of the library happens?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23219,441696971,2020-06-17T17:06:24Z,src/core/lib/transport/byte_stream.cc,"@@ -55,8 +55,11 @@ void SliceBufferByteStream::Orphan() {  bool SliceBufferByteStream::Next(size_t /*max_size_hint*/,                                  grpc_closure* /*on_complete*/) {-  GPR_DEBUG_ASSERT(backing_buffer_.count > 0);-  return true;+  if (cursor_ < backing_buffer_.count) {","This change doesn't look right to me.  A `SliceBufferByteStream` should always return true from `Next()`, because the bytes are always available.As I mentioned elsewhere, `ByteStream::Next()` should never be called when there is no data left on the stream to read (which is what happens when you read the last slice out of the slice buffer), so there should be no case where this returns false.I think that if you change the cronet code as I suggested, then the change in this file will not be necessary.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23219,441701107,2020-06-17T17:13:35Z,src/core/ext/transport/cronet/transport/cronet_transport.cc,"@@ -1090,22 +1094,21 @@ static enum e_op_result execute_stream_op(struct op_and_state* oas) {       grpc_slice_buffer write_slice_buffer;       grpc_slice slice;       grpc_slice_buffer_init(&write_slice_buffer);-      if (1 != stream_op->payload->send_message.send_message->Next(-                   stream_op->payload->send_message.send_message->length(),-                   nullptr)) {-        /* Should never reach here */-        GPR_ASSERT(false);-      }-      if (GRPC_ERROR_NONE !=+      while (1 == stream_op->payload->send_message.send_message->Next(","The contract for `ByteStream::Next()` is that it should not be called once all bytes have been read from the stream:https://github.com/grpc/grpc/blob/5c715f2696a5144daf664020ee372db4d2eaf0f0/src/core/lib/transport/byte_stream.h#L48So instead of checking the result of `Next()` here, this should check to see if there are more bytes still to be read, using something like this:```while (write_slice_buffer.length < stream_op->payload->send_message.send_message->length()) {```You can then call `Next()` within the body of the loop.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23219,441703984,2020-06-17T17:18:47Z,src/core/ext/transport/cronet/transport/cronet_transport.cc,"@@ -1090,22 +1094,21 @@ static enum e_op_result execute_stream_op(struct op_and_state* oas) {       grpc_slice_buffer write_slice_buffer;       grpc_slice slice;       grpc_slice_buffer_init(&write_slice_buffer);-      if (1 != stream_op->payload->send_message.send_message->Next(-                   stream_op->payload->send_message.send_message->length(),-                   nullptr)) {-        /* Should never reach here */-        GPR_ASSERT(false);-      }-      if (GRPC_ERROR_NONE !=+      while (1 == stream_op->payload->send_message.send_message->Next(","BTW, when you do call `Next()`, you can assert that it returns true, because currently all send_message ByteStreams have the data synchronously available.  But please add the following comment:TODO(roth): When we add support for incremental sending, this code will need to be changed to support asynchronous delivery of the send_message payload.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/23228,441727294,2020-06-17T17:57:29Z,src/compiler/objective_c_plugin.cc,"@@ -52,15 +52,16 @@ inline ::grpc::string ImportProtoHeaders(    ::grpc::string base_name = header;   grpc_generator::StripPrefix(&base_name, ""google/protobuf/"");+  ::grpc::string file_name = ""GPB"" + base_name;",@ thomasvl - could you confirm that your change in protobuf follows the pattern of 1) moving WKT files from `objectivec/google/protobuf/` to `objectivec/` and 2) add `GPB` prefix to the headers?,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23245,441932973,2020-06-18T02:27:52Z,src/core/lib/security/authorization/cel_evaluation_engine.h,"@@ -0,0 +1,42 @@+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_CEL_EVALUATION_ENGINE_H+#define GRPC_CORE_LIB_SECURITY_AUTHORIZATION_CEL_EVALUATION_ENGINE_H++#include <map>+#include <memory>+#include ""src/core/ext/upb-generated/envoy/config/rbac/v2/rbac.upb.h""+#include ""src/core/ext/upb-generated/google/api/expr/v1alpha1/syntax.upb.h""++/* Evaluates a request and returns an authorization decision based on the ",I suggest updating the wording a little.CelEvaluationEngine makes an AuthorizationDecision to ALLOW or DENY the current action based on the condition fields in provided RBAC policy. The engine returns UNDECIDED decision if it fails to find a match in RBAC policy...Also add...  This engine ignores the principal and permission fields in RBAC policy. It is the caller's responsibility to provide an appropriate RBAC policy to this engine.,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23245,442573230,2020-06-19T00:43:26Z,BUILD,"@@ -1761,6 +1761,7 @@ grpc_cc_library(     name = ""grpc_secure"",     srcs = [         ""src/core/lib/http/httpcli_security_connector.cc"",+        ""src/core/lib/security/authorization/cel_evaluation_engine.cc"",","I prefer having a separate grpc_cc_library for CelEvaluationEngine.As we will plan to add CELWrapper, CELStub etc. it can all be included in this separate library then.@markdroth WDYT?",
28123841,guantaol,https://api.github.com/repos/grpc/grpc/pulls/22030,442592864,2020-06-19T02:06:09Z,cmake/modules/FindLibUV.cmake,"@@ -0,0 +1,138 @@+# This is adapted from+# https://github.com/Kitware/CMake/blob/master/Source/Modules/FindLibUV.cmake,","I do not know when FindLibUV.cmake will become official in cmake as I do not find any discussion on this.For LibUV_ROOT, it is necessary as our distribution tests (specifically this one examples/cpp/helloworld/cmake_externalproject/CMakeLists.txt) do not install libuv under the default path for cmake. The test needs to tell the find module where to search for the libuv installation. This idea is borrowed from the standard [FindZLIB.cmake module](https://github.com/Kitware/CMake/blob/master/Modules/FindZLIB.cmake#L46).Unfortunately we must keep this find module as libuv does not generate a config.cmake file (thus find_package(libuv CONFIG) does not work). @muxi will take this over in the future as I am going to focus more on grpc development inside Google after this project. (I have asked Muxi to monitor all the changes in this PR).",
28123841,guantaol,https://api.github.com/repos/grpc/grpc/pulls/22030,442595598,2020-06-19T02:18:12Z,src/libuv/gen_build_yaml.py,"@@ -0,0 +1,127 @@+#!/usr/bin/env python2.7++# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import os+import sys+import yaml+import glob++out = {}","Yes, it is a pain to maintain the list of source files for different systems, but I don't find a better way to solve it. Even [the official libuv build files](https://github.com/libuv/libuv/blob/v1.35.0/CMakeLists.txt#L87-L112) list them manually.On each system, only corresponding library will be built from the source, as their build rules are guarded behind [the system check](https://github.com/grpc/grpc/blob/19df07ef98532b109c1e8556a596239c5726bba7/templates/Makefile.template#L1565). The other two libraries (both sources and build rules) will be ignored in the build.",
28123841,guantaol,https://api.github.com/repos/grpc/grpc/pulls/22030,442597020,2020-06-19T02:24:21Z,cmake/libuv.cmake,"@@ -0,0 +1,45 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++if(gRPC_LIBUV_PROVIDER STREQUAL ""module"")+  if(NOT LIBUV_ROOT_DIR)+    set(LIBUV_ROOT_DIR ${CMAKE_CURRENT_SOURCE_DIR}/third_party/libuv)+  endif()+  if(EXISTS ""${LIBUV_ROOT_DIR}/CMakeLists.txt"")+    add_subdirectory(${LIBUV_ROOT_DIR} third_party/libuv)+    if(TARGET uv_a)","If I understand it correctly, this target name must be the same as that created in the libuv CMakeLists.txt, which uses [uv_a](https://github.com/libuv/libuv/blob/v1.35.0/CMakeLists.txt#L271) for the static library.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22498,443507098,2020-06-22T11:58:39Z,cmake/gRPCConfig.cmake.in,"@@ -10,3 +10,16 @@ list(APPEND CMAKE_MODULE_PATH ${CMAKE_CURRENT_LIST_DIR}/modules)  # Targets include(${CMAKE_CURRENT_LIST_DIR}/gRPCTargets.cmake)+if(CMAKE_CROSSCOMPILING)","this really seems just as a convenience, I don't think it enables anything that wasn't possible before. As such, I don't think we want this improvement due to its complexity.It looks like all the other changes have been added just to be able to add this snippet, so IMHO also the other changes from this PR changes aren't really needed.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22856,443559137,2020-06-22T13:31:10Z,src/csharp/Grpc.Core/Internal/NativeExtension.cs,"@@ -129,14 +129,11 @@ private static NativeMethods LoadNativeMethods()         /// </summary>         private static NativeMethods LoadNativeMethodsUnity()         {-            switch (PlatformApis.GetUnityRuntimePlatform())-            {-                case ""IPhonePlayer"":-                    return new NativeMethods(new NativeMethods.DllImportsFromStaticLib());-                default:-                    // most other platforms load unity plugins as a shared library-                    return new NativeMethods(new NativeMethods.DllImportsFromSharedLib());-            }+#if UNITY_IOS","We use this code for platform detection https://github.com/grpc/grpc/blob/master/src/csharp/Grpc.Core/Internal/PlatformApis.cs,but the codepath for detecting unity is know not to be ideal and there's been complaints about it in other issues.But for now this is the best we have.",X
2306212,herbrechtsmeier,https://api.github.com/repos/grpc/grpc/pulls/22498,443682881,2020-06-22T16:28:22Z,cmake/gRPCConfig.cmake.in,"@@ -10,3 +10,16 @@ list(APPEND CMAKE_MODULE_PATH ${CMAKE_CURRENT_LIST_DIR}/modules)  # Targets include(${CMAKE_CURRENT_LIST_DIR}/gRPCTargets.cmake)+if(CMAKE_CROSSCOMPILING)","> this really seems just as a convenience, I don't think it enables anything that wasn't possible before.At the moment every project inclusive the grpc examples have to check the variable `CMAKE_CROSSCOMPILING` to find the plugin `grpc_cpp_plugin` or to use the target `grpc::grpc_cpp_plugin`:```  if(CMAKE_CROSSCOMPILING)  find_program(_GRPC_CPP_PLUGIN_EXECUTABLE grpc_cpp_plugin)else()  set(_GRPC_CPP_PLUGIN_EXECUTABLE $<TARGET_FILE:grpc_cpp_plugin>)endif()``` After the patch every project could simple use the target.This> As such, I don't think we want this improvement due to its complexity.This pull request moves the complexity from every project into the source project and thereby remove code duplication.> It looks like all the other changes have been added just to be able to add this snippet, so IMHO also the other changes from this PR changes aren't really needed.Which changes do you mean?The PR segregate the host tools from the libraries and search for the host tools during a cross compile.Please check your examples. At the moment you force everybody to copy code into its `CMakeLists.txt`.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23256,444147659,2020-06-23T11:17:54Z,test/cpp/qps/qps_worker.cc,"@@ -280,6 +280,7 @@ QpsWorker::QpsWorker(int driver_port, int server_port,   gpr_atm_rel_store(&done_, static_cast<gpr_atm>(0));    std::unique_ptr<ServerBuilder> builder = CreateQpsServerBuilder();+  builder->AddChannelArgument(GRPC_ARG_ALLOW_REUSEPORT, 0);","FTR, note that this only sets GRPC_ARG_ALLOW_REUSEPORT=0 for the worker_service (the one that listens on the driver port). Not sure if that's sufficient for avoiding the port collision or not (more reasoning is probably needed), just wanted to point that out.Also, when we select both `driver_port` and `server_port` with `grpc_pick_unused_port_or_die()` (see above), it isn't clear why we are also setting GRPC_ARG_ALLOW_REUSEPORT=0 - what is it trying to prevent?under normal circumstances, the worker service is exposed on the driver_port (which now uses  GRPC_ARG_ALLOW_REUSEPORT=0) before any benchmarking services are started (because the command to start a benchmarking service is only received through an already-running worker service.So is the point of GRPC_ARG_ALLOW_REUSEPORT=0 to prevent accidentally trying to start benchmarking service on the same port? (which is a fair strategy, but this intention should be clear from the PR).Btw, it's pretty discouraging how complicated our benchmarking harness is and how many subtle technicalities need to be reasoned about before we can figure out what's the right solution for fixing json_run_localhost. :-(On a similar note, what is the reason for GRPC_ARG_ALLOW_REUSEPORT=1 being the default? As far as I can remember, this setting has caused a lot of trouble in the past, especially when testing (e.g. C# explicitly disables the reuseport in all its tests to avoid crosstalk between unrelated tests).",
109690,davidben,https://api.github.com/repos/grpc/grpc/pulls/23165,444351444,2020-06-23T16:24:47Z,test/core/handshake/client_ssl.cc,"@@ -161,7 +161,9 @@ static void server_thread(void* arg) {   // Set the cipher list to match the one expressed in   // src/core/tsi/ssl_transport_security.c.   const char* cipher_list =-      ""ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-""+      ""TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_""+      ""SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-""+      ""AES256-""       ""SHA384:ECDHE-RSA-AES256-GCM-SHA384"";","I don't believe adding TLS 1.3 ciphers actually does anything here. In BoringSSL, we do not allow configuring TLS 1.3 ciphers at all and just have hardcoded preferences within the library. (This helps us ensure we get the AES vs ChaCha20 negotiation correct. We've found that, most of the time, external folks configuring it for TLS 1.2 got it wrong.) In OpenSSL, it is possible to configure, but only via a different API. If you only call this API, it'll use its defaults (which are also sensible).Ditto for the server change below.",
109690,davidben,https://api.github.com/repos/grpc/grpc/pulls/23165,444372115,2020-06-23T16:58:38Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1523,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.",This comment doesn't make sense. It's not possible for application data to be in the same record as handshake in any TLS version. A record has a single content type.What behavior were you observing here / what assumption was gRPC making?,
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,444405331,2020-06-23T17:54:32Z,test/core/handshake/client_ssl.cc,"@@ -161,7 +161,9 @@ static void server_thread(void* arg) {   // Set the cipher list to match the one expressed in   // src/core/tsi/ssl_transport_security.c.   const char* cipher_list =-      ""ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-AES256-""+      ""TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_""+      ""SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-RSA-""+      ""AES256-""       ""SHA384:ECDHE-RSA-AES256-GCM-SHA384"";","Thanks, I didn't realize! I've removed the TLS 1.3 ciphersuites from this list.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22987,444408219,2020-06-23T17:59:20Z,build_handwritten.yaml,"@@ -253,6 +253,7 @@ python_dependencies:   - address_sorting   - ares   - boringssl+  - re2",this looks legit. It's a list of libraries that should be compiled into python artifacts.,
109690,davidben,https://api.github.com/repos/grpc/grpc/pulls/23165,444409290,2020-06-23T18:01:09Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1523,22 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the client may send application data records in the same+    // flight of messages as the record containing the ClientFinished message.+    // In TLS 1.2, this is not allowed; both the client and server must complete+    // the handshake before any application data may be sent.","This still looks off. On a TLS 1.2 resumption, the client speaks last, so you can get application data and client Finished in the same flight. TLS 1.2 full handshakes do end with a server flight, but all browsers implement an optimization where they [send data earlier anyway](https://tools.ietf.org/html/rfc7918) for some connections, notably all HTTP/2 ones.And, conversely, in any flow where the server Finished is last, the server may send application data immediately after its handshake message.",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,444421236,2020-06-23T18:23:19Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1523,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.","Sorry the comment was not very clear - I've changed the comment and I will give a full explanation below.What the comment was trying to convey is: in TLS 1.3, the client can send a flight of messages to the server that contains (a) a TLS handshake record containing the ClientFinished, and (b) TLS application data records.When this occurred, the application data records were left in the ""read"" BIO, and the gRPC stack had no way of knowing that it had any data that needed to be decrypted. Instead, the code I added does the following:(1) remove the application data records from the ""read"" BIO at the end of the handshake, and(2) set the unused bytes equal to these application data records so that the gRPC stack knows there is something to be decrypted.",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,444426994,2020-06-23T18:33:57Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1523,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.","Per your other comment, there may be other scenarios where application data is sent in a flight of messages with a handshake message. I singled out the case of ClientFinished + application data because that was the only case where any error seemed to occur, and I added the comment to try to explain why `unused_bytes` was being populated in this way.",
109690,davidben,https://api.github.com/repos/grpc/grpc/pulls/23165,444473843,2020-06-23T20:01:10Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1523,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.","> What the comment was trying to convey is: in TLS 1.3, the client can send a flight of messages to the server that contains (a) a TLS handshake record containing the ClientFinished, and (b) TLS application data records.This is true with TLS 1.2 as well. It's not new with TLS 1.3. Fundamentally, one of the client or server is going to speak last in the handshake. If the side which speaks last in the handshake also speaks first in the application protocol, it will send handshake data immediately followed by application data. When this happens, whether or not they end up in the same write() call on the sender (BoringSSL doesn't behave any differently on the sender side between TLS 1.2 and 1.3), they may come in at the same read() call on the receiver.In TLS 1.3, the client speaks last. In TLS 1.2 full handshakes, the server speaks last. In TLS 1.2 resumption, the client speaks last. In TLS 1.2 False Start both sides effectively speak last in the handshake. TLS 1.3 0-RTT is extra fun. Also, in HTTP/2, both sides speak first in the application protocol because they need to send SETTINGS. Thus this scenario can happen all over the place.> When this occurred, the application data records were left in the ""read"" BIO, and the gRPC stack had no way of knowing that it had any data that needed to be decrypted.It sounds like this code had a bug, which equally applied to TLS 1.2 and TLS 1.3, but due to some quirk of testing, it only showed up on TLS 1.2 unit tests but not TLS 1.3? You cannot assume that the handshake will consume all data that you received in any version of TLS.> (1) remove the application data records from the ""read"" BIO at the end of the handshake, andNote this is only reasonable if the BIO is just a buffer rather than a socket. This is also implicitly assuming that BoringSSL hasn't overread the data and buffered it internally. (That's probably an okay assumption though. There actually used to be code to overread and we may restore it, but it was always off by default.)",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23245,444507818,2020-06-23T21:04:42Z,src/core/lib/security/authorization/cel_evaluation_engine.cc,"@@ -0,0 +1,45 @@+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/ext/upb-generated/envoy/config/rbac/v2/rbac.upb.h""+#include ""src/core/ext/upb-generated/google/api/expr/v1alpha1/syntax.upb.h""+#include ""src/core/lib/security/authorization/cel_evaluation_engine.h""++CelEvaluationEngine::CelEvaluationEngine(+    const envoy_config_rbac_v2_RBAC& rbac_policy) {+  // Extract array of policies and store their condition fields in policies_+  size_t size;+  const envoy_config_rbac_v2_RBAC_PoliciesEntry* const* +   policies = envoy_config_rbac_v2_RBAC_policies(&rbac_policy, &size);+  upb::Arena temp_arena;++  for (size_t i = 0; i < size; ++i) {+    std::string key = envoy_config_rbac_v2_RBAC_PoliciesEntry_key(","I am yet to take a closer look at arena logic. Overall it looks good.Few nits:1. use const here2. since we are using policy for value, maybe use policy_name instead of key for consistency.3. I prefer using size when doing upb_strview to std::string conversion    std::string(upb_strview.data, upb_strview.size)4. Recommend changing variable name size to num_policies.",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,444543914,2020-06-23T22:30:18Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1523,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.","Thanks for explaining David, that was super helpful. I didn't realize this could also occur in TLS 1.2, and that this specific case could occur in the case of TLS 1.2 resumption. Looking back at the tests where this error occurred, they only ever do full handshakes, which explains why this error did not present itself when only TLS 1.2 was used.In fact, the only e2e test involving TLS resumption is [h2_ssl_session_reuse_test.cc](https://github.com/grpc/grpc/blob/master/test/core/end2end/h2_ssl_session_reuse_test.cc).Additionally, the BIO is indeed a buffer (built using `BIO_new_bio_pair`, and my understanding is that BoringSSL's `SSL_do_handshake` API does not overread. (Please correct me if that's not the case though.)RE the comment itself: I propose removing any reference to TLS versions and simply writing`When the handshake is complete, there may be records containing application data in |impl->ssl|'s read BIO. These bytes must be removed from the BIO and passed to the |ssl_tsi_handshaker_result| as unused bytes so that the gRPC stack knows that it has application data that must be decrypted.`WDYT?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23280,444544670,2020-06-23T22:32:27Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/common.pyx.pxi,"@@ -165,3 +167,22 @@ async def generator_to_async_generator(object gen, object loop, object thread_po      # Port the exception if there is any     await future+++if PY_MAJOR_VERSION >=3 and PY_MINOR_VERSION >=7:",The comment in source code recommended using `if/else` then `IF/ELSE` (https://github.com/cython/cython/blob/master/Cython/Includes/cpython/version.pxd#L3).The C++ compiler is going to optimize this part anyway. The only difference is whether this check gonna appear in the generated cpp file.,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23245,444611606,2020-06-24T02:36:19Z,src/core/lib/security/authorization/cel_evaluation_engine.h,"@@ -0,0 +1,52 @@++// Copyright 2020 gRPC authors.+// +// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+// +//     http://www.apache.org/licenses/LICENSE-2.0+// +// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_CEL_EVALUATION_ENGINE_H+#define GRPC_CORE_LIB_SECURITY_AUTHORIZATION_CEL_EVALUATION_ENGINE_H++#include <map>+#include <memory>+#include <string>++#include ""src/core/ext/upb-generated/envoy/config/rbac/v2/rbac.upb.h""+#include ""src/core/ext/upb-generated/google/api/expr/v1alpha1/syntax.upb.h""+#include ""upb/upb.h""++// CelEvaluationEngine makes an AuthorizationDecision to ALLOW or DENY the+// current action based on the condition fields in the provided RBAC policy.+// The engine returns UNDECIDED decision if it fails to find a match in RBAC +// policy. This engine ignores the principal and permission fields in RBAC +// policy. It is the caller's responsibility to provide an RBAC policy that +// is compatible with this engine.+//+// Example:+// CelEvaluationEngine* cel_engine = new CelEvaluationEngine(rbac_policy);+// cel_engine->Evaluate(evaluate_args); // returns authorization decision.+class CelEvaluationEngine {+ public:+  explicit CelEvaluationEngine(const envoy_config_rbac_v2_RBAC& rbac_policy);+  // TODO(mywang@google.com): add an Evaluate member function.++ private:+  // If action_allow is set to true, the policy acts as an allowlist (equivalent+  // to ALLOW in RBAC). If false, the policy acts as a blocklist (equivalent to +  // DENY in RBAC).+  bool action_allow_;","Use const here, and initialize this variable using initializer list.Just a suggestion... how about allow_policy_ or allowed_if_matched_?",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/23165,444651781,2020-06-24T05:26:09Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1523,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.","Thanks @davidben and @matthewstevenson88 for the discussion. The ssl_transport_security code has been there for a long time, created before my time. It is true that there is only one e2e test related to TLS resumption. But if indeed there is a bug that ssl_transport_security does not process first application data correctly when application data are sent along with handshake bytes in TLS 1.2, it is strange that nobody reports this bug. @yang-g In gRPC streaming RPC, can server send the application data to client first (before client sending any data to server)? It looks like client needs to send some metadata to server first.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/23219,444661745,2020-06-24T05:59:44Z,src/core/ext/transport/cronet/transport/cronet_transport.cc,"@@ -1090,22 +1093,28 @@ static enum e_op_result execute_stream_op(struct op_and_state* oas) {       grpc_slice_buffer write_slice_buffer;       grpc_slice slice;       grpc_slice_buffer_init(&write_slice_buffer);-      if (1 != stream_op->payload->send_message.send_message->Next(-                   stream_op->payload->send_message.send_message->length(),-                   nullptr)) {-        /* Should never reach here */-        GPR_ASSERT(false);-      }-      if (GRPC_ERROR_NONE !=-          stream_op->payload->send_message.send_message->Pull(&slice)) {-        /* Should never reach here */-        GPR_ASSERT(false);-      }-      grpc_slice_buffer_add(&write_slice_buffer, slice);-      if (GPR_UNLIKELY(write_slice_buffer.count != 1)) {+      do {","I suspect that even though this works for now, it may not be ideal and might not work in the future if, e.g., the byte stream in `stream_op->payload->send_message.send_message` changed type. The reason is that the `ByteStream` interface does not guarantee that there'll always be an empty slice in an empty byte stream. Particularly, the interface [said](https://github.com/grpc/grpc/blob/88af358f9ec9b2a84f82b3f4e795df62c920c29b/src/core/lib/transport/byte_stream.h#L48) the Next() method ""should not be called if there is no data left on the stream"". That means the `while` loop in the previous revision was actually the right thing to do. We need to take care of the assert in `write_slice_buffer.count < 1` branch instead.The limitation of `write_slice_buffer.count >= 1` comes from Cronet IIUC, so we probably can't directly eliminate it (or even if we can, it probably goes to another PR). So I would suggest an alternative workaround: if `write_slice_buffer` is empty after the while loop, add a zero length slice to it. This will replacing the original `write_slice_buffer.count < 1` branch which has the assert.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22987,444699942,2020-06-24T07:32:38Z,src/re2/gen_build_yaml.py,"@@ -0,0 +1,48 @@+#!/usr/bin/env python2.7++# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++from __future__ import print_function+import os+import sys+import glob+import yaml++os.chdir(os.path.dirname(sys.argv[0]) + '/../..')++out = {}++out['libs'] = [{+    'name':","nit: replace ""for optimization"" / ""to reduce duplication"".",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22030,444827406,2020-06-24T11:29:05Z,cmake/libuv.cmake,"@@ -0,0 +1,45 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++if(gRPC_LIBUV_PROVIDER STREQUAL ""module"")+  if(NOT LIBUV_ROOT_DIR)+    set(LIBUV_ROOT_DIR ${CMAKE_CURRENT_SOURCE_DIR}/third_party/libuv)+  endif()+  if(EXISTS ""${LIBUV_ROOT_DIR}/CMakeLists.txt"")+    add_subdirectory(${LIBUV_ROOT_DIR} third_party/libuv)+    if(TARGET uv_a)+      set(_gRPC_LIBUV_LIBRARIES uv_a)+      set(_gRPC_LIBUV_INCLUDE_DIR ""${LIBUV_ROOT_DIR}/include"")+      if(gRPC_INSTALL AND _gRPC_INSTALL_SUPPORTED_FROM_MODULE)+        install(TARGETS uv_a EXPORT gRPCTargets+          RUNTIME DESTINATION ${gRPC_INSTALL_BINDIR}+          LIBRARY DESTINATION ${gRPC_INSTALL_LIBDIR}+          ARCHIVE DESTINATION ${gRPC_INSTALL_LIBDIR})+      endif()+    endif()+  else()+	  message(WARNING ""gRPC_LIBUV_PROVIDER is \""module\"" but LIBUV_ROOT_DIR is wrong"")+  endif()+  if(gRPC_INSTALL AND NOT _gRPC_INSTALL_SUPPORTED_FROM_MODULE)+    message(WARNING ""gRPC_INSTALL will be forced to FALSE because gRPC_LIBUV_PROVIDER is \""module\"" and CMake version (${CMAKE_VERSION}) is less than 3.13."")+    set(gRPC_INSTALL FALSE)+  endif()+elseif(gRPC_LIBUV_PROVIDER STREQUAL ""package"")+  # libuv installation directory can be configured by setting LibUV_ROOT.+  find_package(LibUV 1.35.0 REQUIRED)",why the version number here? We don't mention version in other *.cmake files (and also you'd have to keep the version up to data in this file).,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22030,444829706,2020-06-24T11:33:59Z,setup.py,"@@ -213,8 +229,48 @@ def check_linker_need_libatomic(): CYTHON_HELPER_C_FILES = ()  CORE_C_FILES = tuple(grpc_core_dependencies.CORE_SOURCE_FILES)+excluded_core_c_files = []+if ""linux"" in sys.platform:+    CORE_C_FILES = filter(lambda x: 'third_party/libuv/src/win' not in x,+                          CORE_C_FILES)+    # The following source files are under libuv/src/unix but only for Darwin.+    # They need to be excluded when running on Linux, and must keep synchronous+    # with darwin_uv_srcs in src/libuv/gen_build_yaml.py, except",":-( good luck maintaining this and gen_build_yaml.pyNow we have like 3 sources of truth for libuv sources on different platforms.If python owners are ok with this, I'm ""OK"" with this too, but please note that it's very dirty and I explicitly refuse to maintain this in the future.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22030,444835918,2020-06-24T11:46:36Z,grpc.gemspec,"@@ -1655,6 +1655,99 @@ Gem::Specification.new do |s|   s.files += %w( third_party/cares/config_freebsd/ares_config.h )   s.files += %w( third_party/cares/config_linux/ares_config.h )   s.files += %w( third_party/cares/config_openbsd/ares_config.h )+  s.files += %w( third_party/libuv/include/uv.h )","I don't understand how ruby is going to work when other languages (python, C++ etc) have to go out of their way to build the right subset of the sources for given platform (uv_linux, uv_darwin, un_linux), but ruby is just including a list of source files regardless of the platform. So it seems that either a) the platform specific libraries aren't necessary b) ruby won't work.I think PHP might be a similar story?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22987,444984867,2020-06-24T15:33:43Z,src/core/ext/filters/client_channel/lb_policy/xds/xds_routing.cc,"@@ -227,6 +228,10 @@ XdsRoutingLb::PickResult XdsRoutingLb::RoutePicker::Pick(PickArgs args) {   }   std::vector<absl::string_view> path_elements =       absl::StrSplit(path.substr(1), '/');+  static LazyRE2 hostportish_re = {""([^:{}\\s,]+):([0-9]+)""};+  if (RE2::FullMatch(path.data(), *hostportish_re)) {+    gpr_log(GPR_INFO, ""test re2"");",+1.  We don't really have any use for RE2 in this code without the code from #22951.The easiest way of handling this might be to remove the regex support from #22951 and get that PR merged first.  Then this PR can add the RE2 dependency and the regex matcher support at the same time.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22987,444985535,2020-06-24T15:34:44Z,src/re2/gen_build_yaml.py,"@@ -0,0 +1,49 @@+#!/usr/bin/env python2.7++# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++from __future__ import print_function+import os+import sys+import glob+import yaml++os.chdir(os.path.dirname(sys.argv[0]) + '/../..')++out = {}++out['libs'] = [{+    #TODO @donnadionne: extracting the list of source files from bazel build for optimization+    'name':+        're2',+    'build':+        'all',+    'language':+        'c',+    'secure':+        False,+    'src':+        sorted(+            glob.glob('third_party/re2/re2/*.cc') ++            glob.glob('third_party/re2/util/pcre.cc') +","Nit: These last three aren't actually glob patterns, so this can just be a list of files.",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,445038727,2020-06-24T16:58:12Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1523,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.","@jiangtaoli2016 If I understand how TLS 1.2 resumption works (@davidben please keep me honest), then I don't **think** this bug would surface to the user, it would just cause TLS 1.2 resumption to be slower than it should be. The flow would be:1. Client sends ClientHello + ticket to the server.2. Server responds.3. Client sends ClientFinished + application data to the server.4. Server completes handshake but then the application data is sitting in `impl->ssl`'s read BIO without the server knowing it is there, so the server hangs.5. Now, either (a) the client sends more application data to the server, or (b) the client pings the server. In either case, this adds to the server's read BIO and causes **everything** in the read BIO to be decrypted. From this point, the server can continue as normal.If this is correct, then none of this would be exposed to the user, aside from perhaps a bit of latency. After logging all the calls to read/write, this is what occurs in the `h2_ssl_session_reuse_test.cc` currently (i.e. without this PR), and I'm assuming the same occurs for users.Moving forward, I think that we should add a unit test that checks this case. This can be done by modifying [`ssl_tsi_test_do_handshake_session_cache`](https://github.com/grpc/grpc/blob/0f28314557fbe77e61e0e44f4159b774710f65b3/test/core/tsi/ssl_transport_security_test.cc#L707).",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/23286,445054397,2020-06-24T17:25:39Z,tools/interop_matrix/client_matrix.py,"@@ -221,6 +221,7 @@ def __init__(self, patch=[], runtimes=[], testcases_file=None):             ('v1.29.0', ReleaseInfo()),             ('v1.30.0', ReleaseInfo()),             ('v1.30.1', ReleaseInfo()),+            ('v1.30.2', ReleaseInfo()),","This should replace v1.30.1, and it seems we accidentally didn't replace v1.30.0.",
109690,davidben,https://api.github.com/repos/grpc/grpc/pulls/23165,445058666,2020-06-24T17:33:18Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1523,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.","I guess I don't really understand the way gRPC handles I/O here, so I'm not sure what are the implications of leaving things in the BIO. I would have expected TLS 1.3 and TLS 1.2 resumption to have the same behavior either way. Their I/O patterns are pretty similar.",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,445101397,2020-06-24T18:50:36Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1523,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.","Agreed that TLS 1.3 and TLS 1.2 resumption should have the same behavior. The reason for this discussion was that gRPC e2e tests do not use resumption (with 1 exception), so this bug with TLS 1.2 resumption was not discovered until we turned on TLS 1.3 (which ran against all the e2e tests). At the moment, I'm writing a test that explicitly checks that the TLS 1.2 resumption behavior works as expected when application data is sent in the same flight of messages as the ClientFinished. Such checks already exist for TLS 1.3, but not for TLS 1.2 resumption. I should be able to add that test to this PR this afternoon.@davidben @jiangtaoli2016 If I missed any concerns or confusion, please let me know. For context, here's a brief description of how gRPC's TLS stack handles I/O:1. During the handshake, add everything that was received from the peer to read BIO.2. At the end of the handshake, anything that is left unused should be added to the `unused_bytes` field of the handshake result struct.3. The next thing that gRPC does is decrypt lies in the unused bytes field. It does this by adding the unused bytes to the read BIO and calling `SSL_read`.4. Anything further that is received from the peer is added to the read BIO and decrypted by calling `SSL_read`.If we don't populate the unused bytes from the read BIO, then we require something else to trigger the call to `SSL_read`. This is what currently (i.e. before this PR) happens for TLS 1.2 resumption, and it breaks the above model.",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/23165,445212378,2020-06-24T22:45:35Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1523,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.","I agree with Matt's description on gRPC TLS stack handles IO.In gRPC, it is always the client that sends the first application data, confirmed with @yang-g. So server finishes TLS handshake last and sends along with application data is not a possible situation for gRPC.Now back to TLS 1.2 resumption.> 5. Now, either (a) the client sends more application data to the server, or (b) the client pings the server.""For unary RPC, once client sends a request, it will wait server for response. I don't think client will ping server right away if server does not respond. Or a streaming RPC, it is normal for client to wait for server's data without sending request to server (except the initial metadata). @matthewstevenson88 Please let us know how the TLS 1.2 resumption test progresses on this particular case.",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,445640001,2020-06-25T15:20:44Z,src/core/tsi/ssl_transport_security.cc,"@@ -1450,9 +1523,19 @@ static tsi_result ssl_handshaker_next(   if (ssl_handshaker_get_result(impl) == TSI_HANDSHAKE_IN_PROGRESS) {     *handshaker_result = nullptr;   } else {-    size_t unused_bytes_size = received_bytes_size - bytes_consumed;-    const unsigned char* unused_bytes =-        unused_bytes_size == 0 ? nullptr : received_bytes + bytes_consumed;+    // In TLS 1.3, the ClientFinished or ServerFinished record may have+    // (encrypted) application data appended to the end of the record. In TLS+    // 1.2, this is explicitly disallowed by the RFC; application data will+    // never be appended to a handshake record.","In the most recent commit, I've (a) changed the unit tests so that TLS 1.2 resumption is tested when application data is sent in the same flight as the last client handshake record, and (b) amended the comment (that sparked this discussion) to explain why the unused bytes must be extracted.Note that there was already a boolean variable in `ssl_transport_security_test.cc` that, when set to ""true"", sends application data along with the final handshake message. Before this PR, this [variable was set to ""false""](https://github.com/grpc/grpc/blob/635ded1749f990ffe6be0ca403e4b255cf62742f/test/core/tsi/ssl_transport_security_test.cc#L424) for some reason, and this PR sets it to ""true"". If you set this variable to true in the current gRPC master, then the TLS 1.2 resumption test fails for exactly the reason we discussed earlier. I'm not sure why this variable was ever set to ""false"".> For unary RPC, once client sends a request, it will wait server for response. I don't think client will ping server right away if server does not respond. Or a streaming RPC, it is normal for client to wait for server's data without sending request to server (except the initial metadata).To elaborate on my previous point: I observed that `h2_ssl_session_reuse_test.cc` was not failing because the client was sending another flight of application data messages and, when the server received this next flight, the server read everything (i.e. the application data received immediately after the handshake, plus the application data in the next flight). My comment on the client ping was just a guess as to why this error was not visible to the user.",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/23165,445661635,2020-06-25T15:52:28Z,src/core/tsi/ssl_transport_security.h,"@@ -21,6 +21,7 @@  #include <grpc/support/port_platform.h> +#include <grpc/grpc_security_constants.h>","TSI can be called by other callers (independent of gRPC), thus TSI should have minimum dependency on gRPC.We shall define in transport_security_interface.h```typedef enum {  TSI_TLS1_2,  TSI_TLS1_3,} tsi_tls_version; ```Since TSI is C header, thus it does not have namespace. You can see `TSI_DONT_REQUEST_CLIENT_CERTIFICATE` and `GRPC_SSL_DONT_REQUEST_CLIENT_CERTIFICATE` as example. Basically, in TSI code (ssl_transport_security*), use tsi_tls_version. In gRPC code (security_connector, credentials, etc.), uses grpc_tls_version. Security connector will do to conversion from grpc_tls_version to ssl_tls_version.",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/23165,445665337,2020-06-25T15:57:44Z,src/core/lib/security/credentials/ssl/ssl_credentials.h,"@@ -38,6 +38,11 @@ class grpc_ssl_credentials : public grpc_channel_credentials {       const char* target, const grpc_channel_args* args,       grpc_channel_args** new_args) override; +  // TODO(mattstev): Plumb to wrapped languages. Until then, setting the TLS","[optional] For completeness, could you please also change C core tls_credentials and tls_security_connector. ssl_credentials/ssl_security_connector will be deprecated eventually. If you don't have time, I can make a follow-up change for tls_creds.",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/23165,445682871,2020-06-25T16:24:00Z,src/core/tsi/ssl_transport_security.cc,"@@ -1805,8 +1888,11 @@ tsi_result tsi_create_ssl_client_handshaker_factory_with_options(     return TSI_INVALID_ARGUMENT;   } -#if defined(OPENSSL_NO_TLS1_2_METHOD) || OPENSSL_API_COMPAT >= 0x10100000L+#if OPENSSL_VERSION_NUMBER >= 0x10100000   ssl_context = SSL_CTX_new(TLS_method());+  result = tsi_set_min_and_max_tls_versions(+      ssl_context, options->min_tls_version, options->max_tls_version);+  if (result != TSI_OK) return result;","TLSv1_2_method() is a deprecated method. Let's remove it here.We need to change tsi_set_min_and_max_tls_versions() to be no-op if OPENSSL_VERSION_NUMBER < 0x10100000.It is possible that gRPC user using an old version of OpenSSL/BoringSSL (e.g., 1.0.x) may get downgrade attack, if both client and server use old version of SSL library. But OpenSSL 1.1 has been released for 3+ years and TLSv1_2_method() is a depreciated method. I recommend removing it. @davidben what is your recommendation?",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,445722313,2020-06-25T17:30:09Z,src/core/lib/security/credentials/ssl/ssl_credentials.h,"@@ -38,6 +38,11 @@ class grpc_ssl_credentials : public grpc_channel_credentials {       const char* target, const grpc_channel_args* args,       grpc_channel_args** new_args) override; +  // TODO(mattstev): Plumb to wrapped languages. Until then, setting the TLS","Thanks for mentioning this. I was intending to make this change in a follow-up PR, since I'm a little concerned about making this PR too large. Is that ok with you?",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/23165,445866316,2020-06-25T22:09:33Z,src/core/tsi/ssl_transport_security.cc,"@@ -1805,8 +1888,11 @@ tsi_result tsi_create_ssl_client_handshaker_factory_with_options(     return TSI_INVALID_ARGUMENT;   } -#if defined(OPENSSL_NO_TLS1_2_METHOD) || OPENSSL_API_COMPAT >= 0x10100000L+#if OPENSSL_VERSION_NUMBER >= 0x10100000   ssl_context = SSL_CTX_new(TLS_method());+  result = tsi_set_min_and_max_tls_versions(+      ssl_context, options->min_tls_version, options->max_tls_version);+  if (result != TSI_OK) return result;","I suggest that we remove `#if OPENSSL_VERSION_NUMBER >= 0x10100000` in line 1891 and remove deprecated TLSv1_2_method().By doing so, In `tsi_set_min_and_max_tls_versions()`, if application uses OpenSSL 1.0.x, it will fall into Line 914 and fail. This will makes any OpenSSL 1.0.x user cannot use gRPC. That is not what we want.Thus I suggest the following:``` ssl_context = SSL_CTX_new(TLS_method());#if OPENSSL_VERSION_NUMBER >= 0x10100000  result = tsi_set_min_and_max_tls_versions(      ssl_context, options->min_tls_version, options->max_tls_version);  if (result != TSI_OK) return result;#endif```You can also remove `#if OPENSSL_VERSION_NUMBER >= 0x10100000` inside tsi_set_min_and_max_tls_versions()",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,445888674,2020-06-25T23:16:46Z,src/core/tsi/ssl_transport_security.cc,"@@ -1805,8 +1888,11 @@ tsi_result tsi_create_ssl_client_handshaker_factory_with_options(     return TSI_INVALID_ARGUMENT;   } -#if defined(OPENSSL_NO_TLS1_2_METHOD) || OPENSSL_API_COMPAT >= 0x10100000L+#if OPENSSL_VERSION_NUMBER >= 0x10100000   ssl_context = SSL_CTX_new(TLS_method());+  result = tsi_set_min_and_max_tls_versions(+      ssl_context, options->min_tls_version, options->max_tls_version);+  if (result != TSI_OK) return result;","I'm in agreement on the changes to `tsi_set_min_and_max_tls_versions()`.However, if we remove `TLSv1_2_method`, then an OpenSSL 1.0 user might regress to using TLS 1.0 or 1.1, which would be bad. If we want to support OpenSSL 1.0 users, then I think we need to still use `TLSv1_2_method`. (@davidben Please correct me if I'm wrong.)",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23165,445902050,2020-06-26T00:04:26Z,src/core/tsi/ssl_transport_security.cc,"@@ -1805,8 +1888,11 @@ tsi_result tsi_create_ssl_client_handshaker_factory_with_options(     return TSI_INVALID_ARGUMENT;   } -#if defined(OPENSSL_NO_TLS1_2_METHOD) || OPENSSL_API_COMPAT >= 0x10100000L+#if OPENSSL_VERSION_NUMBER >= 0x10100000   ssl_context = SSL_CTX_new(TLS_method());+  result = tsi_set_min_and_max_tls_versions(+      ssl_context, options->min_tls_version, options->max_tls_version);+  if (result != TSI_OK) return result;","Per an offline discussion with @jiangtaoli2016, I've made the following changes:- Calling `tsi_set_min_and_max_tls_versions` is a no-op when using OpenSSL versions < 1.1.- Removed the use of `TLSv1_2_method`. This API is deprecated.Since the API `SSL_CTX_set_min_proto_version` requires OpenSSL >= 1.1, a user of OpenSSL 1.0 could regress to using TLS 1.0 or 1.1 when the `TLS_method` is called. This is acceptable because OpenSSL 1.0.2 is now end-of-life.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/23288,445950455,2020-06-26T03:36:50Z,tools/dockerfile/grpc_artifact_python_manylinux2014_x64/Dockerfile,"@@ -0,0 +1,28 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# Docker file for building gRPC manylinux Python artifacts.++FROM quay.io/pypa/manylinux2014_x86_64++# Update the package manager+RUN yum update -y+RUN yum install -y curl-devel expat-devel gettext-devel openssl-devel zlib-devel++###################################+# Install Python build requirements+RUN /opt/python/cp35-cp35m/bin/pip install --upgrade cython+RUN /opt/python/cp36-cp36m/bin/pip install --upgrade cython+RUN /opt/python/cp37-cp37m/bin/pip install --upgrade cython+RUN /opt/python/cp38-cp38/bin/pip install --upgrade cython",Sure. I added them along with the update-date comment so that we can upgrade docker images easily by modifying its date.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23203,446441399,2020-06-26T22:45:36Z,src/core/lib/security/credentials/google_default/google_default_credentials.cc,"@@ -89,7 +88,7 @@ grpc_google_default_channel_credentials::create_security_connector(   bool use_alts =       is_grpclb_load_balancer || is_backend_from_grpclb_load_balancer;   /* Return failure if ALTS is selected but not running on GCE. */-  if (use_alts && !g_is_on_gce) {+  if (use_alts && !grpc_core::internal::is_on_gce()) {","As far as I can tell, this was actually a *double* cache. The concrete implementation provided for `g_gce_tenancy_checker` is [here](https://github.com/grpc/grpc/blob/master/src/core/lib/security/credentials/alts/check_gcp_environment_linux.cc#L59). (the Windows implementation is similarly cached)",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23203,446441417,2020-06-26T22:45:41Z,src/core/lib/security/credentials/google_default/compute_engine_channel_credentials.cc,"@@ -0,0 +1,70 @@+/*+ *+ * Copyright 2020 The gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include ""src/core/lib/security/credentials/credentials.h""++#include <string.h>++#include <grpc/support/alloc.h>+#include <grpc/support/log.h>+#include <grpc/support/sync.h>++#include ""src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.h""+#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/gpr/env.h""+#include ""src/core/lib/gpr/string.h""+#include ""src/core/lib/gprpp/ref_counted_ptr.h""+#include ""src/core/lib/http/httpcli.h""+#include ""src/core/lib/http/parser.h""+#include ""src/core/lib/iomgr/load_file.h""+#include ""src/core/lib/iomgr/polling_entity.h""+#include ""src/core/lib/security/credentials/alts/alts_credentials.h""+#include ""src/core/lib/security/credentials/alts/check_gcp_environment.h""+#include ""src/core/lib/security/credentials/google_default/google_default_credentials.h""+#include ""src/core/lib/security/credentials/jwt/jwt_credentials.h""+#include ""src/core/lib/security/credentials/oauth2/oauth2_credentials.h""+#include ""src/core/lib/slice/slice_internal.h""+#include ""src/core/lib/slice/slice_string_helpers.h""+#include ""src/core/lib/surface/api_trace.h""++grpc_channel_credentials* grpc_compute_engine_channel_credentials_create(","This is based on a proposal that @apolcyn put together at go/yoshi-direct-path. He can correct anything I get wrong. The problem is that `grpc_google_default_credentials_create` creates its own call creds based on the default service account of the VM. However, the auth libraries for wrapped languages, being **dependents** of gRPC and not dependencies, may actually assert a different identity than the default service account of the VM.When this mismatch occurs and Directpath is not enabled, traffic will flow just fine. When it's enabled in the console, however, RPCs will start to fail.Unfortunately, `grpc_google_default_credentials_create` didn't add a `void* reserved` argument, so we can't just extend that function. You'll notice I tried to reuse as much of the existing implementation as possible. You can more or less think of this as `grpc_google_default_credentials_create2` with naming aligned to the other languages.With that said, I'm more than open to alternate solutions to that problem.",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/23298,446458658,2020-06-27T00:16:09Z,src/compiler/objective_c_plugin.cc,"@@ -104,6 +110,19 @@ class ObjectiveCGrpcGenerator : public grpc::protobuf::compiler::CodeGenerator {           return false;         }         framework = param[1];+      } else if (param[0] == ""runtime_import_prefix"") {+        if (param.size() != 2) {+          *error = grpc::string(""Format: runtime_import_prefix=dir/"");+          return false;+        } else if (param[1].empty()) {+          *error = grpc::string(","I thought an empty parameter value is equivalent to the parameter not exists in this case, but correct me if I'm wrong.",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/23203,447052736,2020-06-29T15:20:22Z,src/core/lib/security/credentials/google_default/google_default_credentials.cc,"@@ -89,7 +88,7 @@ grpc_google_default_channel_credentials::create_security_connector(   bool use_alts =       is_grpclb_load_balancer || is_backend_from_grpclb_load_balancer;   /* Return failure if ALTS is selected but not running on GCE. */-  if (use_alts && !g_is_on_gce) {+  if (use_alts && !grpc_core::internal::is_on_gce()) {","@markdroth That's correct. `g_is_on_gce` is introduced to avoid unnecessary GCP residency check, which needs to be performed only once. @gnossen the performance overhead can also be introduced by lock contention https://github.com/grpc/grpc/blob/master/src/core/lib/security/credentials/alts/check_gcp_environment_linux.cc#L58 IMO, `g_gce_tenancy_checker` is designed to be called only once. ",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/23203,447073062,2020-06-29T15:48:49Z,src/core/lib/security/credentials/google_default/compute_engine_channel_credentials.cc,"@@ -0,0 +1,70 @@+/*+ *+ * Copyright 2020 The gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include ""src/core/lib/security/credentials/credentials.h""++#include <string.h>++#include <grpc/support/alloc.h>+#include <grpc/support/log.h>+#include <grpc/support/sync.h>++#include ""src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.h""+#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/gpr/env.h""+#include ""src/core/lib/gpr/string.h""+#include ""src/core/lib/gprpp/ref_counted_ptr.h""+#include ""src/core/lib/http/httpcli.h""+#include ""src/core/lib/http/parser.h""+#include ""src/core/lib/iomgr/load_file.h""+#include ""src/core/lib/iomgr/polling_entity.h""+#include ""src/core/lib/security/credentials/alts/alts_credentials.h""+#include ""src/core/lib/security/credentials/alts/check_gcp_environment.h""+#include ""src/core/lib/security/credentials/google_default/google_default_credentials.h""+#include ""src/core/lib/security/credentials/jwt/jwt_credentials.h""+#include ""src/core/lib/security/credentials/oauth2/oauth2_credentials.h""+#include ""src/core/lib/slice/slice_internal.h""+#include ""src/core/lib/slice/slice_string_helpers.h""+#include ""src/core/lib/surface/api_trace.h""++grpc_channel_credentials* grpc_compute_engine_channel_credentials_create(","As @gnossen mentioned, using compute engine default engine is the way we agreed with the client library to address the identity mismatch issue (ALTS only uses the VM service account as identity, but GDC may use identity from environment or file which causes identity mismatch). Java and Go both have Compute Engine Creds/Channels.",
109690,davidben,https://api.github.com/repos/grpc/grpc/pulls/23165,447162613,2020-06-29T18:17:21Z,src/core/lib/security/security_connector/ssl_utils.cc,"@@ -67,6 +67,9 @@ static const char* cipher_suites = nullptr; // All cipher suites for default are compliant with HTTP2. GPR_GLOBAL_CONFIG_DEFINE_STRING(     grpc_ssl_cipher_suites,+    ""TLS_AES_128_GCM_SHA256:""+    ""TLS_AES_256_GCM_SHA384:""+    ""TLS_CHACHA20_POLY1305_SHA256:""","Poking around the code, it looks like this also is just used in `SSL_CTX_set_cipher_list`? In that case, while adding them isn't harmful, I don't think it does anything. (For the same reason as the other cipher lists.)",
1054404,muxi,https://api.github.com/repos/grpc/grpc/pulls/23298,447418211,2020-06-30T05:28:19Z,src/objective-c/tests/run_plugin_option_tests.sh,"@@ -0,0 +1,61 @@+#!/bin/bash+# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# Don't run this script standalone. Instead, run from the repository root:+# ./tools/run_tests/run_tests.py -l objc++set -ev++cd $(dirname $0)++# Run the tests server.++ROOT_DIR=../../..+BAZEL=$ROOT_DIR/tools/bazel+PROTOC=$ROOT_DIR/bazel-bin/external/com_google_protobuf/protoc+PLUGIN=$ROOT_DIR/bazel-bin/src/compiler/grpc_objective_c_plugin+RUNTIME_IMPORT_PREFIX=prefix/dir/++[ -f $PROTOC ] && [ -f $PLUGIN ] || {+    BAZEL build @com_google_protobuf//:protoc //src/compiler:grpc_objective_c_plugin+}++rm -rf RemoteTestClient/*pb*++$PROTOC \+    --plugin=protoc-gen-grpc=$PLUGIN \+    --objc_out=RemoteTestClient \+    --grpc_out=runtime_import_prefix=$RUNTIME_IMPORT_PREFIX:RemoteTestClient \+    -I $ROOT_DIR \+    -I ../../../third_party/protobuf/src \+    $ROOT_DIR/src/objective-c/examples/RemoteTestClient/*.proto++# Verify the output proto filename+[ -e ./RemoteTestClient/src/objective-c/examples/RemoteTestClient/Test.pbrpc.m ] || {+    echo >&2 ""protoc outputs wrong filename.""+    exit 1+}++# Verify paths of protobuf WKTs in generated code contain runtime import prefix.+[ ""`cat RemoteTestClient/src/objective-c/examples/RemoteTestClient/Test.pbrpc.m |+    egrep '#import "".*\.pbobjc\.h""$' |","The test condition is a little bit vague here: we want to test that all the WKTs contain runtime import prefix, but not the local imports (CMIIW). The two egrep sentences here appear to be filtering ""lines with #import.*prefix/dir/.*pbobjc\.h"" exists. That is a little bit too general I think. Since we know that test.proto is importing `empty.proto`, maybe check `#import.*/prefix/dir/GPBEmpty\.pbobjc\.h` and #import ""Messages\.pbobjc\.h""?Also, let's have another sentence to verify the same thing in header files as well.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23203,447778387,2020-06-30T15:33:55Z,src/core/lib/security/credentials/google_default/compute_engine_channel_credentials.cc,"@@ -0,0 +1,70 @@+/*+ *+ * Copyright 2020 The gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include ""src/core/lib/security/credentials/credentials.h""++#include <string.h>++#include <grpc/support/alloc.h>+#include <grpc/support/log.h>+#include <grpc/support/sync.h>++#include ""src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.h""+#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/gpr/env.h""+#include ""src/core/lib/gpr/string.h""+#include ""src/core/lib/gprpp/ref_counted_ptr.h""+#include ""src/core/lib/http/httpcli.h""+#include ""src/core/lib/http/parser.h""+#include ""src/core/lib/iomgr/load_file.h""+#include ""src/core/lib/iomgr/polling_entity.h""+#include ""src/core/lib/security/credentials/alts/alts_credentials.h""+#include ""src/core/lib/security/credentials/alts/check_gcp_environment.h""+#include ""src/core/lib/security/credentials/google_default/google_default_credentials.h""+#include ""src/core/lib/security/credentials/jwt/jwt_credentials.h""+#include ""src/core/lib/security/credentials/oauth2/oauth2_credentials.h""+#include ""src/core/lib/slice/slice_internal.h""+#include ""src/core/lib/slice/slice_string_helpers.h""+#include ""src/core/lib/surface/api_trace.h""++grpc_channel_credentials* grpc_compute_engine_channel_credentials_create(","> Unfortunately, `grpc_google_default_credentials_create` didn't add a `void* reserved` argument, so we can't just extend that function.We have no requirement to maintain backward compatibility in the C-core API.  The only requirement is that we publish a gRFC describing those changes.If this is essentially just a way of overriding the call creds attached to GoogleDefaultCreds, then let's just add an optional parameter to the existing function that allows specifying custom call creds.",
2030323,JunTaoLuo,https://api.github.com/repos/grpc/grpc/pulls/22067,448008990,2020-06-30T22:13:25Z,src/csharp/Grpc.Tools/build/_protobuf/Google.Protobuf.Tools.targets,"@@ -111,7 +111,7 @@        Protobuf_Compile directly where required. -->   <!-- TODO(kkm): Do shared compile in outer multitarget project? -->   <Target Name=""_Protobuf_Compile_BeforeCsCompile""-          BeforeTargets=""BeforeCompile""+          BeforeTargets=""CoreCompile""","Hmm this is a pretty significant change since it does move a large amount of logic to a later part of the build. Since I didn't make the original change, it's hard for me to gauge all the consequences of this change. It's hard to say that this change would not break other non WPF projects due to some other build order oddities. It's probably valuable to figure out why it was originally set to BeforeCompile in the first place. ",
539951,uecasm,https://api.github.com/repos/grpc/grpc/pulls/22067,449475242,2020-07-03T09:16:13Z,src/csharp/Grpc.Tools/build/_protobuf/Google.Protobuf.Tools.targets,"@@ -111,7 +111,7 @@        Protobuf_Compile directly where required. -->   <!-- TODO(kkm): Do shared compile in outer multitarget project? -->   <Target Name=""_Protobuf_Compile_BeforeCsCompile""-          BeforeTargets=""BeforeCompile""+          BeforeTargets=""CoreCompile""","I don't know why you're making that big a deal of this.  It is literally the very next target, so it barely changes the time of execution at all.  Except that it actually works.",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/23380,450601031,2020-07-07T04:13:21Z,src/core/ext/transport/inproc/inproc_transport.cc,"@@ -847,14 +865,13 @@ void op_state_machine_locked(inproc_stream* s, grpc_error* error) {         ""op_state_machine scheduling recv-message-on-complete"");     s->recv_message_op = nullptr;   }-  if (s->trailing_md_recvd && (s->trailing_md_sent || s->t->is_client) &&-      s->send_message_op) {+  if (s->trailing_md_recvd && s->send_message_op && s->t->is_client) {","with this change -1) the server will try to send messages even after it has sent trailing metadata..well, this case should not happen in the first place, so it would be fine2) If the client has already sent trailing md, it will still try to send messages but again this case should not happen.This change should be fine but I don't understand what this is solving",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/20857,450627691,2020-07-07T05:58:41Z,src/core/lib/iomgr/tcp_windows.cc,"@@ -482,7 +482,10 @@ static grpc_resource_user* win_get_resource_user(grpc_endpoint* ep) {   return tcp->resource_user; } -static int win_get_fd(grpc_endpoint* ep) { return -1; }+static int win_get_fd(grpc_endpoint* ep) {+  grpc_tcp* tcp = reinterpret_cast<grpc_tcp*>(ep);","This might work but I'm not certain it makes sense to implement this interface on Windows.For example this `grpc_endpoint` `get_fd` method is supposed to return an fd as an `int`, with -1 to represent ""invalid"".On Windows, though, this field being returned here is a `SOCKET`, which is an unsigned data type having the `INVALID_SOCKET` macro to represent ""invalid"".I think it might be better to put the platform-specific at a higher level, e.g. extracting the chunk of code from `local_check_peer` that deals with OS handles and creating a Windows-specific version of it.Also, we should have some test that proves this works - @yihuazhang is there a simple client/server local credentials test that is perhaps currently skipped on Windows, which could just be enabled to show that it works?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23377,451038573,2020-07-07T17:46:17Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pxd.pxi,"@@ -49,6 +49,12 @@ cdef class BaseCompletionQueue:      cdef grpc_completion_queue* c_ptr(self) ++cdef class _EventLoopBound:+    cdef readonly object loop+    cdef readonly object read_socket","Type annotation added as comments, also for other class variables defined as `object` in this file.",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23245,451174767,2020-07-07T22:16:14Z,src/core/lib/security/authorization/cel_evaluation_engine.h,"@@ -0,0 +1,66 @@++// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_CEL_EVALUATION_ENGINE_H+#define GRPC_CORE_LIB_SECURITY_AUTHORIZATION_CEL_EVALUATION_ENGINE_H++#include <grpc/support/port_platform.h>++#include <map>+#include <memory>+#include <string>+#include <vector>++#include ""src/core/ext/upb-generated/envoy/config/rbac/v2/rbac.upb.h""+#include ""src/core/ext/upb-generated/google/api/expr/v1alpha1/syntax.upb.h""+#include ""upb/upb.hpp""++// CelEvaluationEngine makes an AuthorizationDecision to ALLOW or DENY the+// current action based on the condition fields in two provided RBAC policies.",The comment seems confusing. .. based on condition fields in provided RBAC policies?Also update the example below.,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23245,451184064,2020-07-07T22:41:23Z,src/core/lib/security/authorization/cel_evaluation_engine.cc,"@@ -0,0 +1,69 @@+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include ""src/core/lib/security/authorization/cel_evaluation_engine.h""++std::unique_ptr<CelEvaluationEngine>+CelEvaluationEngine::CreateCelEvaluationEngine(+    const std::vector<envoy_config_rbac_v2_RBAC*>& rbac_policies) {+  if (rbac_policies.size() == NumPolicies &&+      envoy_config_rbac_v2_RBAC_action(rbac_policies[0]) == DENY &&+      envoy_config_rbac_v2_RBAC_action(rbac_policies[1]) == ALLOW) {+    return std::unique_ptr<CelEvaluationEngine>(+        new CelEvaluationEngine(rbac_policies));+  } else if (rbac_policies.size() == 1 and rbac_policies[0] != nullptr) {+    return std::unique_ptr<CelEvaluationEngine>(+        new CelEvaluationEngine(rbac_policies));+  } else {+    return nullptr;",Preferred pattern is returning early on errors. Also add logs.if(rbac_policies.size() < 1 || rbac_policies.size() >2 ) {// Add Logreturn nullptr;} if( rbac_policies.size() == 2 && (first policy is not deny || second policy is not allow) {// Add Logreturn nullptr;}...,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23245,451192065,2020-07-07T23:05:41Z,test/core/security/cel_engine_test.cc,"@@ -0,0 +1,76 @@+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/lib/security/authorization/cel_evaluation_engine.h""++#include <gtest/gtest.h>++class CelEngineTest : public ::testing::Test {+ protected:+  void SetUp() override {+    deny_policy = envoy_config_rbac_v2_RBAC_new(arena.ptr());+    envoy_config_rbac_v2_RBAC_set_action(deny_policy, 1);+    allow_policy = envoy_config_rbac_v2_RBAC_new(arena.ptr());+    envoy_config_rbac_v2_RBAC_set_action(allow_policy, 0);+  }++  upb::Arena arena;+  envoy_config_rbac_v2_RBAC* deny_policy;+  envoy_config_rbac_v2_RBAC* allow_policy;+};++TEST_F(CelEngineTest, CreateEngineSuccessOnePolicy) {+  std::vector<envoy_config_rbac_v2_RBAC*> policies{allow_policy};+  std::unique_ptr<CelEvaluationEngine> engine =+      CelEvaluationEngine::CreateCelEvaluationEngine(policies);+  EXPECT_NE(engine, nullptr)+      << ""Failed to create a CelEvaluationEngine with one policy."";+}++TEST_F(CelEngineTest, CreateEngineSuccessTwoPolicies) {+  std::vector<envoy_config_rbac_v2_RBAC*> policies{deny_policy, allow_policy};+  std::unique_ptr<CelEvaluationEngine> engine =+      CelEvaluationEngine::CreateCelEvaluationEngine(policies);+  EXPECT_NE(engine, nullptr)+      << ""Failed to create a CelEvaluationEngine with two policies."";+}++TEST_F(CelEngineTest, CreateEngineFailNoPolicies) {+  std::vector<envoy_config_rbac_v2_RBAC*> policies{};+  std::unique_ptr<CelEvaluationEngine> engine =+      CelEvaluationEngine::CreateCelEvaluationEngine(policies);+  EXPECT_EQ(engine, nullptr)+      << ""Created a CelEvaluationEngine without policies."";+}++TEST_F(CelEngineTest, CreateEngineFailWrongPolicyOrder) {+  std::vector<envoy_config_rbac_v2_RBAC*> policies{allow_policy, deny_policy};+  std::unique_ptr<CelEvaluationEngine> engine =+      CelEvaluationEngine::CreateCelEvaluationEngine(policies);+  EXPECT_EQ(engine, nullptr)+      << ""Created a CelEvaluationEngine with policies in the wrong order."";+}++TEST_F(CelEngineTest, CreateEngineFailMissingPolicyType) {",This test doesn't add much value. Its similar to CreateEngineFailWrongPolicyOrder.Maybe we can remove this.,
66329299,michaelywg,https://api.github.com/repos/grpc/grpc/pulls/23245,451201694,2020-07-07T23:36:54Z,test/core/security/cel_engine_test.cc,"@@ -0,0 +1,76 @@+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/lib/security/authorization/cel_evaluation_engine.h""++#include <gtest/gtest.h>++class CelEngineTest : public ::testing::Test {+ protected:+  void SetUp() override {+    deny_policy = envoy_config_rbac_v2_RBAC_new(arena.ptr());+    envoy_config_rbac_v2_RBAC_set_action(deny_policy, 1);+    allow_policy = envoy_config_rbac_v2_RBAC_new(arena.ptr());+    envoy_config_rbac_v2_RBAC_set_action(allow_policy, 0);+  }++  upb::Arena arena;+  envoy_config_rbac_v2_RBAC* deny_policy;+  envoy_config_rbac_v2_RBAC* allow_policy;+};++TEST_F(CelEngineTest, CreateEngineSuccessOnePolicy) {+  std::vector<envoy_config_rbac_v2_RBAC*> policies{allow_policy};+  std::unique_ptr<CelEvaluationEngine> engine =+      CelEvaluationEngine::CreateCelEvaluationEngine(policies);+  EXPECT_NE(engine, nullptr)+      << ""Failed to create a CelEvaluationEngine with one policy."";+}++TEST_F(CelEngineTest, CreateEngineSuccessTwoPolicies) {+  std::vector<envoy_config_rbac_v2_RBAC*> policies{deny_policy, allow_policy};+  std::unique_ptr<CelEvaluationEngine> engine =+      CelEvaluationEngine::CreateCelEvaluationEngine(policies);+  EXPECT_NE(engine, nullptr)+      << ""Failed to create a CelEvaluationEngine with two policies."";+}++TEST_F(CelEngineTest, CreateEngineFailNoPolicies) {+  std::vector<envoy_config_rbac_v2_RBAC*> policies{};+  std::unique_ptr<CelEvaluationEngine> engine =+      CelEvaluationEngine::CreateCelEvaluationEngine(policies);+  EXPECT_EQ(engine, nullptr)+      << ""Created a CelEvaluationEngine without policies."";","My understanding is that these log messages are transmitted in the event that the EXPECT_EQ fails, i.e. if we *do* end up creating a CelEvaluationEngine. Perhaps I could preface them with ""FAIL: "" or ""ERROR: "" to make that more clear?https://github.com/google/googletest/blob/master/googletest/docs/primer.md#assertions",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23396,451229845,2020-07-08T01:24:09Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,289 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <gflags/gflags.h>+#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+// ""INSECURE_CREDENTIALS"" - InsecureChannelCredentials+// ""alts"" - AltsCredentials+// ""ssl"" - SslCredentials+// ""google_default_credentials"" - GoogleDefaultCredentials: tls connection ++// oauth token++using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;++using grpc::ClientContext;+using grpc::Status;++int64_t GetServerID(grpc::channelz::v1::Server& server) {+  return server.ref().server_id();+}+int64_t GetChannelID(grpc::channelz::v1::Channel& channel) {+  return channel.ref().channel_id();+}+int64_t GetSubchannelID(grpc::channelz::v1::Subchannel& subchannel) {+  return subchannel.ref().subchannel_id();+}+int64_t GetSocketID(grpc::channelz::v1::Socket& socket) {+  return socket.ref().socket_id();+}++void GetChannelRPC(int64_t channel_id,+                   grpc::channelz::v1::Channelz::Stub* channelz_stub,+                   std::queue<grpc::channelz::v1::Channel>& channel_queue) {+  GetChannelRequest get_channel_request;+  get_channel_request.set_channel_id(channel_id);+  GetChannelResponse get_channel_response;+  ClientContext get_channel_context;+  channelz_stub->GetChannel(&get_channel_context, get_channel_request,+                            &get_channel_response);+  channel_queue.push(get_channel_response.channel());+}++void GetSubchannelRPC(+    int64_t subchannel_id, grpc::channelz::v1::Channelz::Stub* channelz_stub,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  GetSubchannelRequest get_subchannel_request;+  get_subchannel_request.set_subchannel_id(subchannel_id);+  GetSubchannelResponse get_subchannel_response;+  ClientContext get_subchannel_context;+  channelz_stub->GetSubchannel(&get_subchannel_context, get_subchannel_request,+                               &get_subchannel_response);+  subchannel_queue.push(get_subchannel_response.subchannel());+}++void GetChannelDescedence(+    grpc::channelz::v1::Channelz::Stub* channelz_stub,+    grpc::channelz::v1::Channel& channel,+    std::queue<grpc::channelz::v1::Channel>& channel_queue,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+  if (channel.channel_ref_size() > 0) {+    std::cout << ""channel: "";+    for (auto& i : channel.channel_ref()) {+      int64_t ch_id = i.channel_id();+      std::cout << ch_id << "" "";+      GetChannelRPC(ch_id, channelz_stub, channel_queue);+    }+  }+  if (channel.subchannel_ref_size() > 0) {+    std::cout << ""subchannel: "";+    for (auto& i : channel.subchannel_ref()) {+      int64_t subch_id = i.subchannel_id();+      std::cout << subch_id << "" "";+      GetSubchannelRPC(subch_id, channelz_stub, subchannel_queue);+    }+  }+  if (channel.socket_ref_size() > 0) {+    std::cout << ""socket: "";+    for (auto& i : channel.socket_ref()) {+      int64_t so_id = i.socket_id();+      std::cout << so_id << "" "";+    }+  }+  std::cout << std::endl;+}++void GetSubchannelDescedence(+    grpc::channelz::v1::Channelz::Stub* channelz_stub,+    grpc::channelz::v1::Subchannel& subchannel,+    std::queue<grpc::channelz::v1::Channel>& channel_queue,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+            << "" descendence - "";+  if (subchannel.channel_ref_size() > 0) {+    std::cout << ""channel: "";+    for (auto& i : subchannel.channel_ref()) {+      int64_t ch_id = i.channel_id();+      std::cout << ch_id << "" "";+      GetChannelRPC(ch_id, channelz_stub, channel_queue);+    }+  }+  if (subchannel.subchannel_ref_size() > 0) {+    std::cout << ""subchannel: "";+    for (auto& i : subchannel.subchannel_ref()) {+      int64_t subch_id = i.subchannel_id();+      std::cout << subch_id << "" "";+      GetSubchannelRPC(subch_id, channelz_stub, subchannel_queue);+    }+  }+  if (subchannel.socket_ref_size() > 0) {+    std::cout << ""socket: "";+    for (auto& i : subchannel.socket_ref()) {+      int64_t so_id = i.socket_id();+      std::cout << so_id << "" "";+    }+  }+  std::cout << std::endl;+}++int main(int argc, char** argv) {+  // make sure flags can be used+  grpc::testing::InitTest(&argc, &argv, true);+  std::cout << ""server address: "" << FLAGS_server_address << std::endl;+  std::cout << ""custom credentials type: "" << FLAGS_custom_credentials_type+            << std::endl;++  // create a channelz client+  ::grpc::channelz::experimental::InitChannelzService();+  grpc::ChannelArguments channel_args;+  std::shared_ptr<grpc::ChannelCredentials> channel_creds =+      grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+          FLAGS_custom_credentials_type, &channel_args);+  std::shared_ptr<grpc::Channel> channel =+      CreateChannel(FLAGS_server_address, channel_creds);+  std::unique_ptr<grpc::channelz::v1::Channelz::Stub> channelz_stub =+      grpc::channelz::v1::Channelz::NewStub(channel);++  // Server side code+  // Get all servers by calling GetServers+  GetServersRequest get_server_request;+  GetServersResponse get_server_response;+  ClientContext get_server_context;","It's not safe to reuse the same `ClientContext` object across multiple RPCs (also note https://github.com/grpc/grpc/blob/3201d7f7bfca6e6d3883da710c105d3e8167e0a3/include/grpcpp/impl/codegen/client_context_impl.h#L181).We should restructure things to ensure that the same we use a new/fresh `ClientContext` object on each RPC, e.g. in the `GetServers` loop below.",X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23245,451231136,2020-07-08T01:29:14Z,test/core/security/cel_engine_test.cc,"@@ -0,0 +1,76 @@+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/lib/security/authorization/cel_evaluation_engine.h""++#include <gtest/gtest.h>++class CelEngineTest : public ::testing::Test {+ protected:+  void SetUp() override {+    deny_policy = envoy_config_rbac_v2_RBAC_new(arena.ptr());+    envoy_config_rbac_v2_RBAC_set_action(deny_policy, 1);+    allow_policy = envoy_config_rbac_v2_RBAC_new(arena.ptr());+    envoy_config_rbac_v2_RBAC_set_action(allow_policy, 0);+  }++  upb::Arena arena;+  envoy_config_rbac_v2_RBAC* deny_policy;+  envoy_config_rbac_v2_RBAC* allow_policy;+};++TEST_F(CelEngineTest, CreateEngineSuccessOnePolicy) {+  std::vector<envoy_config_rbac_v2_RBAC*> policies{allow_policy};+  std::unique_ptr<CelEvaluationEngine> engine =+      CelEvaluationEngine::CreateCelEvaluationEngine(policies);+  EXPECT_NE(engine, nullptr)+      << ""Failed to create a CelEvaluationEngine with one policy."";+}++TEST_F(CelEngineTest, CreateEngineSuccessTwoPolicies) {+  std::vector<envoy_config_rbac_v2_RBAC*> policies{deny_policy, allow_policy};+  std::unique_ptr<CelEvaluationEngine> engine =+      CelEvaluationEngine::CreateCelEvaluationEngine(policies);+  EXPECT_NE(engine, nullptr)+      << ""Failed to create a CelEvaluationEngine with two policies."";+}++TEST_F(CelEngineTest, CreateEngineFailNoPolicies) {+  std::vector<envoy_config_rbac_v2_RBAC*> policies{};+  std::unique_ptr<CelEvaluationEngine> engine =+      CelEvaluationEngine::CreateCelEvaluationEngine(policies);+  EXPECT_EQ(engine, nullptr)+      << ""Created a CelEvaluationEngine without policies."";","That is right. My bad! You can leave it the way it is or add prefacing, anything works. Make sure it is consistent across tests. ",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23396,451234039,2020-07-08T01:40:37Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,289 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <gflags/gflags.h>+#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+// ""INSECURE_CREDENTIALS"" - InsecureChannelCredentials+// ""alts"" - AltsCredentials+// ""ssl"" - SslCredentials+// ""google_default_credentials"" - GoogleDefaultCredentials: tls connection ++// oauth token++using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;++using grpc::ClientContext;+using grpc::Status;++int64_t GetServerID(grpc::channelz::v1::Server& server) {+  return server.ref().server_id();+}+int64_t GetChannelID(grpc::channelz::v1::Channel& channel) {+  return channel.ref().channel_id();+}+int64_t GetSubchannelID(grpc::channelz::v1::Subchannel& subchannel) {+  return subchannel.ref().subchannel_id();+}+int64_t GetSocketID(grpc::channelz::v1::Socket& socket) {+  return socket.ref().socket_id();+}++void GetChannelRPC(int64_t channel_id,+                   grpc::channelz::v1::Channelz::Stub* channelz_stub,+                   std::queue<grpc::channelz::v1::Channel>& channel_queue) {+  GetChannelRequest get_channel_request;+  get_channel_request.set_channel_id(channel_id);+  GetChannelResponse get_channel_response;+  ClientContext get_channel_context;+  channelz_stub->GetChannel(&get_channel_context, get_channel_request,+                            &get_channel_response);+  channel_queue.push(get_channel_response.channel());+}++void GetSubchannelRPC(+    int64_t subchannel_id, grpc::channelz::v1::Channelz::Stub* channelz_stub,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  GetSubchannelRequest get_subchannel_request;+  get_subchannel_request.set_subchannel_id(subchannel_id);+  GetSubchannelResponse get_subchannel_response;+  ClientContext get_subchannel_context;+  channelz_stub->GetSubchannel(&get_subchannel_context, get_subchannel_request,+                               &get_subchannel_response);+  subchannel_queue.push(get_subchannel_response.subchannel());+}++void GetChannelDescedence(+    grpc::channelz::v1::Channelz::Stub* channelz_stub,+    grpc::channelz::v1::Channel& channel,+    std::queue<grpc::channelz::v1::Channel>& channel_queue,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+  if (channel.channel_ref_size() > 0) {+    std::cout << ""channel: "";+    for (auto& i : channel.channel_ref()) {+      int64_t ch_id = i.channel_id();+      std::cout << ch_id << "" "";+      GetChannelRPC(ch_id, channelz_stub, channel_queue);+    }+  }+  if (channel.subchannel_ref_size() > 0) {+    std::cout << ""subchannel: "";+    for (auto& i : channel.subchannel_ref()) {+      int64_t subch_id = i.subchannel_id();+      std::cout << subch_id << "" "";+      GetSubchannelRPC(subch_id, channelz_stub, subchannel_queue);+    }+  }+  if (channel.socket_ref_size() > 0) {+    std::cout << ""socket: "";+    for (auto& i : channel.socket_ref()) {+      int64_t so_id = i.socket_id();+      std::cout << so_id << "" "";+    }+  }+  std::cout << std::endl;+}++void GetSubchannelDescedence(+    grpc::channelz::v1::Channelz::Stub* channelz_stub,+    grpc::channelz::v1::Subchannel& subchannel,+    std::queue<grpc::channelz::v1::Channel>& channel_queue,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+            << "" descendence - "";+  if (subchannel.channel_ref_size() > 0) {+    std::cout << ""channel: "";+    for (auto& i : subchannel.channel_ref()) {+      int64_t ch_id = i.channel_id();+      std::cout << ch_id << "" "";+      GetChannelRPC(ch_id, channelz_stub, channel_queue);+    }+  }+  if (subchannel.subchannel_ref_size() > 0) {+    std::cout << ""subchannel: "";+    for (auto& i : subchannel.subchannel_ref()) {+      int64_t subch_id = i.subchannel_id();+      std::cout << subch_id << "" "";+      GetSubchannelRPC(subch_id, channelz_stub, subchannel_queue);+    }+  }+  if (subchannel.socket_ref_size() > 0) {+    std::cout << ""socket: "";+    for (auto& i : subchannel.socket_ref()) {+      int64_t so_id = i.socket_id();+      std::cout << so_id << "" "";+    }+  }+  std::cout << std::endl;+}++int main(int argc, char** argv) {+  // make sure flags can be used+  grpc::testing::InitTest(&argc, &argv, true);+  std::cout << ""server address: "" << FLAGS_server_address << std::endl;+  std::cout << ""custom credentials type: "" << FLAGS_custom_credentials_type+            << std::endl;++  // create a channelz client+  ::grpc::channelz::experimental::InitChannelzService();+  grpc::ChannelArguments channel_args;+  std::shared_ptr<grpc::ChannelCredentials> channel_creds =+      grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+          FLAGS_custom_credentials_type, &channel_args);+  std::shared_ptr<grpc::Channel> channel =+      CreateChannel(FLAGS_server_address, channel_creds);+  std::unique_ptr<grpc::channelz::v1::Channelz::Stub> channelz_stub =+      grpc::channelz::v1::Channelz::NewStub(channel);++  // Server side code+  // Get all servers by calling GetServers+  GetServersRequest get_server_request;+  GetServersResponse get_server_response;+  ClientContext get_server_context;+  std::vector<grpc::channelz::v1::Server> top_servers;+  int64_t server_start_id = 0;+  while (true) {+    get_server_request.set_start_server_id(server_start_id);+    Status status = channelz_stub->GetServers(+        &get_server_context, get_server_request, &get_server_response);+    if (!status.ok()) {+      std::cout << ""Channelz failed: "" << status.error_message() << std::endl;+      return 1;+    }+    for (auto& i : get_server_response.server()) {+      top_servers.push_back(i);+    }+    if (!get_server_response.end()) {+      server_start_id = GetServerID(top_servers.back()) + 1;+    } else {+      break;+    }+  }+  std::cout << ""Number of servers = "" << top_servers.size() << std::endl;++  // print out sockets of each server+  for (auto& i : top_servers) {+    std::cout << ""Server "" << GetServerID(i) << "" listen_socket: "";+    for (auto& j : i.listen_socket()) {+      std::cout << j.socket_id() << "" "";",It would be more interesting for debug purposes to see the entire `Server` data structure.Can we use the protobuf text format APIs to dump the entire `Server` objects (https://developers.google.com/protocol-buffers/docs/reference/cpp/google.protobuf.text_format),
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23396,451234204,2020-07-08T01:41:10Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,289 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <gflags/gflags.h>+#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+// ""INSECURE_CREDENTIALS"" - InsecureChannelCredentials+// ""alts"" - AltsCredentials+// ""ssl"" - SslCredentials+// ""google_default_credentials"" - GoogleDefaultCredentials: tls connection ++// oauth token++using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;++using grpc::ClientContext;+using grpc::Status;++int64_t GetServerID(grpc::channelz::v1::Server& server) {+  return server.ref().server_id();+}+int64_t GetChannelID(grpc::channelz::v1::Channel& channel) {+  return channel.ref().channel_id();+}+int64_t GetSubchannelID(grpc::channelz::v1::Subchannel& subchannel) {+  return subchannel.ref().subchannel_id();+}+int64_t GetSocketID(grpc::channelz::v1::Socket& socket) {+  return socket.ref().socket_id();+}++void GetChannelRPC(int64_t channel_id,+                   grpc::channelz::v1::Channelz::Stub* channelz_stub,+                   std::queue<grpc::channelz::v1::Channel>& channel_queue) {+  GetChannelRequest get_channel_request;+  get_channel_request.set_channel_id(channel_id);+  GetChannelResponse get_channel_response;+  ClientContext get_channel_context;+  channelz_stub->GetChannel(&get_channel_context, get_channel_request,+                            &get_channel_response);+  channel_queue.push(get_channel_response.channel());+}++void GetSubchannelRPC(+    int64_t subchannel_id, grpc::channelz::v1::Channelz::Stub* channelz_stub,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  GetSubchannelRequest get_subchannel_request;+  get_subchannel_request.set_subchannel_id(subchannel_id);+  GetSubchannelResponse get_subchannel_response;+  ClientContext get_subchannel_context;+  channelz_stub->GetSubchannel(&get_subchannel_context, get_subchannel_request,+                               &get_subchannel_response);+  subchannel_queue.push(get_subchannel_response.subchannel());+}++void GetChannelDescedence(+    grpc::channelz::v1::Channelz::Stub* channelz_stub,+    grpc::channelz::v1::Channel& channel,+    std::queue<grpc::channelz::v1::Channel>& channel_queue,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+  if (channel.channel_ref_size() > 0) {+    std::cout << ""channel: "";+    for (auto& i : channel.channel_ref()) {+      int64_t ch_id = i.channel_id();+      std::cout << ch_id << "" "";+      GetChannelRPC(ch_id, channelz_stub, channel_queue);+    }+  }+  if (channel.subchannel_ref_size() > 0) {+    std::cout << ""subchannel: "";+    for (auto& i : channel.subchannel_ref()) {+      int64_t subch_id = i.subchannel_id();+      std::cout << subch_id << "" "";+      GetSubchannelRPC(subch_id, channelz_stub, subchannel_queue);+    }+  }+  if (channel.socket_ref_size() > 0) {+    std::cout << ""socket: "";+    for (auto& i : channel.socket_ref()) {+      int64_t so_id = i.socket_id();+      std::cout << so_id << "" "";+    }+  }+  std::cout << std::endl;+}++void GetSubchannelDescedence(+    grpc::channelz::v1::Channelz::Stub* channelz_stub,+    grpc::channelz::v1::Subchannel& subchannel,+    std::queue<grpc::channelz::v1::Channel>& channel_queue,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+            << "" descendence - "";+  if (subchannel.channel_ref_size() > 0) {+    std::cout << ""channel: "";+    for (auto& i : subchannel.channel_ref()) {+      int64_t ch_id = i.channel_id();+      std::cout << ch_id << "" "";+      GetChannelRPC(ch_id, channelz_stub, channel_queue);+    }+  }+  if (subchannel.subchannel_ref_size() > 0) {+    std::cout << ""subchannel: "";+    for (auto& i : subchannel.subchannel_ref()) {+      int64_t subch_id = i.subchannel_id();+      std::cout << subch_id << "" "";+      GetSubchannelRPC(subch_id, channelz_stub, subchannel_queue);+    }+  }+  if (subchannel.socket_ref_size() > 0) {+    std::cout << ""socket: "";+    for (auto& i : subchannel.socket_ref()) {+      int64_t so_id = i.socket_id();+      std::cout << so_id << "" "";+    }+  }+  std::cout << std::endl;+}++int main(int argc, char** argv) {+  // make sure flags can be used+  grpc::testing::InitTest(&argc, &argv, true);+  std::cout << ""server address: "" << FLAGS_server_address << std::endl;+  std::cout << ""custom credentials type: "" << FLAGS_custom_credentials_type+            << std::endl;++  // create a channelz client+  ::grpc::channelz::experimental::InitChannelzService();+  grpc::ChannelArguments channel_args;+  std::shared_ptr<grpc::ChannelCredentials> channel_creds =+      grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+          FLAGS_custom_credentials_type, &channel_args);+  std::shared_ptr<grpc::Channel> channel =+      CreateChannel(FLAGS_server_address, channel_creds);+  std::unique_ptr<grpc::channelz::v1::Channelz::Stub> channelz_stub =+      grpc::channelz::v1::Channelz::NewStub(channel);++  // Server side code+  // Get all servers by calling GetServers+  GetServersRequest get_server_request;+  GetServersResponse get_server_response;+  ClientContext get_server_context;+  std::vector<grpc::channelz::v1::Server> top_servers;+  int64_t server_start_id = 0;+  while (true) {+    get_server_request.set_start_server_id(server_start_id);+    Status status = channelz_stub->GetServers(+        &get_server_context, get_server_request, &get_server_response);+    if (!status.ok()) {+      std::cout << ""Channelz failed: "" << status.error_message() << std::endl;+      return 1;+    }+    for (auto& i : get_server_response.server()) {+      top_servers.push_back(i);+    }+    if (!get_server_response.end()) {+      server_start_id = GetServerID(top_servers.back()) + 1;+    } else {+      break;+    }+  }+  std::cout << ""Number of servers = "" << top_servers.size() << std::endl;++  // print out sockets of each server+  for (auto& i : top_servers) {+    std::cout << ""Server "" << GetServerID(i) << "" listen_socket: "";+    for (auto& j : i.listen_socket()) {+      std::cout << j.socket_id() << "" "";+    }+    std::cout << std::endl;+  }+  std::cout << std::endl;++  // Client side code+  // Get all top channels by calling GetTopChannels+  GetTopChannelsRequest get_top_channels_request;+  GetTopChannelsResponse get_top_channels_response;+  ClientContext get_top_channels_context;","similar to above, we'll need to make sure this context object isn't reused across RPCs",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23396,451234976,2020-07-08T01:44:15Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,289 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <gflags/gflags.h>+#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+// ""INSECURE_CREDENTIALS"" - InsecureChannelCredentials+// ""alts"" - AltsCredentials+// ""ssl"" - SslCredentials+// ""google_default_credentials"" - GoogleDefaultCredentials: tls connection ++// oauth token++using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;++using grpc::ClientContext;+using grpc::Status;++int64_t GetServerID(grpc::channelz::v1::Server& server) {+  return server.ref().server_id();+}+int64_t GetChannelID(grpc::channelz::v1::Channel& channel) {+  return channel.ref().channel_id();+}+int64_t GetSubchannelID(grpc::channelz::v1::Subchannel& subchannel) {+  return subchannel.ref().subchannel_id();+}+int64_t GetSocketID(grpc::channelz::v1::Socket& socket) {+  return socket.ref().socket_id();+}++void GetChannelRPC(int64_t channel_id,+                   grpc::channelz::v1::Channelz::Stub* channelz_stub,+                   std::queue<grpc::channelz::v1::Channel>& channel_queue) {+  GetChannelRequest get_channel_request;+  get_channel_request.set_channel_id(channel_id);+  GetChannelResponse get_channel_response;+  ClientContext get_channel_context;+  channelz_stub->GetChannel(&get_channel_context, get_channel_request,+                            &get_channel_response);+  channel_queue.push(get_channel_response.channel());+}++void GetSubchannelRPC(+    int64_t subchannel_id, grpc::channelz::v1::Channelz::Stub* channelz_stub,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  GetSubchannelRequest get_subchannel_request;+  get_subchannel_request.set_subchannel_id(subchannel_id);+  GetSubchannelResponse get_subchannel_response;+  ClientContext get_subchannel_context;+  channelz_stub->GetSubchannel(&get_subchannel_context, get_subchannel_request,+                               &get_subchannel_response);+  subchannel_queue.push(get_subchannel_response.subchannel());+}++void GetChannelDescedence(+    grpc::channelz::v1::Channelz::Stub* channelz_stub,+    grpc::channelz::v1::Channel& channel,+    std::queue<grpc::channelz::v1::Channel>& channel_queue,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+  if (channel.channel_ref_size() > 0) {+    std::cout << ""channel: "";+    for (auto& i : channel.channel_ref()) {+      int64_t ch_id = i.channel_id();+      std::cout << ch_id << "" "";+      GetChannelRPC(ch_id, channelz_stub, channel_queue);+    }+  }+  if (channel.subchannel_ref_size() > 0) {+    std::cout << ""subchannel: "";+    for (auto& i : channel.subchannel_ref()) {+      int64_t subch_id = i.subchannel_id();+      std::cout << subch_id << "" "";+      GetSubchannelRPC(subch_id, channelz_stub, subchannel_queue);+    }+  }+  if (channel.socket_ref_size() > 0) {+    std::cout << ""socket: "";+    for (auto& i : channel.socket_ref()) {+      int64_t so_id = i.socket_id();+      std::cout << so_id << "" "";+    }+  }+  std::cout << std::endl;+}++void GetSubchannelDescedence(+    grpc::channelz::v1::Channelz::Stub* channelz_stub,+    grpc::channelz::v1::Subchannel& subchannel,+    std::queue<grpc::channelz::v1::Channel>& channel_queue,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+            << "" descendence - "";+  if (subchannel.channel_ref_size() > 0) {+    std::cout << ""channel: "";+    for (auto& i : subchannel.channel_ref()) {+      int64_t ch_id = i.channel_id();+      std::cout << ch_id << "" "";","similar to above, it would be more interesting to dump the full proto messages, rather than just the channel/subchannel/socket id's",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/20857,451710650,2020-07-08T17:29:03Z,src/core/lib/iomgr/tcp_windows.cc,"@@ -482,7 +482,10 @@ static grpc_resource_user* win_get_resource_user(grpc_endpoint* ep) {   return tcp->resource_user; } -static int win_get_fd(grpc_endpoint* ep) { return -1; }+static int win_get_fd(grpc_endpoint* ep) {+  grpc_tcp* tcp = reinterpret_cast<grpc_tcp*>(ep);","@yihuazhang I think in order to enable on Windows, those test configurations need to add ""windows"" to the `_platforms` field, e.g. in https://github.com/grpc/grpc/blob/ff8ceb700e8a53ed4087edc006830da372b1199a/test/core/end2end/generate_tests.bzl#L97",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23420,451979018,2020-07-09T05:52:33Z,test/core/util/test_config.cc,"@@ -135,26 +97,42 @@ static void print_stack_from_context(CONTEXT c) { #endif    HANDLE process = GetCurrentProcess();-  HANDLE thread = GetCurrentThread();    SYMBOL_INFOW* symbol =       (SYMBOL_INFOW*)calloc(sizeof(SYMBOL_INFOW) + 256 * sizeof(wchar_t), 1);   symbol->MaxNameLen = 255;   symbol->SizeOfStruct = sizeof(SYMBOL_INFOW); -  while (StackWalk(imageType, process, thread, &s, &c, 0,-                   SymFunctionTableAccess, SymGetModuleBase, 0)) {-    BOOL has_symbol =-        SymFromAddrW(process, (DWORD64)(s.AddrPC.Offset), 0, symbol);-    fwprintf(-        stderr, L""*** %016I64X %ls - %016I64X\n"", (DWORD64)(s.AddrPC.Offset),-        has_symbol ? symbol->Name : L""<<no symbol>>"", (DWORD64)symbol->Address);+  const unsigned short MAX_CALLERS_SHOWN = 32;+  for (int frame = 0; frame < MAX_CALLERS_SHOWN && StackWalk(imageType, process, thread, &s, &c, 0,","Looks like before, the abort handler would limit the number of stack frames to display up to `MAX_CALLERS_SHOWN`, but the exception/crash handler would not, and would continue walking the stack as long as `StackWalk` returned true.Is it necessary to limit the number of frames displayed to `MAX_CALLERS_SHOWN` in both cases? If not, maybe pass the limit on number of frames to display as a parameter?",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23437,452529771,2020-07-09T22:44:36Z,src/python/grpcio/support.py,"@@ -28,18 +28,20 @@ """""" C_PYTHON_DEV_ERROR_MESSAGE = """""" Could not find <Python.h>. This could mean the following:-  * You're on Ubuntu and haven't run `apt-get install python-dev`.-  * You're on RHEL/Fedora and haven't run `yum install python-devel` or-    `dnf install python-devel` (make sure you also have redhat-rpm-config+  * You're on Ubuntu and haven't run `apt-get install <PY_REPR>-dev`.+  * You're on RHEL/Fedora and haven't run `yum install <PY_REPR>-devel` or+    `dnf install <PY_REPR>-devel` (make sure you also have redhat-rpm-config     installed)   * You're on Mac OS X and the usual Python framework was somehow corrupted     (check your environment variables or try re-installing?)   * You're on Windows and your Python installation was somehow corrupted     (check your environment variables or try re-installing?) """"""+PYTHON_REPRESENTATION = 'python' if sys.version_info[0] == 2 else 'python3'","Possible. But there's no way to know until they make the decision. Hopefully, they'll never have to 🤞, but forever is a long time.",
67390330,mohanli-ml,https://api.github.com/repos/grpc/grpc/pulls/23396,452587757,2020-07-10T02:23:09Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,289 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <gflags/gflags.h>+#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+// ""INSECURE_CREDENTIALS"" - InsecureChannelCredentials+// ""alts"" - AltsCredentials+// ""ssl"" - SslCredentials+// ""google_default_credentials"" - GoogleDefaultCredentials: tls connection ++// oauth token++using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;++using grpc::ClientContext;+using grpc::Status;++int64_t GetServerID(grpc::channelz::v1::Server& server) {+  return server.ref().server_id();+}+int64_t GetChannelID(grpc::channelz::v1::Channel& channel) {+  return channel.ref().channel_id();+}+int64_t GetSubchannelID(grpc::channelz::v1::Subchannel& subchannel) {+  return subchannel.ref().subchannel_id();+}+int64_t GetSocketID(grpc::channelz::v1::Socket& socket) {+  return socket.ref().socket_id();+}++void GetChannelRPC(int64_t channel_id,+                   grpc::channelz::v1::Channelz::Stub* channelz_stub,+                   std::queue<grpc::channelz::v1::Channel>& channel_queue) {+  GetChannelRequest get_channel_request;+  get_channel_request.set_channel_id(channel_id);+  GetChannelResponse get_channel_response;+  ClientContext get_channel_context;+  channelz_stub->GetChannel(&get_channel_context, get_channel_request,+                            &get_channel_response);+  channel_queue.push(get_channel_response.channel());+}++void GetSubchannelRPC(+    int64_t subchannel_id, grpc::channelz::v1::Channelz::Stub* channelz_stub,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  GetSubchannelRequest get_subchannel_request;+  get_subchannel_request.set_subchannel_id(subchannel_id);+  GetSubchannelResponse get_subchannel_response;+  ClientContext get_subchannel_context;+  channelz_stub->GetSubchannel(&get_subchannel_context, get_subchannel_request,+                               &get_subchannel_response);+  subchannel_queue.push(get_subchannel_response.subchannel());+}++void GetChannelDescedence(+    grpc::channelz::v1::Channelz::Stub* channelz_stub,+    grpc::channelz::v1::Channel& channel,+    std::queue<grpc::channelz::v1::Channel>& channel_queue,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+  if (channel.channel_ref_size() > 0) {+    std::cout << ""channel: "";+    for (auto& i : channel.channel_ref()) {+      int64_t ch_id = i.channel_id();+      std::cout << ch_id << "" "";+      GetChannelRPC(ch_id, channelz_stub, channel_queue);+    }+  }+  if (channel.subchannel_ref_size() > 0) {+    std::cout << ""subchannel: "";+    for (auto& i : channel.subchannel_ref()) {+      int64_t subch_id = i.subchannel_id();+      std::cout << subch_id << "" "";+      GetSubchannelRPC(subch_id, channelz_stub, subchannel_queue);+    }+  }+  if (channel.socket_ref_size() > 0) {+    std::cout << ""socket: "";+    for (auto& i : channel.socket_ref()) {+      int64_t so_id = i.socket_id();+      std::cout << so_id << "" "";+    }+  }+  std::cout << std::endl;+}++void GetSubchannelDescedence(+    grpc::channelz::v1::Channelz::Stub* channelz_stub,+    grpc::channelz::v1::Subchannel& subchannel,+    std::queue<grpc::channelz::v1::Channel>& channel_queue,+    std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+  std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+            << "" descendence - "";+  if (subchannel.channel_ref_size() > 0) {+    std::cout << ""channel: "";+    for (auto& i : subchannel.channel_ref()) {+      int64_t ch_id = i.channel_id();+      std::cout << ch_id << "" "";","Keep the current output format for quickly showing the tree structure, and print out entity (i.e., channel, subchannel, server, socket) data in the end.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22916,452640023,2020-07-10T06:07:52Z,src/csharp/Grpc.Tools/ProtoCompile.cs,"@@ -375,60 +375,48 @@ protected override bool ValidateParameters()         static readonly Encoding s_utf8WithoutBom = new UTF8Encoding(false);         protected override Encoding ResponseFileEncoding => s_utf8WithoutBom; -        // Protoc takes one argument per line from the response file, and does not-        // require any quoting whatsoever. Otherwise, this is similar to the-        // standard CommandLineBuilder-        class ProtocResponseFileBuilder+        class ProtocCommandLineBuilder : CommandLineBuilder         {-            StringBuilder _data = new StringBuilder(1000);-            public override string ToString() => _data.ToString();--            // If 'value' is not empty, append '--name=value\n'.-            public void AddSwitchMaybe(string name, string value)+            // If 'value' is not empty, append '--name=value'.+            public void AppendSwitchUnquotedIfNotNullOrEmpty(string name, string value)             {                 if (!string.IsNullOrEmpty(value))                 {-                    _data.Append(""--"").Append(name).Append(""="")-                         .Append(value).Append('\n');+                    AppendSwitchIfNotNull($""--{name}="", value);                 }             }              // Add switch with the 'values' separated by commas, for options.-            public void AddSwitchMaybe(string name, string[] values)+            public void AppendSwitchIfNotNullOrEmpty(string name, string[] values)             {                 if (values?.Length > 0)                 {-                    _data.Append(""--"").Append(name).Append(""="")-                         .Append(string.Join("","", values)).Append('\n');+                    AppendSwitchIfNotNull($""--{name}="", values, "","");                 }             }+        } -            // Add a positional argument to the file data.-            public void AddArg(string arg)-            {-                _data.Append(arg).Append('\n');-            }-        };--        // Called by the base ToolTask to get response file contents.-        protected override string GenerateResponseFileCommands()+        // Called by the base ToolTask to get the command line.+        protected override string GenerateCommandLineCommands()         {-            var cmd = new ProtocResponseFileBuilder();-            cmd.AddSwitchMaybe(Generator + ""_out"", TrimEndSlash(OutputDir));-            cmd.AddSwitchMaybe(Generator + ""_opt"", OutputOptions);-            cmd.AddSwitchMaybe(""plugin=protoc-gen-grpc"", GrpcPluginExe);-            cmd.AddSwitchMaybe(""grpc_out"", TrimEndSlash(GrpcOutputDir));-            cmd.AddSwitchMaybe(""grpc_opt"", GrpcOutputOptions);+            var cmd = new ProtocCommandLineBuilder();",I'm worried about the switch from ResponseFile to CommandLine args. Can't this:1. cause problems because we need to use quoting now?2. cause issues like command line ending up too long etc.?I'm actually not entirely sure why ResponseFile way of passing arguments was originally used.,
940619,Falco20019,https://api.github.com/repos/grpc/grpc/pulls/22916,452748947,2020-07-10T10:01:47Z,src/csharp/Grpc.Tools/ProtoCompile.cs,"@@ -375,60 +375,48 @@ protected override bool ValidateParameters()         static readonly Encoding s_utf8WithoutBom = new UTF8Encoding(false);         protected override Encoding ResponseFileEncoding => s_utf8WithoutBom; -        // Protoc takes one argument per line from the response file, and does not-        // require any quoting whatsoever. Otherwise, this is similar to the-        // standard CommandLineBuilder-        class ProtocResponseFileBuilder+        class ProtocCommandLineBuilder : CommandLineBuilder         {-            StringBuilder _data = new StringBuilder(1000);-            public override string ToString() => _data.ToString();--            // If 'value' is not empty, append '--name=value\n'.-            public void AddSwitchMaybe(string name, string value)+            // If 'value' is not empty, append '--name=value'.+            public void AppendSwitchUnquotedIfNotNullOrEmpty(string name, string value)             {                 if (!string.IsNullOrEmpty(value))                 {-                    _data.Append(""--"").Append(name).Append(""="")-                         .Append(value).Append('\n');+                    AppendSwitchIfNotNull($""--{name}="", value);                 }             }              // Add switch with the 'values' separated by commas, for options.-            public void AddSwitchMaybe(string name, string[] values)+            public void AppendSwitchIfNotNullOrEmpty(string name, string[] values)             {                 if (values?.Length > 0)                 {-                    _data.Append(""--"").Append(name).Append(""="")-                         .Append(string.Join("","", values)).Append('\n');+                    AppendSwitchIfNotNull($""--{name}="", values, "","");                 }             }+        } -            // Add a positional argument to the file data.-            public void AddArg(string arg)-            {-                _data.Append(arg).Append('\n');-            }-        };--        // Called by the base ToolTask to get response file contents.-        protected override string GenerateResponseFileCommands()+        // Called by the base ToolTask to get the command line.+        protected override string GenerateCommandLineCommands()         {-            var cmd = new ProtocResponseFileBuilder();-            cmd.AddSwitchMaybe(Generator + ""_out"", TrimEndSlash(OutputDir));-            cmd.AddSwitchMaybe(Generator + ""_opt"", OutputOptions);-            cmd.AddSwitchMaybe(""plugin=protoc-gen-grpc"", GrpcPluginExe);-            cmd.AddSwitchMaybe(""grpc_out"", TrimEndSlash(GrpcOutputDir));-            cmd.AddSwitchMaybe(""grpc_opt"", GrpcOutputOptions);+            var cmd = new ProtocCommandLineBuilder();","Usually, this is called with one proto file per call (even though Protobuf would allow for multiple items). So it would only get too long if too many proto paths are appended. So yes, the response file would be preferable. But it seems, that protoc won't get fixed soon, so this would only be good as workaround and should be reverted once protoc is fixed.",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/23362,453287633,2020-07-12T08:42:33Z,src/core/lib/surface/init.cc,"@@ -193,8 +223,13 @@ void grpc_shutdown_internal_locked(void) {   grpc_core::ApplicationCallbackExecCtx::GlobalShutdown();   g_shutting_down = false;   gpr_cv_broadcast(g_shutting_down_cv);+  free(g_shutting_down_cv);   // Absolute last action will be to delete static metadata context.   grpc_destroy_static_metadata_ctx();++  // Executes all custom cleanup functions, which were registered via+  // grpc_on_shutdown_callback() and grpc_on_shutdown_callback_with_arg()+  delete ShutdownData::get();","In addition to the comments I made in my review, note that the grpc library can be init'ed and shutdown numerous times during the running of an application. It does not need to stay alive for the entire duration of the application, and it can be restarted later. I think doing these sorts of frees on function-static and global variables without actually recreating them one a later init is not going to work for the actual usage allowed by gRPC.",
32997632,stefan301,https://api.github.com/repos/grpc/grpc/pulls/23362,453483860,2020-07-13T08:35:25Z,src/core/lib/surface/init.cc,"@@ -193,8 +223,13 @@ void grpc_shutdown_internal_locked(void) {   grpc_core::ApplicationCallbackExecCtx::GlobalShutdown();   g_shutting_down = false;   gpr_cv_broadcast(g_shutting_down_cv);+  free(g_shutting_down_cv);   // Absolute last action will be to delete static metadata context.   grpc_destroy_static_metadata_ctx();++  // Executes all custom cleanup functions, which were registered via+  // grpc_on_shutdown_callback() and grpc_on_shutdown_callback_with_arg()+  delete ShutdownData::get();","I think, then there should be a function`GRPCAPI void grpc_shutdown_grpc_library();`to trigger that cleanup. This function can be called at process exit or when unloading a DLL that uses gRPC.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23420,453715242,2020-07-13T15:01:57Z,test/core/util/test_config.cc,"@@ -135,26 +97,42 @@ static void print_stack_from_context(CONTEXT c) { #endif    HANDLE process = GetCurrentProcess();-  HANDLE thread = GetCurrentThread();    SYMBOL_INFOW* symbol =       (SYMBOL_INFOW*)calloc(sizeof(SYMBOL_INFOW) + 256 * sizeof(wchar_t), 1);   symbol->MaxNameLen = 255;   symbol->SizeOfStruct = sizeof(SYMBOL_INFOW); -  while (StackWalk(imageType, process, thread, &s, &c, 0,-                   SymFunctionTableAccess, SymGetModuleBase, 0)) {-    BOOL has_symbol =-        SymFromAddrW(process, (DWORD64)(s.AddrPC.Offset), 0, symbol);-    fwprintf(-        stderr, L""*** %016I64X %ls - %016I64X\n"", (DWORD64)(s.AddrPC.Offset),-        has_symbol ? symbol->Name : L""<<no symbol>>"", (DWORD64)symbol->Address);+  const unsigned short MAX_CALLERS_SHOWN = 32;+  for (int frame = 0; frame < MAX_CALLERS_SHOWN && StackWalk(imageType, process, thread, &s, &c, 0,","I think the previous limit was due to the fact that the method to grab the stacktrace had a hard limit on number of frames.This limitation no longer exists, so I set the print limit to 8192 frames (seems pretty high) to avoid completely flooding the log if things go haywire for some reason.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23203,453803817,2020-07-13T17:15:01Z,src/core/lib/security/credentials/google_default/google_default_credentials.cc,"@@ -314,18 +289,56 @@ grpc_channel_credentials* grpc_google_default_credentials_create() {     g_metadata_server_available = is_metadata_server_reachable();   }   gpr_mu_unlock(&g_state_mu);+}++static void default_call_creds(+    grpc_core::RefCountedPtr<grpc_call_credentials>* call_creds,","Instead of returning this via an output parameter, just make it the return value.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23203,453804455,2020-07-13T17:16:02Z,src/core/lib/security/credentials/google_default/google_default_credentials.cc,"@@ -314,18 +289,56 @@ grpc_channel_credentials* grpc_google_default_credentials_create() {     g_metadata_server_available = is_metadata_server_reachable();   }   gpr_mu_unlock(&g_state_mu);+}++static void default_call_creds(+    grpc_core::RefCountedPtr<grpc_call_credentials>* call_creds,+    grpc_error* error) {","I assume this is meant to be an output parameter?  If so, it will need to be `grpc_error**`.The way this is currently written, you're resetting `error` within the function, but that modified value will never be visible outside of it.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23455,453821122,2020-07-13T17:44:10Z,tools/run_tests/python_utils/start_port_server.py,"@@ -73,12 +73,15 @@ def start_port_server():             # Working directory of port server needs to be outside of Jenkins             # workspace to prevent file lock issues.             tempdir = tempfile.mkdtemp()-            port_server = subprocess.Popen(-                args,-                env=env,-                cwd=tempdir,-                creationflags=0x00000008,  # detached process-                close_fds=True)+            if sys.version_info.major == 2:+                creationflags = 0x00000008  # detached process+            else:+                creationflags = 0  # DETACHED_PROCESS doesn't seem to work with python3",This is surprising. The docs indicate that DETACHED_PROCESS wasn't actually even introduced until 3.7. Have you tried using [the symbol](https://docs.python.org/3/library/subprocess.html#subprocess.DETACHED_PROCESS)?,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23396,453856324,2020-07-13T18:44:22Z,test/cpp/util/channelz_sampler_test.cc,"@@ -0,0 +1,167 @@+/*+ *+ * Copyright 2016 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <stdlib.h>+#include <unistd.h>+#include <cstdlib>+#include <iostream>+#include <memory>+#include <string>+#include <thread>++#include ""include/grpcpp/ext/proto_server_reflection_plugin_impl.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/testing/test.grpc.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/subprocess.h""+#include ""test/cpp/util/test_credentials_provider.h""++using grpc::Channel;+using grpc::ClientContext;+using grpc::Server;+using grpc::ServerBuilder;+using grpc::ServerContext;+using grpc::Status;++#include <gtest/gtest.h>++// Test variables+std::string server_address(""0.0.0.0:10000"");+std::string custom_credentials_type(""INSECURE_CREDENTIALS"");++// Creata an echo server - randomly delay 0.1 to 0.2 s+class EchoServerImpl final : public grpc::testing::TestService::Service {+  Status EmptyCall(::grpc::ServerContext* context,+                   const grpc::testing::Empty* request,+                   grpc::testing::Empty* response) {+    srand(unsigned(time(0)));+    unsigned int server_delay_microseconds = 100000;+    server_delay_microseconds += rand() % server_delay_microseconds;+    usleep(server_delay_microseconds);+    return Status::OK;+  }+};++// Run server in a thread+void RunServer() {+  // register channelz service+  ::grpc::channelz::experimental::InitChannelzService();++  EchoServerImpl service;+  grpc::EnableDefaultHealthCheckService(true);+  grpc_impl::reflection::InitProtoReflectionServerBuilderPlugin();+  grpc::ServerBuilder builder;+  auto server_creds =+      grpc::testing::GetCredentialsProvider()->GetServerCredentials(+          custom_credentials_type);+  builder.AddListeningPort(server_address, server_creds);++  // forces channelz and channel tracing to be enabled.+  builder.AddChannelArgument(GRPC_ARG_ENABLE_CHANNELZ, 1);+  builder.AddChannelArgument(GRPC_ARG_MAX_CHANNEL_TRACE_EVENT_MEMORY_PER_NODE,+                             1024);+  builder.RegisterService(&service);+  std::unique_ptr<Server> server(builder.BuildAndStart());+  std::cout << ""Server listening on "" << server_address << std::endl;++  server->Wait();+}++// Creata an echo client - set timeout as 0.15s+class EchoClientImpl {+ public:+  EchoClientImpl(std::shared_ptr<Channel> channel)+      : stub_(grpc::testing::TestService::NewStub(channel)) {}+  Status EmptyCall() {+    grpc::testing::Empty request;+    grpc::testing::Empty response;+    ClientContext context;+    int64_t timeout_microseconds = 150;+    context.set_deadline(+        grpc_timeout_milliseconds_to_deadline(timeout_microseconds));+    Status status = stub_->EmptyCall(&context, request, &response);+    return status;+  }++ private:+  std::unique_ptr<grpc::testing::TestService::Stub> stub_;+};++// Run client in a thread+void RunClient(std::string client_id) {+  // std::string target_str = ""localhost:10000"";+  grpc::ChannelArguments channel_args;+  std::shared_ptr<grpc::ChannelCredentials> channel_creds =+      grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+          custom_credentials_type, &channel_args);+  EchoClientImpl echoer(grpc::CreateChannel(server_address, channel_creds));+  unsigned int client_echo_sleep_second = 1;++  std::cout << ""Client "" << client_id << "" is echoing!"" << std::endl;+  while (true) {+    Status status = echoer.EmptyCall();+    sleep(client_echo_sleep_second);+  }+}++// Test the channelz sampler+TEST(ChannelzSamplerTest, SimpleTest) {+  // server thread+  std::thread server_thread(RunServer);+  std::cout << ""Wait 3 seconds to make sure server is started..."" << std::endl;+  float server_start_seconds = 3.0;+  sleep(server_start_seconds);++  // client thread+  std::thread client_thread_1(RunClient, ""1"");+  std::thread client_thread_2(RunClient, ""2"");+  float run_services_seconds = 5.0;+  std::cout << ""Run echo service for "" << run_services_seconds << "" seconds.""+            << std::endl;+  sleep(run_services_seconds);++  // Run the channelz sampler+  std::string channelz_sampler_bin_path =+      ""./bazel-bin/test/cpp/util/channelz_sampler"";+  grpc::SubProcess* test_driver = new grpc::SubProcess(",we'll need to wait for this process's completion and make sure the status (e.g. as is done in https://github.com/grpc/grpc/blob/6ff6066dd59aff38fec77a25127427609983b650/test/cpp/naming/resolver_component_tests_runner_invoker.cc#L130),
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/23344,453944509,2020-07-13T21:29:33Z,third_party/rake-compiler-dock/rake_x86_64-linux/Dockerfile,"@@ -0,0 +1,91 @@+FROM dockcross/manylinux2010-x64++# Docker file for building gRPC manylinux-based Ruby artifacts.+# Updated: 2020-07-03++# install packages which rvm will require+RUN yum install -y autoconf gcc-c++ libtool readline-devel ruby sqlite-devel openssl-devel xz++# install rvm, RVM 1.26.0+ has signed releases, source rvm for usage outside of package scripts+RUN gpg --keyserver hkp://pool.sks-keyservers.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB && \+    (curl -L http://get.rvm.io | sudo bash)+RUN echo ""source /etc/profile.d/rvm.sh"" >> /etc/rubybashrc+RUN bash -c "" \+        source /etc/rubybashrc && \+        rvm cleanup all ""++# Import patch files for ruby and gems+COPY build/patches /work/patches/+ENV BASH_ENV /etc/rubybashrc++# install rubies and fix permissions on+RUN bash -c "" \+    export CFLAGS='-s -O3 -fno-fast-math -fPIC' && \+    for v in 2.5.7 ; do \+        rvm install \$v --patch \$(echo /work/patches/ruby-\$v/* | tr ' ' ','); \+    done && \+    rvm cleanup all && \+    find /usr/local/rvm -type d -print0 | sudo xargs -0 chmod g+sw ""++# Install rake-compiler and typical gems in all Rubies+# do not generate documentation for gems+RUN echo ""gem: --no-ri --no-rdoc"" >> ~/.gemrc && \+    bash -c "" \+        rvm all do gem update --system --no-document && \+        rvm all do gem install --no-document bundler 'bundler:~>1.16' rake-compiler:1.1.0 hoe:3.20.0 mini_portile rubygems-tasks mini_portile2 && \+        find /usr/local/rvm -type d -print0 | sudo xargs -0 chmod g+sw ""++# Install rake-compiler's cross rubies in global dir instead of /root+RUN mkdir -p /usr/local/rake-compiler && \+    ln -s /usr/local/rake-compiler ~/.rake-compiler++RUN bash -c "" \+        rvm alias create default 2.5.7 && \+        rvm use default ""++# Patch rake-compiler and hoe package+COPY build/patches2 /work/patches/+RUN cd /usr/local/rvm/gems/ruby-2.5.7/gems/rake-compiler-1.1.0 && \+    ( git apply /work/patches/rake-compiler-1.1.0/*.patch || true )+RUN cd /usr/local/rvm/gems/ruby-2.5.7/gems/hoe-3.20.0 && \+    ( git apply /work/patches/hoe-3.20.0/*.patch || true )++# Patch ruby-2.7.0 for cross build+RUN curl -SL http://cache.ruby-lang.org/pub/ruby/2.7/ruby-2.7.0.tar.xz | tar -xJC /root/ && \+    cd /root/ruby-2.7.0 && \+    git apply /work/patches/ruby-2.7.0/*.patch && \+    cd .. && \+    mkdir -p /usr/local/rake-compiler/sources/ && \+    tar cjf /usr/local/rake-compiler/sources/ruby-2.7.0.tar.bz2 ruby-2.7.0 && \+    rm -rf /root/ruby-2.7.0++ENV XRUBIES 2.7.0:2.6.0:2.5.0:2.4.0:2.3.0:2.2.2","I like to keep this not because we need them but this is exactly what the upstream docker image has. To avoid the divergence from this, I want to keep what they have as much as possible.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23203,454003722,2020-07-13T23:27:46Z,src/core/lib/security/credentials/google_default/google_default_credentials.cc,"@@ -314,18 +289,58 @@ grpc_channel_credentials* grpc_google_default_credentials_create() {     g_metadata_server_available = is_metadata_server_reachable();   }   gpr_mu_unlock(&g_state_mu);+}++static grpc_core::RefCountedPtr<grpc_call_credentials> make_default_call_creds(+    grpc_error** error) {+  grpc_core::RefCountedPtr<grpc_call_credentials> call_creds;+  grpc_error* err;++  /* First, try the environment variable. */+  char* path_from_env = gpr_getenv(GRPC_GOOGLE_CREDENTIALS_ENV_VAR);+  if (path_from_env != nullptr) {+    err = create_default_creds_from_path(path_from_env, &call_creds);+    gpr_free(path_from_env);+    if (err == GRPC_ERROR_NONE) return call_creds;+    *error = grpc_error_add_child(*error, err);+  }++  /* Then the well-known file. */+  err = create_default_creds_from_path(+      grpc_get_well_known_google_credentials_file_path(), &call_creds);+  if (err == GRPC_ERROR_NONE) return call_creds;+  *error = grpc_error_add_child(*error, err);    if (g_metadata_server_available) {     call_creds = grpc_core::RefCountedPtr<grpc_call_credentials>(         grpc_google_compute_engine_credentials_create(nullptr));     if (call_creds == nullptr) {-      error = grpc_error_add_child(-          error, GRPC_ERROR_CREATE_FROM_STATIC_STRING(-                     ""Failed to get credentials from network""));+      *error = grpc_error_add_child(+          *error, GRPC_ERROR_CREATE_FROM_STATIC_STRING(+                      ""Failed to get credentials from network""));     }   } -end:+  return call_creds;+}++grpc_channel_credentials* grpc_google_default_credentials_create(+    grpc_call_credentials* call_credentials) {+  grpc_channel_credentials* result = nullptr;+  grpc_core::RefCountedPtr<grpc_call_credentials> call_creds(call_credentials);+  grpc_error* error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(+      ""Failed to create Google credentials"");",Good idea! That way we only allocate the error in case we actually hit an error condition.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23203,454003787,2020-07-13T23:27:59Z,test/core/security/credentials_test.cc,"@@ -1512,16 +1512,105 @@ static void test_no_google_default_creds(void) {       default_creds_gce_detection_httpcli_get_failure_override,       httpcli_post_should_not_be_called);   /* Simulate a successful detection of GCE. */-  GPR_ASSERT(grpc_google_default_credentials_create() == nullptr);+  GPR_ASSERT(grpc_google_default_credentials_create(nullptr) == nullptr);   /* Try a second one. GCE detection should occur again. */   g_test_gce_tenancy_checker_called = false;-  GPR_ASSERT(grpc_google_default_credentials_create() == nullptr);+  GPR_ASSERT(grpc_google_default_credentials_create(nullptr) == nullptr);   GPR_ASSERT(g_test_gce_tenancy_checker_called == true);   /* Cleanup. */   grpc_override_well_known_credentials_path_getter(nullptr);   grpc_httpcli_set_override(nullptr, nullptr); } +static void test_google_default_creds_call_creds_specified(void) {+  expected_md emd[] = {+      {""authorization"", ""Bearer ya29.AHES6ZRN3-HlhAPya30GnW_bHSb_""}};+  request_metadata_state* state =+      make_request_metadata_state(GRPC_ERROR_NONE, emd, GPR_ARRAY_SIZE(emd));+  grpc_auth_metadata_context auth_md_ctx = {test_service_url, test_method,+                                            nullptr, nullptr};+  grpc_core::ExecCtx exec_ctx;+  grpc_flush_cached_google_default_credentials();+  grpc_call_credentials* call_creds =+      grpc_google_compute_engine_credentials_create(nullptr);+  set_gce_tenancy_checker_for_testing(test_gce_tenancy_checker);+  g_test_gce_tenancy_checker_called = false;+  g_test_is_on_gce = true;+  grpc_httpcli_set_override(+      default_creds_metadata_server_detection_httpcli_get_success_override,+      httpcli_post_should_not_be_called);+  grpc_composite_channel_credentials* channel_creds =+      reinterpret_cast<grpc_composite_channel_credentials*>(+          grpc_google_default_credentials_create(call_creds));+  GPR_ASSERT(g_test_gce_tenancy_checker_called == true);+  GPR_ASSERT(channel_creds != nullptr);+  GPR_ASSERT(channel_creds->call_creds() != nullptr);+  grpc_httpcli_set_override(compute_engine_httpcli_get_success_override,+                            httpcli_post_should_not_be_called);+  run_request_metadata_test(channel_creds->mutable_call_creds(), auth_md_ctx,+                            state);+  grpc_core::ExecCtx::Get()->Flush();+  channel_creds->Unref();+  grpc_httpcli_set_override(nullptr, nullptr);+}++struct fake_call_creds : public grpc_call_credentials {+ public:+  explicit fake_call_creds() : grpc_call_credentials(""fake"") {+    grpc_slice key = grpc_slice_from_static_string(""foo"");+    grpc_slice value = grpc_slice_from_static_string(""oof"");+    dummy_md_ = grpc_mdelem_from_slices(key, value);+    grpc_slice_unref(key);+    grpc_slice_unref(value);+  }++  ~fake_call_creds() { GRPC_MDELEM_UNREF(dummy_md_); }++  bool get_request_metadata(grpc_polling_entity* pollent,+                            grpc_auth_metadata_context context,+                            grpc_credentials_mdelem_array* md_array,+                            grpc_closure* on_request_metadata,+                            grpc_error** error) {+    grpc_credentials_mdelem_array_add(md_array, dummy_md_);+    return true;+  }++  void cancel_get_request_metadata(grpc_credentials_mdelem_array* md_array,+                                   grpc_error* error) {}++ private:+  grpc_mdelem dummy_md_;+};++static void test_google_default_creds_not_default(void) {+  expected_md emd[] = {{""foo"", ""oof""}};+  request_metadata_state* state =+      make_request_metadata_state(GRPC_ERROR_NONE, emd, GPR_ARRAY_SIZE(emd));+  grpc_auth_metadata_context auth_md_ctx = {test_service_url, test_method,+                                            nullptr, nullptr};+  grpc_core::ExecCtx exec_ctx;+  grpc_flush_cached_google_default_credentials();+  grpc_core::RefCountedPtr<grpc_call_credentials> call_creds =+      grpc_core::MakeRefCounted<fake_call_creds>();+  set_gce_tenancy_checker_for_testing(test_gce_tenancy_checker);+  g_test_gce_tenancy_checker_called = false;+  g_test_is_on_gce = true;+  grpc_httpcli_set_override(+      default_creds_metadata_server_detection_httpcli_get_success_override,+      httpcli_post_should_not_be_called);+  grpc_composite_channel_credentials* channel_creds =+      reinterpret_cast<grpc_composite_channel_credentials*>(+          grpc_google_default_credentials_create(call_creds.release()));+  GPR_ASSERT(g_test_gce_tenancy_checker_called == true);+  GPR_ASSERT(channel_creds != nullptr);+  GPR_ASSERT(channel_creds->call_creds() != nullptr);+  run_request_metadata_test(channel_creds->mutable_call_creds(), auth_md_ctx,+                            state);","This test doesn't actually ever send an RPC. It just runs the metadata call credentials code path, so we can't make such an expectation. Setting up a test that actually sent an RPC like this would require a hermetic server that accepts a Google default credentials, which AFAIK, has never been done.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23203,454003963,2020-07-13T23:28:37Z,src/python/grpcio_tests/tests/interop/client.py,"@@ -89,8 +101,27 @@ def _create_call_credentials(args): def get_secure_channel_parameters(args):     call_credentials = _create_call_credentials(args) -    channel_opts = None-    if args.use_tls:+    channel_opts = ()+    if args.grpc_test_use_grpclb_with_child_policy:+        channel_opts += ((+            ""grpc.service_config"",+            '{""loadBalancingConfig"": [{""grpclb"": {""childPolicy"": [{""%s"": {}}]}}]}'+            % args.grpc_test_use_grpclb_with_child_policy),)+    if args.custom_credentials_type is not None:+        if args.custom_credentials_type == ""compute_engine_channel_creds"":+            assert call_credentials is None+            google_credentials, unused_project_id = google_auth.default(+                scopes=[args.oauth_scope])+            call_creds = grpc.metadata_call_credentials(+                google_auth.transport.grpc.AuthMetadataPlugin(+                    credentials=google_credentials,+                    request=google_auth.transport.requests.Request()))+            channel_credentials = grpc.compute_engine_channel_credentials(+                call_creds)","The only intended users of this are going to be the cloud client library authors, who we'll be working with directly. I don't think it's desirable to advertise this in general, since it's only meant to be consumed by higher-level auth libraries. If we add an example, it should show how to enable directpath using the google auth library, but that won't be possible until after they've implemented their part.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23203,454017760,2020-07-14T00:03:22Z,test/core/security/credentials_test.cc,"@@ -1512,16 +1512,105 @@ static void test_no_google_default_creds(void) {       default_creds_gce_detection_httpcli_get_failure_override,       httpcli_post_should_not_be_called);   /* Simulate a successful detection of GCE. */-  GPR_ASSERT(grpc_google_default_credentials_create() == nullptr);+  GPR_ASSERT(grpc_google_default_credentials_create(nullptr) == nullptr);   /* Try a second one. GCE detection should occur again. */   g_test_gce_tenancy_checker_called = false;-  GPR_ASSERT(grpc_google_default_credentials_create() == nullptr);+  GPR_ASSERT(grpc_google_default_credentials_create(nullptr) == nullptr);   GPR_ASSERT(g_test_gce_tenancy_checker_called == true);   /* Cleanup. */   grpc_override_well_known_credentials_path_getter(nullptr);   grpc_httpcli_set_override(nullptr, nullptr); } +static void test_google_default_creds_call_creds_specified(void) {+  expected_md emd[] = {+      {""authorization"", ""Bearer ya29.AHES6ZRN3-HlhAPya30GnW_bHSb_""}};+  request_metadata_state* state =+      make_request_metadata_state(GRPC_ERROR_NONE, emd, GPR_ARRAY_SIZE(emd));+  grpc_auth_metadata_context auth_md_ctx = {test_service_url, test_method,+                                            nullptr, nullptr};+  grpc_core::ExecCtx exec_ctx;+  grpc_flush_cached_google_default_credentials();+  grpc_call_credentials* call_creds =+      grpc_google_compute_engine_credentials_create(nullptr);+  set_gce_tenancy_checker_for_testing(test_gce_tenancy_checker);+  g_test_gce_tenancy_checker_called = false;+  g_test_is_on_gce = true;+  grpc_httpcli_set_override(+      default_creds_metadata_server_detection_httpcli_get_success_override,+      httpcli_post_should_not_be_called);+  grpc_composite_channel_credentials* channel_creds =+      reinterpret_cast<grpc_composite_channel_credentials*>(+          grpc_google_default_credentials_create(call_creds));+  GPR_ASSERT(g_test_gce_tenancy_checker_called == true);+  GPR_ASSERT(channel_creds != nullptr);+  GPR_ASSERT(channel_creds->call_creds() != nullptr);+  grpc_httpcli_set_override(compute_engine_httpcli_get_success_override,+                            httpcli_post_should_not_be_called);+  run_request_metadata_test(channel_creds->mutable_call_creds(), auth_md_ctx,+                            state);+  grpc_core::ExecCtx::Get()->Flush();+  channel_creds->Unref();+  grpc_httpcli_set_override(nullptr, nullptr);+}++struct fake_call_creds : public grpc_call_credentials {+ public:+  explicit fake_call_creds() : grpc_call_credentials(""fake"") {+    grpc_slice key = grpc_slice_from_static_string(""foo"");+    grpc_slice value = grpc_slice_from_static_string(""oof"");+    dummy_md_ = grpc_mdelem_from_slices(key, value);+    grpc_slice_unref(key);+    grpc_slice_unref(value);+  }++  ~fake_call_creds() { GRPC_MDELEM_UNREF(dummy_md_); }++  bool get_request_metadata(grpc_polling_entity* pollent,+                            grpc_auth_metadata_context context,+                            grpc_credentials_mdelem_array* md_array,+                            grpc_closure* on_request_metadata,+                            grpc_error** error) {+    grpc_credentials_mdelem_array_add(md_array, dummy_md_);+    return true;+  }++  void cancel_get_request_metadata(grpc_credentials_mdelem_array* md_array,+                                   grpc_error* error) {}++ private:+  grpc_mdelem dummy_md_;+};++static void test_google_default_creds_not_default(void) {+  expected_md emd[] = {{""foo"", ""oof""}};+  request_metadata_state* state =+      make_request_metadata_state(GRPC_ERROR_NONE, emd, GPR_ARRAY_SIZE(emd));+  grpc_auth_metadata_context auth_md_ctx = {test_service_url, test_method,+                                            nullptr, nullptr};+  grpc_core::ExecCtx exec_ctx;+  grpc_flush_cached_google_default_credentials();+  grpc_core::RefCountedPtr<grpc_call_credentials> call_creds =+      grpc_core::MakeRefCounted<fake_call_creds>();+  set_gce_tenancy_checker_for_testing(test_gce_tenancy_checker);+  g_test_gce_tenancy_checker_called = false;+  g_test_is_on_gce = true;+  grpc_httpcli_set_override(+      default_creds_metadata_server_detection_httpcli_get_success_override,+      httpcli_post_should_not_be_called);+  grpc_composite_channel_credentials* channel_creds =+      reinterpret_cast<grpc_composite_channel_credentials*>(+          grpc_google_default_credentials_create(call_creds.release()));+  GPR_ASSERT(g_test_gce_tenancy_checker_called == true);+  GPR_ASSERT(channel_creds != nullptr);+  GPR_ASSERT(channel_creds->call_creds() != nullptr);+  run_request_metadata_test(channel_creds->mutable_call_creds(), auth_md_ctx,+                            state);","> If used with any other sort of call credential, the connection may suddenly and unexpectedly begin failing RPCs.`I wanted to do a sanity check here, might be wrong. This tests seems not failing the call credentials and still invokes it (metadata changed). Is the statement above tested?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23203,454021755,2020-07-14T00:17:17Z,test/core/security/credentials_test.cc,"@@ -1512,16 +1512,105 @@ static void test_no_google_default_creds(void) {       default_creds_gce_detection_httpcli_get_failure_override,       httpcli_post_should_not_be_called);   /* Simulate a successful detection of GCE. */-  GPR_ASSERT(grpc_google_default_credentials_create() == nullptr);+  GPR_ASSERT(grpc_google_default_credentials_create(nullptr) == nullptr);   /* Try a second one. GCE detection should occur again. */   g_test_gce_tenancy_checker_called = false;-  GPR_ASSERT(grpc_google_default_credentials_create() == nullptr);+  GPR_ASSERT(grpc_google_default_credentials_create(nullptr) == nullptr);   GPR_ASSERT(g_test_gce_tenancy_checker_called == true);   /* Cleanup. */   grpc_override_well_known_credentials_path_getter(nullptr);   grpc_httpcli_set_override(nullptr, nullptr); } +static void test_google_default_creds_call_creds_specified(void) {+  expected_md emd[] = {+      {""authorization"", ""Bearer ya29.AHES6ZRN3-HlhAPya30GnW_bHSb_""}};+  request_metadata_state* state =+      make_request_metadata_state(GRPC_ERROR_NONE, emd, GPR_ARRAY_SIZE(emd));+  grpc_auth_metadata_context auth_md_ctx = {test_service_url, test_method,+                                            nullptr, nullptr};+  grpc_core::ExecCtx exec_ctx;+  grpc_flush_cached_google_default_credentials();+  grpc_call_credentials* call_creds =+      grpc_google_compute_engine_credentials_create(nullptr);+  set_gce_tenancy_checker_for_testing(test_gce_tenancy_checker);+  g_test_gce_tenancy_checker_called = false;+  g_test_is_on_gce = true;+  grpc_httpcli_set_override(+      default_creds_metadata_server_detection_httpcli_get_success_override,+      httpcli_post_should_not_be_called);+  grpc_composite_channel_credentials* channel_creds =+      reinterpret_cast<grpc_composite_channel_credentials*>(+          grpc_google_default_credentials_create(call_creds));+  GPR_ASSERT(g_test_gce_tenancy_checker_called == true);+  GPR_ASSERT(channel_creds != nullptr);+  GPR_ASSERT(channel_creds->call_creds() != nullptr);+  grpc_httpcli_set_override(compute_engine_httpcli_get_success_override,+                            httpcli_post_should_not_be_called);+  run_request_metadata_test(channel_creds->mutable_call_creds(), auth_md_ctx,+                            state);+  grpc_core::ExecCtx::Get()->Flush();+  channel_creds->Unref();+  grpc_httpcli_set_override(nullptr, nullptr);+}++struct fake_call_creds : public grpc_call_credentials {+ public:+  explicit fake_call_creds() : grpc_call_credentials(""fake"") {+    grpc_slice key = grpc_slice_from_static_string(""foo"");+    grpc_slice value = grpc_slice_from_static_string(""oof"");+    dummy_md_ = grpc_mdelem_from_slices(key, value);+    grpc_slice_unref(key);+    grpc_slice_unref(value);+  }++  ~fake_call_creds() { GRPC_MDELEM_UNREF(dummy_md_); }++  bool get_request_metadata(grpc_polling_entity* pollent,+                            grpc_auth_metadata_context context,+                            grpc_credentials_mdelem_array* md_array,+                            grpc_closure* on_request_metadata,+                            grpc_error** error) {+    grpc_credentials_mdelem_array_add(md_array, dummy_md_);+    return true;+  }++  void cancel_get_request_metadata(grpc_credentials_mdelem_array* md_array,+                                   grpc_error* error) {}++ private:+  grpc_mdelem dummy_md_;+};++static void test_google_default_creds_not_default(void) {+  expected_md emd[] = {{""foo"", ""oof""}};+  request_metadata_state* state =+      make_request_metadata_state(GRPC_ERROR_NONE, emd, GPR_ARRAY_SIZE(emd));+  grpc_auth_metadata_context auth_md_ctx = {test_service_url, test_method,+                                            nullptr, nullptr};+  grpc_core::ExecCtx exec_ctx;+  grpc_flush_cached_google_default_credentials();+  grpc_core::RefCountedPtr<grpc_call_credentials> call_creds =+      grpc_core::MakeRefCounted<fake_call_creds>();+  set_gce_tenancy_checker_for_testing(test_gce_tenancy_checker);+  g_test_gce_tenancy_checker_called = false;+  g_test_is_on_gce = true;+  grpc_httpcli_set_override(+      default_creds_metadata_server_detection_httpcli_get_success_override,+      httpcli_post_should_not_be_called);+  grpc_composite_channel_credentials* channel_creds =+      reinterpret_cast<grpc_composite_channel_credentials*>(+          grpc_google_default_credentials_create(call_creds.release()));+  GPR_ASSERT(g_test_gce_tenancy_checker_called == true);+  GPR_ASSERT(channel_creds != nullptr);+  GPR_ASSERT(channel_creds->call_creds() != nullptr);+  run_request_metadata_test(channel_creds->mutable_call_creds(), auth_md_ctx,+                            state);","This is not a statement of expected behavior of the function, but rather, of explicitly *undefined behavior*. The key word is ""_may_"". We would much rather reject a call credential that does not implement the Application Default Credentials mechanism; the problem is that this mechanism is defined by a convention, rather than by any C++ or Python type.If we took a dependency on the auth library instead of the other way around, then we could identify their implementation of the Application Default Credentials mechanism and allow it and only it, failing any other type of call credential. Unfortunately, since the dependency is in the other direction, we have no way of knowing *which* call credential that might be. So, instead, we opted to add this warning to the docstring for the Core API. ",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23396,454030252,2020-07-14T00:44:30Z,test/cpp/util/channelz_sampler_test.cc,"@@ -0,0 +1,172 @@+/*+ *+ * Copyright 2016 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <stdlib.h>+#include <unistd.h>+#include <cstdlib>+#include <iostream>+#include <memory>+#include <string>+#include <thread>++#include ""include/grpcpp/ext/proto_server_reflection_plugin_impl.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/testing/test.grpc.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/subprocess.h""+#include ""test/cpp/util/test_credentials_provider.h""++using grpc::Channel;+using grpc::ClientContext;+using grpc::Server;+using grpc::ServerBuilder;+using grpc::ServerContext;+using grpc::Status;++#include <gtest/gtest.h>++// Test variables+std::string server_address(""0.0.0.0:10000"");+std::string custom_credentials_type(""INSECURE_CREDENTIALS"");+std::string sampling_times = ""2"";+std::string sampling_interval_seconds = ""5"";+std::string output_json(""./output.json"");++// Creata an echo server - randomly delay 0.1 to 0.2 s+class EchoServerImpl final : public grpc::testing::TestService::Service {+  Status EmptyCall(::grpc::ServerContext* context,+                   const grpc::testing::Empty* request,+                   grpc::testing::Empty* response) {+    srand(unsigned(time(0)));+    unsigned int server_delay_microseconds = 100000;+    server_delay_microseconds += rand() % server_delay_microseconds;+    usleep(server_delay_microseconds);+    return Status::OK;+  }+};++// Run server in a thread+void RunServer() {+  // register channelz service+  ::grpc::channelz::experimental::InitChannelzService();++  EchoServerImpl service;+  grpc::EnableDefaultHealthCheckService(true);+  grpc_impl::reflection::InitProtoReflectionServerBuilderPlugin();+  grpc::ServerBuilder builder;+  auto server_creds =+      grpc::testing::GetCredentialsProvider()->GetServerCredentials(+          custom_credentials_type);+  builder.AddListeningPort(server_address, server_creds);++  // forces channelz and channel tracing to be enabled.+  builder.AddChannelArgument(GRPC_ARG_ENABLE_CHANNELZ, 1);+  builder.AddChannelArgument(GRPC_ARG_MAX_CHANNEL_TRACE_EVENT_MEMORY_PER_NODE,+                             1024);+  builder.RegisterService(&service);+  std::unique_ptr<Server> server(builder.BuildAndStart());+  gpr_log(GPR_INFO, ""Server listening on %s"", server_address.c_str());+  server->Wait();+}++// Creata an echo client - set timeout as 0.15s+class EchoClientImpl {+ public:+  EchoClientImpl(std::shared_ptr<Channel> channel)+      : stub_(grpc::testing::TestService::NewStub(channel)) {}+  Status EmptyCall() {+    grpc::testing::Empty request;+    grpc::testing::Empty response;+    ClientContext context;+    int64_t timeout_microseconds = 150;+    context.set_deadline(+        grpc_timeout_milliseconds_to_deadline(timeout_microseconds));+    Status status = stub_->EmptyCall(&context, request, &response);+    return status;+  }++ private:+  std::unique_ptr<grpc::testing::TestService::Stub> stub_;+};++// Run client in a thread+void RunClient(std::string client_id) {+  // std::string target_str = ""localhost:10000"";+  grpc::ChannelArguments channel_args;+  std::shared_ptr<grpc::ChannelCredentials> channel_creds =+      grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+          custom_credentials_type, &channel_args);+  EchoClientImpl echoer(grpc::CreateChannel(server_address, channel_creds));+  unsigned int client_echo_sleep_second = 1;++  gpr_log(GPR_INFO, ""Client %s is echoing!"", client_id.c_str());+  while (true) {+    Status status = echoer.EmptyCall();+    sleep(client_echo_sleep_second);+  }+}++// Test the channelz sampler+TEST(ChannelzSamplerTest, SimpleTest) {+  // server thread+  std::thread server_thread(RunServer);+  gpr_log(GPR_INFO, ""Wait 3 seconds to make sure server is started..."");+  float server_start_seconds = 3.0;+  sleep(server_start_seconds);","instead of sleeping for 3 seconds, can we create a channel that tries to connect to the server, and returns when it does connect, using the `WaitForConnected` channel API?E.g., https://github.com/grpc/grpc/blob/master/test/cpp/end2end/grpclb_end2end_test.cc#L959",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23396,454030666,2020-07-14T00:45:47Z,test/cpp/util/channelz_sampler_test.cc,"@@ -0,0 +1,172 @@+/*+ *+ * Copyright 2016 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <stdlib.h>+#include <unistd.h>+#include <cstdlib>+#include <iostream>+#include <memory>+#include <string>+#include <thread>++#include ""include/grpcpp/ext/proto_server_reflection_plugin_impl.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/testing/test.grpc.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/subprocess.h""+#include ""test/cpp/util/test_credentials_provider.h""++using grpc::Channel;+using grpc::ClientContext;+using grpc::Server;+using grpc::ServerBuilder;+using grpc::ServerContext;+using grpc::Status;++#include <gtest/gtest.h>++// Test variables+std::string server_address(""0.0.0.0:10000"");+std::string custom_credentials_type(""INSECURE_CREDENTIALS"");+std::string sampling_times = ""2"";+std::string sampling_interval_seconds = ""5"";+std::string output_json(""./output.json"");++// Creata an echo server - randomly delay 0.1 to 0.2 s+class EchoServerImpl final : public grpc::testing::TestService::Service {+  Status EmptyCall(::grpc::ServerContext* context,+                   const grpc::testing::Empty* request,+                   grpc::testing::Empty* response) {+    srand(unsigned(time(0)));+    unsigned int server_delay_microseconds = 100000;+    server_delay_microseconds += rand() % server_delay_microseconds;+    usleep(server_delay_microseconds);+    return Status::OK;+  }+};++// Run server in a thread+void RunServer() {+  // register channelz service+  ::grpc::channelz::experimental::InitChannelzService();++  EchoServerImpl service;+  grpc::EnableDefaultHealthCheckService(true);+  grpc_impl::reflection::InitProtoReflectionServerBuilderPlugin();+  grpc::ServerBuilder builder;+  auto server_creds =+      grpc::testing::GetCredentialsProvider()->GetServerCredentials(+          custom_credentials_type);+  builder.AddListeningPort(server_address, server_creds);++  // forces channelz and channel tracing to be enabled.+  builder.AddChannelArgument(GRPC_ARG_ENABLE_CHANNELZ, 1);+  builder.AddChannelArgument(GRPC_ARG_MAX_CHANNEL_TRACE_EVENT_MEMORY_PER_NODE,+                             1024);+  builder.RegisterService(&service);+  std::unique_ptr<Server> server(builder.BuildAndStart());+  gpr_log(GPR_INFO, ""Server listening on %s"", server_address.c_str());+  server->Wait();+}++// Creata an echo client - set timeout as 0.15s+class EchoClientImpl {+ public:+  EchoClientImpl(std::shared_ptr<Channel> channel)+      : stub_(grpc::testing::TestService::NewStub(channel)) {}+  Status EmptyCall() {+    grpc::testing::Empty request;+    grpc::testing::Empty response;+    ClientContext context;+    int64_t timeout_microseconds = 150;+    context.set_deadline(+        grpc_timeout_milliseconds_to_deadline(timeout_microseconds));+    Status status = stub_->EmptyCall(&context, request, &response);+    return status;+  }++ private:+  std::unique_ptr<grpc::testing::TestService::Stub> stub_;+};++// Run client in a thread+void RunClient(std::string client_id) {+  // std::string target_str = ""localhost:10000"";+  grpc::ChannelArguments channel_args;+  std::shared_ptr<grpc::ChannelCredentials> channel_creds =+      grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+          custom_credentials_type, &channel_args);+  EchoClientImpl echoer(grpc::CreateChannel(server_address, channel_creds));+  unsigned int client_echo_sleep_second = 1;++  gpr_log(GPR_INFO, ""Client %s is echoing!"", client_id.c_str());+  while (true) {",we'll need to use some signal to know when to terminate the loop upon cleanup/thread-joinSuggest using `gpr_event`. See e.g. https://github.com/grpc/grpc/blob/8febefc05784fe2e12a5933a96d373297ad0cfdf/test/cpp/naming/resolver_component_test.cc#L312,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23396,454031656,2020-07-14T00:48:56Z,test/cpp/util/channelz_sampler_test.cc,"@@ -0,0 +1,172 @@+/*+ *+ * Copyright 2016 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <stdlib.h>+#include <unistd.h>+#include <cstdlib>+#include <iostream>+#include <memory>+#include <string>+#include <thread>++#include ""include/grpcpp/ext/proto_server_reflection_plugin_impl.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/testing/test.grpc.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/subprocess.h""+#include ""test/cpp/util/test_credentials_provider.h""++using grpc::Channel;+using grpc::ClientContext;+using grpc::Server;+using grpc::ServerBuilder;+using grpc::ServerContext;+using grpc::Status;++#include <gtest/gtest.h>++// Test variables+std::string server_address(""0.0.0.0:10000"");+std::string custom_credentials_type(""INSECURE_CREDENTIALS"");+std::string sampling_times = ""2"";+std::string sampling_interval_seconds = ""5"";+std::string output_json(""./output.json"");++// Creata an echo server - randomly delay 0.1 to 0.2 s+class EchoServerImpl final : public grpc::testing::TestService::Service {+  Status EmptyCall(::grpc::ServerContext* context,+                   const grpc::testing::Empty* request,+                   grpc::testing::Empty* response) {+    srand(unsigned(time(0)));+    unsigned int server_delay_microseconds = 100000;+    server_delay_microseconds += rand() % server_delay_microseconds;+    usleep(server_delay_microseconds);+    return Status::OK;+  }+};++// Run server in a thread+void RunServer() {+  // register channelz service+  ::grpc::channelz::experimental::InitChannelzService();++  EchoServerImpl service;+  grpc::EnableDefaultHealthCheckService(true);+  grpc_impl::reflection::InitProtoReflectionServerBuilderPlugin();+  grpc::ServerBuilder builder;+  auto server_creds =+      grpc::testing::GetCredentialsProvider()->GetServerCredentials(+          custom_credentials_type);+  builder.AddListeningPort(server_address, server_creds);++  // forces channelz and channel tracing to be enabled.+  builder.AddChannelArgument(GRPC_ARG_ENABLE_CHANNELZ, 1);+  builder.AddChannelArgument(GRPC_ARG_MAX_CHANNEL_TRACE_EVENT_MEMORY_PER_NODE,+                             1024);+  builder.RegisterService(&service);+  std::unique_ptr<Server> server(builder.BuildAndStart());+  gpr_log(GPR_INFO, ""Server listening on %s"", server_address.c_str());+  server->Wait();+}++// Creata an echo client - set timeout as 0.15s+class EchoClientImpl {","nit: I don't see a lot of advantage to this being a class, so I suggest removing this, and inlining the fields and logic of this class into the `RunClient` loop.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23455,454223120,2020-07-14T09:23:35Z,tools/run_tests/python_utils/port_server.py,"@@ -31,7 +31,7 @@ # increment this number whenever making a change to ensure that # the changes are picked up by running CI servers # note that all changes must be backwards compatible-_MY_VERSION = 20+_MY_VERSION = 21","There's no sanity test for this and also it's mostly a legacy thing - our kokoro jobs get a new VM each time a build runs, so they always end up with the freshest version of port server anyway (This used to be a big deal where our test VMs weren't ephemeral so we needed to make sure they have an up-to-date version of port server).Right now the only thing it does is restart port server on people's workstation (if they are still using run_tests.py - which we are slowly trying to get rid of), so I'll leave as is.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23455,454224800,2020-07-14T09:26:26Z,tools/run_tests/python_utils/start_port_server.py,"@@ -73,12 +73,15 @@ def start_port_server():             # Working directory of port server needs to be outside of Jenkins             # workspace to prevent file lock issues.             tempdir = tempfile.mkdtemp()-            port_server = subprocess.Popen(-                args,-                env=env,-                cwd=tempdir,-                creationflags=0x00000008,  # detached process-                close_fds=True)+            if sys.version_info.major == 2:+                creationflags = 0x00000008  # detached process+            else:+                creationflags = 0  # DETACHED_PROCESS doesn't seem to work with python3","Yes, I tried that and it didn't work either (not sure why, but it tried several times and whenever I used it, the port server process just failed to start).I think the detached_process options was more important in the past (when we were using jenkins and long-lived worker VMs), because we needed port server to survive the CI build - this is not the case anymore.I tried the port server locally and it worked, so I'm not too afraid to leave as is.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23464,454679220,2020-07-14T22:21:46Z,test/cpp/interop/xds_interop_client.cc,"@@ -123,7 +142,9 @@ class TestClient {   TestClient(const std::shared_ptr<Channel>& channel)       : stub_(TestService::NewStub(channel)) {} -  void AsyncUnaryCall() {+  void AsyncUnaryCall(+      absl::optional<std::vector<std::pair<std::string, std::string>>>",Why use `absl::optional<>` here?  Why not just pass in an empty map if there's no metadata?,
303201,JamesNK,https://api.github.com/repos/grpc/grpc/pulls/23485,454960942,2020-07-15T10:46:09Z,src/csharp/Grpc.Examples/MathGrpc.cs,"@@ -27,10 +27,33 @@ public static partial class Math   {     static readonly string __ServiceName = ""math.Math""; -    static readonly grpc::Marshaller<global::Math.DivArgs> __Marshaller_math_DivArgs = grpc::Marshallers.Create((arg) => global::Google.Protobuf.MessageExtensions.ToByteArray(arg), global::Math.DivArgs.Parser.ParseFrom);-    static readonly grpc::Marshaller<global::Math.DivReply> __Marshaller_math_DivReply = grpc::Marshallers.Create((arg) => global::Google.Protobuf.MessageExtensions.ToByteArray(arg), global::Math.DivReply.Parser.ParseFrom);-    static readonly grpc::Marshaller<global::Math.FibArgs> __Marshaller_math_FibArgs = grpc::Marshallers.Create((arg) => global::Google.Protobuf.MessageExtensions.ToByteArray(arg), global::Math.FibArgs.Parser.ParseFrom);-    static readonly grpc::Marshaller<global::Math.Num> __Marshaller_math_Num = grpc::Marshallers.Create((arg) => global::Google.Protobuf.MessageExtensions.ToByteArray(arg), global::Math.Num.Parser.ParseFrom);+    static void __Helper_SerializeMessage(global::Google.Protobuf.IMessage message, grpc::SerializationContext context)+    {+      #if !GRPC_DISABLE_PROTOBUF_BUFFER_SERIALIZATION+      if (message is global::Google.Protobuf.IBufferMessage)+      {+        context.SetPayloadLength(message.CalculateSize());+        global::Google.Protobuf.MessageExtensions.WriteTo(message, context.GetBufferWriter());+        context.Complete();+        return;+      }+      #endif+      context.Complete(global::Google.Protobuf.MessageExtensions.ToByteArray(message));+    }++    static T __Helper_DeserializeMessage<T>(grpc::DeserializationContext context, global::Google.Protobuf.MessageParser<T> parser) where T : global::Google.Protobuf.IMessage<T>+    {+      #if !GRPC_DISABLE_PROTOBUF_BUFFER_SERIALIZATION+      return parser.ParseFrom(context.PayloadAsReadOnlySequence());","I think you could cache the result of `typeof(global::Google.Protobuf.IBufferMessage).IsAssignableFrom(typeof(T))` in a generic static type.```csprivate static class BufferMessageHelper<T>{    public static readonly bool IsBufferMessage = typeof(global::Google.Protobuf.IBufferMessage).IsAssignableFrom(typeof(T));}static T __Helper_DeserializeMessage<T>(grpc::DeserializationContext context, global::Google.Protobuf.MessageParser<T> parser) where T : global::Google.Protobuf.IMessage<T>{    if (BufferMessageHelper<T>.IsBufferMessage)    {        // ...    }}```",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/23441,455279645,2020-07-15T19:08:51Z,include/grpcpp/security/alts_context.h,"@@ -59,6 +61,7 @@ class AltsContext {   std::string local_service_account_;   grpc_security_level security_level_ = GRPC_SECURITY_NONE;   RpcProtocolVersions peer_rpc_versions_ = {{0, 0}, {0, 0}};+  std::map<std::string, std::string> peer_attributes_map;",nit: rename this to `peer_attributes_map_`. We use a suffix `_` to indicate it is a class member.,
303201,JamesNK,https://api.github.com/repos/grpc/grpc/pulls/23485,455368585,2020-07-15T21:35:22Z,src/compiler/csharp_generator.cc,"@@ -325,13 +325,76 @@ std::vector<const Descriptor*> GetUsedMessages(  void GenerateMarshallerFields(Printer* out, const ServiceDescriptor* service) {   std::vector<const Descriptor*> used_messages = GetUsedMessages(service);+  if (used_messages.size() != 0) {+    // Generate static helper methods for serialization/deserialization+    out->Print(+        ""static void __Helper_SerializeMessage(""+        ""global::Google.Protobuf.IMessage message, ""+        ""grpc::SerializationContext context)\n""+        ""{\n"");+    out->Indent();+    out->Print(+        ""#if !GRPC_DISABLE_PROTOBUF_BUFFER_SERIALIZATION\n""+        ""if (message is global::Google.Protobuf.IBufferMessage)\n""+        ""{\n"");+    out->Indent();+    out->Print(+        ""context.SetPayloadLength(message.CalculateSize());\n""+        ""global::Google.Protobuf.MessageExtensions.WriteTo(message, ""+        ""context.GetBufferWriter());\n""+        ""context.Complete();\n""+        ""return;\n"");+    out->Outdent();+    out->Print(+        ""}\n""+        ""#endif\n"");+    out->Print(+        ""context.Complete(""+        ""global::Google.Protobuf.MessageExtensions.ToByteArray(message));\n"");+    out->Outdent();+    out->Print(""}\n\n"");++    out->Print(+        ""static class __BufferMessageHelper<T>\n""+        ""{\n"");+    out->Indent();+    out->Print(+        ""public static readonly bool __IsBufferMessage = ""+        ""global::System.Reflection.IntrospectionExtensions.GetTypeInfo(typeof(""+        ""global::Google.Protobuf.IBufferMessage)).IsAssignableFrom(typeof(T));""+        ""\n"");+    out->Outdent();+    out->Print(""}\n\n"");++    out->Print(+        ""static T __Helper_DeserializeMessage<T>(""+        ""grpc::DeserializationContext context, ""+        ""global::Google.Protobuf.MessageParser<T> parser) ""+        ""where T : global::Google.Protobuf.IMessage<T>\n""+        ""{\n"");+    out->Indent();+    out->Print(+        ""#if !GRPC_DISABLE_PROTOBUF_BUFFER_SERIALIZATION\n""+        ""if (__BufferMessageHelper<T>.__IsBufferMessage)\n""",nit: `__Helper_MessageCache<T>.IsBufferMessage`To be consistent with the other helper members. I remember we picked the `__Helper_` prefix because there was no way it could conflict with any generated code.Also I don't think you need underscores with `__IsBufferMessage`. I don't see how a property on a private class could conflict with anything.,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23484,456020159,2020-07-16T19:24:37Z,test/cpp/util/channelz_sampler_test.cc,"@@ -0,0 +1,181 @@+/*+ *+ * Copyright 2016 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <stdlib.h>+#include <unistd.h>+#include <cstdlib>+#include <iostream>+#include <memory>+#include <string>+#include <thread>++#include ""include/grpcpp/ext/proto_server_reflection_plugin_impl.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/testing/test.grpc.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/subprocess.h""+#include ""test/cpp/util/test_credentials_provider.h""++using grpc::Channel;+using grpc::ClientContext;+using grpc::Server;+using grpc::ServerBuilder;+using grpc::ServerContext;+using grpc::Status;++#include <gtest/gtest.h>++// Test variables+std::string server_address(""0.0.0.0:10000"");+std::string custom_credentials_type(""INSECURE_CREDENTIALS"");+std::string sampling_times = ""2"";+std::string sampling_interval_seconds = ""5"";+std::string output_json(""./output.json"");++// Creata an echo server - randomly delay 0.1 to 0.2 s+class EchoServerImpl final : public grpc::testing::TestService::Service {+  Status EmptyCall(::grpc::ServerContext* context,+                   const grpc::testing::Empty* request,+                   grpc::testing::Empty* response) {+    srand(unsigned(time(0)));+    unsigned int server_delay_microseconds = 100000;+    server_delay_microseconds += rand() % server_delay_microseconds;+    usleep(server_delay_microseconds);+    return Status::OK;+  }+};++// Run server in a thread+void RunServer(gpr_event* done_ev) {+  // register channelz service+  ::grpc::channelz::experimental::InitChannelzService();++  EchoServerImpl service;+  grpc::EnableDefaultHealthCheckService(true);+  grpc_impl::reflection::InitProtoReflectionServerBuilderPlugin();+  grpc::ServerBuilder builder;+  auto server_creds =+      grpc::testing::GetCredentialsProvider()->GetServerCredentials(+          custom_credentials_type);+  builder.AddListeningPort(server_address, server_creds);++  // forces channelz and channel tracing to be enabled.+  builder.AddChannelArgument(GRPC_ARG_ENABLE_CHANNELZ, 1);+  builder.AddChannelArgument(GRPC_ARG_MAX_CHANNEL_TRACE_EVENT_MEMORY_PER_NODE,+                             1024);+  builder.RegisterService(&service);+  std::unique_ptr<Server> server(builder.BuildAndStart());+  gpr_log(GPR_INFO, ""Server listening on %s"", server_address.c_str());+  while (true) {+    if (gpr_event_get(done_ev)) {+      return;+    }+  }+}++// Run client in a thread - - set timeout as 0.15s+void RunClient(std::string client_id, gpr_event* done_ev) {+  grpc::ChannelArguments channel_args;+  std::shared_ptr<grpc::ChannelCredentials> channel_creds =+      grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+          custom_credentials_type, &channel_args);+  std::unique_ptr<grpc::testing::TestService::Stub> stub =+      grpc::testing::TestService::NewStub(+          grpc::CreateChannel(server_address, channel_creds));+  unsigned int client_echo_sleep_second = 1;++  gpr_log(GPR_INFO, ""Client %s is echoing!"", client_id.c_str());+  while (true) {+    if (gpr_event_get(done_ev)) {+      return;+    }+    sleep(client_echo_sleep_second);+    // Rcho RPC+    grpc::testing::Empty request;+    grpc::testing::Empty response;+    ClientContext context;+    int64_t timeout_microseconds = 150;+    context.set_deadline(+        grpc_timeout_milliseconds_to_deadline(timeout_microseconds));+    stub->EmptyCall(&context, request, &response);+  }+}++// Create the channelz to test the connection to the server+void WaitForConnection() {+  grpc::ChannelArguments channel_args;+  std::shared_ptr<grpc::ChannelCredentials> channel_creds =+      grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+          custom_credentials_type, &channel_args);+  auto channel = grpc::CreateChannel(server_address, channel_creds);+  channel->WaitForConnected(grpc_timeout_seconds_to_deadline(3));","we should increase this timeout to a value after which we will just fail the test if surpassed, and then change this code to not silently continue in case the deadline is reached and the server isn't up",
67390330,mohanli-ml,https://api.github.com/repos/grpc/grpc/pulls/23484,456094166,2020-07-16T21:41:19Z,test/cpp/util/channelz_sampler_test.cc,"@@ -0,0 +1,181 @@+/*+ *+ * Copyright 2016 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <stdlib.h>+#include <unistd.h>+#include <cstdlib>+#include <iostream>+#include <memory>+#include <string>+#include <thread>++#include ""include/grpcpp/ext/proto_server_reflection_plugin_impl.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/testing/test.grpc.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/subprocess.h""+#include ""test/cpp/util/test_credentials_provider.h""++using grpc::Channel;+using grpc::ClientContext;+using grpc::Server;+using grpc::ServerBuilder;+using grpc::ServerContext;+using grpc::Status;++#include <gtest/gtest.h>++// Test variables+std::string server_address(""0.0.0.0:10000"");+std::string custom_credentials_type(""INSECURE_CREDENTIALS"");+std::string sampling_times = ""2"";+std::string sampling_interval_seconds = ""5"";+std::string output_json(""./output.json"");++// Creata an echo server - randomly delay 0.1 to 0.2 s+class EchoServerImpl final : public grpc::testing::TestService::Service {+  Status EmptyCall(::grpc::ServerContext* context,+                   const grpc::testing::Empty* request,+                   grpc::testing::Empty* response) {+    srand(unsigned(time(0)));+    unsigned int server_delay_microseconds = 100000;+    server_delay_microseconds += rand() % server_delay_microseconds;+    usleep(server_delay_microseconds);+    return Status::OK;+  }+};++// Run server in a thread+void RunServer(gpr_event* done_ev) {","It seems we need to do so. Say if we run the server in the main thread, then the main thread have to keep listening (either use `server->Wait()` or `While(true)`), and we could not start the client thread. What do you think?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23528,457065613,2020-07-20T05:31:23Z,src/boringssl/gen_build_yaml.py,"@@ -14,20 +14,20 @@ # limitations under the License.  from __future__ import print_function+from collections import OrderedDict++import json import shutil import sys import os import yaml -sys.dont_write_bytecode = True--boring_ssl_root = os.path.abspath(-    os.path.join(os.path.dirname(sys.argv[0]),-                 '../../third_party/boringssl-with-bazel/src'))-sys.path.append(os.path.join(boring_ssl_root, 'util'))- try:-    import generate_build_files+    sources_path = os.path.abspath(","Why is this being changed?I don't like that these changes are hiding under the disguise of just upgrading boringssl version.Please explain why these changes were necessary and what's the design behind them.This really seems the be the most significant change of this PR so I'd actually prefer this:1. perform a routing boringssl update according to our instructions (https://github.com/grpc/grpc/tree/master/third_party#updating-third_partyboringssl-with-bazel)2. change the way the boringssl sources are extracted, but do that in a small separate PR so that it's clear what's being done and why. ",
17869838,emkornfield,https://api.github.com/repos/grpc/grpc/pulls/23528,457079172,2020-07-20T05:59:48Z,src/boringssl/gen_build_yaml.py,"@@ -14,20 +14,20 @@ # limitations under the License.  from __future__ import print_function+from collections import OrderedDict++import json import shutil import sys import os import yaml -sys.dont_write_bytecode = True--boring_ssl_root = os.path.abspath(-    os.path.join(os.path.dirname(sys.argv[0]),-                 '../../third_party/boringssl-with-bazel/src'))-sys.path.append(os.path.join(boring_ssl_root, 'util'))- try:-    import generate_build_files+    sources_path = os.path.abspath(","To give you context, the path that led me here is:1.  python gRPC is being compiled without ASM optimizations (https://github.com/grpc/grpc/pull/23483)2.  the way this code currently hacks around the boringssl build system doesn't work for capturing ASM files.  When I attempted a small change to BoringSSL to capture the ASM files, the BoringSSL team [pushed](https://boringssl-review.googlesource.com/c/boringssl/+/42164) for cleaning up technical debt.I didn't intend to hide this change, I noted why I thought it might be worthwhile to include this in [a comment on this PR](https://github.com/grpc/grpc/pull/23528#issuecomment-660335200).  Is it OK if Include this change in the [PR to turn on ASM for python gRPC](https://github.com/grpc/grpc/pull/23483)?  Or would you prefer two distinct PRs (1  with this change and 1 for the python changes)?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23540,457090079,2020-07-20T06:22:07Z,tools/run_tests/artifacts/artifact_targets.py,"@@ -286,7 +286,7 @@ def pre_build_jobspecs(self):     def build_jobspec(self):         return create_docker_jobspec(             self.name,-            'tools/dockerfile/grpc_artifact_centos6_{}'.format(self.arch),+            'tools/dockerfile/test/php73_zts_stretch_{}'.format(self.arch),","Fair enough but note that other languages generally don't use their ""test"" images for building the artifacts.Mostly this is because the ""test"" docker images are kept on comparatively newer distros, which can cause trouble with portability of the binary artifacts they produce (if you build a library on a new linux distro, there might be backward compatibility issues when trying to run on older distros. That's why the umbrella ""artifact"" image is on such an old distro as centos6.But if for PHP specifically building on stretch is fine (distribtests would ideally tell you that), than I'm fine with that.",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/23540,457101116,2020-07-20T06:42:22Z,tools/run_tests/artifacts/artifact_targets.py,"@@ -286,7 +286,7 @@ def pre_build_jobspecs(self):     def build_jobspec(self):         return create_docker_jobspec(             self.name,-            'tools/dockerfile/grpc_artifact_centos6_{}'.format(self.arch),+            'tools/dockerfile/test/php73_zts_stretch_{}'.format(self.arch),","This is in some way necessary for PHP - the centos6 image only contains PHP 5.3.3 which is _way_ too old.We are currently working separately on deprecating PHP 5 - it will take some work to make sure we get rid of PHP 5 across these testing frameworks.So in the meantime, can we switch to this docker image for now? In the future (when we are working on deprecating PHP 5), I will look into switching to using `tools/dockerfile/grpc_artifact_linux_x64/Dockerfile`, but, changing that docker image from PHP 5 to PHP 7.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23485,457304594,2020-07-20T11:39:49Z,src/compiler/csharp_generator.cc,"@@ -325,13 +325,76 @@ std::vector<const Descriptor*> GetUsedMessages(  void GenerateMarshallerFields(Printer* out, const ServiceDescriptor* service) {   std::vector<const Descriptor*> used_messages = GetUsedMessages(service);+  if (used_messages.size() != 0) {+    // Generate static helper methods for serialization/deserialization+    out->Print(+        ""static void __Helper_SerializeMessage(""+        ""global::Google.Protobuf.IMessage message, ""+        ""grpc::SerializationContext context)\n""+        ""{\n"");+    out->Indent();+    out->Print(+        ""#if !GRPC_DISABLE_PROTOBUF_BUFFER_SERIALIZATION\n""+        ""if (message is global::Google.Protobuf.IBufferMessage)\n""+        ""{\n"");+    out->Indent();+    out->Print(+        ""context.SetPayloadLength(message.CalculateSize());\n""+        ""global::Google.Protobuf.MessageExtensions.WriteTo(message, ""+        ""context.GetBufferWriter());\n""+        ""context.Complete();\n""+        ""return;\n"");+    out->Outdent();+    out->Print(+        ""}\n""+        ""#endif\n"");+    out->Print(+        ""context.Complete(""+        ""global::Google.Protobuf.MessageExtensions.ToByteArray(message));\n"");+    out->Outdent();+    out->Print(""}\n\n"");++    out->Print(+        ""static class __BufferMessageHelper<T>\n""+        ""{\n"");+    out->Indent();+    out->Print(+        ""public static readonly bool __IsBufferMessage = ""+        ""global::System.Reflection.IntrospectionExtensions.GetTypeInfo(typeof(""+        ""global::Google.Protobuf.IBufferMessage)).IsAssignableFrom(typeof(T));""+        ""\n"");+    out->Outdent();+    out->Print(""}\n\n"");++    out->Print(+        ""static T __Helper_DeserializeMessage<T>(""+        ""grpc::DeserializationContext context, ""+        ""global::Google.Protobuf.MessageParser<T> parser) ""+        ""where T : global::Google.Protobuf.IMessage<T>\n""+        ""{\n"");+    out->Indent();+    out->Print(+        ""#if !GRPC_DISABLE_PROTOBUF_BUFFER_SERIALIZATION\n""+        ""if (__BufferMessageHelper<T>.__IsBufferMessage)\n""","The name `__Helper_MessageCache` seems reasonable, I'll update the PR.I also don't see how a property on a private class could conflict with anything but I wanted to be really sure it's not going to conflict with anything. I will consider removing the prefix.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23484,457614786,2020-07-20T18:39:24Z,test/cpp/util/channelz_sampler_test.cc,"@@ -0,0 +1,181 @@+/*+ *+ * Copyright 2016 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <stdlib.h>+#include <unistd.h>+#include <cstdlib>+#include <iostream>+#include <memory>+#include <string>+#include <thread>++#include ""include/grpcpp/ext/proto_server_reflection_plugin_impl.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/testing/test.grpc.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/subprocess.h""+#include ""test/cpp/util/test_credentials_provider.h""++using grpc::Channel;+using grpc::ClientContext;+using grpc::Server;+using grpc::ServerBuilder;+using grpc::ServerContext;+using grpc::Status;++#include <gtest/gtest.h>++// Test variables+std::string server_address(""0.0.0.0:10000"");+std::string custom_credentials_type(""INSECURE_CREDENTIALS"");+std::string sampling_times = ""2"";+std::string sampling_interval_seconds = ""5"";+std::string output_json(""./output.json"");++// Creata an echo server - randomly delay 0.1 to 0.2 s+class EchoServerImpl final : public grpc::testing::TestService::Service {+  Status EmptyCall(::grpc::ServerContext* context,+                   const grpc::testing::Empty* request,+                   grpc::testing::Empty* response) {+    srand(unsigned(time(0)));+    unsigned int server_delay_microseconds = 100000;+    server_delay_microseconds += rand() % server_delay_microseconds;+    usleep(server_delay_microseconds);+    return Status::OK;+  }+};++// Run server in a thread+void RunServer(gpr_event* done_ev) {","Note that the server itself will launch background threads, because it has synchronous method handlers, and the `while (true)` loop isn't necessary for the server to listen. So I think this can run on the main thread?",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23484,457617249,2020-07-20T18:43:47Z,test/cpp/util/channelz_sampler_test.cc,"@@ -0,0 +1,190 @@+/*+ *+ * Copyright 2016 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>++#include <stdlib.h>+#include <unistd.h>+#include <cstdlib>+#include <iostream>+#include <memory>+#include <string>+#include <thread>++#include ""include/grpcpp/ext/proto_server_reflection_plugin_impl.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/testing/test.grpc.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/subprocess.h""+#include ""test/cpp/util/test_credentials_provider.h""++using grpc::Channel;+using grpc::ClientContext;+using grpc::Server;+using grpc::ServerBuilder;+using grpc::ServerContext;+using grpc::Status;++#include <gtest/gtest.h>++// Test variables+std::string server_address(""0.0.0.0:10000"");+std::string custom_credentials_type(""INSECURE_CREDENTIALS"");+std::string sampling_times = ""2"";+std::string sampling_interval_seconds = ""1"";+std::string output_json(""output.json"");++// Creata an echo server+class EchoServerImpl final : public grpc::testing::TestService::Service {+  Status EmptyCall(::grpc::ServerContext* context,+                   const grpc::testing::Empty* request,+                   grpc::testing::Empty* response) {+    return Status::OK;+  }+};++// Run server in a thread+void RunServer(gpr_event* done_ev) {+  // register channelz service+  ::grpc::channelz::experimental::InitChannelzService();++  EchoServerImpl service;+  grpc::EnableDefaultHealthCheckService(true);+  grpc_impl::reflection::InitProtoReflectionServerBuilderPlugin();+  grpc::ServerBuilder builder;+  auto server_creds =+      grpc::testing::GetCredentialsProvider()->GetServerCredentials(+          custom_credentials_type);+  builder.AddListeningPort(server_address, server_creds);++  builder.RegisterService(&service);+  std::unique_ptr<Server> server(builder.BuildAndStart());+  gpr_log(GPR_INFO, ""Server listening on %s"", server_address.c_str());+  while (true) {+    if (gpr_event_get(done_ev)) {+      return;+    }+  }+}++// Run client in a thread+void RunClient(std::string client_id, gpr_event* done_ev) {+  grpc::ChannelArguments channel_args;+  std::shared_ptr<grpc::ChannelCredentials> channel_creds =+      grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+          custom_credentials_type, &channel_args);+  std::unique_ptr<grpc::testing::TestService::Stub> stub =+      grpc::testing::TestService::NewStub(+          grpc::CreateChannel(server_address, channel_creds));+  unsigned int client_echo_sleep_second = 1;++  gpr_log(GPR_INFO, ""Client %s is echoing!"", client_id.c_str());+  while (true) {+    if (gpr_event_get(done_ev)) {","nit: we can change this to:```if (gpr_event_wait(done_ev, grpc_timeout_seconds_to_deadline(1) != nullptr) {  return;}```and then remove the sleep below",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23203,457624377,2020-07-20T18:56:37Z,src/core/lib/security/credentials/google_default/google_default_credentials.cc,"@@ -314,18 +291,59 @@ grpc_channel_credentials* grpc_google_default_credentials_create() {     g_metadata_server_available = is_metadata_server_reachable();   }   gpr_mu_unlock(&g_state_mu);+}++static grpc_core::RefCountedPtr<grpc_call_credentials> make_default_call_creds(+    grpc_error** error) {+  grpc_core::RefCountedPtr<grpc_call_credentials> call_creds;+  grpc_error* err;++  /* First, try the environment variable. */+  char* path_from_env = gpr_getenv(GRPC_GOOGLE_CREDENTIALS_ENV_VAR);+  if (path_from_env != nullptr) {+    err = create_default_creds_from_path(path_from_env, &call_creds);+    gpr_free(path_from_env);+    if (err == GRPC_ERROR_NONE) return call_creds;+    *error = grpc_error_add_child(*error, err);+  }++  /* Then the well-known file. */+  err = create_default_creds_from_path(+      grpc_get_well_known_google_credentials_file_path(), &call_creds);+  if (err == GRPC_ERROR_NONE) return call_creds;+  *error = grpc_error_add_child(*error, err);    if (g_metadata_server_available) {","pre-existing comment, but looks like we're accessing `g_metadata_server_available` outside of `g_state_mu`.Can we fix that here?",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23484,457669910,2020-07-20T20:23:36Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,512 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <gflags/gflags.h>+#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>+#include <unistd.h>++#include <google/protobuf/text_format.h>+#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>+#include ""absl/strings/str_format.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;++using grpc::ClientContext;+using grpc::Status;++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;+    channelz_stub->GetChannel(&get_channel_context, get_channel_request,+                              &get_channel_response);+    return get_channel_response.channel();+  }++  // Get a subchannel based on subchannel_id+  grpc::channelz::v1::Subchannel GetSubchannelRPC(int64_t subchannel_id) {+    GetSubchannelRequest get_subchannel_request;+    get_subchannel_request.set_subchannel_id(subchannel_id);+    GetSubchannelResponse get_subchannel_response;+    ClientContext get_subchannel_context;+    channelz_stub->GetSubchannel(&get_subchannel_context,+                                 get_subchannel_request,+                                 &get_subchannel_response);+    return get_subchannel_response.subchannel();+  }++  // get a socket based on socket_id+  grpc::channelz::v1::Socket GetSocketRPC(int64_t socket_id) {+    GetSocketRequest get_socket_request;+    get_socket_request.set_socket_id(socket_id);+    GetSocketResponse get_socket_response;+    ClientContext get_socket_context;+    channelz_stub->GetSocket(&get_socket_context, get_socket_request,+                             &get_socket_response);+    return get_socket_response.socket();+  }++  // get the descedent channels/subchannels/sockets of a channel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetChannelDescedence(+      const grpc::channelz::v1::Channel& channel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+    if (channel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : channel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (channel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : channel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (channel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : channel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // get the descedent channels/subchannels/sockets of a subchannel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetSubchannelDescedence(+      grpc::channelz::v1::Subchannel& subchannel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+              << "" descendence - "";+    if (subchannel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : subchannel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (subchannel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : subchannel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (subchannel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : subchannel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // Set up the channelz sampler client+  // Initialize json as an array+  void Setup(std::string custom_credentials_type, std::string server_address) {+    json = grpc_core::Json::Array();+    grpc::ChannelArguments channel_args;+    std::shared_ptr<grpc::ChannelCredentials> channel_creds =+        grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+            custom_credentials_type, &channel_args);+    std::shared_ptr<grpc::Channel> channel =+        CreateChannel(server_address, channel_creds);+    channelz_stub = grpc::channelz::v1::Channelz::NewStub(channel);+  }++  // Get all servers, keep querying until getting all+  // Store servers for dumping data+  // Need to check id repeating for servers+  bool GetServersRPC() {+    int64_t server_start_id = 0;+    while (true) {+      GetServersRequest get_server_request;+      GetServersResponse get_server_response;+      ClientContext get_server_context;+      get_server_request.set_start_server_id(server_start_id);+      Status status = channelz_stub->GetServers(+          &get_server_context, get_server_request, &get_server_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR, ""%s"",+                get_server_context.debug_error_string().c_str());+        return false;+      }+      for (const auto& _server : get_server_response.server()) {+        all_servers.push_back(_server);+        StoreServerInJson(_server);+      }+      if (!get_server_response.end()) {+        server_start_id = GetServerID(all_servers.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of servers = "" << all_servers.size() << std::endl;+    return true;+  }++  // Get sockets that belongs to servers+  // Store sockets for dumping data+  void GetSocketsOfServers() {+    for (const auto& _server : all_servers) {+      std::cout << ""Server "" << GetServerID(_server) << "" listen_socket: "";+      for (const auto& _socket : _server.listen_socket()) {+        int64_t so_id = _socket.socket_id();+        std::cout << so_id << "" "";+        if (CheckID(so_id)) {+          grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+          all_sockets.push_back(so);+          StoreSocketInJson(so);+        }+      }+      std::cout << std::endl;+    }+  }++  // Get all top channels, keep querying until getting all+  // Store channels for dumping data+  // No need to check id repeating for top channels+  bool GetTopChannelsRPC() {+    int64_t channel_start_id = 0;+    while (true) {+      GetTopChannelsRequest get_top_channels_request;+      GetTopChannelsResponse get_top_channels_response;+      ClientContext get_top_channels_context;+      get_top_channels_request.set_start_channel_id(channel_start_id);+      Status status = channelz_stub->GetTopChannels(&get_top_channels_context,+                                                    get_top_channels_request,+                                                    &get_top_channels_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR, ""%s"",+                get_top_channels_context.debug_error_string().c_str());+        return false;+      }+      for (const auto& _topchannel : get_top_channels_response.channel()) {+        top_channels.push_back(_topchannel);+        all_channels.push_back(_topchannel);+        StoreChannelInJson(_topchannel);+      }+      if (!get_top_channels_response.end()) {+        channel_start_id = GetChannelID(top_channels.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of top channels = "" << top_channels.size()+              << std::endl;+    return true;+  }++  // layer traverse for each top channel+  void TraverseTopChannels() {+    for (const auto& _topchannel : top_channels) {+      int tree_depth = 0;+      std::queue<grpc::channelz::v1::Channel> channel_queue;+      std::queue<grpc::channelz::v1::Subchannel> subchannel_queue;+      std::cout << ""Tree depth = "" << tree_depth << std::endl;+      GetChannelDescedence(_topchannel, channel_queue, subchannel_queue);++      while (!channel_queue.empty() || !subchannel_queue.empty()) {+        ++tree_depth;+        std::cout << ""Tree depth = "" << tree_depth << std::endl;+        int ch_q_size = channel_queue.size();+        int subch_q_size = subchannel_queue.size();+        for (int i = 0; i < ch_q_size; ++i) {+          grpc::channelz::v1::Channel ch = channel_queue.front();+          channel_queue.pop();+          GetChannelDescedence(ch, channel_queue, subchannel_queue);+        }+        for (int i = 0; i < subch_q_size; ++i) {+          grpc::channelz::v1::Subchannel subch = subchannel_queue.front();+          subchannel_queue.pop();+          GetSubchannelDescedence(subch, channel_queue, subchannel_queue);+        }+      }+    }+  }++  // dump data of all entities to stdout+  void DumpStdout() {+    std::string data_str;+    for (const auto& _channel : all_channels) {+      std::cout << ""channel "" << GetChannelID(_channel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_channel.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _subchannel : all_subchannels) {+      std::cout << ""subchannel "" << GetSubchannelID(_subchannel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_subchannel.data(),+                                                    &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _server : all_servers) {+      std::cout << ""server "" << GetServerID(_server) << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_server.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _socket : all_sockets) {+      std::cout << ""socket "" << GetSocketID(_socket) << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_socket.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+  }++  // Clear stored entities to prepare for the next scan+  void ClearEntities() {+    all_channels.clear();+    all_subchannels.clear();+    all_servers.clear();+    all_sockets.clear();+    top_channels.clear();+    id_set.clear();+  }++  // Store a channel in Json+  void StoreChannelInJson(const grpc::channelz::v1::Channel& channel) {+    std::string id = grpc::to_string(GetChannelID(channel));+    std::string type = ""Channel"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(channel.data(), &description);+    StoreEntityInJson(id, type, description);+  }++  // Store a subchannel in Json+  void StoreSubchannelInJson(const grpc::channelz::v1::Subchannel& subchannel) {+    std::string id = grpc::to_string(GetSubchannelID(subchannel));+    std::string type = ""Subchannel"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(subchannel.data(),+                                                  &description);+    StoreEntityInJson(id, type, description);+  }++  // Store a server in Json+  void StoreServerInJson(const grpc::channelz::v1::Server& server) {+    std::string id = grpc::to_string(GetServerID(server));+    std::string type = ""Server"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(server.data(), &description);+    StoreEntityInJson(id, type, description);+  }++  // Store a socket in Json+  void StoreSocketInJson(const grpc::channelz::v1::Socket& socket) {+    std::string id = grpc::to_string(GetSocketID(socket));+    std::string type = ""Socket"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(socket.data(), &description);+    StoreEntityInJson(id, type, description);+  }++  // Store an entity in Json+  void StoreEntityInJson(std::string& id, std::string& type,+                         std::string& description) {+    std::string start, finish;+    time_t ago = now - int64_t(FLAGS_sampling_interval_seconds);+    std::stringstream ss;+    ss << std::put_time(std::localtime(&now), ""%F %T"");+    finish = ss.str();  // example: ""2019-02-01 12:12:18""+    ss.str("""");+    ss << std::put_time(std::localtime(&ago), ""%F %T"");+    start = ss.str();++    grpc_core::Json obj =+        grpc_core::Json::Object{{""Task"", absl::StrFormat(""%s_ID%s"", type, id)},+                                {""Start"", start},+                                {""Finish"", finish},+                                {""ID"", id},+                                {""Type"", type},+                                {""Description"", description}};+    json.mutable_array()->push_back(obj);+  }++  // Dump data in json+  std::string DumpJson() { return json.Dump(); }++  // Check if one entity has been recorded+  bool CheckID(int64_t id) {+    if (id_set.count(id) == 0) {+      id_set.insert(id);+      return true;+    } else {+      return false;+    }+  }++  time_t now;+  // private:+  std::unique_ptr<grpc::channelz::v1::Channelz::Stub> channelz_stub;+  std::vector<grpc::channelz::v1::Channel> top_channels;+  std::vector<grpc::channelz::v1::Server> all_servers;+  std::vector<grpc::channelz::v1::Channel> all_channels;+  std::vector<grpc::channelz::v1::Subchannel> all_subchannels;+  std::vector<grpc::channelz::v1::Socket> all_sockets;+  std::unordered_set<int64_t> id_set;+  grpc_core::Json json;+};++int main(int argc, char** argv) {+  // make sure flags can be used+  grpc::testing::InitTest(&argc, &argv, true);++  // create a channelz client+  ChannelzSampler channelz_sampler;+  channelz_sampler.Setup(FLAGS_custom_credentials_type, FLAGS_server_address);++  // Keep sampling based on FLAGS_sampling_times and+  for (int i = 0; i < FLAGS_sampling_times; ++i) {+    std::cout << ""Wait for sampling interval ""+              << FLAGS_sampling_interval_seconds << ""s..."" << std::endl;+    sleep(FLAGS_sampling_interval_seconds);+    std::cout << ""##### "" << i << ""th sampling #####"" << std::endl;+    // Get sampling time+    channelz_sampler.now = time(0);","nit: here and elsewhere, lets use `gpr_timespec` and the gpr time library for time calculations in general (e.g. https://github.com/grpc/grpc/blob/master/include/grpc/support/time.h)",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23484,457673912,2020-07-20T20:31:16Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,512 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <gflags/gflags.h>+#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>+#include <unistd.h>++#include <google/protobuf/text_format.h>+#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>+#include ""absl/strings/str_format.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;++using grpc::ClientContext;+using grpc::Status;++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;+    channelz_stub->GetChannel(&get_channel_context, get_channel_request,+                              &get_channel_response);+    return get_channel_response.channel();+  }++  // Get a subchannel based on subchannel_id+  grpc::channelz::v1::Subchannel GetSubchannelRPC(int64_t subchannel_id) {+    GetSubchannelRequest get_subchannel_request;+    get_subchannel_request.set_subchannel_id(subchannel_id);+    GetSubchannelResponse get_subchannel_response;+    ClientContext get_subchannel_context;+    channelz_stub->GetSubchannel(&get_subchannel_context,+                                 get_subchannel_request,+                                 &get_subchannel_response);+    return get_subchannel_response.subchannel();+  }++  // get a socket based on socket_id+  grpc::channelz::v1::Socket GetSocketRPC(int64_t socket_id) {+    GetSocketRequest get_socket_request;+    get_socket_request.set_socket_id(socket_id);+    GetSocketResponse get_socket_response;+    ClientContext get_socket_context;+    channelz_stub->GetSocket(&get_socket_context, get_socket_request,+                             &get_socket_response);+    return get_socket_response.socket();+  }++  // get the descedent channels/subchannels/sockets of a channel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetChannelDescedence(+      const grpc::channelz::v1::Channel& channel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+    if (channel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : channel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (channel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : channel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (channel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : channel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // get the descedent channels/subchannels/sockets of a subchannel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetSubchannelDescedence(+      grpc::channelz::v1::Subchannel& subchannel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+              << "" descendence - "";+    if (subchannel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : subchannel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (subchannel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : subchannel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (subchannel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : subchannel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // Set up the channelz sampler client+  // Initialize json as an array+  void Setup(std::string custom_credentials_type, std::string server_address) {+    json = grpc_core::Json::Array();+    grpc::ChannelArguments channel_args;+    std::shared_ptr<grpc::ChannelCredentials> channel_creds =+        grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+            custom_credentials_type, &channel_args);+    std::shared_ptr<grpc::Channel> channel =+        CreateChannel(server_address, channel_creds);+    channelz_stub = grpc::channelz::v1::Channelz::NewStub(channel);+  }++  // Get all servers, keep querying until getting all+  // Store servers for dumping data+  // Need to check id repeating for servers+  bool GetServersRPC() {+    int64_t server_start_id = 0;+    while (true) {+      GetServersRequest get_server_request;+      GetServersResponse get_server_response;+      ClientContext get_server_context;+      get_server_request.set_start_server_id(server_start_id);+      Status status = channelz_stub->GetServers(+          &get_server_context, get_server_request, &get_server_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR, ""%s"",+                get_server_context.debug_error_string().c_str());+        return false;+      }+      for (const auto& _server : get_server_response.server()) {+        all_servers.push_back(_server);+        StoreServerInJson(_server);+      }+      if (!get_server_response.end()) {+        server_start_id = GetServerID(all_servers.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of servers = "" << all_servers.size() << std::endl;+    return true;+  }++  // Get sockets that belongs to servers+  // Store sockets for dumping data+  void GetSocketsOfServers() {+    for (const auto& _server : all_servers) {+      std::cout << ""Server "" << GetServerID(_server) << "" listen_socket: "";+      for (const auto& _socket : _server.listen_socket()) {+        int64_t so_id = _socket.socket_id();+        std::cout << so_id << "" "";+        if (CheckID(so_id)) {+          grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+          all_sockets.push_back(so);+          StoreSocketInJson(so);+        }+      }+      std::cout << std::endl;+    }+  }++  // Get all top channels, keep querying until getting all+  // Store channels for dumping data+  // No need to check id repeating for top channels+  bool GetTopChannelsRPC() {+    int64_t channel_start_id = 0;+    while (true) {+      GetTopChannelsRequest get_top_channels_request;+      GetTopChannelsResponse get_top_channels_response;+      ClientContext get_top_channels_context;","nit: lets put (generous) deadlines on all of these RPCs, just so that the tool doesn't get ""stuck"" somehow in one of these RPCs.We can do something like, `get_top_channels_context.set_deadline(grpc_timeout_seconds_to_deadline(20)`",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23527,457704548,2020-07-20T21:33:41Z,test/cpp/util/test_credentials_provider.cc,"@@ -115,7 +121,22 @@ class DefaultCredentialsProvider : public CredentialsProvider {     } else if (type == grpc::testing::kAltsCredentialsType) {       grpc::experimental::AltsServerCredentialsOptions alts_opts;       return grpc::experimental::AltsServerCredentials(alts_opts);-    } else if (type == grpc::testing::kTlsCredentialsType) {+    } else if (type == grpc::testing::kTls12CredentialsType) {",Could we combine the `else if`'s for TLS 1.2 and 1.3 (to avoid a lot of duplicate code) and then have an additional if statement when we go to set the min/max TLS version?,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23489,457797974,2020-07-21T02:25:19Z,test/core/util/reconnect_server.cc,"@@ -59,27 +60,25 @@ static void on_connect(void* arg, grpc_endpoint* tcp,                        grpc_pollset* /*accepting_pollset*/,                        grpc_tcp_server_acceptor* acceptor) {   gpr_free(acceptor);-  char* peer;-  char* last_colon;+  absl::string_view peer;+  int last_colon;   reconnect_server* server = static_cast<reconnect_server*>(arg);   gpr_timespec now = gpr_now(GPR_CLOCK_REALTIME);   timestamp_list* new_tail;   peer = grpc_endpoint_get_peer(tcp);   grpc_endpoint_shutdown(tcp,                          GRPC_ERROR_CREATE_FROM_STATIC_STRING(""Connected""));   grpc_endpoint_destroy(tcp);-  if (peer) {-    last_colon = strrchr(peer, ':');-    if (server->peer == nullptr) {-      server->peer = peer;-    } else {-      if (last_colon == nullptr) {-        gpr_log(GPR_ERROR, ""peer does not contain a ':'"");-      } else if (strncmp(server->peer, peer,-                         static_cast<size_t>(last_colon - peer)) != 0) {-        gpr_log(GPR_ERROR, ""mismatched peer! %s vs %s"", server->peer, peer);-      }-      gpr_free(peer);+  last_colon = peer.rfind(':');+  if (server->peer == nullptr) {+    server->peer = new std::string(peer);+  } else {+    if (last_colon == std::string::npos) {+      gpr_log(GPR_ERROR, ""peer does not contain a ':'"");+    } else if (peer.compare(0, static_cast<size_t>(last_colon),+                            *server->peer) != 0) {+      gpr_log(GPR_ERROR, ""mismatched peer! %s vs %s"", server->peer->c_str(),+              peer);",We may need peer.data() here.,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23484,457827957,2020-07-21T04:24:16Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,512 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <gflags/gflags.h>+#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>+#include <unistd.h>++#include <google/protobuf/text_format.h>+#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>+#include ""absl/strings/str_format.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;++using grpc::ClientContext;+using grpc::Status;++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;+    channelz_stub->GetChannel(&get_channel_context, get_channel_request,+                              &get_channel_response);+    return get_channel_response.channel();+  }++  // Get a subchannel based on subchannel_id+  grpc::channelz::v1::Subchannel GetSubchannelRPC(int64_t subchannel_id) {+    GetSubchannelRequest get_subchannel_request;+    get_subchannel_request.set_subchannel_id(subchannel_id);+    GetSubchannelResponse get_subchannel_response;+    ClientContext get_subchannel_context;+    channelz_stub->GetSubchannel(&get_subchannel_context,+                                 get_subchannel_request,+                                 &get_subchannel_response);+    return get_subchannel_response.subchannel();+  }++  // get a socket based on socket_id+  grpc::channelz::v1::Socket GetSocketRPC(int64_t socket_id) {+    GetSocketRequest get_socket_request;+    get_socket_request.set_socket_id(socket_id);+    GetSocketResponse get_socket_response;+    ClientContext get_socket_context;+    channelz_stub->GetSocket(&get_socket_context, get_socket_request,+                             &get_socket_response);+    return get_socket_response.socket();+  }++  // get the descedent channels/subchannels/sockets of a channel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetChannelDescedence(+      const grpc::channelz::v1::Channel& channel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+    if (channel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : channel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (channel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : channel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (channel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : channel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // get the descedent channels/subchannels/sockets of a subchannel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetSubchannelDescedence(+      grpc::channelz::v1::Subchannel& subchannel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+              << "" descendence - "";+    if (subchannel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : subchannel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (subchannel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : subchannel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (subchannel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : subchannel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // Set up the channelz sampler client+  // Initialize json as an array+  void Setup(std::string custom_credentials_type, std::string server_address) {+    json = grpc_core::Json::Array();+    grpc::ChannelArguments channel_args;+    std::shared_ptr<grpc::ChannelCredentials> channel_creds =+        grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+            custom_credentials_type, &channel_args);+    std::shared_ptr<grpc::Channel> channel =+        CreateChannel(server_address, channel_creds);+    channelz_stub = grpc::channelz::v1::Channelz::NewStub(channel);+  }++  // Get all servers, keep querying until getting all+  // Store servers for dumping data+  // Need to check id repeating for servers+  bool GetServersRPC() {+    int64_t server_start_id = 0;+    while (true) {+      GetServersRequest get_server_request;+      GetServersResponse get_server_response;+      ClientContext get_server_context;+      get_server_request.set_start_server_id(server_start_id);+      Status status = channelz_stub->GetServers(+          &get_server_context, get_server_request, &get_server_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR, ""%s"",+                get_server_context.debug_error_string().c_str());+        return false;+      }+      for (const auto& _server : get_server_response.server()) {+        all_servers.push_back(_server);+        StoreServerInJson(_server);+      }+      if (!get_server_response.end()) {+        server_start_id = GetServerID(all_servers.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of servers = "" << all_servers.size() << std::endl;+    return true;+  }++  // Get sockets that belongs to servers+  // Store sockets for dumping data+  void GetSocketsOfServers() {+    for (const auto& _server : all_servers) {+      std::cout << ""Server "" << GetServerID(_server) << "" listen_socket: "";+      for (const auto& _socket : _server.listen_socket()) {+        int64_t so_id = _socket.socket_id();+        std::cout << so_id << "" "";+        if (CheckID(so_id)) {+          grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+          all_sockets.push_back(so);+          StoreSocketInJson(so);+        }+      }+      std::cout << std::endl;+    }+  }++  // Get all top channels, keep querying until getting all+  // Store channels for dumping data+  // No need to check id repeating for top channels+  bool GetTopChannelsRPC() {+    int64_t channel_start_id = 0;+    while (true) {+      GetTopChannelsRequest get_top_channels_request;+      GetTopChannelsResponse get_top_channels_response;+      ClientContext get_top_channels_context;+      get_top_channels_request.set_start_channel_id(channel_start_id);+      Status status = channelz_stub->GetTopChannels(&get_top_channels_context,+                                                    get_top_channels_request,+                                                    &get_top_channels_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR, ""%s"",+                get_top_channels_context.debug_error_string().c_str());+        return false;+      }+      for (const auto& _topchannel : get_top_channels_response.channel()) {+        top_channels.push_back(_topchannel);+        all_channels.push_back(_topchannel);+        StoreChannelInJson(_topchannel);+      }+      if (!get_top_channels_response.end()) {+        channel_start_id = GetChannelID(top_channels.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of top channels = "" << top_channels.size()+              << std::endl;+    return true;+  }++  // layer traverse for each top channel+  void TraverseTopChannels() {+    for (const auto& _topchannel : top_channels) {+      int tree_depth = 0;+      std::queue<grpc::channelz::v1::Channel> channel_queue;+      std::queue<grpc::channelz::v1::Subchannel> subchannel_queue;+      std::cout << ""Tree depth = "" << tree_depth << std::endl;+      GetChannelDescedence(_topchannel, channel_queue, subchannel_queue);++      while (!channel_queue.empty() || !subchannel_queue.empty()) {+        ++tree_depth;+        std::cout << ""Tree depth = "" << tree_depth << std::endl;+        int ch_q_size = channel_queue.size();+        int subch_q_size = subchannel_queue.size();+        for (int i = 0; i < ch_q_size; ++i) {+          grpc::channelz::v1::Channel ch = channel_queue.front();+          channel_queue.pop();+          GetChannelDescedence(ch, channel_queue, subchannel_queue);+        }+        for (int i = 0; i < subch_q_size; ++i) {+          grpc::channelz::v1::Subchannel subch = subchannel_queue.front();+          subchannel_queue.pop();+          GetSubchannelDescedence(subch, channel_queue, subchannel_queue);+        }+      }+    }+  }++  // dump data of all entities to stdout+  void DumpStdout() {+    std::string data_str;+    for (const auto& _channel : all_channels) {+      std::cout << ""channel "" << GetChannelID(_channel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_channel.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _subchannel : all_subchannels) {+      std::cout << ""subchannel "" << GetSubchannelID(_subchannel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_subchannel.data(),+                                                    &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _server : all_servers) {+      std::cout << ""server "" << GetServerID(_server) << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_server.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _socket : all_sockets) {+      std::cout << ""socket "" << GetSocketID(_socket) << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_socket.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+  }++  // Clear stored entities to prepare for the next scan+  void ClearEntities() {+    all_channels.clear();+    all_subchannels.clear();+    all_servers.clear();+    all_sockets.clear();+    top_channels.clear();+    id_set.clear();+  }++  // Store a channel in Json+  void StoreChannelInJson(const grpc::channelz::v1::Channel& channel) {+    std::string id = grpc::to_string(GetChannelID(channel));+    std::string type = ""Channel"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(channel.data(), &description);+    StoreEntityInJson(id, type, description);+  }++  // Store a subchannel in Json+  void StoreSubchannelInJson(const grpc::channelz::v1::Subchannel& subchannel) {+    std::string id = grpc::to_string(GetSubchannelID(subchannel));+    std::string type = ""Subchannel"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(subchannel.data(),+                                                  &description);+    StoreEntityInJson(id, type, description);+  }++  // Store a server in Json+  void StoreServerInJson(const grpc::channelz::v1::Server& server) {+    std::string id = grpc::to_string(GetServerID(server));+    std::string type = ""Server"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(server.data(), &description);+    StoreEntityInJson(id, type, description);+  }++  // Store a socket in Json+  void StoreSocketInJson(const grpc::channelz::v1::Socket& socket) {+    std::string id = grpc::to_string(GetSocketID(socket));+    std::string type = ""Socket"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(socket.data(), &description);+    StoreEntityInJson(id, type, description);+  }++  // Store an entity in Json+  void StoreEntityInJson(std::string& id, std::string& type,+                         std::string& description) {+    std::string start, finish;+    time_t ago = now - int64_t(FLAGS_sampling_interval_seconds);+    std::stringstream ss;+    ss << std::put_time(std::localtime(&now), ""%F %T"");+    finish = ss.str();  // example: ""2019-02-01 12:12:18""+    ss.str("""");+    ss << std::put_time(std::localtime(&ago), ""%F %T"");+    start = ss.str();++    grpc_core::Json obj =+        grpc_core::Json::Object{{""Task"", absl::StrFormat(""%s_ID%s"", type, id)},+                                {""Start"", start},+                                {""Finish"", finish},+                                {""ID"", id},+                                {""Type"", type},+                                {""Description"", description}};+    json.mutable_array()->push_back(obj);+  }++  // Dump data in json+  std::string DumpJson() { return json.Dump(); }++  // Check if one entity has been recorded+  bool CheckID(int64_t id) {+    if (id_set.count(id) == 0) {+      id_set.insert(id);+      return true;+    } else {+      return false;+    }+  }++  time_t now;+  // private:+  std::unique_ptr<grpc::channelz::v1::Channelz::Stub> channelz_stub;+  std::vector<grpc::channelz::v1::Channel> top_channels;",please suffice all class member fields with `_` (https://google.github.io/styleguide/cppguide.html#Variable_Comments),
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23484,457828434,2020-07-21T04:26:13Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,512 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <gflags/gflags.h>+#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>+#include <unistd.h>++#include <google/protobuf/text_format.h>+#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>+#include ""absl/strings/str_format.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;++using grpc::ClientContext;+using grpc::Status;++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;+    channelz_stub->GetChannel(&get_channel_context, get_channel_request,+                              &get_channel_response);+    return get_channel_response.channel();+  }++  // Get a subchannel based on subchannel_id+  grpc::channelz::v1::Subchannel GetSubchannelRPC(int64_t subchannel_id) {+    GetSubchannelRequest get_subchannel_request;+    get_subchannel_request.set_subchannel_id(subchannel_id);+    GetSubchannelResponse get_subchannel_response;+    ClientContext get_subchannel_context;+    channelz_stub->GetSubchannel(&get_subchannel_context,+                                 get_subchannel_request,+                                 &get_subchannel_response);+    return get_subchannel_response.subchannel();+  }++  // get a socket based on socket_id+  grpc::channelz::v1::Socket GetSocketRPC(int64_t socket_id) {+    GetSocketRequest get_socket_request;+    get_socket_request.set_socket_id(socket_id);+    GetSocketResponse get_socket_response;+    ClientContext get_socket_context;+    channelz_stub->GetSocket(&get_socket_context, get_socket_request,+                             &get_socket_response);+    return get_socket_response.socket();+  }++  // get the descedent channels/subchannels/sockets of a channel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetChannelDescedence(+      const grpc::channelz::v1::Channel& channel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+    if (channel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : channel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (channel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : channel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (channel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : channel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // get the descedent channels/subchannels/sockets of a subchannel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetSubchannelDescedence(+      grpc::channelz::v1::Subchannel& subchannel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+              << "" descendence - "";+    if (subchannel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : subchannel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (subchannel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : subchannel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (subchannel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : subchannel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // Set up the channelz sampler client+  // Initialize json as an array+  void Setup(std::string custom_credentials_type, std::string server_address) {+    json = grpc_core::Json::Array();+    grpc::ChannelArguments channel_args;+    std::shared_ptr<grpc::ChannelCredentials> channel_creds =+        grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+            custom_credentials_type, &channel_args);+    std::shared_ptr<grpc::Channel> channel =+        CreateChannel(server_address, channel_creds);+    channelz_stub = grpc::channelz::v1::Channelz::NewStub(channel);+  }++  // Get all servers, keep querying until getting all+  // Store servers for dumping data+  // Need to check id repeating for servers+  bool GetServersRPC() {+    int64_t server_start_id = 0;+    while (true) {+      GetServersRequest get_server_request;+      GetServersResponse get_server_response;+      ClientContext get_server_context;+      get_server_request.set_start_server_id(server_start_id);+      Status status = channelz_stub->GetServers(+          &get_server_context, get_server_request, &get_server_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR, ""%s"",+                get_server_context.debug_error_string().c_str());+        return false;+      }+      for (const auto& _server : get_server_response.server()) {+        all_servers.push_back(_server);+        StoreServerInJson(_server);+      }+      if (!get_server_response.end()) {+        server_start_id = GetServerID(all_servers.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of servers = "" << all_servers.size() << std::endl;+    return true;+  }++  // Get sockets that belongs to servers+  // Store sockets for dumping data+  void GetSocketsOfServers() {+    for (const auto& _server : all_servers) {+      std::cout << ""Server "" << GetServerID(_server) << "" listen_socket: "";+      for (const auto& _socket : _server.listen_socket()) {+        int64_t so_id = _socket.socket_id();+        std::cout << so_id << "" "";+        if (CheckID(so_id)) {+          grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+          all_sockets.push_back(so);+          StoreSocketInJson(so);+        }+      }+      std::cout << std::endl;+    }+  }++  // Get all top channels, keep querying until getting all+  // Store channels for dumping data+  // No need to check id repeating for top channels+  bool GetTopChannelsRPC() {+    int64_t channel_start_id = 0;+    while (true) {+      GetTopChannelsRequest get_top_channels_request;+      GetTopChannelsResponse get_top_channels_response;+      ClientContext get_top_channels_context;+      get_top_channels_request.set_start_channel_id(channel_start_id);+      Status status = channelz_stub->GetTopChannels(&get_top_channels_context,+                                                    get_top_channels_request,+                                                    &get_top_channels_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR, ""%s"",+                get_top_channels_context.debug_error_string().c_str());+        return false;+      }+      for (const auto& _topchannel : get_top_channels_response.channel()) {+        top_channels.push_back(_topchannel);+        all_channels.push_back(_topchannel);+        StoreChannelInJson(_topchannel);+      }+      if (!get_top_channels_response.end()) {+        channel_start_id = GetChannelID(top_channels.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of top channels = "" << top_channels.size()+              << std::endl;+    return true;+  }++  // layer traverse for each top channel+  void TraverseTopChannels() {+    for (const auto& _topchannel : top_channels) {+      int tree_depth = 0;+      std::queue<grpc::channelz::v1::Channel> channel_queue;+      std::queue<grpc::channelz::v1::Subchannel> subchannel_queue;+      std::cout << ""Tree depth = "" << tree_depth << std::endl;+      GetChannelDescedence(_topchannel, channel_queue, subchannel_queue);++      while (!channel_queue.empty() || !subchannel_queue.empty()) {+        ++tree_depth;+        std::cout << ""Tree depth = "" << tree_depth << std::endl;+        int ch_q_size = channel_queue.size();+        int subch_q_size = subchannel_queue.size();+        for (int i = 0; i < ch_q_size; ++i) {+          grpc::channelz::v1::Channel ch = channel_queue.front();+          channel_queue.pop();+          GetChannelDescedence(ch, channel_queue, subchannel_queue);+        }+        for (int i = 0; i < subch_q_size; ++i) {+          grpc::channelz::v1::Subchannel subch = subchannel_queue.front();+          subchannel_queue.pop();+          GetSubchannelDescedence(subch, channel_queue, subchannel_queue);+        }+      }+    }+  }++  // dump data of all entities to stdout+  void DumpStdout() {+    std::string data_str;+    for (const auto& _channel : all_channels) {+      std::cout << ""channel "" << GetChannelID(_channel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_channel.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _subchannel : all_subchannels) {+      std::cout << ""subchannel "" << GetSubchannelID(_subchannel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_subchannel.data(),+                                                    &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _server : all_servers) {+      std::cout << ""server "" << GetServerID(_server) << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_server.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _socket : all_sockets) {+      std::cout << ""socket "" << GetSocketID(_socket) << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_socket.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+  }++  // Clear stored entities to prepare for the next scan+  void ClearEntities() {+    all_channels.clear();+    all_subchannels.clear();+    all_servers.clear();+    all_sockets.clear();+    top_channels.clear();+    id_set.clear();+  }++  // Store a channel in Json+  void StoreChannelInJson(const grpc::channelz::v1::Channel& channel) {+    std::string id = grpc::to_string(GetChannelID(channel));+    std::string type = ""Channel"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(channel.data(), &description);+    StoreEntityInJson(id, type, description);+  }++  // Store a subchannel in Json+  void StoreSubchannelInJson(const grpc::channelz::v1::Subchannel& subchannel) {+    std::string id = grpc::to_string(GetSubchannelID(subchannel));+    std::string type = ""Subchannel"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(subchannel.data(),+                                                  &description);+    StoreEntityInJson(id, type, description);+  }++  // Store a server in Json+  void StoreServerInJson(const grpc::channelz::v1::Server& server) {+    std::string id = grpc::to_string(GetServerID(server));+    std::string type = ""Server"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(server.data(), &description);+    StoreEntityInJson(id, type, description);+  }++  // Store a socket in Json+  void StoreSocketInJson(const grpc::channelz::v1::Socket& socket) {+    std::string id = grpc::to_string(GetSocketID(socket));+    std::string type = ""Socket"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(socket.data(), &description);+    StoreEntityInJson(id, type, description);+  }++  // Store an entity in Json+  void StoreEntityInJson(std::string& id, std::string& type,+                         std::string& description) {+    std::string start, finish;+    time_t ago = now - int64_t(FLAGS_sampling_interval_seconds);+    std::stringstream ss;+    ss << std::put_time(std::localtime(&now), ""%F %T"");+    finish = ss.str();  // example: ""2019-02-01 12:12:18""+    ss.str("""");+    ss << std::put_time(std::localtime(&ago), ""%F %T"");+    start = ss.str();++    grpc_core::Json obj =+        grpc_core::Json::Object{{""Task"", absl::StrFormat(""%s_ID%s"", type, id)},+                                {""Start"", start},+                                {""Finish"", finish},+                                {""ID"", id},+                                {""Type"", type},+                                {""Description"", description}};+    json.mutable_array()->push_back(obj);+  }++  // Dump data in json+  std::string DumpJson() { return json.Dump(); }++  // Check if one entity has been recorded+  bool CheckID(int64_t id) {+    if (id_set.count(id) == 0) {+      id_set.insert(id);+      return true;+    } else {+      return false;+    }+  }++  time_t now;+  // private:+  std::unique_ptr<grpc::channelz::v1::Channelz::Stub> channelz_stub;+  std::vector<grpc::channelz::v1::Channel> top_channels;+  std::vector<grpc::channelz::v1::Server> all_servers;+  std::vector<grpc::channelz::v1::Channel> all_channels;+  std::vector<grpc::channelz::v1::Subchannel> all_subchannels;+  std::vector<grpc::channelz::v1::Socket> all_sockets;+  std::unordered_set<int64_t> id_set;+  grpc_core::Json json;+};++int main(int argc, char** argv) {+  // make sure flags can be used+  grpc::testing::InitTest(&argc, &argv, true);++  // create a channelz client+  ChannelzSampler channelz_sampler;+  channelz_sampler.Setup(FLAGS_custom_credentials_type, FLAGS_server_address);++  // Keep sampling based on FLAGS_sampling_times and+  for (int i = 0; i < FLAGS_sampling_times; ++i) {+    std::cout << ""Wait for sampling interval ""+              << FLAGS_sampling_interval_seconds << ""s..."" << std::endl;+    sleep(FLAGS_sampling_interval_seconds);+    std::cout << ""##### "" << i << ""th sampling #####"" << std::endl;+    // Get sampling time+    channelz_sampler.now = time(0);+    // Server side code+    bool isRPCOK = channelz_sampler.GetServersRPC();+    if (!isRPCOK) GPR_ASSERT(0);+    channelz_sampler.GetSocketsOfServers();+    // Client side code+    isRPCOK = channelz_sampler.GetTopChannelsRPC();+    if (!isRPCOK) GPR_ASSERT(0);+    channelz_sampler.TraverseTopChannels();+    // dump data to stdout+    channelz_sampler.DumpStdout();+    // clear stored entities to prepare for the next sampling+    channelz_sampler.ClearEntities();",Manually resetting state in the `ChannelzSampler` objects after each loop iteration may be error prone. Can we instead instantiate a new `ChannelzSampler fresh during reach round?,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23484,457829513,2020-07-21T04:30:42Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,512 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <gflags/gflags.h>+#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>+#include <unistd.h>++#include <google/protobuf/text_format.h>+#include <cstdlib>",please place C++ standard library headers above grpc/protobuf/gflags headers (https://google.github.io/styleguide/cppguide.html#Names_and_Order_of_Includes),
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23484,457829778,2020-07-21T04:31:53Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,539 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <gflags/gflags.h>+#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>+#include <unistd.h>++#include <google/protobuf/text_format.h>+#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>+#include ""absl/strings/str_format.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;++using grpc::ClientContext;+using grpc::Status;++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;",can we please also put the same 20 second deadlines on these channelz RPCs?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23203,458138240,2020-07-21T14:25:09Z,src/core/lib/security/credentials/google_default/google_default_credentials.cc,"@@ -273,59 +274,82 @@ static grpc_error* create_default_creds_from_path(   return error; } -grpc_channel_credentials* grpc_google_default_credentials_create() {-  grpc_channel_credentials* result = nullptr;+static void update_tenancy() {+  gpr_once_init(&g_once, init_default_credentials);+  gpr_mu_lock(&g_state_mu);++  /* Try a platform-provided hint for GCE. */+  if (!g_metadata_server_available) {+    g_metadata_server_available = g_gce_tenancy_checker();+  }+  /* TODO: Add a platform-provided hint for GAE. */++  /* Do a network test for metadata server. */+  if (!g_metadata_server_available) {+    g_metadata_server_available = is_metadata_server_reachable();+  }+  gpr_mu_unlock(&g_state_mu);+}++static bool metadata_server_available() {+  bool available = false;",This could be written as:```grpc_core::MutexLock lock(&g_state_mu);return static_cast<bool>(g_metadata_server_available);```,
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/23203,458189590,2020-07-21T15:31:49Z,src/core/lib/security/credentials/google_default/google_default_credentials.cc,"@@ -273,59 +274,82 @@ static grpc_error* create_default_creds_from_path(   return error; } -grpc_channel_credentials* grpc_google_default_credentials_create() {-  grpc_channel_credentials* result = nullptr;+static void update_tenancy() {+  gpr_once_init(&g_once, init_default_credentials);+  gpr_mu_lock(&g_state_mu);++  /* Try a platform-provided hint for GCE. */+  if (!g_metadata_server_available) {+    g_metadata_server_available = g_gce_tenancy_checker();+  }+  /* TODO: Add a platform-provided hint for GAE. */++  /* Do a network test for metadata server. */+  if (!g_metadata_server_available) {+    g_metadata_server_available = is_metadata_server_reachable();+  }+  gpr_mu_unlock(&g_state_mu);+}++static bool metadata_server_available() {+  bool available = false;+  gpr_mu_lock(&g_state_mu);+  available = static_cast<bool>(g_metadata_server_available);+  gpr_mu_unlock(&g_state_mu);+  return available;+}++static grpc_core::RefCountedPtr<grpc_call_credentials> make_default_call_creds(+    grpc_error** error) {   grpc_core::RefCountedPtr<grpc_call_credentials> call_creds;-  grpc_error* error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(-      ""Failed to create Google credentials"");   grpc_error* err;-  grpc_core::ExecCtx exec_ctx; -  GRPC_API_TRACE(""grpc_google_default_credentials_create(void)"", 0, ());--  gpr_once_init(&g_once, init_default_credentials);+  update_tenancy();","Can we move this to line 323? The tenancy check does not seem necessary if the call credentials is not read from the metadata server. By doing that, it will be also consistent with the old code path.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23203,458291048,2020-07-21T18:06:59Z,src/core/lib/security/credentials/google_default/google_default_credentials.cc,"@@ -288,15 +288,12 @@ static void update_tenancy() {   if (!g_metadata_server_available) {     g_metadata_server_available = is_metadata_server_reachable();   }-  gpr_mu_unlock(&g_state_mu); }  static bool metadata_server_available() {   bool available = false;",This variable is no longer needed.,
15756578,Ryanfsdf,https://api.github.com/repos/grpc/grpc/pulls/23527,458398368,2020-07-21T21:27:25Z,include/grpc/grpc_security.h,"@@ -240,8 +240,11 @@ GRPCAPI grpc_channel_credentials* grpc_ssl_credentials_create(     const char* pem_root_certs, grpc_ssl_pem_key_cert_pair* pem_key_cert_pair,     const verify_peer_options* verify_options, void* reserved); +/** Sets the min TLS version in a channel credentials object. */","I'm not sure what else I could add to this, since the function is very small. I could add `grpc_channel_credentials should not be NULL` but some of the examples below don't do that as well. Do you have any suggestions?```/** Creates a credentials object from a plugin with a specified minimum security * level. */GRPCAPI grpc_call_credentials* grpc_metadata_credentials_create_from_plugin(    grpc_metadata_credentials_plugin plugin,    grpc_security_level min_security_level, void* reserved);``````/** Creates an IAM credentials object for connecting to Google. */GRPCAPI grpc_call_credentials* grpc_google_iam_credentials_create(    const char* authorization_token, const char* authority_selector,    void* reserved);```",
15756578,Ryanfsdf,https://api.github.com/repos/grpc/grpc/pulls/23527,458400078,2020-07-21T21:30:58Z,test/cpp/util/test_credentials_provider.cc,"@@ -153,23 +140,16 @@ class DefaultCredentialsProvider : public CredentialsProvider {                                                             test_server1_cert};         ssl_opts.pem_key_cert_pairs.push_back(pkcp);       }-      ssl_opts.min_tls_version = TLS1_2;-      ssl_opts.max_tls_version = TLS1_2;-      return SslServerCredentials(ssl_opts);-    } else if (type == grpc::testing::kTls13CredentialsType) {-      SslServerCredentialsOptions ssl_opts;-      ssl_opts.pem_root_certs = """";-      if (!custom_server_key_.empty() && !custom_server_cert_.empty()) {-        SslServerCredentialsOptions::PemKeyCertPair pkcp = {-            custom_server_key_, custom_server_cert_};-        ssl_opts.pem_key_cert_pairs.push_back(pkcp);-      } else {-        SslServerCredentialsOptions::PemKeyCertPair pkcp = {test_server1_key,-                                                            test_server1_cert};-        ssl_opts.pem_key_cert_pairs.push_back(pkcp);+      if (type == grpc::testing::kTlsCredentialsType) {","I think it may be cleaner just to remove it as you initially suggested. I put the first case initially because I thought being explicit may be useful, but it seems much cleaner just removing it. I'm not sure if a switch statement would be necessary and failing seems excessive since we check the condition right above in the if condition. WDYT?",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/23527,458471284,2020-07-22T00:53:57Z,test/cpp/util/test_credentials_provider.h,"@@ -68,6 +74,10 @@ class CredentialsProvider {    // Provide a list of secure credentials type.   virtual std::vector<std::string> GetSecureCredentialsTypeList() = 0;++  // Provide a transport security type given a credentials type.+  virtual std::string GetTransportSecurityType(",I think this function can be removed from the class and made a standalone function (since it does not depend on the partial `CredentialProvider` implementation.),
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22628,458821675,2020-07-22T14:10:35Z,src/csharp/BUILD-INTEGRATION.md,"@@ -105,6 +105,39 @@ button](https://stackoverflow.com/a/9770061). Click on it, and choose ""Add as link"". If you do not select this option, Visual Studio will copy files to the project directory instead. +#### My .proto files have same filename in different folders++Starting from Grpc.Tools version 2.31, protocol buffers compilation preserves original folder structure for generated files. Eg.","That's not actually what https://github.com/grpc/grpc/pull/22869 does. We do use directory hashes to disambiguate the generated files.`ProjectFolder/obj/hash_generated_from_relative_path/Greet.cs`@Falco20019  to avoid confusion, maybe we should close this PR and you can open a new one with up-to-date description of what https://github.com/grpc/grpc/pull/22869 actually does?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23557,458899825,2020-07-22T15:56:07Z,test/cpp/util/grpc_tool.cc,"@@ -541,6 +541,8 @@ bool GrpcTool::CallMethod(int argc, const char** argv,     PrintMetadata(client_metadata, ""Sending client initial metadata:"");      CliCall call(channel, formatted_method_name, client_metadata);+    fprintf(stderr, ""New call for method_name:%s has peer address:|%s|\n"",",How about putting these messages behind a flag rather than logging them unconditionally?,X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23553,459199437,2020-07-23T03:43:09Z,src/core/lib/security/authorization/mock_cel_library/activation.h,"@@ -0,0 +1,106 @@+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_MOCK_CEL_LIBRARY_ACTIVATION_H+#define GRPC_CORE_LIB_SECURITY_AUTHORIZATION_MOCK_CEL_LIBRARY_ACTIVATION_H++#include <grpc/support/port_platform.h>++#include <vector>+#include ""absl/strings/string_view.h""++#include <google/protobuf/util/field_mask_util.h>++#include ""src/core/lib/security/authorization/mock_cel_library/cel_value.h""++namespace google {+namespace api {+namespace expr {+namespace runtime {++class CelAttributePattern {","We can remove this, since we won't be needing attribute pattern related member functions of activation.",
67390330,mohanli-ml,https://api.github.com/repos/grpc/grpc/pulls/23484,459229585,2020-07-23T06:01:13Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,539 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <gflags/gflags.h>+#include <grpc/grpc.h>+#include <grpc/support/port_platform.h>+#include <grpcpp/channel.h>+#include <grpcpp/client_context.h>+#include <grpcpp/create_channel.h>+#include <grpcpp/ext/channelz_service_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/security/credentials.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/server.h>+#include <grpcpp/server_builder.h>+#include <grpcpp/server_context.h>+#include <unistd.h>++#include <google/protobuf/text_format.h>+#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>+#include ""absl/strings/str_format.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;++using grpc::ClientContext;+using grpc::Status;++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;",Make sense. Have add deadline and status check for all RPCs.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23255,459381789,2020-07-23T11:28:00Z,templates/tools/dockerfile/test/sanity/Dockerfile.template,"@@ -19,7 +19,7 @@    #========================   # Sanity test dependencies-  RUN apt-get update && apt-get -t buster install -y python3.7 python3-all-dev+  RUN apt-get update && apt-get -t buster install -y python3.7 python3-all-dev gawk","I don't think we should add a new dependency into the dockerimage just because of this pretty specialized PR.If we can't make it happen with what's already in the sanity dockerimage, I'd say let's not do this at all.",
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/23439,460223087,2020-07-24T18:35:23Z,tools/run_tests/run_xds_tests.py,"@@ -56,17 +56,27 @@     'secondary_locality_gets_requests_on_primary_failure',     'traffic_splitting', ]+# Valid test cases, but not in all. So the tests can only run manually, and+# aren't enabled automatically for all languages. Move them into _TEST_CASES+# when support is ready in all languages.+_ALTERNATIVE_TEST_CASES = ['path_matching', 'header_matching']   def parse_test_cases(arg):-    if arg == 'all':-        return _TEST_CASES     if arg == '':         return []-    test_cases = arg.split(',')-    if all([test_case in _TEST_CASES for test_case in test_cases]):-        return test_cases-    raise Exception('Failed to parse test cases %s' % arg)+    arg_split = arg.split(',')+    test_cases = set()+    all_test_cases = _TEST_CASES + _ALTERNATIVE_TEST_CASES+    for arg in arg_split:+        if arg == ""all"":+            test_cases = test_cases.union(_TEST_CASES)+        else:+            test_cases = test_cases.union([arg])+    if not all([test_case in all_test_cases for test_case in test_cases]):","Is this comment just to invert `not`?The union is to avoid having duplicate tests (which will happen when the xds.sh runs `all,path_matching`, and we add `path_matching` into `all`)We will need `all_test_cases` later to perserve the test orders (not sure if the order will ever matter).",
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/23439,460226360,2020-07-24T18:42:24Z,tools/run_tests/run_xds_tests.py,"@@ -1504,22 +1713,41 @@ def __init__(self, compute, alpha_compute, project):             test_log_filename = os.path.join(log_dir, _SPONGE_LOG_NAME)             test_log_file = open(test_log_filename, 'w+')             client_process = None++            if test_case in _TESTS_TO_RUN_MULTIPLE_RPCS:+                run_all_rpcs = '--rpc=""UnaryCall,EmptyCall""'+            else:+                run_all_rpcs = '--rpc=""UnaryCall""'++            if test_case in _TESTS_TO_SEND_METADATA:+                send_metadata = '--metadata=""EmptyCall:{key}:{value}""'.format(+                    key=_TEST_METADATA_KEY, value=_TEST_METADATA_VALUE)+            else:+                send_metadata = '--metadata=""""'+             if test_case in _TESTS_TO_FAIL_ON_RPC_FAILURE:                 wait_for_config_propagation(                     gcp, instance_group,                     args.client_cmd.format(server_uri=server_uri,                                            stats_port=args.stats_port,                                            qps=args.qps,-                                           fail_on_failed_rpc=False),+                                           fail_on_failed_rpc=False,+                                           run_all_rpcs=run_all_rpcs,",`rpcs` is quite a short name. I was thinking it may conflict with other variables. Same reason `metadata` is named `send_metadata`,
28025951,HannahShiSFB,https://api.github.com/repos/grpc/grpc/pulls/23607,460232645,2020-07-24T18:56:28Z,src/php/bin/php_extension_doxygen_filter.awk,"@@ -12,38 +12,63 @@ # See the License for the specific language governing permissions and # limitations under the License. +function sed_gensub(regexp, replacement, how, target,     cmd_, ret_) {",the spaces in the parameter list are necessary to separate out local variables,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23553,460238351,2020-07-24T19:08:47Z,src/core/lib/security/authorization/mock_cel_library/cel_value.h,"@@ -0,0 +1,128 @@+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_MOCK_CEL_LIBRARY_CEL_VALUE_H+#define GRPC_CORE_LIB_SECURITY_AUTHORIZATION_MOCK_CEL_LIBRARY_CEL_VALUE_H++// CelValue is a holder, capable of storing all kinds of data+// supported by CEL.+// CelValue defines explicitly typed/named getters/setters.+// When storing pointers to objects, CelValue does not accept ownership+// to them and does not control their lifecycle. Instead objects are expected+// to be either external to expression evaluation, and controlled beyond the+// scope or to be allocated and associated with some allocation/ownership+// controller (Arena).+// Usage examples:+// (a) For primitive types:+//    CelValue value = CelValue::CreateInt64(1);+// (b) For string:+//    string* msg = google::protobuf::Arena::Create<string>(arena,""test"");+//    CelValue value = CelValue::CreateString(msg);+// (c) For messages:+//    const MyMessage * msg =+//    google::protobuf::Arena::CreateMessage<MyMessage>(arena); CelValue value =+//    CelValue::CreateMessage(msg, &arena);++#include <grpc/support/port_platform.h>++#include ""absl/status/status.h""+#include ""absl/types/optional.h""++#include <google/protobuf/message.h>++#include ""src/core/lib/security/authorization/mock_cel_library/cel_value_internal.h""++namespace google {+namespace api {+namespace expr {+namespace runtime {++using CelError = absl::Status;++// Break cyclic depdendencies for container types.+class CelList;+class CelMap;+class UnknownSet;++class CelValue {+ public:+  // Default constructor.+  // Creates CelValue with null data type.+  CelValue() : CelValue(nullptr) {}++  // We will use factory methods instead of public constructors+  // The reason for this is the high risk of implicit type conversions+  // between bool/int/pointer types.+  // We rely on copy elision to avoid extra copying.+  static CelValue CreateNull() { return CelValue(nullptr); }++  static CelValue CreateBool(bool value) { return CreateNull(); }++  static CelValue CreateInt64(int64_t value) { return CreateNull(); }++  static CelValue CreateUint64(uint64_t value) { return CreateNull(); }++  static CelValue CreateDouble(double value) { return CreateNull(); }++  static CelValue CreateString(StringHolder holder) { return CreateNull(); }++  static CelValue CreateStringView(absl::string_view value) {+    return CreateNull();+  }++  static CelValue CreateString(const std::string* str) { return CreateNull(); }++  static CelValue CreateBytes(BytesHolder holder) { return CreateNull(); }++  static CelValue CreateBytesView(absl::string_view value) {+    return CreateNull();+  }++  static CelValue CreateBytes(const std::string* str) { return CreateNull(); }++  // CreateMessage creates CelValue from google::protobuf::Message.+  // As some of CEL basic types are subclassing google::protobuf::Message,+  // this method contains type checking and downcasts.+  static CelValue CreateMessage(const google::protobuf::Message* value,+                                google::protobuf::Arena* arena) {+    return CreateNull();+  }",We can't have protobuf dependency. Will we need this function?,
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/23439,460243829,2020-07-24T19:21:07Z,tools/run_tests/run_xds_tests.py,"@@ -1504,22 +1713,41 @@ def __init__(self, compute, alpha_compute, project):             test_log_filename = os.path.join(log_dir, _SPONGE_LOG_NAME)             test_log_file = open(test_log_filename, 'w+')             client_process = None++            if test_case in _TESTS_TO_RUN_MULTIPLE_RPCS:+                run_all_rpcs = '--rpc=""UnaryCall,EmptyCall""'+            else:+                run_all_rpcs = '--rpc=""UnaryCall""'++            if test_case in _TESTS_TO_SEND_METADATA:+                send_metadata = '--metadata=""EmptyCall:{key}:{value}""'.format(+                    key=_TEST_METADATA_KEY, value=_TEST_METADATA_VALUE)+            else:+                send_metadata = '--metadata=""""'+             if test_case in _TESTS_TO_FAIL_ON_RPC_FAILURE:                 wait_for_config_propagation(                     gcp, instance_group,                     args.client_cmd.format(server_uri=server_uri,                                            stats_port=args.stats_port,                                            qps=args.qps,-                                           fail_on_failed_rpc=False),+                                           fail_on_failed_rpc=False,+                                           run_all_rpcs=run_all_rpcs,",Renamed to `rpcs_to_send` and `metadata_to_send`,
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/23527,461105561,2020-07-27T19:03:52Z,include/grpc/grpc_security.h,"@@ -240,6 +240,24 @@ GRPCAPI grpc_channel_credentials* grpc_ssl_credentials_create(     const char* pem_root_certs, grpc_ssl_pem_key_cert_pair* pem_key_cert_pair,     const verify_peer_options* verify_options, void* reserved); +/** Sets the minimum TLS version that will be negotiated during the TLS+   handshake. The caller MUST ensure that |creds| is an instance of","nit: it is not a grpc tradition to wrap parameter with ""|"" in comment. you can just remove ""|"".",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/23553,461221210,2020-07-27T23:08:48Z,src/core/lib/security/authorization/mock_cel/activation.h,"@@ -0,0 +1,61 @@+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_MOCK_CEL_ACTIVATION_H+#define GRPC_CORE_LIB_SECURITY_AUTHORIZATION_MOCK_CEL_ACTIVATION_H++#include <grpc/support/port_platform.h>++#include ""src/core/lib/security/authorization/mock_cel/cel_value.h""++namespace google {+namespace api {+namespace expr {+namespace runtime {++// Base class for an activation.","Add a comment that```// Base class for an activation. This is a temporary stub implementation of CEL APIs.// Once gRPC imports the CEL library, this class will be removed.```Similarly for Activation and CelValue class.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23634,461394370,2020-07-28T08:04:03Z,tools/codegen/core/gen_upb_api.sh,"@@ -124,3 +124,18 @@ do     --upb_out=$UPB_OUTPUT_DIR \     --plugin=protoc-gen-upb=$UPB_PLUGIN done++# In PHP build Makefile, the files with .upb.c suffix collide .upbdefs.c suffix due to a PHP buildsystem bug.+# Work around this by placing the generated files with "".upbdefs.c"" suffix under a different directory.+# See https://github.com/grpc/grpc/issues/23307++# move all .upbdefs.c files from under src/core/ext/upb-generated to src/core/ext/upbdefs-generated+UPBDEFS_OUTPUT_DIR=$PWD/src/core/ext/upbdefs-generated+rm -rf $UPBDEFS_OUTPUT_DIR","Good point. I didn't want to get into details like that in the first iteration of the PR as it's just a technicality.We could either:- stop supporting the `$1` argument (which I'd be in favor of if this option it's used very often as these special cases add complexity to the script and I'd like to avoid unnecessary complexity as much as possible).- make sure the $1 still works fine (doable, it's just a technicality).",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23648,461652783,2020-07-28T15:01:25Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -567,6 +595,16 @@ def _generate_build_extra_metadata_for_tests(tests, bazel_rules):     return test_metadata  +def _detect_and_print_issues(build_yaml_like):+    """"""Try detecting some unusual situations and warn about them.""""""+    for tgt in build_yaml_like['targets']:+        if tgt['build'] == 'test':+            for src in tgt['src']:+                if src.startswith('src/') and not src.endswith('.proto'):+                    print('source file from under ""src/"" tree used in test ' ++                          tgt['name'] + ': ' + src)++ # extra metadata that will be used to construct build.yaml","It seems strange to hard-code all of this data inside of this script instead of putting it in an external data file somewhere.  Could this be moved to build_handwritten.yaml, or something similar?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23648,461654584,2020-07-28T15:03:57Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -410,8 +437,8 @@ def _extract_cc_tests(bazel_rules):     return list(sorted(result))  -def _filter_cc_tests(tests):-    """"""Filters out tests that we don't want or we cannot build them reasonably""""""+def _exclude_unwanted_cc_tests(tests):+    """"""Filters out bazel tests that we don't want to run with other build systems or we cannot build them reasonably""""""      # most qps tests are autogenerated, we are fine without them","It would be nice to move the specific cases we're excluding here out of this file and into a data file, so that the specific inputs can be modified without changing this script.Alternatively, why do we still need any of the C/C++ tests to run in non-bazel builds?  Couldn't we just ignore all tests here?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23648,461655240,2020-07-28T15:04:53Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -937,31 +975,86 @@ def _generate_build_extra_metadata_for_tests(tests, bazel_rules):     'deps(""//src/proto/..."")', ] +# Step 1: run a bunch of ""bazel query --output xml"" queries to collect+# the raw build metadata from the bazel build.+# At the end of this step we will have a dictionary of bazel rules+# that are interesting to us (libraries, binaries, etc.) along+# with their most important metadata (sources, headers, dependencies) bazel_rules = {} for query in _BAZEL_DEPS_QUERIES:     bazel_rules.update(         _extract_rules_from_bazel_xml(_bazel_query_xml_tree(query))) +# Step 1a: Knowing the transitive closure of dependencies will make+# the postprocessing simpler, so compute the info for all our rules. _populate_transitive_deps(bazel_rules) -tests = _filter_cc_tests(_extract_cc_tests(bazel_rules))-test_metadata = _generate_build_extra_metadata_for_tests(tests, bazel_rules)--all_metadata = {}-all_metadata.update(_BUILD_EXTRA_METADATA)-all_metadata.update(test_metadata)--all_targets_dict = _generate_build_metadata(all_metadata, bazel_rules)+# Step 2: Extract the known bazel cc_test tests. While most tests+# will be buildable with other build systems just fine, some of these tests+# would be too difficult to build and run with other build systems,+# so we simply the ones we don't want.+tests = _exclude_unwanted_cc_tests(_extract_cc_tests(bazel_rules))++# Step 3: Generate the ""extra metadata"" for all our build targets.+# While the bazel rules give us most of the information we need,+# the legacy ""build.yaml"" format requires some additional fields that",Can you list the specific extra fields that need to be populated and what each of them indicates?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23648,461656228,2020-07-28T15:06:10Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -937,31 +975,86 @@ def _generate_build_extra_metadata_for_tests(tests, bazel_rules):     'deps(""//src/proto/..."")', ] +# Step 1: run a bunch of ""bazel query --output xml"" queries to collect+# the raw build metadata from the bazel build.+# At the end of this step we will have a dictionary of bazel rules+# that are interesting to us (libraries, binaries, etc.) along+# with their most important metadata (sources, headers, dependencies) bazel_rules = {} for query in _BAZEL_DEPS_QUERIES:     bazel_rules.update(         _extract_rules_from_bazel_xml(_bazel_query_xml_tree(query))) +# Step 1a: Knowing the transitive closure of dependencies will make+# the postprocessing simpler, so compute the info for all our rules.","I assume this step modifies `bazel_rules` in place.  Can you show an example of how a given rule might be modified, so a reader can tell what the data should look like at this point?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23648,461667956,2020-07-28T15:22:24Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -937,31 +975,86 @@ def _generate_build_extra_metadata_for_tests(tests, bazel_rules):     'deps(""//src/proto/..."")', ] +# Step 1: run a bunch of ""bazel query --output xml"" queries to collect+# the raw build metadata from the bazel build.+# At the end of this step we will have a dictionary of bazel rules+# that are interesting to us (libraries, binaries, etc.) along+# with their most important metadata (sources, headers, dependencies) bazel_rules = {} for query in _BAZEL_DEPS_QUERIES:     bazel_rules.update(         _extract_rules_from_bazel_xml(_bazel_query_xml_tree(query))) +# Step 1a: Knowing the transitive closure of dependencies will make+# the postprocessing simpler, so compute the info for all our rules. _populate_transitive_deps(bazel_rules) -tests = _filter_cc_tests(_extract_cc_tests(bazel_rules))-test_metadata = _generate_build_extra_metadata_for_tests(tests, bazel_rules)--all_metadata = {}-all_metadata.update(_BUILD_EXTRA_METADATA)-all_metadata.update(test_metadata)--all_targets_dict = _generate_build_metadata(all_metadata, bazel_rules)+# Step 2: Extract the known bazel cc_test tests. While most tests+# will be buildable with other build systems just fine, some of these tests+# would be too difficult to build and run with other build systems,+# so we simply the ones we don't want.+tests = _exclude_unwanted_cc_tests(_extract_cc_tests(bazel_rules))++# Step 3: Generate the ""extra metadata"" for all our build targets.+# While the bazel rules give us most of the information we need,+# the legacy ""build.yaml"" format requires some additional fields that+# we cannot get just from bazel alone (we call that ""extra metadata"").+# In this step, we basically analyze the build metadata we have from bazel+# and use heuristics to determine (and sometimes guess) the right+# extra metadata to use for each target.+#+# - For some targets (such as the public libraries, helper libraries+#   and executables) determining the right extra metadata is hard to do+#   automatically. For these targets, the extra metadata is supplied ""manually""+#   in form of the _BUILD_EXTRA_METADATA dictionary. That allows us to match+#   the semantics of the legacy ""build.yaml"" as closely as possible.+#+# - For test binaries, it is possible to generate the ""extra metadata"" mostly+#   automatically using a rule-based heuristic approach because most tests+#   look and behave alike from the build's perspective.+#+# TODO(jtattermusch): Of course neither ""_BUILD_EXTRA_METADATA"" or+# the heuristic approach used for tests are ideal and they cannot be made+# to cover all possible situations (and are tailored to work with the way+# the grpc build currently works), but the idea was to start with something+# reasonably simple that matches the ""build.yaml""-like semantics as closely+# as possible (to avoid changing too many things at once) and gradually get+# rid of the legacy ""build.yaml""-specific fields one by one. Once that is done,+# only very little ""extra metadata"" would be needed and/or it would be trivial+# to generate it automatically.+all_extra_metadata = {}+all_extra_metadata.update(_BUILD_EXTRA_METADATA)+all_extra_metadata.update(+    _generate_build_extra_metadata_for_tests(tests, bazel_rules))++# Step 4: Generate the final metadata for all the targets.+# This is done by combining the bazel build metadata and the ""extra metadata""+# we obtained in the previous step.+# In this step, we also perform some interesting massaging of the target metadata+# to end up with a result that is as similar to the legacy build.yaml data+# as possible.+# - Some targets get renamed (to match the legacy build.yaml target names)","Basically I think it's these reasons:- in the original PR I wanted to avoid changing too many things at the same time and avoid people getting confused by some well know targets suddenly being missing (the PR was already a big conceptual change)- Makefile/cmake and also language-specific generators rely on some build targets being called exactly as they are.- I wanted to test to have similar names as before (otherwise they would be called like `test_cpp_TEST_NAME` because some bazel targets are not top-level.- many of our other scripts invoke executables (e.g. ""qps_json_driver"") by their name. Without renaming, the target binary name would have been `test_cpp_qps_qps_json_driver` which is both ugly and would also have broken our tests.The renaming is something that can be revisited in the future.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23648,461675806,2020-07-28T15:33:13Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -410,8 +437,8 @@ def _extract_cc_tests(bazel_rules):     return list(sorted(result))  -def _filter_cc_tests(tests):-    """"""Filters out tests that we don't want or we cannot build them reasonably""""""+def _exclude_unwanted_cc_tests(tests):+    """"""Filters out bazel tests that we don't want to run with other build systems or we cannot build them reasonably""""""      # most qps tests are autogenerated, we are fine without them","In the long run, we probably can remove some C/C++ tests in  non-bazel builds but there are some concerns:- if the Make/cmake builds don't have any tests at all, how can we be sure what they've build actually works? (so we need at least some ""smoke tests""). Btw, the build flags between bazel / non-bazel flag might differ (sometimes it's for interesting reasons that are not easy to overcome) which makes it even more important to have at least some tests for make/cmake.- our current portability suite actually runs make/cmake tests and we'd need to migrate it to use bazel first (which brings the question whether it's possible to get a good enough coverage of different compilers / distros etc, with bazel - it's something we haven't really tried yet).- some things that are considered ""tests"" in build.yaml based builds are actually binaries you'd want to be able to build anyway (qps_json_worker, interop_client, interop_server, grpc_cli) so it's unclear how much make/cmake simplification we would gain by removing just some (but not all) tests.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23648,461690025,2020-07-28T15:53:03Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -937,31 +975,86 @@ def _generate_build_extra_metadata_for_tests(tests, bazel_rules):     'deps(""//src/proto/..."")', ] +# Step 1: run a bunch of ""bazel query --output xml"" queries to collect+# the raw build metadata from the bazel build.+# At the end of this step we will have a dictionary of bazel rules+# that are interesting to us (libraries, binaries, etc.) along+# with their most important metadata (sources, headers, dependencies) bazel_rules = {} for query in _BAZEL_DEPS_QUERIES:     bazel_rules.update(         _extract_rules_from_bazel_xml(_bazel_query_xml_tree(query))) +# Step 1a: Knowing the transitive closure of dependencies will make+# the postprocessing simpler, so compute the info for all our rules. _populate_transitive_deps(bazel_rules) -tests = _filter_cc_tests(_extract_cc_tests(bazel_rules))-test_metadata = _generate_build_extra_metadata_for_tests(tests, bazel_rules)--all_metadata = {}-all_metadata.update(_BUILD_EXTRA_METADATA)-all_metadata.update(test_metadata)--all_targets_dict = _generate_build_metadata(all_metadata, bazel_rules)+# Step 2: Extract the known bazel cc_test tests. While most tests+# will be buildable with other build systems just fine, some of these tests+# would be too difficult to build and run with other build systems,+# so we simply the ones we don't want.+tests = _exclude_unwanted_cc_tests(_extract_cc_tests(bazel_rules))++# Step 3: Generate the ""extra metadata"" for all our build targets.+# While the bazel rules give us most of the information we need,+# the legacy ""build.yaml"" format requires some additional fields that","The fields being set and explanation for some of them is here:https://github.com/grpc/grpc/blob/9e095b2076e1df72d2241128b9c6714d4cc8264b/tools/buildgen/extract_metadata_from_bazel_xml.py#L575General documentation of ""build.yaml"" contents is here:https://github.com/grpc/grpc/blob/master/templates/README.mdThat is good enough for general understanding what the fields mean, but the docs might be out of date or incorrect in some cases (build.yaml has been with us from pretty much the project start). While some of the field name seem self-explanatory, I don't really have a way to know what exactly the fields are supposed to mean because they way the they were added to `build.yaml` was mostly random and they were added on ad-hoc and as-needed fashion. So when someone needed a new field to achieve something in our build system, they added a new attribute and the exact meaning was defined implicitly by implementing its behavior in our build file generators.For the above reasons, there's actually no clearly defined list of extra fields that need to be populated - it's basically empirical (I basically just tried to match exactly what was in the original build.yaml because that was the only way to prevent things from exploding - there are too many special cases in our build generators to address all of them at once).In the future, the right thing to do is to revisit the ""extra metadata"" fields being used in _BUILD_EXTRA_METADATA and get rid of all of them one by one  (either by removing the need for a field entirely or somehow adding the information to the bazel rule so we can consume it) - unlike before, this is now possible because bazel build is now our source of truth and the big bazel related changes have already settled down (and most builds are mostly green).",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23639,462553186,2020-07-29T20:00:54Z,test/distrib/ruby/run_distrib_test.sh,"@@ -17,12 +17,26 @@ set -ex  cd ""$(dirname ""$0"")"" -ARCH=$1-PLATFORM=$2+function die {+  echo ""$1""+  exit 1+}++ARCH=""$1""+PLATFORM=""$2""+PACKAGE_TYPE=""$3""+echo ""$EXTERNAL_GIT_ROOT""+GRPC_VERSION=""$(ruby -e 'require ENV[""EXTERNAL_GIT_ROOT""] + ""/src/ruby/lib/grpc/version.rb""; puts GRPC::VERSION')""+if [[ ""$PACKAGE_TYPE"" == ""source"" ]]; then+  GEM_NAME=""grpc-${GRPC_VERSION}.gem""+else+  [[ ""$PACKAGE_TYPE"" == ""binary"" ]] || die ""unexpeced package type: $PACKAGE_TYPE""+  GEM_NAME=""grpc-${GRPC_VERSION}-${ARCH}-${PLATFORM}.gem""+fi # Create an indexed local gem source with gRPC gems to test GEM_SOURCE=../../../gem_source mkdir -p ""${GEM_SOURCE}/gems""-cp ""$EXTERNAL_GIT_ROOT""/input_artifacts/grpc-*""$ARCH-$PLATFORM"".gem ""${GEM_SOURCE}/gems""+cp ""${EXTERNAL_GIT_ROOT}/input_artifacts/${GEM_NAME}"" ""${GEM_SOURCE}/gems""","Yep, it's built in the artifacts job that is chained to the distrib tests job as a pre-req, and passed to this job via this `EXTERNAL_GIT_ROOT/input_artifacts` directory.",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23665,462669603,2020-07-30T00:46:43Z,src/core/lib/security/authorization/eval_args_util.cc,"@@ -0,0 +1,94 @@+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_EVAL_ARGS_UTIL_CC",I suggest renaming the file to authorization_engine_util instead. We can then use this file to add other helper methods that may be required for AuthorizationEngine.,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23665,462680984,2020-07-30T01:30:01Z,src/core/lib/security/authorization/eval_args_util.cc,"@@ -0,0 +1,94 @@+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_EVAL_ARGS_UTIL_CC+#define GRPC_CORE_LIB_SECURITY_AUTHORIZATION_EVAL_ARGS_UTIL_CC++#include ""src/core/lib/security/authorization/eval_args_util.h""++grpc::string_ref get_key(grpc_linked_mdelem* elem) {+  const grpc_slice& key = GRPC_MDKEY(elem->md);+  return grpc::string_ref(reinterpret_cast<const char*>(+      GRPC_SLICE_START_PTR(key), GRPC_SLICE_LENGTH(key)));+}++grpc::string_ref get_value(grpc_linked_mdelem* elem) {+  const grpc_slice& val = GRPC_MDVALUE(elem->md);+  return grpc::string_ref(reinterpret_cast<const char*>(+      GRPC_SLICE_START_PTR(val), GRPC_SLICE_LENGTH(val)));+}++grpc::string_ref get_path(grpc_metadata_batch* metadata) {+  grpc_linked_mdelem* path = metadata->idx.array[GRPC_BATCH_PATH];+  return get_value(path);+}","Can we use string_view(more common) here and for rest of the functions instead of string_ref? Is there any benefit of using the latter?Something like```absl::string_view get_path_from_metadata(const grpc_metadata_batch* metadata) {  absl::string_view path;  if(metadata!=nullptr && metadata->idx.named.path != nullptr) {    path = absl::string_view(reinterpret_cast<const char*>(      GRPC_SLICE_START_PTR(GRPC_MDVALUE(metadata->idx.named.path->md))),        GRPC_SLICE_LENGTH(GRPC_MDVALUE(metadata->idx.named.path->md)));  }  return path;}```The snippet above has some other recommendations too1. We want to check if the field is set in the input struct.2. Maybe rename the functions to get_path_from_metadata? Similarly for the other functions.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23620,463166562,2020-07-30T17:46:23Z,tools/run_tests/run_tests.py,"@@ -926,11 +926,31 @@ def test_specs(self):                                  timeout_seconds=10 * 60,                                  environ=_FORCE_ENVIRON_FOR_WRAPPERS)         ]-        tests.append(-            self.config.job_spec(-                ['tools/run_tests/helper_scripts/run_ruby_end2end_tests.sh'],-                timeout_seconds=20 * 60,-                environ=_FORCE_ENVIRON_FOR_WRAPPERS))+        for test in [+                'src/ruby/end2end/sig_handling_test.rb',","There isn't. But there wasn't a way to be sure this this previously when test cases were listed in the shell script, either.One thing we could do which could perhaps clean this up and avoid listing test cases here is for `run_tests.py` to run every ruby script under `src/ruby/end2end` that ends with `_test.rb`.Any thoughts?",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/23694,463345923,2020-07-31T00:41:06Z,test/core/tsi/alts/handshaker/alts_concurrent_connectivity_test.cc,"@@ -334,9 +334,17 @@ class FakeTcpServer {     CLOSE_SOCKET,   }; -  FakeTcpServer(+  enum AcceptMode {","I think the C++ style guide suggests using enum classes https://google.github.io/styleguide/cppguide.html#Enumerator_Namesenum class AcceptMode { kWaitForClientToSendFirstBytes, kEagerlySendSettings };",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23665,463744631,2020-07-31T17:43:23Z,src/core/lib/security/authorization/evaluate_args.h,"@@ -0,0 +1,54 @@+//+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//+//++#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_EVALUATE_ARGS_H+#define GRPC_CORE_LIB_SECURITY_AUTHORIZATION_EVALUATE_ARGS_H++#include <map>++#include ""src/core/lib/iomgr/endpoint.h""+#include ""src/core/lib/security/context/security_context.h""+#include ""src/core/lib/transport/metadata_batch.h""++namespace grpc_core {++class EvaluateArgs {","EvaluateArgs is the struct name with metadata, auth and endpoint fields. That is different from this class. Please choose a name that represents what this class does. Ex. AuthorizationAttributeProvider as Mark suggested or something similar.",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23665,463745476,2020-07-31T17:45:04Z,src/core/lib/security/authorization/evaluate_args.h,"@@ -0,0 +1,54 @@+//+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//+//++#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_EVALUATE_ARGS_H+#define GRPC_CORE_LIB_SECURITY_AUTHORIZATION_EVALUATE_ARGS_H++#include <map>++#include ""src/core/lib/iomgr/endpoint.h""+#include ""src/core/lib/security/context/security_context.h""+#include ""src/core/lib/transport/metadata_batch.h""++namespace grpc_core {++class EvaluateArgs {+  public:+    EvaluateArgs(+      grpc_metadata_batch* metadata,+      grpc_auth_context* auth_context,+      grpc_endpoint* endpoint);++    absl::string_view get_path_from_metadata();","The naming gets too long for some functions, so I will take back my previous recommendation. Sorry about that. You can remove ""_from_metadata"" suffix. Similarly for rest of the functions.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23648,464958929,2020-08-04T10:38:23Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -13,6 +13,15 @@ # See the License for the specific language governing permissions and # limitations under the License. +# Script to extract build metadata from bazel BUILD.+# To avoid having two sources of truth for the build metadata (build+# targets, source files, header files etc.), this script analyzes the contents+# of bazel BUILD files and generates a YAML file (currently called+# build_autogenerated.yaml). The format and semantics of the generated YAML files+# is chosen to match the format of a ""build.yaml"" file, which used","General documentation of ""build.yaml"" contents is here:https://github.com/grpc/grpc/blob/master/templates/README.md, but as noted in another comment, there is really no spec and meaning of some of the constructs is defined empirically.I do not plan to provide better documentation for the legacy build.yaml format as the plan is to simplify build.yaml significantly or get rid of it entirely in the future.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23648,464962190,2020-08-04T10:45:13Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -312,22 +328,33 @@ def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):   def _generate_build_metadata(build_extra_metadata, bazel_rules):+    """"""Generate build metadata in build.yaml-like format bazel build metadata and build.yaml-specific ""extra metadata"".""""""     lib_names = build_extra_metadata.keys()     result = {}      for lib_name in lib_names:         lib_dict = _create_target_from_bazel_rule(lib_name, bazel_rules) +        # Figure out the final list of headers and sources for given target.+        # While this is mostly based on bazel build metadata, build.yaml does+        # not necessarily expose all the targets that are present in bazel build.+        # These ""intermediate dependencies"" might get flattened.+        # TODO(jtattermusch): This is done to avoid introducing too many intermediate+        # libraries into the build.yaml-based builds (which might in turn cause issues+        # building language-specific artifacts). The need for elision (and expansion)+        # of intermediate libraries can be re-evaluated in the future.         _expand_intermediate_deps(lib_dict, lib_names, bazel_rules) -        # populate extra properties from build metadata+        # populate extra properties from the build.yaml-specific ""extra metadata""         lib_dict.update(build_extra_metadata.get(lib_name, {}))          # store to results         result[lib_name] = lib_dict -    # rename some targets to something else-    # this needs to be made after we're done with most of processing logic+    # Rename targets marked with ""_RENAME"" extra metadata.+    # This is mostly a cosmetic change to ensure that we end up with build.yaml target",The need for renaming is explained in another comment (line 1036),
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23620,465067726,2020-08-04T13:55:12Z,tools/run_tests/run_tests.py,"@@ -926,11 +926,31 @@ def test_specs(self):                                  timeout_seconds=10 * 60,                                  environ=_FORCE_ENVIRON_FOR_WRAPPERS)         ]-        tests.append(-            self.config.job_spec(-                ['tools/run_tests/helper_scripts/run_ruby_end2end_tests.sh'],-                timeout_seconds=20 * 60,-                environ=_FORCE_ENVIRON_FOR_WRAPPERS))+        for test in [+                'src/ruby/end2end/sig_handling_test.rb',","What I ended up doing in C# is to add an extra C# test (""c# sanity test"") that only does one thing - lists all the existing C# tests and makes sure that matches https://github.com/grpc/grpc/blob/master/src/csharp/tests.json (which is where run_tests.py gets the list of C# tests from).https://github.com/grpc/grpc/blob/fc4c9d74eebfdfbf99f181ebfd377f6a01db2946/src/csharp/Grpc.Core.Tests/SanityTest.cs#L44For ruby you could do something similar - have a ruby tests.json, use it in run_tests.py and also check its contents by a dedicated ruby test (which reads the .json and also lists all the *_test.rb files and compares the two sets).",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/23740,466946596,2020-08-07T10:04:44Z,tools/internal_ci/linux/grpc_xds_csharp_test_in_docker.sh,"@@ -48,9 +48,14 @@ touch ""$TOOLS_DIR""/src/proto/grpc/testing/__init__.py  python tools/run_tests/run_tests.py -l csharp -c opt --build_only +# Test cases ""path_matching"" and ""header_matching"" are not included in ""all"",+# because not all interop clients in all languages support these new tests.+#+# TODO(jtattermusch): remove ""path_matching"" and ""header_matching"" from+# --test_case after they are added into ""all"". GRPC_VERBOSITY=debug GRPC_TRACE=xds_client,xds_resolver,xds_routing_lb,cds_lb,eds_lb,priority_lb,weighted_target_lb,lrs_lb ""$PYTHON"" \   tools/run_tests/run_xds_tests.py \-    --test_case=all \+    --test_case=""all,path_matching,header_matching"" \     --project_id=grpc-testing \     --source_image=projects/grpc-testing/global/images/xds-test-server \     --path_to_server_binary=/java_server/grpc-java/interop-testing/build/install/grpc-interop-testing/bin/xds-test-server \","Do we need to fix the `--client_cmd` (3 lines below this), to include these new parameters `{fail_on_failed_rpc} {rpcs_to_send} {metadata_to_send}`?C++: https://github.com/grpc/grpc/blob/master/tools/internal_ci/linux/grpc_xds_bazel_test_in_docker.sh#L64-L66, and Go: https://github.com/grpc/grpc-go/blob/master/test/kokoro/xds.sh#L40-L42.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/23740,467183980,2020-08-07T17:48:52Z,tools/internal_ci/linux/grpc_xds_csharp_test_in_docker.sh,"@@ -48,9 +48,14 @@ touch ""$TOOLS_DIR""/src/proto/grpc/testing/__init__.py  python tools/run_tests/run_tests.py -l csharp -c opt --build_only +# Test cases ""path_matching"" and ""header_matching"" are not included in ""all"",+# because not all interop clients in all languages support these new tests.+#+# TODO(jtattermusch): remove ""path_matching"" and ""header_matching"" from+# --test_case after they are added into ""all"". GRPC_VERBOSITY=debug GRPC_TRACE=xds_client,xds_resolver,xds_routing_lb,cds_lb,eds_lb,priority_lb,weighted_target_lb,lrs_lb ""$PYTHON"" \   tools/run_tests/run_xds_tests.py \-    --test_case=all \+    --test_case=""all,path_matching,header_matching"" \     --project_id=grpc-testing \     --source_image=projects/grpc-testing/global/images/xds-test-server \     --path_to_server_binary=/java_server/grpc-java/interop-testing/build/install/grpc-interop-testing/bin/xds-test-server \",`{rpcs_to_send}` and `{metadata_to_send}` are necessary here,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/23766,467198971,2020-08-07T18:20:01Z,src/proto/grpc/tls/provider/meshca/meshca.proto,"@@ -0,0 +1,52 @@+// Copyright 2019 Istio Authors. All Rights Reserved.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++syntax = ""proto3"";++package google.security.meshca.v1;","I think that for now, we should be using the copy of this from the grpc-proto repo:https://github.com/grpc/grpc-proto/blob/master/grpc/tls/provider/meshca/experimental/config.proto",
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/23740,467204287,2020-08-07T18:31:32Z,tools/internal_ci/linux/grpc_xds_csharp_test_in_docker.sh,"@@ -48,9 +48,14 @@ touch ""$TOOLS_DIR""/src/proto/grpc/testing/__init__.py  python tools/run_tests/run_tests.py -l csharp -c opt --build_only +# Test cases ""path_matching"" and ""header_matching"" are not included in ""all"",+# because not all interop clients in all languages support these new tests.+#+# TODO(jtattermusch): remove ""path_matching"" and ""header_matching"" from+# --test_case after they are added into ""all"". GRPC_VERBOSITY=debug GRPC_TRACE=xds_client,xds_resolver,xds_routing_lb,cds_lb,eds_lb,priority_lb,weighted_target_lb,lrs_lb ""$PYTHON"" \   tools/run_tests/run_xds_tests.py \-    --test_case=all \+    --test_case=""all,path_matching,header_matching"" \     --project_id=grpc-testing \     --source_image=projects/grpc-testing/global/images/xds-test-server \     --path_to_server_binary=/java_server/grpc-java/interop-testing/build/install/grpc-interop-testing/bin/xds-test-server \","Yes, the parameters are necessary. And also need to change `--source_image` to `projects/grpc-testing/global/images/xds-test-server-2` (Append `-2`)https://github.com/grpc/grpc-go/pull/3764/files",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21458,468776034,2020-08-11T18:20:47Z,tools/distrib/python/grpcio_tools/grpc_tools/main.cc,"@@ -36,3 +49,137 @@ int protoc_main(int argc, char* argv[]) {    return cli.Run(argc, argv); }++namespace internal {++class GeneratorContextImpl+    : public ::google::protobuf::compiler::GeneratorContext {+ public:+  GeneratorContextImpl(+      const std::vector<const ::google::protobuf::FileDescriptor*>&+          parsed_files,+      std::vector<std::pair<std::string, std::string>>* files_out)+      : files_(files_out), parsed_files_(parsed_files) {}++  ::google::protobuf::io::ZeroCopyOutputStream* Open(+      const std::string& filename) {+    files_->emplace_back(filename, """");+    return new ::google::protobuf::io::StringOutputStream(+        &(files_->back().second));+  }++  // NOTE(rbellevi): Equivalent to Open, since all files start out empty.+  ::google::protobuf::io::ZeroCopyOutputStream* OpenForAppend(+      const std::string& filename) {+    return Open(filename);+  }++  // NOTE(rbellevi): Equivalent to Open, since all files start out empty.+  ::google::protobuf::io::ZeroCopyOutputStream* OpenForInsert(+      const std::string& filename, const std::string& insertion_point) {+    return Open(filename);+  }++  void ListParsedFiles(+      std::vector<const ::google::protobuf::FileDescriptor*>* output) {+    *output = parsed_files_;+  }++ private:+  std::vector<std::pair<std::string, std::string>>* files_;+  const std::vector<const ::google::protobuf::FileDescriptor*>& parsed_files_;+};++class ErrorCollectorImpl+    : public ::google::protobuf::compiler::MultiFileErrorCollector {+ public:+  ErrorCollectorImpl(std::vector<::grpc_tools::ProtocError>* errors,+                     std::vector<::grpc_tools::ProtocWarning>* warnings)+      : errors_(errors), warnings_(warnings) {}++  void AddError(const std::string& filename, int line, int column,+                const std::string& message) {+    errors_->emplace_back(filename, line, column, message);+  }++  void AddWarning(const std::string& filename, int line, int column,+                  const std::string& message) {+    warnings_->emplace_back(filename, line, column, message);+  }++ private:+  std::vector<::grpc_tools::ProtocError>* errors_;+  std::vector<::grpc_tools::ProtocWarning>* warnings_;+};++static void calculate_transitive_closure(+    const ::google::protobuf::FileDescriptor* descriptor,+    std::vector<const ::google::protobuf::FileDescriptor*>* transitive_closure,+    std::unordered_set<const ::google::protobuf::FileDescriptor*>* visited) {+  for (int i = 0; i < descriptor->dependency_count(); ++i) {+    const ::google::protobuf::FileDescriptor* dependency =+        descriptor->dependency(i);+    if (visited->find(dependency) == visited->end()) {+      calculate_transitive_closure(dependency, transitive_closure, visited);+    }+  }+  transitive_closure->push_back(descriptor);+  visited->insert(descriptor);+}","Discussed offline (months ago). This is not actually removing duplicates. This is an auxiliary data structure that reduces the asymptotic complexity of the overall algorithm. There *is* something similar in the command-line portion of the protoc codebase, but it's less optimal and not accessible as a programmatic API.",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23786,468805155,2020-08-11T19:10:15Z,src/core/lib/security/authorization/evaluate_args.h,"@@ -0,0 +1,57 @@++// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_EVALUATE_ARGS_H+#define GRPC_CORE_LIB_SECURITY_AUTHORIZATION_EVALUATE_ARGS_H++#include <map>++#include ""src/core/ext/filters/client_channel/parse_address.h""+#include ""src/core/lib/iomgr/endpoint.h""+#include ""src/core/lib/iomgr/resolve_address.h""+#include ""src/core/lib/iomgr/sockaddr_utils.h""+#include ""src/core/lib/security/context/security_context.h""+#include ""src/core/lib/transport/metadata_batch.h""++namespace grpc_core {++class EvaluateArgs {+ public:+  EvaluateArgs(grpc_metadata_batch* metadata, grpc_auth_context* auth_context,+               grpc_endpoint* endpoint)+      : metadata_(metadata), auth_context_(auth_context), endpoint_(endpoint) {}++  absl::string_view path() const;+  absl::string_view host() const;+  absl::string_view method() const;+  std::multimap<absl::string_view, absl::string_view> headers() const;+  absl::string_view sourceAddress() const;+  int sourcePort() const;+  absl::string_view destinationAddress() const;+  int destinationPort() const;+  absl::string_view spiffeID() const;+  absl::string_view certServerName() const;",Follow C++ function naming stylehttps://google.github.io/styleguide/cppguide.html#Function_Namesex. GetSourcePort,
66329299,michaelywg,https://api.github.com/repos/grpc/grpc/pulls/23786,468867897,2020-08-11T21:12:44Z,src/core/lib/security/authorization/authorization_engine.cc,"@@ -74,4 +74,77 @@ AuthorizationEngine::AuthorizationEngine(   } } +std::unique_ptr<google::api::expr::runtime::Activation>+AuthorizationEngine::CreateActivation(const EvaluateArgs& args) {+  std::unique_ptr<google::api::expr::runtime::Activation> activation;+  for (const auto& elem : envoy_attributes_) {+    if (elem == ""url_path"") {+      activation->InsertValue(+          ""url_path"",+          google::api::expr::runtime::CelValue::CreateStringView(args.path()));+    } else if (elem == ""host"") {+      activation->InsertValue(+          ""host"",+          google::api::expr::runtime::CelValue::CreateStringView(args.host()));+    } else if (elem == ""method"") {+      activation->InsertValue(+          ""method"", google::api::expr::runtime::CelValue::CreateStringView(+                        args.method()));+    } else if (elem == ""headers"") {+      std::multimap<absl::string_view, absl::string_view> headers =+          args.headers();+      std::vector<std::pair<google::api::expr::runtime::CelValue,+                            google::api::expr::runtime::CelValue>>+          header_items;+      for (const auto& header_key : header_keys_) {+        auto header_item = headers.find(header_key);+        if (header_item != headers.end()) {+          header_items.push_back(+              std::pair<google::api::expr::runtime::CelValue,+                        google::api::expr::runtime::CelValue>(+                  google::api::expr::runtime::CelValue::CreateStringView(+                      header_key),+                  google::api::expr::runtime::CelValue::CreateStringView(+                      header_item->second)));+        }+      }+      headers_ = google::api::expr::runtime::ContainerBackedMapImpl::Create(","As specified [here](https://github.com/google/cel-cpp/blob/master/eval/public/cel_value.h#L7), CelValue does not accept ownership of pointers. In particular, CreateMap takes a bare pointer to a CelMap and stores only that pointer. This means that AuthorizationEngine needs to be responsible for the life cycle of any CelMap it creates. Without headers_, the headers CelMap would immediately go out of scope and be deleted.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23783,469496771,2020-08-12T19:41:24Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/completion_queue.pyx.pxi,"@@ -17,10 +17,26 @@ import socket cdef gpr_timespec _GPR_INF_FUTURE = gpr_inf_future(GPR_CLOCK_REALTIME) cdef float _POLL_AWAKE_INTERVAL_S = 0.2 +# This bool indicates if the event loop impl can monitor a given fd, or has+# loop.add_reader method.+cdef bint _has_fd_monitoring = True  IF UNAME_SYSNAME == ""Windows"":     cdef void _unified_socket_write(int fd) nogil:         win_socket_send(<WIN_SOCKET>fd, b""1"", 1, 0)++    # If the event loop policy is Proactor, then immediately turn on fall back+    # mode.+    try:+        if hasattr(asyncio, 'WindowsProactorEventLoopPolicy') and \+            isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):","Correct me if I'm wrong, but this code will run when the shared object library is loaded (i.e. when the user imports grpc), but there is a mutating `asyncio.set_event_loop_policy()` which they are likely to call only *after* the import. To do otherwise would be a lint error. Can this check be deferred until the first time a call is made into `grpc.aio`?",
16571023,ananda1066,https://api.github.com/repos/grpc/grpc/pulls/23588,470082493,2020-08-13T16:39:06Z,test/cpp/util/grpc_tool.cc,"@@ -74,6 +74,7 @@ DEFINE_bool(batch, false,             ""more than a few RPCs. gRPC CLI has very different performance ""             ""characteristics compared with normal RPC calls which make it ""             ""unsuitable for loadtesting or significant production traffic."");+DEFINE_uint32(timeout, 0, ""Call timeout in seconds (0 disables timeout)"");","timeout should be a double for more user flexibility (for instance, setting a deadline of microseconds).",
16571023,ananda1066,https://api.github.com/repos/grpc/grpc/pulls/23588,470112845,2020-08-13T17:20:56Z,test/cpp/util/grpc_tool.cc,"@@ -527,6 +530,10 @@ bool GrpcTool::CallMethod(int argc, const char** argv,     }   } +  if (FLAGS_timeout > 0) {+    deadline.reset(new std::chrono::system_clock::time_point(std::chrono::system_clock::now() + std::chrono::seconds(FLAGS_timeout)));","Is there a reason deadline is reset to now() + timeout, instead of now() being added when setting the deadline for the ClientContext in cli_call.cc?",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23786,470232677,2020-08-13T20:36:27Z,src/core/lib/security/authorization/authorization_engine.cc,"@@ -74,4 +90,103 @@ AuthorizationEngine::AuthorizationEngine(   } } +std::unique_ptr<google::api::expr::runtime::Activation>+AuthorizationEngine::CreateActivation(const EvaluateArgs& args) {+  std::unique_ptr<google::api::expr::runtime::Activation> activation;+  for (const auto& elem : envoy_attributes_) {+    if (elem == kUrlPath) {+      absl::string_view url_path(args.Path());+      if (url_path != """") {",I would recommend using url_path.empty(). Similarly for rest of the conditions,
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/23839,470780236,2020-08-14T18:14:02Z,tools/run_tests/run_xds_tests.py,"@@ -109,6 +110,10 @@ def parse_port_range(port_arg):     default='',     help='File to reference via GRPC_XDS_BOOTSTRAP. Disables built-in '     'bootstrap generation')+argp.add_argument(+    '--xds_v3_support',+    default=False,+    help='Support xDS v3 via GRPC_XDS_EXPERIMENTAL_V3_SUPPORT')","Should this say ""Support xDS v3 via GRPC_XDS_EXPERIMENTAL_V3_SUPPORT. If a pre-created bootstrap file is provided via the --bootstrap_file parameter, it should include xds_v3 in its server_features field.""?Since it looks like this also requires putting `xds_v3` in the bootstrap file `server_features`, which is done here when we generate the bootstrap file, but it could be overlooked when using a separately created bootstrap file (which is used in staging).",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23843,470861613,2020-08-14T20:57:23Z,src/python/grpcio_tests/tests/interop/BUILD.bazel,"@@ -88,6 +88,18 @@ py_library(     ], ) +py_binary(",It would be nice to add this `py_binary` rule. We have a similar one for the interop client.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/21458,470910507,2020-08-15T00:04:21Z,tools/distrib/python/grpcio_tools/setup.py,"@@ -223,4 +225,5 @@ def extension_modules():         'grpcio>={version}'.format(version=grpc_version.VERSION),     ],     package_data=package_data(),+    test_suite=_TEST_CLASS,","Not necessarily, but since we're already testing under both frameworks in the rest of the project, I figured it was best to follow the same pattern here. I'll remove if you feel strongly about it.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23648,471335368,2020-08-17T08:52:05Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -567,6 +595,16 @@ def _generate_build_extra_metadata_for_tests(tests, bazel_rules):     return test_metadata  +def _detect_and_print_issues(build_yaml_like):+    """"""Try detecting some unusual situations and warn about them.""""""+    for tgt in build_yaml_like['targets']:+        if tgt['build'] == 'test':+            for src in tgt['src']:+                if src.startswith('src/') and not src.endswith('.proto'):+                    print('source file from under ""src/"" tree used in test ' ++                          tgt['name'] + ': ' + src)++ # extra metadata that will be used to construct build.yaml","I did not want to put the ""extra metadata"" in a separate file because the contents are relatively tightly bound to the extract_metadata_bazel_xml.py logic (and e.g. the ""extra metadata"" for test are generated automatically) and I did not want to introduce another yaml file that has format of ""additions for build.yaml format"" (= a new format which is close to build.yaml but not quite). Hopefully over time less and less ""extra metadata"" will be needed here. In the ideal world, we would be able to competely elimitinate the need for _BUILD_EXTRA_METADATA.There is already a TODO (at ""Step 3"") that covers the plan to gradually eliminate the need for having handwritten extra metadata.",
13220781,pszemus,https://api.github.com/repos/grpc/grpc/pulls/23588,471370633,2020-08-17T09:57:00Z,test/cpp/util/grpc_tool.cc,"@@ -527,6 +530,10 @@ bool GrpcTool::CallMethod(int argc, const char** argv,     }   } +  if (FLAGS_timeout > 0) {+    deadline.reset(new std::chrono::system_clock::time_point(std::chrono::system_clock::now() + std::chrono::seconds(FLAGS_timeout)));","I wanted the `CliCall` constructor parameters to be consistent with gRPC terminology, so I passed a `deadline` parameter instead of `timeout`, and because of that `deadline` has to be constructed before `CliCall`.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23763,471652460,2020-08-17T17:37:25Z,src/ruby/pb/test/xds_client.rb,"@@ -95,17 +96,45 @@ def get_client_stats(req, _call)       end       $watchers.delete_at($watchers.index(watcher))     end+    # convert results into proper proto object+    rpcs_by_method = {}+    watcher['rpcs_by_method'].each do | rpc_name, rpcs_by_peer |","small nit: we don't have spaces before or after opening and closing `|` parameter brackets in do blocks elsewhere in the code, lets remove for consistency",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23763,471656499,2020-08-17T17:45:21Z,src/ruby/pb/test/xds_client.rb,"@@ -95,17 +96,45 @@ def get_client_stats(req, _call)       end       $watchers.delete_at($watchers.index(watcher))     end+    # convert results into proper proto object+    rpcs_by_method = {}+    watcher['rpcs_by_method'].each do | rpc_name, rpcs_by_peer |+      rpcs_by_method[rpc_name] = LoadBalancerStatsResponse::RpcsByPeer.new(+        rpcs_by_peer: rpcs_by_peer+      )+    end     LoadBalancerStatsResponse.new(+      rpcs_by_method: rpcs_by_method,       rpcs_by_peer: watcher['rpcs_by_peer'],       num_failures: watcher['no_remote_peer'] + watcher['rpcs_needed']     );   end end +# execute 1 RPC and return remote hostname+def execute_rpc(op, fail_on_failed_rpcs)+  remote_peer = """"+  begin+    op.execute+    if op.metadata.key?('hostname')",Where is the `hostname` metadata key getting populated?,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23763,471658902,2020-08-17T17:49:30Z,src/ruby/pb/test/xds_client.rb,"@@ -121,25 +150,42 @@ def run_test_loop(stub, target_seconds_between_rpcs, fail_on_failed_rpcs)       target_next_start += target_seconds_between_rpcs       sleep(sleep_seconds)     end-    begin-      deadline = GRPC::Core::TimeConsts::from_relative_time(30) # 30 seconds-      resp = stub.unary_call(req, deadline: deadline)-      remote_peer = resp.hostname-    rescue GRPC::BadStatus => e-      remote_peer = """"-      GRPC.logger.info(""ruby xds: rpc failed:|#{e.message}|, "" \-                       ""this may or may not be expected"")-      if fail_on_failed_rpcs-        raise e+    deadline = GRPC::Core::TimeConsts::from_relative_time(30) # 30 seconds+    results = {}+    rpcs_to_send.each do |rpc|+      metadata = metadata_to_send.key?(rpc) ? metadata_to_send[rpc] : {}+      if rpc == 'UnaryCall'+        op = stub.unary_call(simple_req,+                             metadata: metadata,+                             deadline: deadline,+                             return_op: true)+      elsif rpc == 'EmptyCall'+        op = stub.empty_call(empty_req,+                             metadata: metadata,+                             deadline: deadline,+                             return_op: true)+      else+        raise ""Unsupported rpc %s"" % [rpc]       end+      results[rpc] = execute_rpc(op, fail_on_failed_rpcs)     end     $watchers_mutex.synchronize do       $watchers.each do |watcher|+        # this is counted once when each group of all rpcs_to_send were done         watcher['rpcs_needed'] -= 1-        if remote_peer.strip.empty?-          watcher['no_remote_peer'] += 1-        else-          watcher['rpcs_by_peer'][remote_peer] += 1+        results.each do | rpc_name, remote_peer |",same nit here for spaces around do/end block parameter brackets,
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/23763,471709647,2020-08-17T18:51:48Z,src/ruby/pb/test/xds_client.rb,"@@ -95,17 +96,45 @@ def get_client_stats(req, _call)       end       $watchers.delete_at($watchers.index(watcher))     end+    # convert results into proper proto object+    rpcs_by_method = {}+    watcher['rpcs_by_method'].each do | rpc_name, rpcs_by_peer |+      rpcs_by_method[rpc_name] = LoadBalancerStatsResponse::RpcsByPeer.new(+        rpcs_by_peer: rpcs_by_peer+      )+    end     LoadBalancerStatsResponse.new(+      rpcs_by_method: rpcs_by_method,       rpcs_by_peer: watcher['rpcs_by_peer'],       num_failures: watcher['no_remote_peer'] + watcher['rpcs_needed']     );   end end +# execute 1 RPC and return remote hostname+def execute_rpc(op, fail_on_failed_rpcs)+  remote_peer = """"+  begin+    op.execute+    if op.metadata.key?('hostname')","This is the updated spec for this xDS interop test - that the remote `hostname` will be returned as part of the call's _initial_ metadata, instead of in the RPC's response proto.In the past, the remote `hostname` is being returned as a field in the `SimpleResponse` proto https://github.com/grpc/grpc/blob/v1.31.x/src/ruby/pb/test/xds_client.rb#L127, https://github.com/grpc/grpc/blob/master/src/proto/grpc/testing/messages.proto#L120. But since now, after the updated spec, we could be sending `EmptyCall` RPC, we can't be expecting this field to be in the response proto uniformly. So the updated spec specified that this key will be populated as part of the initial metadata.Also, an RPC may be designed to not be successful as part of the test, so we need to check whether that key exists in the initial metadata as well.",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/23763,471709957,2020-08-17T18:52:24Z,src/ruby/pb/test/xds_client.rb,"@@ -95,17 +96,45 @@ def get_client_stats(req, _call)       end       $watchers.delete_at($watchers.index(watcher))     end+    # convert results into proper proto object+    rpcs_by_method = {}+    watcher['rpcs_by_method'].each do | rpc_name, rpcs_by_peer |+      rpcs_by_method[rpc_name] = LoadBalancerStatsResponse::RpcsByPeer.new(+        rpcs_by_peer: rpcs_by_peer+      )+    end     LoadBalancerStatsResponse.new(+      rpcs_by_method: rpcs_by_method,       rpcs_by_peer: watcher['rpcs_by_peer'],       num_failures: watcher['no_remote_peer'] + watcher['rpcs_needed']     );   end end +# execute 1 RPC and return remote hostname+def execute_rpc(op, fail_on_failed_rpcs)+  remote_peer = """"","Similar response to above - according to the new updated test spec, we need to expect this `hostname` key in the call's initial metadata.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23763,471756332,2020-08-17T20:26:54Z,src/ruby/pb/test/xds_client.rb,"@@ -95,17 +96,45 @@ def get_client_stats(req, _call)       end       $watchers.delete_at($watchers.index(watcher))     end+    # convert results into proper proto object+    rpcs_by_method = {}+    watcher['rpcs_by_method'].each do | rpc_name, rpcs_by_peer |+      rpcs_by_method[rpc_name] = LoadBalancerStatsResponse::RpcsByPeer.new(+        rpcs_by_peer: rpcs_by_peer+      )+    end     LoadBalancerStatsResponse.new(+      rpcs_by_method: rpcs_by_method,       rpcs_by_peer: watcher['rpcs_by_peer'],       num_failures: watcher['no_remote_peer'] + watcher['rpcs_needed']     );   end end +# execute 1 RPC and return remote hostname+def execute_rpc(op, fail_on_failed_rpcs)+  remote_peer = """"+  begin+    op.execute+    if op.metadata.key?('hostname')","Got it, thanks.> Also, an RPC may be designed to not be successful as part of the test, so we need to check whether that key exists in the initial metadata as well.Slightly confused about this part, though. Looks like we currently don't check the peer via metadata at all if the call fails.",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/23763,471765787,2020-08-17T20:46:28Z,src/ruby/pb/test/xds_client.rb,"@@ -121,25 +150,42 @@ def run_test_loop(stub, target_seconds_between_rpcs, fail_on_failed_rpcs)       target_next_start += target_seconds_between_rpcs       sleep(sleep_seconds)     end-    begin-      deadline = GRPC::Core::TimeConsts::from_relative_time(30) # 30 seconds-      resp = stub.unary_call(req, deadline: deadline)-      remote_peer = resp.hostname-    rescue GRPC::BadStatus => e-      remote_peer = """"-      GRPC.logger.info(""ruby xds: rpc failed:|#{e.message}|, "" \-                       ""this may or may not be expected"")-      if fail_on_failed_rpcs-        raise e+    deadline = GRPC::Core::TimeConsts::from_relative_time(30) # 30 seconds+    results = {}+    rpcs_to_send.each do |rpc|+      metadata = metadata_to_send.key?(rpc) ? metadata_to_send[rpc] : {}+      if rpc == 'UnaryCall'+        op = stub.unary_call(simple_req,+                             metadata: metadata,+                             deadline: deadline,+                             return_op: true)+      elsif rpc == 'EmptyCall'+        op = stub.empty_call(empty_req,+                             metadata: metadata,+                             deadline: deadline,+                             return_op: true)+      else+        raise ""Unsupported rpc %s"" % [rpc]       end+      results[rpc] = execute_rpc(op, fail_on_failed_rpcs)     end     $watchers_mutex.synchronize do       $watchers.each do |watcher|+        # this is counted once when each group of all rpcs_to_send were done","The ""change"" to the spec, which this PR addressed, hasn't been merged to the main xDS test description doc yet. It's just in an internal doc linked from the internal bug b/161801377.In there, it specified that the new `--rpc` param will specify the list of RPCs to make (i.e. `UnaryCall`, `EmptyCall`, etc). And `--qps=10` means to _make 10 RPCs/second for each type_.",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/23763,471767112,2020-08-17T20:49:20Z,src/ruby/pb/test/xds_client.rb,"@@ -95,17 +96,45 @@ def get_client_stats(req, _call)       end       $watchers.delete_at($watchers.index(watcher))     end+    # convert results into proper proto object+    rpcs_by_method = {}+    watcher['rpcs_by_method'].each do | rpc_name, rpcs_by_peer |+      rpcs_by_method[rpc_name] = LoadBalancerStatsResponse::RpcsByPeer.new(+        rpcs_by_peer: rpcs_by_peer+      )+    end     LoadBalancerStatsResponse.new(+      rpcs_by_method: rpcs_by_method,       rpcs_by_peer: watcher['rpcs_by_peer'],       num_failures: watcher['no_remote_peer'] + watcher['rpcs_needed']     );   end end +# execute 1 RPC and return remote hostname+def execute_rpc(op, fail_on_failed_rpcs)+  remote_peer = """"+  begin+    op.execute+    if op.metadata.key?('hostname')","The spec mentioned that `All clients read hostname from header[“hostname”] instead of from RPC response`. It seems that it's unclear whether the initial metadata should be read if the RPC failed. I will follow up with Menghan and Eric to see what the behavior should be. As it stands, it passes all the test cases so perhaps it doesn't matter for now. I am inclined to merge this PR as is. If there's an issue with this particular piece of logic we can always fix it later.",
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/23763,471770358,2020-08-17T20:56:06Z,src/ruby/pb/test/xds_client.rb,"@@ -95,17 +96,45 @@ def get_client_stats(req, _call)       end       $watchers.delete_at($watchers.index(watcher))     end+    # convert results into proper proto object+    rpcs_by_method = {}+    watcher['rpcs_by_method'].each do | rpc_name, rpcs_by_peer |+      rpcs_by_method[rpc_name] = LoadBalancerStatsResponse::RpcsByPeer.new(+        rpcs_by_peer: rpcs_by_peer+      )+    end     LoadBalancerStatsResponse.new(+      rpcs_by_method: rpcs_by_method,       rpcs_by_peer: watcher['rpcs_by_peer'],       num_failures: watcher['no_remote_peer'] + watcher['rpcs_needed']     );   end end +# execute 1 RPC and return remote hostname+def execute_rpc(op, fail_on_failed_rpcs)+  remote_peer = """"+  begin+    op.execute+    if op.metadata.key?('hostname')","Right, whether to read ""hostname"" from metadata when the RPC fails is  currently not defined. Mostly because none of the existing tests depends on hostname when the RPC fails. It's OK for the client to ignore it.This may change in the future, to test failure cases (fault injection for example). But it not super clear how the behavior will be, so let's worry about it later.",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/23871,472617289,2020-08-19T02:51:11Z,test/core/security/evaluate_args_test.cc,"@@ -68,6 +68,182 @@ TEST_F(EvaluateArgsTest, TestEvaluateArgsPeerPort) {       << ""Error: Failed to extract correct Peer port from EvaluateArgs.""; } +TEST(EvaluateArgsMetadataTest, HandlesNullMetadata) {+  EvaluateArgs evalArgs(nullptr, nullptr, nullptr);+  absl::string_view path = evalArgs.GetPath();+  absl::string_view method = evalArgs.GetMethod();+  absl::string_view host = evalArgs.GetHost();+  std::multimap<absl::string_view, absl::string_view> headers = evalArgs.GetHeaders();+  EXPECT_EQ(path, nullptr) << ""Failed to return nullptr with null metadata_batch."";+  EXPECT_EQ(method, nullptr) << ""Failed to return nullptr with null metadata_batch."";+  EXPECT_EQ(host, nullptr) << ""Failed to return nullptr with null metadata_batch."";+  EXPECT_EQ(headers.size(), 0) << ""Failed to return nullptr with null metadata_batch."";+}++TEST(EvaluateArgsMetadataTest, HandlesEmptyMetadata) {+  grpc_metadata_batch metadata_;",Use metadata. Trailing underscore is used only for class data members.Please do update for remaining tests as well.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23892,473407564,2020-08-19T22:54:54Z,src/python/grpcio_tests/tests_py3_only/interop/xds_interop_client.py,"@@ -251,4 +325,18 @@ def _run(args: argparse.Namespace) -> None:         file_handler = logging.FileHandler(args.log_file, mode='a')         file_handler.setFormatter(formatter)         logger.addHandler(file_handler)-    _run(args)+    methods = args.rpc.split("","")+    if set(methods) - set(_SUPPORTED_METHODS):+        raise ValueError(""--rpc supported methods: {}"".format(+            "", "".join(_SUPPORTED_METHODS)))+    per_method_metadata = collections.defaultdict(list)+    metadata = args.metadata.split("","") if args.metadata else []+    for metadatum in metadata:+        elems = metadatum.split("":"")+        if len(elems) != 3:+            raise ValueError(+                f""'{metadatum}' was not in the form 'METHOD:KEY:VALUE'"")+        if elems[0] not in _SUPPORTED_METHODS:+            raise ValueError(f""Unrecognized method '{elems[0]}'"")+        per_method_metadata[elems[0]].append((elems[1], elems[2]))",nit: We could encapsulate this metadata parsing logic to a function to help readability. I think one reason why `run_tests.py` is hard to understand is because the parser definition is mixed with processing logic.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23892,473409790,2020-08-19T22:57:56Z,tools/internal_ci/linux/grpc_xds_bazel_python_test_in_docker.sh,"@@ -48,12 +48,14 @@ touch ""$TOOLS_DIR""/src/proto/grpc/testing/__init__.py  bazel build //src/python/grpcio_tests/tests_py3_only/interop:xds_interop_client +# Test cases ""path_matching"" and ""header_matching"" are not included in ""all"",+# because not all interop clients in all languages support these new tests.","optional: the definition of all is in https://github.com/grpc/grpc/blob/master/tools/run_tests/run_interop_tests.py. The most correct way should be adding these two cases to `all`, then exclude it from unsupported languages. But it is probably out of the scope of this PR.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23892,473412727,2020-08-19T23:01:57Z,tools/run_tests/run_xds_tests.py,"@@ -1856,17 +1891,13 @@ def __init__(self, compute, alpha_compute, project):                 result.state = 'FAILED'                 result.message = str(e)             finally:-                if client_process and not client_process.returncode:+                if client_process and client_process.returncode is None:                     client_process.terminate()-                test_log_file.close()                 # Workaround for Python 3, as report_utils will invoke decode() on                 # result.message, which has a default value of ''.                 result.message = result.message.encode('UTF-8')                 test_results[test_case] = [result]-                if args.log_client_output:-                    logger.info('Client output:')-                    with open(test_log_filename, 'r') as client_output:-                        logger.info(client_output.read())+                client_logged.wait()",nit: Do we want a deadline here? What should we do if client hang?,
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/23923,474370983,2020-08-21T02:20:27Z,tools/run_tests/run_xds_tests.py,"@@ -1002,14 +1004,30 @@ def test_header_matching(gcp, original_backend_service, instance_group,   def set_serving_status(instances, service_port, serving):+    logger.info('setting %s serving status to %s', instances, serving)     for instance in instances:         with grpc.insecure_channel('%s:%d' %                                    (instance, service_port)) as channel:+            logger.info('setting %s serving status to %s', instance, serving)             stub = test_pb2_grpc.XdsUpdateHealthServiceStub(channel)-            if serving:-                stub.SetServing(empty_pb2.Empty())-            else:-                stub.SetNotServing(empty_pb2.Empty())+            health_stub = health_pb2_grpc.HealthStub(channel)++            retry_count = 5+            for i in range(5):+                if serving:+                    stub.SetServing(empty_pb2.Empty())+                else:+                    stub.SetNotServing(empty_pb2.Empty())+                serving_status = health_stub.Check(+                    health_pb2.HealthCheckRequest())+                logger.info('got instance service status %s', serving_status)+                want_status = health_pb2.HealthCheckResponse.SERVING if serving else health_pb2.HealthCheckResponse.NOT_SERVING","Sanity test failed, maybe this line is too long?",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/23923,474371203,2020-08-21T02:21:23Z,tools/run_tests/run_xds_tests.py,"@@ -1002,14 +1004,30 @@ def test_header_matching(gcp, original_backend_service, instance_group,   def set_serving_status(instances, service_port, serving):+    logger.info('setting %s serving status to %s', instances, serving)     for instance in instances:         with grpc.insecure_channel('%s:%d' %                                    (instance, service_port)) as channel:+            logger.info('setting %s serving status to %s', instance, serving)             stub = test_pb2_grpc.XdsUpdateHealthServiceStub(channel)-            if serving:-                stub.SetServing(empty_pb2.Empty())-            else:-                stub.SetNotServing(empty_pb2.Empty())+            health_stub = health_pb2_grpc.HealthStub(channel)++            retry_count = 5+            for i in range(5):+                if serving:+                    stub.SetServing(empty_pb2.Empty())+                else:+                    stub.SetNotServing(empty_pb2.Empty())+                serving_status = health_stub.Check(+                    health_pb2.HealthCheckRequest())+                logger.info('got instance service status %s', serving_status)+                want_status = health_pb2.HealthCheckResponse.SERVING if serving else health_pb2.HealthCheckResponse.NOT_SERVING+                if serving_status.status == want_status:+                    break","Suggest changing this to `return`, then drop the `if` below and just raise the exception if the for loop terminates",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22906,474643257,2020-08-21T11:39:00Z,src/csharp/ext/grpc_csharp_ext.c,"@@ -1076,6 +1076,82 @@ grpcsharp_ssl_server_credentials_create(   return creds; } +GPR_EXPORT grpc_ssl_server_certificate_config* GPR_CALLTYPE+grpcsharp_ssl_server_certificate_config_create(+    const char* pem_root_certs, const char** key_cert_pair_cert_chain_array,+    const char** key_cert_pair_private_key_array, size_t num_key_cert_pairs) {+  size_t i;+  grpc_ssl_server_certificate_config* config;+  grpc_ssl_pem_key_cert_pair* key_cert_pairs =+      gpr_malloc(sizeof(grpc_ssl_pem_key_cert_pair) * num_key_cert_pairs);+  memset(key_cert_pairs, 0,+         sizeof(grpc_ssl_pem_key_cert_pair) * num_key_cert_pairs);++  for (i = 0; i < num_key_cert_pairs; i++) {+    if (key_cert_pair_cert_chain_array[i] ||+        key_cert_pair_private_key_array[i]) {+      key_cert_pairs[i].cert_chain = key_cert_pair_cert_chain_array[i];+      key_cert_pairs[i].private_key = key_cert_pair_private_key_array[i];+    }+  }++  config = grpc_ssl_server_certificate_config_create(+      pem_root_certs, key_cert_pairs, num_key_cert_pairs);+  gpr_free(key_cert_pairs);+  return config;+}++GPR_EXPORT void GPR_CALLTYPE grpcsharp_ssl_server_certificate_config_destroy(+    grpc_ssl_server_certificate_config* config) {+  return grpc_ssl_server_certificate_config_destroy(config);+}++GPR_EXPORT grpc_ssl_server_credentials_options* GPR_CALLTYPE+grpcsharp_ssl_server_credentials_create_options_using_config(+    grpc_ssl_client_certificate_request_type client_certificate_request,+    grpc_ssl_server_certificate_config* certificate_config) {+  return grpc_ssl_server_credentials_create_options_using_config(+      client_certificate_request, certificate_config);+}++grpc_ssl_server_certificate_config_callback saved_cb = NULL;","No, using global variable is wrong because it wouldn't work for multiple parallel connections.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22906,474653576,2020-08-21T12:02:59Z,src/csharp/ext/grpc_csharp_ext.c,"@@ -1076,6 +1076,82 @@ grpcsharp_ssl_server_credentials_create(   return creds; } +GPR_EXPORT grpc_ssl_server_certificate_config* GPR_CALLTYPE+grpcsharp_ssl_server_certificate_config_create(+    const char* pem_root_certs, const char** key_cert_pair_cert_chain_array,+    const char** key_cert_pair_private_key_array, size_t num_key_cert_pairs) {+  size_t i;+  grpc_ssl_server_certificate_config* config;+  grpc_ssl_pem_key_cert_pair* key_cert_pairs =+      gpr_malloc(sizeof(grpc_ssl_pem_key_cert_pair) * num_key_cert_pairs);+  memset(key_cert_pairs, 0,+         sizeof(grpc_ssl_pem_key_cert_pair) * num_key_cert_pairs);++  for (i = 0; i < num_key_cert_pairs; i++) {+    if (key_cert_pair_cert_chain_array[i] ||+        key_cert_pair_private_key_array[i]) {+      key_cert_pairs[i].cert_chain = key_cert_pair_cert_chain_array[i];+      key_cert_pairs[i].private_key = key_cert_pair_private_key_array[i];+    }+  }++  config = grpc_ssl_server_certificate_config_create(+      pem_root_certs, key_cert_pairs, num_key_cert_pairs);+  gpr_free(key_cert_pairs);+  return config;+}++GPR_EXPORT void GPR_CALLTYPE grpcsharp_ssl_server_certificate_config_destroy(+    grpc_ssl_server_certificate_config* config) {+  return grpc_ssl_server_certificate_config_destroy(config);+}++GPR_EXPORT grpc_ssl_server_credentials_options* GPR_CALLTYPE+grpcsharp_ssl_server_credentials_create_options_using_config(+    grpc_ssl_client_certificate_request_type client_certificate_request,+    grpc_ssl_server_certificate_config* certificate_config) {+  return grpc_ssl_server_credentials_create_options_using_config(+      client_certificate_request, certificate_config);+}++grpc_ssl_server_certificate_config_callback saved_cb = NULL;++// Definition of grpcsharp_ssl_server_certificate_config_callback_handler+// needs to be the same as the definition of the callback.+static grpc_ssl_certificate_config_reload_status+grpcsharp_ssl_server_certificate_config_callback_handler(+    void* user_data, grpc_ssl_server_certificate_config** config) {+  return native_callback_dispatcher(saved_cb, user_data, (void**)config, NULL,+                                    NULL, NULL, NULL);+}++GPR_EXPORT grpc_ssl_server_credentials_options* GPR_CALLTYPE+grpcsharp_ssl_server_credentials_create_options_using_config_fetcher(","you should model this after https://github.com/grpc/grpc/blob/560932cd8226108031893effec633c199060de5c/src/csharp/ext/grpc_csharp_ext.c#L1150what you want is to pass `void* callback_tag` from the C# code (which is going to be an GCHandle of the native callback handler, and NativeCallbackDispatcher will allow you to turn the IntPtr into a previously registered C# delegate).https://github.com/grpc/grpc/blob/560932cd8226108031893effec633c199060de5c/src/csharp/Grpc.Core/Internal/NativeCallbackDispatcher.cs#L59That way you shouldn't need to store any cb (just point it to the handler function you have) and store callback_tag in user_data - so that you have access to the callback_tag when your handler gets invoked by the native code gets invoked.If you need to store more data/state, you should create a new struct, store all the data you need in it (similar to the case of metadata plugin) and pass a pointer to it in user_data.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/22906,474658085,2020-08-21T12:13:15Z,src/csharp/Grpc.Core/Internal/ServerCertificateConfigCallbackRegistration.cs,"@@ -0,0 +1,75 @@+#region Copyright notice and license+// Copyright 2015 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+#endregion+using System;+using System.Collections.Generic;+using System.Linq;+using System.Runtime.InteropServices;+using System.Text;+using System.Threading.Tasks;+using Grpc.Core.Logging;++namespace Grpc.Core.Internal+{+    internal class ServerCertificateConfigCallbackRegistration+    {+        static readonly ILogger Logger = GrpcEnvironment.Logger.ForType<ServerCertificateConfigCallbackRegistration>();+        static readonly NativeMethods Native = NativeMethods.Get();++        readonly ServerCertificateConfigCallback serverCertificateConfigCallback;+        readonly NativeCallbackRegistration callbackRegistration;++        public ServerCertificateConfigCallbackRegistration(ServerCertificateConfigCallback serverCertificateConfigCallback)+        {+            this.serverCertificateConfigCallback = serverCertificateConfigCallback;+            this.callbackRegistration = NativeCallbackDispatcher.RegisterCallback(HandleUniversalCallback);+        }++        public NativeCallbackRegistration CallbackRegistration => callbackRegistration;++        private int HandleUniversalCallback(IntPtr arg0, IntPtr arg1, IntPtr arg2, IntPtr arg3, IntPtr arg4, IntPtr arg5)+        {+            return ServerCertificateConfigCallback(arg0, arg1);+        }++        private int ServerCertificateConfigCallback(IntPtr arg0, IntPtr arg1)+        {+            try+            {+                // Get the certificate from users callback:+                ServerCertificateConfig serverCertificateConfig = this.serverCertificateConfigCallback();++                // Write the pointer to the certificate to the location where arg1 is pointing to:+                ServerCertificateConfigSafeHandle nativeServerCertificateConfig = serverCertificateConfig.ToNative();+                Native.grpcsharp_write_ssl_server_certificate_config_to_pointer(arg1, nativeServerCertificateConfig);++                // Perhaps the pointer arithmetic should be done in Native should be done in CLR:+                    //GCHandle handle = GCHandle.Alloc(nativeServerCertificateConfig);+                    //IntPtr pointerToNativeHandle = GCHandle.ToIntPtr(handle);+                    //Marshal.WriteIntPtr(arg1, pointerToNativeHandle);++                // We could somehow retrieve the old cert config and compare instead of passing .New unconditinally.+                return (int)SslCertificateConfigReloadStatus.New;",You'd probably need a bunch of native functions to be able to retrieve the native data of the old config (from the pointer) and turn it into a c# object that represents the config data. Then you could compare the C# objects. So basically you need native functions that allow you to populate ServerCertificateConfig from a native pointer (probably needs to be done field by field).,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23935,474882288,2020-08-21T19:15:45Z,setup.py,"@@ -171,7 +171,17 @@ def check_linker_need_libatomic():                                stdout=PIPE,                                stderr=PIPE)     cc_test.communicate(input=code_test)-    return cc_test.returncode != 0+    if cc_test.returncode == 0:+        return False+    # Double-check to see if -latomic actually can solve the problem.+    # https://github.com/grpc/grpc/issues/22491+    cc_test = subprocess.Popen(+        ['cc', '-x', 'c++', '-std=c++11', '-latomic', '-'],",I don't think we can assume that `cc` is the proper compiler across all platforms. You can access the correct compiler executable from the `self.compiler.compiler_type` attribute from a subclass of `distutils.command.build_ext.build_ext`. We already have one of those [here](https://github.com/grpc/grpc/blob/7c343ee45ea8244c011ae46ae8e2d6b88b6d749a/src/python/grpcio/commands.py#L214).,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/23935,475150710,2020-08-22T23:59:05Z,setup.py,"@@ -171,7 +171,17 @@ def check_linker_need_libatomic():                                stdout=PIPE,                                stderr=PIPE)     cc_test.communicate(input=code_test)-    return cc_test.returncode != 0+    if cc_test.returncode == 0:+        return False+    # Double-check to see if -latomic actually can solve the problem.+    # https://github.com/grpc/grpc/issues/22491+    cc_test = subprocess.Popen(+        ['cc', '-x', 'c++', '-std=c++11', '-latomic', '-'],","I guess a certain version of clang doesn't add libc++ by default when running `cc -x cc` so I'm trying to test whether `c++` instead of `cc` would solve the libc++ problem. Since there are two different libraries for c++ library; libstdc++ and libc++, it could not be portable to specify them explicitly.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23964,476646658,2020-08-25T18:16:42Z,tools/distrib/docgen/all_lang_docgen.sh,"@@ -0,0 +1,117 @@+#! /bin/bash+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+#+# A script to automatically generate API references and push to GitHub.+# This script covers Core/C++/ObjC/C#/PHP/Python. Due to lack of tooling for+# Cython, Python document generation unfortunately needs to compile everything.+# So, this script will take couple minutes to run.+#+# Generate and push:+#+#     tools/distrib/docgen/all_lang-docgen.sh YOUR_GITHUB_USERNAME+#+# Just generate:+#+#     tools/distrib/docgen/all_lang-docgen.sh+#++set -e++# Find out the gRPC version and print it+GRPC_VERSION=""$(grep -m1 -Eo ' version: .*' build_handwritten.yaml | grep -Eo '[0-9].*')""+echo ""Generating documents for version ${GRPC_VERSION}...""++# Specifies your GitHub user name or generates documents locally+if [ $# -eq 0 ]; then+    read -r -p ""- Are you sure to generate documents without push to GitHub? [y/N] "" response+    if [[ ""${response[0]}"" =~ ^([yY][eE][sS]|[yY])$ ]]; then+        GITHUB_USER=''+    else+        echo ""Generation stopped""+        exit 1+    fi+else+    if [ $# -eq 1 ]; then+        GITHUB_USER=$1+    else+        echo ""Too many arguments!""+        exit 1+    fi+fi++# Exits on pending changes; please double check for unwanted code changes+git diff --exit-code+git submodule update --init --recursive++# Changes to project root+dir=$(dirname ""${0}"")+cd ""${dir}/../../..""++# Clones the API reference GitHub Pages branch+PAGES_PATH=""/tmp/gh-pages""+rm -rf ""${PAGES_PATH}""+git clone https://github.com/grpc/grpc -b gh-pages ""${PAGES_PATH}""++# Generates Core / C++ / ObjC / PHP documents+rm -rf ""${PAGES_PATH}/core"" ""${PAGES_PATH}/cpp"" ""${PAGES_PATH}/objc"" ""${PAGES_PATH}/php""","The folder to store the GitHub Pages branch needs to be cleaned before trying to clone. And subsequence `rm -rf`s are meant for removal of each language's documents.The language coverage is not complete, like Node.js doc generation is missing. It would be better to keep this pattern to update individual languages one-by-one.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/23964,476662706,2020-08-25T18:45:37Z,tools/distrib/docgen/all_lang_docgen.sh,"@@ -0,0 +1,117 @@+#! /bin/bash+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+#+# A script to automatically generate API references and push to GitHub.+# This script covers Core/C++/ObjC/C#/PHP/Python. Due to lack of tooling for+# Cython, Python document generation unfortunately needs to compile everything.+# So, this script will take couple minutes to run.+#+# Generate and push:+#+#     tools/distrib/docgen/all_lang-docgen.sh YOUR_GITHUB_USERNAME+#+# Just generate:+#+#     tools/distrib/docgen/all_lang-docgen.sh+#++set -e++# Find out the gRPC version and print it+GRPC_VERSION=""$(grep -m1 -Eo ' version: .*' build_handwritten.yaml | grep -Eo '[0-9].*')""+echo ""Generating documents for version ${GRPC_VERSION}...""++# Specifies your GitHub user name or generates documents locally+if [ $# -eq 0 ]; then+    read -r -p ""- Are you sure to generate documents without push to GitHub? [y/N] "" response+    if [[ ""${response[0]}"" =~ ^([yY][eE][sS]|[yY])$ ]]; then+        GITHUB_USER=''+    else+        echo ""Generation stopped""+        exit 1+    fi+else+    if [ $# -eq 1 ]; then+        GITHUB_USER=$1+    else+        echo ""Too many arguments!""+        exit 1+    fi+fi++# Exits on pending changes; please double check for unwanted code changes+git diff --exit-code+git submodule update --init --recursive++# Changes to project root+dir=$(dirname ""${0}"")+cd ""${dir}/../../..""++# Clones the API reference GitHub Pages branch+PAGES_PATH=""/tmp/gh-pages""+rm -rf ""${PAGES_PATH}""+git clone https://github.com/grpc/grpc -b gh-pages ""${PAGES_PATH}""++# Generates Core / C++ / ObjC / PHP documents+rm -rf ""${PAGES_PATH}/core"" ""${PAGES_PATH}/cpp"" ""${PAGES_PATH}/objc"" ""${PAGES_PATH}/php""","Ah, I see. `${PAGES_PATH}/{core,cpp,objc,php}` are getting checked in, so the `git clone` populates them. 👍 ",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23964,476713060,2020-08-25T20:19:36Z,tools/distrib/docgen/all_lang_docgen.sh,"@@ -0,0 +1,117 @@+#! /bin/bash+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+#+# A script to automatically generate API references and push to GitHub.+# This script covers Core/C++/ObjC/C#/PHP/Python. Due to lack of tooling for+# Cython, Python document generation unfortunately needs to compile everything.+# So, this script will take couple minutes to run.+#+# Generate and push:+#+#     tools/distrib/docgen/all_lang-docgen.sh YOUR_GITHUB_USERNAME+#+# Just generate:+#+#     tools/distrib/docgen/all_lang-docgen.sh+#++set -e++# Find out the gRPC version and print it+GRPC_VERSION=""$(grep -m1 -Eo ' version: .*' build_handwritten.yaml | grep -Eo '[0-9].*')""+echo ""Generating documents for version ${GRPC_VERSION}...""++# Specifies your GitHub user name or generates documents locally+if [ $# -eq 0 ]; then+    read -r -p ""- Are you sure to generate documents without push to GitHub? [y/N] "" response+    if [[ ""${response[0]}"" =~ ^([yY][eE][sS]|[yY])$ ]]; then+        GITHUB_USER=''+    else+        echo ""Generation stopped""+        exit 1+    fi+else+    if [ $# -eq 1 ]; then+        GITHUB_USER=$1+    else+        echo ""Too many arguments!""+        exit 1+    fi+fi++# Exits on pending changes; please double check for unwanted code changes+git diff --exit-code+git submodule update --init --recursive++# Changes to project root+dir=$(dirname ""${0}"")+cd ""${dir}/../../..""++# Clones the API reference GitHub Pages branch+PAGES_PATH=""/tmp/gh-pages""+rm -rf ""${PAGES_PATH}""+git clone https://github.com/grpc/grpc -b gh-pages ""${PAGES_PATH}""++# Generates Core / C++ / ObjC / PHP documents+rm -rf ""${PAGES_PATH}/core"" ""${PAGES_PATH}/cpp"" ""${PAGES_PATH}/objc"" ""${PAGES_PATH}/php""+docker run --rm -it \","Log printing added.The output is quite verbose now. The good side is that it makes any failure trace-able, but the bad side is that the printed log about the Docker build might be overwhelmed by other log statements.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/23964,476717567,2020-08-25T20:28:20Z,tools/distrib/docgen/all_lang_docgen.sh,"@@ -0,0 +1,117 @@+#! /bin/bash+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+#+# A script to automatically generate API references and push to GitHub.+# This script covers Core/C++/ObjC/C#/PHP/Python. Due to lack of tooling for+# Cython, Python document generation unfortunately needs to compile everything.+# So, this script will take couple minutes to run.+#+# Generate and push:+#+#     tools/distrib/docgen/all_lang-docgen.sh YOUR_GITHUB_USERNAME+#+# Just generate:+#+#     tools/distrib/docgen/all_lang-docgen.sh+#++set -e++# Find out the gRPC version and print it+GRPC_VERSION=""$(grep -m1 -Eo ' version: .*' build_handwritten.yaml | grep -Eo '[0-9].*')""+echo ""Generating documents for version ${GRPC_VERSION}...""++# Specifies your GitHub user name or generates documents locally+if [ $# -eq 0 ]; then+    read -r -p ""- Are you sure to generate documents without push to GitHub? [y/N] "" response+    if [[ ""${response[0]}"" =~ ^([yY][eE][sS]|[yY])$ ]]; then+        GITHUB_USER=''+    else+        echo ""Generation stopped""+        exit 1+    fi+else+    if [ $# -eq 1 ]; then+        GITHUB_USER=$1+    else+        echo ""Too many arguments!""+        exit 1+    fi+fi++# Exits on pending changes; please double check for unwanted code changes+git diff --exit-code+git submodule update --init --recursive++# Changes to project root+dir=$(dirname ""${0}"")+cd ""${dir}/../../..""++# Clones the API reference GitHub Pages branch+PAGES_PATH=""/tmp/gh-pages""+rm -rf ""${PAGES_PATH}""+git clone https://github.com/grpc/grpc -b gh-pages ""${PAGES_PATH}""",I tried `--depth 1` but GitHub doesn't allow shallow clones to push. The `--single-branch` flag works fine 👍,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23980,478186573,2020-08-27T06:39:04Z,tools/dockerfile/distribtest/python_dev_centos7_x64/Dockerfile,"@@ -18,5 +18,13 @@ RUN yum install -y python RUN yum install -y epel-release RUN yum install -y python-pip RUN pip install virtualenv-RUN yum groupinstall -y 'Development Tools' RUN yum install -y python-devel++# The default gcc of CentOS 7 is gcc 4.8 which is older than gcc 4.9,+# the minimum supported gcc version for gRPC Core so let's upgrade to+# the latest one available on Centos 7.+RUN yum install -y centos-release-scl+RUN yum install -y devtoolset-8-binutils devtoolset-8-gcc devtoolset-8-gcc-c++","sg, thanks for the explantion.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/23935,478875417,2020-08-28T07:13:36Z,setup.py,"@@ -171,7 +171,17 @@ def check_linker_need_libatomic():                                stdout=PIPE,                                stderr=PIPE)     cc_test.communicate(input=code_test)-    return cc_test.returncode != 0+    if cc_test.returncode == 0:+        return False+    # Double-check to see if -latomic actually can solve the problem.+    # https://github.com/grpc/grpc/issues/22491+    cc_test = subprocess.Popen(+        ['cc', '-x', 'c++', '-std=c++11', '-latomic', '-'],",I changed c++ to let it use proper c++ library.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24015,479576890,2020-08-28T23:47:49Z,src/core/lib/security/certificate_provider.h,"@@ -0,0 +1,66 @@+//+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//+//++#ifndef GRPC_CORE_LIB_SECURITY_CERTIFICATE_PROVIDER_H+#define GRPC_CORE_LIB_SECURITY_CERTIFICATE_PROVIDER_H++#include <grpc/support/port_platform.h>++#include ""src/core/lib/gprpp/ref_counted_ptr.h""+#include ""src/core/lib/iomgr/pollset_set.h""++namespace grpc_core {++// TODO(yashkt): After https://github.com/grpc/grpc/pull/23572, remove this+// forward declaration and include the header for the distributor instead.+struct grpc_tls_certificate_distributor;++// Interface for a grpc_tls_certificate_provider that handles the process to+// fetch credentials and validation contexts. Implementations are free to rely+// on local or remote sources to fetch the latest secrets, and free to share any+// state among different instances as they deem fit.+//+// On creation, grpc_tls_certificate_provider creates a+// grpc_tls_certificate_distributor object. When the credentials and validation+// contexts become valid or changed, a grpc_tls_certificate_provider should+// notify its distributor so as to propagate the update to the watchers.+struct grpc_tls_certificate_provider+    : public RefCounted<grpc_tls_certificate_provider> {+ public:+  grpc_tls_certificate_provider()+      : distributor_(MakeRefCounted<grpc_tls_certificate_distributor>()),","As per discussion in the doc, I don't think we want to create the distributor in the base class.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/24016,479610737,2020-08-29T05:31:58Z,test/core/transport/chttp2/remove_stream_from_stalled_lists_test.cc,"@@ -0,0 +1,364 @@+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include <stdlib.h>+#include <string.h>++#include <functional>+#include <set>+#include <thread>++#include <gmock/gmock.h>++#include <grpc/grpc.h>+#include <grpc/grpc_security.h>+#include <grpc/impl/codegen/grpc_types.h>+#include <grpc/slice.h>+#include <grpc/support/alloc.h>+#include <grpc/support/log.h>+#include <grpc/support/string_util.h>+#include <grpc/support/time.h>++#include ""absl/strings/str_cat.h""+#include ""absl/strings/str_format.h""+#include ""absl/types/optional.h""++#include ""src/core/ext/filters/client_channel/backup_poller.h""","Fixed, I think I've removed all unused headers on the latest commit.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23860,480885390,2020-09-01T06:47:13Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,530 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <unistd.h>++#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>++#include ""absl/strings/str_format.h""+#include ""gflags/gflags.h""+#include ""google/protobuf/text_format.h""+#include ""grpc/grpc.h""+#include ""grpc/support/port_platform.h""+#include ""grpcpp/channel.h""+#include ""grpcpp/client_context.h""+#include ""grpcpp/create_channel.h""+#include ""grpcpp/ext/channelz_service_plugin.h""+#include ""grpcpp/grpcpp.h""+#include ""grpcpp/security/credentials.h""+#include ""grpcpp/security/server_credentials.h""+#include ""grpcpp/server.h""+#include ""grpcpp/server_builder.h""+#include ""grpcpp/server_context.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++namespace {+using grpc::ClientContext;+using grpc::Status;+using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;+}  // namespace++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;+    get_channel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetChannel(+        &get_channel_context, get_channel_request, &get_channel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetChannelRPC failed: %s"",+              get_channel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_channel_response.channel();+  }++  // Get a subchannel based on subchannel_id+  grpc::channelz::v1::Subchannel GetSubchannelRPC(int64_t subchannel_id) {+    GetSubchannelRequest get_subchannel_request;+    get_subchannel_request.set_subchannel_id(subchannel_id);+    GetSubchannelResponse get_subchannel_response;+    ClientContext get_subchannel_context;+    get_subchannel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSubchannel(&get_subchannel_context,+                                                  get_subchannel_request,+                                                  &get_subchannel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSubchannelRPC failed: %s"",+              get_subchannel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_subchannel_response.subchannel();+  }++  // get a socket based on socket_id+  grpc::channelz::v1::Socket GetSocketRPC(int64_t socket_id) {+    GetSocketRequest get_socket_request;+    get_socket_request.set_socket_id(socket_id);+    GetSocketResponse get_socket_response;+    ClientContext get_socket_context;+    get_socket_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSocket(+        &get_socket_context, get_socket_request, &get_socket_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSocketRPC failed: %s"",+              get_socket_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_socket_response.socket();+  }++  // get the descedent channels/subchannels/sockets of a channel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetChannelDescedence(+      const grpc::channelz::v1::Channel& channel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+    if (channel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : channel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (channel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : channel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (channel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : channel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // get the descedent channels/subchannels/sockets of a subchannel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetSubchannelDescedence(+      grpc::channelz::v1::Subchannel& subchannel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+              << "" descendence - "";+    if (subchannel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : subchannel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (subchannel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : subchannel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (subchannel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : subchannel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // Set up the channelz sampler client+  // Initialize json as an array+  void Setup(const std::string& custom_credentials_type,+             const std::string& server_address) {+    json_ = grpc_core::Json::Array();+    rpc_timeout_seconds_ = 20;+    grpc::ChannelArguments channel_args;+    std::shared_ptr<grpc::ChannelCredentials> channel_creds =","Can we check that `channel_creds` is not nullptr here?If it is nullptr, can we log an error message which includes:a) the value of the --custom_credentials_type flag which was provided, with explanation of how it's invalidb) list of all valid options (we should be able to get this list by using the `GetSecureCredentialsTypeList` API)",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23860,480900373,2020-09-01T07:06:32Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,530 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <unistd.h>++#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>++#include ""absl/strings/str_format.h""+#include ""gflags/gflags.h""+#include ""google/protobuf/text_format.h""+#include ""grpc/grpc.h""+#include ""grpc/support/port_platform.h""+#include ""grpcpp/channel.h""+#include ""grpcpp/client_context.h""+#include ""grpcpp/create_channel.h""+#include ""grpcpp/ext/channelz_service_plugin.h""+#include ""grpcpp/grpcpp.h""+#include ""grpcpp/security/credentials.h""+#include ""grpcpp/security/server_credentials.h""+#include ""grpcpp/server.h""+#include ""grpcpp/server_builder.h""+#include ""grpcpp/server_context.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++namespace {+using grpc::ClientContext;+using grpc::Status;+using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;+}  // namespace++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;+    get_channel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetChannel(+        &get_channel_context, get_channel_request, &get_channel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetChannelRPC failed: %s"",+              get_channel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_channel_response.channel();+  }++  // Get a subchannel based on subchannel_id+  grpc::channelz::v1::Subchannel GetSubchannelRPC(int64_t subchannel_id) {+    GetSubchannelRequest get_subchannel_request;+    get_subchannel_request.set_subchannel_id(subchannel_id);+    GetSubchannelResponse get_subchannel_response;+    ClientContext get_subchannel_context;+    get_subchannel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSubchannel(&get_subchannel_context,+                                                  get_subchannel_request,+                                                  &get_subchannel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSubchannelRPC failed: %s"",+              get_subchannel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_subchannel_response.subchannel();+  }++  // get a socket based on socket_id+  grpc::channelz::v1::Socket GetSocketRPC(int64_t socket_id) {+    GetSocketRequest get_socket_request;+    get_socket_request.set_socket_id(socket_id);+    GetSocketResponse get_socket_response;+    ClientContext get_socket_context;+    get_socket_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSocket(+        &get_socket_context, get_socket_request, &get_socket_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSocketRPC failed: %s"",+              get_socket_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_socket_response.socket();+  }++  // get the descedent channels/subchannels/sockets of a channel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetChannelDescedence(+      const grpc::channelz::v1::Channel& channel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+    if (channel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : channel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (channel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : channel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (channel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : channel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // get the descedent channels/subchannels/sockets of a subchannel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetSubchannelDescedence(+      grpc::channelz::v1::Subchannel& subchannel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+              << "" descendence - "";+    if (subchannel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : subchannel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (subchannel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : subchannel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (subchannel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : subchannel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // Set up the channelz sampler client+  // Initialize json as an array+  void Setup(const std::string& custom_credentials_type,+             const std::string& server_address) {+    json_ = grpc_core::Json::Array();+    rpc_timeout_seconds_ = 20;+    grpc::ChannelArguments channel_args;+    std::shared_ptr<grpc::ChannelCredentials> channel_creds =+        grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+            custom_credentials_type, &channel_args);+    std::shared_ptr<grpc::Channel> channel =+        CreateChannel(server_address, channel_creds);+    channelz_stub_ = grpc::channelz::v1::Channelz::NewStub(channel);+  }++  // Get all servers, keep querying until getting all+  // Store servers for dumping data+  // Need to check id repeating for servers+  void GetServersRPC() {+    int64_t server_start_id = 0;+    while (true) {+      GetServersRequest get_servers_request;+      GetServersResponse get_servers_response;+      ClientContext get_servers_context;+      get_servers_context.set_deadline(+          grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+      get_servers_request.set_start_server_id(server_start_id);+      Status status = channelz_stub_->GetServers(+          &get_servers_context, get_servers_request, &get_servers_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR,+                ""GetServers RPC with GetServersRequest.server_start_id=%d ""+                ""failed: %s"",+                int(server_start_id),+                get_servers_context.debug_error_string().c_str());+        GPR_ASSERT(0);+      }+      for (const auto& _server : get_servers_response.server()) {+        all_servers_.push_back(_server);+        StoreServerInJson(_server);+      }+      if (!get_servers_response.end()) {+        server_start_id = GetServerID(all_servers_.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of servers = "" << all_servers_.size() << std::endl;+  }++  // Get sockets that belongs to servers+  // Store sockets for dumping data+  void GetSocketsOfServers() {+    for (const auto& _server : all_servers_) {+      std::cout << ""Server "" << GetServerID(_server) << "" listen_socket: "";+      for (const auto& _socket : _server.listen_socket()) {+        int64_t so_id = _socket.socket_id();+        std::cout << so_id << "" "";+        if (CheckID(so_id)) {+          grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+      std::cout << std::endl;+    }+  }++  // Get all top channels, keep querying until getting all+  // Store channels for dumping data+  // No need to check id repeating for top channels+  void GetTopChannelsRPC() {+    int64_t channel_start_id = 0;+    while (true) {+      GetTopChannelsRequest get_top_channels_request;+      GetTopChannelsResponse get_top_channels_response;+      ClientContext get_top_channels_context;+      get_top_channels_context.set_deadline(+          grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+      get_top_channels_request.set_start_channel_id(channel_start_id);+      Status status = channelz_stub_->GetTopChannels(+          &get_top_channels_context, get_top_channels_request,+          &get_top_channels_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR,+                ""GetTopChannels RPC with ""+                ""GetTopChannelsRequest.channel_start_id=%d failed: %s"",+                int(channel_start_id),+                get_top_channels_context.debug_error_string().c_str());+        GPR_ASSERT(0);+      }+      for (const auto& _topchannel : get_top_channels_response.channel()) {+        top_channels_.push_back(_topchannel);+        all_channels_.push_back(_topchannel);+        StoreChannelInJson(_topchannel);+      }+      if (!get_top_channels_response.end()) {+        channel_start_id = GetChannelID(top_channels_.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of top channels = "" << top_channels_.size()+              << std::endl;+  }++  // layer traverse for each top channel+  void TraverseTopChannels() {+    for (const auto& _topchannel : top_channels_) {+      int tree_depth = 0;+      std::queue<grpc::channelz::v1::Channel> channel_queue;+      std::queue<grpc::channelz::v1::Subchannel> subchannel_queue;+      std::cout << ""Tree depth = "" << tree_depth << std::endl;+      GetChannelDescedence(_topchannel, channel_queue, subchannel_queue);+      while (!channel_queue.empty() || !subchannel_queue.empty()) {+        ++tree_depth;+        std::cout << ""Tree depth = "" << tree_depth << std::endl;+        int ch_q_size = channel_queue.size();+        int subch_q_size = subchannel_queue.size();+        for (int i = 0; i < ch_q_size; ++i) {+          grpc::channelz::v1::Channel ch = channel_queue.front();+          channel_queue.pop();+          GetChannelDescedence(ch, channel_queue, subchannel_queue);+        }+        for (int i = 0; i < subch_q_size; ++i) {+          grpc::channelz::v1::Subchannel subch = subchannel_queue.front();+          subchannel_queue.pop();+          GetSubchannelDescedence(subch, channel_queue, subchannel_queue);+        }+      }+    }+  }++  // dump data of all entities to stdout+  void DumpStdout() {+    std::string data_str;+    for (const auto& _channel : all_channels_) {+      std::cout << ""channel "" << GetChannelID(_channel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_channel.data(), &data_str);","It looks like one slightly unfortunate consequence of using `TextFormat::PrintToString` is that timestamps seem to be represented as raw `seconds` and `nanos` fields with their values, and doesn't e.g. print a very human readable timestamp along with it's time zone etc.I'm not sure how else that could be handled, though. Can we add a TODO here to find a way to log the timestamps in each proto in a more human readable way?",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23860,480900933,2020-09-01T07:07:19Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,530 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <unistd.h>++#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>++#include ""absl/strings/str_format.h""+#include ""gflags/gflags.h""+#include ""google/protobuf/text_format.h""+#include ""grpc/grpc.h""+#include ""grpc/support/port_platform.h""+#include ""grpcpp/channel.h""+#include ""grpcpp/client_context.h""+#include ""grpcpp/create_channel.h""+#include ""grpcpp/ext/channelz_service_plugin.h""+#include ""grpcpp/grpcpp.h""+#include ""grpcpp/security/credentials.h""+#include ""grpcpp/security/server_credentials.h""+#include ""grpcpp/server.h""+#include ""grpcpp/server_builder.h""+#include ""grpcpp/server_context.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++namespace {+using grpc::ClientContext;+using grpc::Status;+using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;+}  // namespace++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;+    get_channel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetChannel(+        &get_channel_context, get_channel_request, &get_channel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetChannelRPC failed: %s"",+              get_channel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_channel_response.channel();+  }++  // Get a subchannel based on subchannel_id+  grpc::channelz::v1::Subchannel GetSubchannelRPC(int64_t subchannel_id) {+    GetSubchannelRequest get_subchannel_request;+    get_subchannel_request.set_subchannel_id(subchannel_id);+    GetSubchannelResponse get_subchannel_response;+    ClientContext get_subchannel_context;+    get_subchannel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSubchannel(&get_subchannel_context,+                                                  get_subchannel_request,+                                                  &get_subchannel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSubchannelRPC failed: %s"",+              get_subchannel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_subchannel_response.subchannel();+  }++  // get a socket based on socket_id+  grpc::channelz::v1::Socket GetSocketRPC(int64_t socket_id) {+    GetSocketRequest get_socket_request;+    get_socket_request.set_socket_id(socket_id);+    GetSocketResponse get_socket_response;+    ClientContext get_socket_context;+    get_socket_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSocket(+        &get_socket_context, get_socket_request, &get_socket_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSocketRPC failed: %s"",+              get_socket_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_socket_response.socket();+  }++  // get the descedent channels/subchannels/sockets of a channel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetChannelDescedence(+      const grpc::channelz::v1::Channel& channel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+    if (channel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : channel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (channel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : channel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (channel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : channel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // get the descedent channels/subchannels/sockets of a subchannel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetSubchannelDescedence(+      grpc::channelz::v1::Subchannel& subchannel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+              << "" descendence - "";+    if (subchannel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : subchannel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (subchannel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : subchannel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (subchannel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : subchannel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // Set up the channelz sampler client+  // Initialize json as an array+  void Setup(const std::string& custom_credentials_type,+             const std::string& server_address) {+    json_ = grpc_core::Json::Array();+    rpc_timeout_seconds_ = 20;+    grpc::ChannelArguments channel_args;+    std::shared_ptr<grpc::ChannelCredentials> channel_creds =+        grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+            custom_credentials_type, &channel_args);+    std::shared_ptr<grpc::Channel> channel =+        CreateChannel(server_address, channel_creds);+    channelz_stub_ = grpc::channelz::v1::Channelz::NewStub(channel);+  }++  // Get all servers, keep querying until getting all+  // Store servers for dumping data+  // Need to check id repeating for servers+  void GetServersRPC() {+    int64_t server_start_id = 0;+    while (true) {+      GetServersRequest get_servers_request;+      GetServersResponse get_servers_response;+      ClientContext get_servers_context;+      get_servers_context.set_deadline(+          grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+      get_servers_request.set_start_server_id(server_start_id);+      Status status = channelz_stub_->GetServers(+          &get_servers_context, get_servers_request, &get_servers_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR,+                ""GetServers RPC with GetServersRequest.server_start_id=%d ""+                ""failed: %s"",+                int(server_start_id),+                get_servers_context.debug_error_string().c_str());+        GPR_ASSERT(0);+      }+      for (const auto& _server : get_servers_response.server()) {+        all_servers_.push_back(_server);+        StoreServerInJson(_server);+      }+      if (!get_servers_response.end()) {+        server_start_id = GetServerID(all_servers_.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of servers = "" << all_servers_.size() << std::endl;+  }++  // Get sockets that belongs to servers+  // Store sockets for dumping data+  void GetSocketsOfServers() {+    for (const auto& _server : all_servers_) {+      std::cout << ""Server "" << GetServerID(_server) << "" listen_socket: "";+      for (const auto& _socket : _server.listen_socket()) {+        int64_t so_id = _socket.socket_id();+        std::cout << so_id << "" "";+        if (CheckID(so_id)) {+          grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+      std::cout << std::endl;+    }+  }++  // Get all top channels, keep querying until getting all+  // Store channels for dumping data+  // No need to check id repeating for top channels+  void GetTopChannelsRPC() {+    int64_t channel_start_id = 0;+    while (true) {+      GetTopChannelsRequest get_top_channels_request;+      GetTopChannelsResponse get_top_channels_response;+      ClientContext get_top_channels_context;+      get_top_channels_context.set_deadline(+          grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+      get_top_channels_request.set_start_channel_id(channel_start_id);+      Status status = channelz_stub_->GetTopChannels(+          &get_top_channels_context, get_top_channels_request,+          &get_top_channels_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR,+                ""GetTopChannels RPC with ""+                ""GetTopChannelsRequest.channel_start_id=%d failed: %s"",+                int(channel_start_id),+                get_top_channels_context.debug_error_string().c_str());+        GPR_ASSERT(0);+      }+      for (const auto& _topchannel : get_top_channels_response.channel()) {+        top_channels_.push_back(_topchannel);+        all_channels_.push_back(_topchannel);+        StoreChannelInJson(_topchannel);+      }+      if (!get_top_channels_response.end()) {+        channel_start_id = GetChannelID(top_channels_.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of top channels = "" << top_channels_.size()+              << std::endl;+  }++  // layer traverse for each top channel+  void TraverseTopChannels() {+    for (const auto& _topchannel : top_channels_) {+      int tree_depth = 0;+      std::queue<grpc::channelz::v1::Channel> channel_queue;+      std::queue<grpc::channelz::v1::Subchannel> subchannel_queue;+      std::cout << ""Tree depth = "" << tree_depth << std::endl;+      GetChannelDescedence(_topchannel, channel_queue, subchannel_queue);+      while (!channel_queue.empty() || !subchannel_queue.empty()) {+        ++tree_depth;+        std::cout << ""Tree depth = "" << tree_depth << std::endl;+        int ch_q_size = channel_queue.size();+        int subch_q_size = subchannel_queue.size();+        for (int i = 0; i < ch_q_size; ++i) {+          grpc::channelz::v1::Channel ch = channel_queue.front();+          channel_queue.pop();+          GetChannelDescedence(ch, channel_queue, subchannel_queue);+        }+        for (int i = 0; i < subch_q_size; ++i) {+          grpc::channelz::v1::Subchannel subch = subchannel_queue.front();+          subchannel_queue.pop();+          GetSubchannelDescedence(subch, channel_queue, subchannel_queue);+        }+      }+    }+  }++  // dump data of all entities to stdout+  void DumpStdout() {+    std::string data_str;+    for (const auto& _channel : all_channels_) {+      std::cout << ""channel "" << GetChannelID(_channel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_channel.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _subchannel : all_subchannels_) {+      std::cout << ""subchannel "" << GetSubchannelID(_subchannel)","Nit: for this (and all other objects  where applicable e.g. channels, sockets, etc.), can we please print the `name` of the server along with it's global id -- i.e. this field: https://github.com/grpc/grpc-proto/blob/0020624375a8ee4c7dd9b3e513e443b90bc28990/grpc/channelz/v1/channelz.proto#L193 ?",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23860,480902270,2020-09-01T07:09:09Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,530 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <unistd.h>++#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>++#include ""absl/strings/str_format.h""+#include ""gflags/gflags.h""+#include ""google/protobuf/text_format.h""+#include ""grpc/grpc.h""+#include ""grpc/support/port_platform.h""+#include ""grpcpp/channel.h""+#include ""grpcpp/client_context.h""+#include ""grpcpp/create_channel.h""+#include ""grpcpp/ext/channelz_service_plugin.h""+#include ""grpcpp/grpcpp.h""+#include ""grpcpp/security/credentials.h""+#include ""grpcpp/security/server_credentials.h""+#include ""grpcpp/server.h""+#include ""grpcpp/server_builder.h""+#include ""grpcpp/server_context.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++namespace {+using grpc::ClientContext;+using grpc::Status;+using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;+}  // namespace++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;+    get_channel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetChannel(+        &get_channel_context, get_channel_request, &get_channel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetChannelRPC failed: %s"",+              get_channel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_channel_response.channel();+  }++  // Get a subchannel based on subchannel_id+  grpc::channelz::v1::Subchannel GetSubchannelRPC(int64_t subchannel_id) {+    GetSubchannelRequest get_subchannel_request;+    get_subchannel_request.set_subchannel_id(subchannel_id);+    GetSubchannelResponse get_subchannel_response;+    ClientContext get_subchannel_context;+    get_subchannel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSubchannel(&get_subchannel_context,+                                                  get_subchannel_request,+                                                  &get_subchannel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSubchannelRPC failed: %s"",+              get_subchannel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_subchannel_response.subchannel();+  }++  // get a socket based on socket_id+  grpc::channelz::v1::Socket GetSocketRPC(int64_t socket_id) {+    GetSocketRequest get_socket_request;+    get_socket_request.set_socket_id(socket_id);+    GetSocketResponse get_socket_response;+    ClientContext get_socket_context;+    get_socket_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSocket(+        &get_socket_context, get_socket_request, &get_socket_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSocketRPC failed: %s"",+              get_socket_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_socket_response.socket();+  }++  // get the descedent channels/subchannels/sockets of a channel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetChannelDescedence(+      const grpc::channelz::v1::Channel& channel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+    if (channel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : channel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (channel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : channel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (channel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : channel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // get the descedent channels/subchannels/sockets of a subchannel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetSubchannelDescedence(+      grpc::channelz::v1::Subchannel& subchannel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+              << "" descendence - "";+    if (subchannel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : subchannel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (subchannel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : subchannel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (subchannel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : subchannel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // Set up the channelz sampler client+  // Initialize json as an array+  void Setup(const std::string& custom_credentials_type,+             const std::string& server_address) {+    json_ = grpc_core::Json::Array();+    rpc_timeout_seconds_ = 20;+    grpc::ChannelArguments channel_args;+    std::shared_ptr<grpc::ChannelCredentials> channel_creds =+        grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+            custom_credentials_type, &channel_args);+    std::shared_ptr<grpc::Channel> channel =+        CreateChannel(server_address, channel_creds);+    channelz_stub_ = grpc::channelz::v1::Channelz::NewStub(channel);+  }++  // Get all servers, keep querying until getting all+  // Store servers for dumping data+  // Need to check id repeating for servers+  void GetServersRPC() {+    int64_t server_start_id = 0;+    while (true) {+      GetServersRequest get_servers_request;+      GetServersResponse get_servers_response;+      ClientContext get_servers_context;+      get_servers_context.set_deadline(+          grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+      get_servers_request.set_start_server_id(server_start_id);+      Status status = channelz_stub_->GetServers(+          &get_servers_context, get_servers_request, &get_servers_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR,+                ""GetServers RPC with GetServersRequest.server_start_id=%d ""+                ""failed: %s"",+                int(server_start_id),+                get_servers_context.debug_error_string().c_str());+        GPR_ASSERT(0);+      }+      for (const auto& _server : get_servers_response.server()) {+        all_servers_.push_back(_server);+        StoreServerInJson(_server);+      }+      if (!get_servers_response.end()) {+        server_start_id = GetServerID(all_servers_.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of servers = "" << all_servers_.size() << std::endl;+  }++  // Get sockets that belongs to servers+  // Store sockets for dumping data+  void GetSocketsOfServers() {+    for (const auto& _server : all_servers_) {+      std::cout << ""Server "" << GetServerID(_server) << "" listen_socket: "";+      for (const auto& _socket : _server.listen_socket()) {+        int64_t so_id = _socket.socket_id();+        std::cout << so_id << "" "";+        if (CheckID(so_id)) {+          grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+      std::cout << std::endl;+    }+  }++  // Get all top channels, keep querying until getting all+  // Store channels for dumping data+  // No need to check id repeating for top channels+  void GetTopChannelsRPC() {+    int64_t channel_start_id = 0;+    while (true) {+      GetTopChannelsRequest get_top_channels_request;+      GetTopChannelsResponse get_top_channels_response;+      ClientContext get_top_channels_context;+      get_top_channels_context.set_deadline(+          grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+      get_top_channels_request.set_start_channel_id(channel_start_id);+      Status status = channelz_stub_->GetTopChannels(+          &get_top_channels_context, get_top_channels_request,+          &get_top_channels_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR,+                ""GetTopChannels RPC with ""+                ""GetTopChannelsRequest.channel_start_id=%d failed: %s"",+                int(channel_start_id),+                get_top_channels_context.debug_error_string().c_str());+        GPR_ASSERT(0);+      }+      for (const auto& _topchannel : get_top_channels_response.channel()) {+        top_channels_.push_back(_topchannel);+        all_channels_.push_back(_topchannel);+        StoreChannelInJson(_topchannel);+      }+      if (!get_top_channels_response.end()) {+        channel_start_id = GetChannelID(top_channels_.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of top channels = "" << top_channels_.size()+              << std::endl;+  }++  // layer traverse for each top channel+  void TraverseTopChannels() {+    for (const auto& _topchannel : top_channels_) {+      int tree_depth = 0;+      std::queue<grpc::channelz::v1::Channel> channel_queue;+      std::queue<grpc::channelz::v1::Subchannel> subchannel_queue;+      std::cout << ""Tree depth = "" << tree_depth << std::endl;+      GetChannelDescedence(_topchannel, channel_queue, subchannel_queue);+      while (!channel_queue.empty() || !subchannel_queue.empty()) {+        ++tree_depth;+        std::cout << ""Tree depth = "" << tree_depth << std::endl;+        int ch_q_size = channel_queue.size();+        int subch_q_size = subchannel_queue.size();+        for (int i = 0; i < ch_q_size; ++i) {+          grpc::channelz::v1::Channel ch = channel_queue.front();+          channel_queue.pop();+          GetChannelDescedence(ch, channel_queue, subchannel_queue);+        }+        for (int i = 0; i < subch_q_size; ++i) {+          grpc::channelz::v1::Subchannel subch = subchannel_queue.front();+          subchannel_queue.pop();+          GetSubchannelDescedence(subch, channel_queue, subchannel_queue);+        }+      }+    }+  }++  // dump data of all entities to stdout+  void DumpStdout() {+    std::string data_str;+    for (const auto& _channel : all_channels_) {+      std::cout << ""channel "" << GetChannelID(_channel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_channel.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _subchannel : all_subchannels_) {+      std::cout << ""subchannel "" << GetSubchannelID(_subchannel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_subchannel.data(),+                                                    &data_str);","nit: can we please put a blank line at the end of the printed data?Otherwise, I think it's not totally clear where the text-printed proto message ends and where the next channelz output begins.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/23860,480907108,2020-09-01T07:15:24Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,530 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <unistd.h>++#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>++#include ""absl/strings/str_format.h""+#include ""gflags/gflags.h""+#include ""google/protobuf/text_format.h""+#include ""grpc/grpc.h""+#include ""grpc/support/port_platform.h""+#include ""grpcpp/channel.h""+#include ""grpcpp/client_context.h""+#include ""grpcpp/create_channel.h""+#include ""grpcpp/ext/channelz_service_plugin.h""+#include ""grpcpp/grpcpp.h""+#include ""grpcpp/security/credentials.h""+#include ""grpcpp/security/server_credentials.h""+#include ""grpcpp/server.h""+#include ""grpcpp/server_builder.h""+#include ""grpcpp/server_context.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++namespace {+using grpc::ClientContext;+using grpc::Status;+using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;+}  // namespace++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;+    get_channel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetChannel(+        &get_channel_context, get_channel_request, &get_channel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetChannelRPC failed: %s"",+              get_channel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_channel_response.channel();+  }++  // Get a subchannel based on subchannel_id+  grpc::channelz::v1::Subchannel GetSubchannelRPC(int64_t subchannel_id) {+    GetSubchannelRequest get_subchannel_request;+    get_subchannel_request.set_subchannel_id(subchannel_id);+    GetSubchannelResponse get_subchannel_response;+    ClientContext get_subchannel_context;+    get_subchannel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSubchannel(&get_subchannel_context,+                                                  get_subchannel_request,+                                                  &get_subchannel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSubchannelRPC failed: %s"",+              get_subchannel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_subchannel_response.subchannel();+  }++  // get a socket based on socket_id+  grpc::channelz::v1::Socket GetSocketRPC(int64_t socket_id) {+    GetSocketRequest get_socket_request;+    get_socket_request.set_socket_id(socket_id);+    GetSocketResponse get_socket_response;+    ClientContext get_socket_context;+    get_socket_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSocket(+        &get_socket_context, get_socket_request, &get_socket_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSocketRPC failed: %s"",+              get_socket_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_socket_response.socket();+  }++  // get the descedent channels/subchannels/sockets of a channel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetChannelDescedence(+      const grpc::channelz::v1::Channel& channel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+    if (channel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : channel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (channel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : channel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (channel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : channel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // get the descedent channels/subchannels/sockets of a subchannel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetSubchannelDescedence(+      grpc::channelz::v1::Subchannel& subchannel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+              << "" descendence - "";+    if (subchannel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : subchannel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (subchannel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : subchannel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (subchannel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : subchannel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // Set up the channelz sampler client+  // Initialize json as an array+  void Setup(const std::string& custom_credentials_type,+             const std::string& server_address) {+    json_ = grpc_core::Json::Array();+    rpc_timeout_seconds_ = 20;+    grpc::ChannelArguments channel_args;+    std::shared_ptr<grpc::ChannelCredentials> channel_creds =+        grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+            custom_credentials_type, &channel_args);+    std::shared_ptr<grpc::Channel> channel =+        CreateChannel(server_address, channel_creds);+    channelz_stub_ = grpc::channelz::v1::Channelz::NewStub(channel);+  }++  // Get all servers, keep querying until getting all+  // Store servers for dumping data+  // Need to check id repeating for servers+  void GetServersRPC() {+    int64_t server_start_id = 0;+    while (true) {+      GetServersRequest get_servers_request;+      GetServersResponse get_servers_response;+      ClientContext get_servers_context;+      get_servers_context.set_deadline(+          grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+      get_servers_request.set_start_server_id(server_start_id);+      Status status = channelz_stub_->GetServers(+          &get_servers_context, get_servers_request, &get_servers_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR,+                ""GetServers RPC with GetServersRequest.server_start_id=%d ""+                ""failed: %s"",+                int(server_start_id),+                get_servers_context.debug_error_string().c_str());+        GPR_ASSERT(0);+      }+      for (const auto& _server : get_servers_response.server()) {+        all_servers_.push_back(_server);+        StoreServerInJson(_server);+      }+      if (!get_servers_response.end()) {+        server_start_id = GetServerID(all_servers_.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of servers = "" << all_servers_.size() << std::endl;+  }++  // Get sockets that belongs to servers+  // Store sockets for dumping data+  void GetSocketsOfServers() {+    for (const auto& _server : all_servers_) {+      std::cout << ""Server "" << GetServerID(_server) << "" listen_socket: "";+      for (const auto& _socket : _server.listen_socket()) {+        int64_t so_id = _socket.socket_id();+        std::cout << so_id << "" "";+        if (CheckID(so_id)) {+          grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+      std::cout << std::endl;+    }+  }++  // Get all top channels, keep querying until getting all+  // Store channels for dumping data+  // No need to check id repeating for top channels+  void GetTopChannelsRPC() {+    int64_t channel_start_id = 0;+    while (true) {+      GetTopChannelsRequest get_top_channels_request;+      GetTopChannelsResponse get_top_channels_response;+      ClientContext get_top_channels_context;+      get_top_channels_context.set_deadline(+          grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+      get_top_channels_request.set_start_channel_id(channel_start_id);+      Status status = channelz_stub_->GetTopChannels(+          &get_top_channels_context, get_top_channels_request,+          &get_top_channels_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR,+                ""GetTopChannels RPC with ""+                ""GetTopChannelsRequest.channel_start_id=%d failed: %s"",+                int(channel_start_id),+                get_top_channels_context.debug_error_string().c_str());+        GPR_ASSERT(0);+      }+      for (const auto& _topchannel : get_top_channels_response.channel()) {+        top_channels_.push_back(_topchannel);+        all_channels_.push_back(_topchannel);+        StoreChannelInJson(_topchannel);+      }+      if (!get_top_channels_response.end()) {+        channel_start_id = GetChannelID(top_channels_.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of top channels = "" << top_channels_.size()+              << std::endl;+  }++  // layer traverse for each top channel+  void TraverseTopChannels() {+    for (const auto& _topchannel : top_channels_) {+      int tree_depth = 0;+      std::queue<grpc::channelz::v1::Channel> channel_queue;+      std::queue<grpc::channelz::v1::Subchannel> subchannel_queue;+      std::cout << ""Tree depth = "" << tree_depth << std::endl;+      GetChannelDescedence(_topchannel, channel_queue, subchannel_queue);+      while (!channel_queue.empty() || !subchannel_queue.empty()) {+        ++tree_depth;+        std::cout << ""Tree depth = "" << tree_depth << std::endl;+        int ch_q_size = channel_queue.size();+        int subch_q_size = subchannel_queue.size();+        for (int i = 0; i < ch_q_size; ++i) {+          grpc::channelz::v1::Channel ch = channel_queue.front();+          channel_queue.pop();+          GetChannelDescedence(ch, channel_queue, subchannel_queue);+        }+        for (int i = 0; i < subch_q_size; ++i) {+          grpc::channelz::v1::Subchannel subch = subchannel_queue.front();+          subchannel_queue.pop();+          GetSubchannelDescedence(subch, channel_queue, subchannel_queue);+        }+      }+    }+  }++  // dump data of all entities to stdout+  void DumpStdout() {+    std::string data_str;+    for (const auto& _channel : all_channels_) {+      std::cout << ""channel "" << GetChannelID(_channel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_channel.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _subchannel : all_subchannels_) {+      std::cout << ""subchannel "" << GetSubchannelID(_subchannel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_subchannel.data(),+                                                    &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _server : all_servers_) {+      std::cout << ""server "" << GetServerID(_server) << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_server.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _socket : all_sockets_) {+      std::cout << ""socket "" << GetSocketID(_socket) << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_socket.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+  }++  // Store a channel in Json+  void StoreChannelInJson(const grpc::channelz::v1::Channel& channel) {+    std::string id = grpc::to_string(GetChannelID(channel));+    std::string type = ""Channel"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(channel.data(), &description);+    StoreEntityInJson(id, type, description);+  }++  // Store a subchannel in Json+  void StoreSubchannelInJson(const grpc::channelz::v1::Subchannel& subchannel) {+    std::string id = grpc::to_string(GetSubchannelID(subchannel));+    std::string type = ""Subchannel"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(subchannel.data(),+                                                  &description);+    StoreEntityInJson(id, type, description);",A little worried that the text-printed protobuf might not always parse as JSON Can we instead generate the `description` variable used in `StoreEntityInJson` (here and elsewhere) via on the the protobuf JSON  APIs: https://developers.google.com/protocol-buffers/docs/reference/csharp/class/google/protobuf/json-formatter,
67390330,mohanli-ml,https://api.github.com/repos/grpc/grpc/pulls/23860,481636265,2020-09-02T03:59:43Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,530 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <unistd.h>++#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>++#include ""absl/strings/str_format.h""+#include ""gflags/gflags.h""+#include ""google/protobuf/text_format.h""+#include ""grpc/grpc.h""+#include ""grpc/support/port_platform.h""+#include ""grpcpp/channel.h""+#include ""grpcpp/client_context.h""+#include ""grpcpp/create_channel.h""+#include ""grpcpp/ext/channelz_service_plugin.h""+#include ""grpcpp/grpcpp.h""+#include ""grpcpp/security/credentials.h""+#include ""grpcpp/security/server_credentials.h""+#include ""grpcpp/server.h""+#include ""grpcpp/server_builder.h""+#include ""grpcpp/server_context.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++namespace {+using grpc::ClientContext;+using grpc::Status;+using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;+}  // namespace++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;+    get_channel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetChannel(+        &get_channel_context, get_channel_request, &get_channel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetChannelRPC failed: %s"",+              get_channel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_channel_response.channel();+  }++  // Get a subchannel based on subchannel_id+  grpc::channelz::v1::Subchannel GetSubchannelRPC(int64_t subchannel_id) {+    GetSubchannelRequest get_subchannel_request;+    get_subchannel_request.set_subchannel_id(subchannel_id);+    GetSubchannelResponse get_subchannel_response;+    ClientContext get_subchannel_context;+    get_subchannel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSubchannel(&get_subchannel_context,+                                                  get_subchannel_request,+                                                  &get_subchannel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSubchannelRPC failed: %s"",+              get_subchannel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_subchannel_response.subchannel();+  }++  // get a socket based on socket_id+  grpc::channelz::v1::Socket GetSocketRPC(int64_t socket_id) {+    GetSocketRequest get_socket_request;+    get_socket_request.set_socket_id(socket_id);+    GetSocketResponse get_socket_response;+    ClientContext get_socket_context;+    get_socket_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSocket(+        &get_socket_context, get_socket_request, &get_socket_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSocketRPC failed: %s"",+              get_socket_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_socket_response.socket();+  }++  // get the descedent channels/subchannels/sockets of a channel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetChannelDescedence(+      const grpc::channelz::v1::Channel& channel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+    if (channel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : channel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (channel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : channel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (channel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : channel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // get the descedent channels/subchannels/sockets of a subchannel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetSubchannelDescedence(+      grpc::channelz::v1::Subchannel& subchannel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+              << "" descendence - "";+    if (subchannel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : subchannel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (subchannel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : subchannel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (subchannel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : subchannel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // Set up the channelz sampler client+  // Initialize json as an array+  void Setup(const std::string& custom_credentials_type,+             const std::string& server_address) {+    json_ = grpc_core::Json::Array();+    rpc_timeout_seconds_ = 20;+    grpc::ChannelArguments channel_args;+    std::shared_ptr<grpc::ChannelCredentials> channel_creds =+        grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+            custom_credentials_type, &channel_args);+    std::shared_ptr<grpc::Channel> channel =+        CreateChannel(server_address, channel_creds);+    channelz_stub_ = grpc::channelz::v1::Channelz::NewStub(channel);+  }++  // Get all servers, keep querying until getting all+  // Store servers for dumping data+  // Need to check id repeating for servers+  void GetServersRPC() {+    int64_t server_start_id = 0;+    while (true) {+      GetServersRequest get_servers_request;+      GetServersResponse get_servers_response;+      ClientContext get_servers_context;+      get_servers_context.set_deadline(+          grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+      get_servers_request.set_start_server_id(server_start_id);+      Status status = channelz_stub_->GetServers(+          &get_servers_context, get_servers_request, &get_servers_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR,+                ""GetServers RPC with GetServersRequest.server_start_id=%d ""+                ""failed: %s"",+                int(server_start_id),+                get_servers_context.debug_error_string().c_str());+        GPR_ASSERT(0);+      }+      for (const auto& _server : get_servers_response.server()) {+        all_servers_.push_back(_server);+        StoreServerInJson(_server);+      }+      if (!get_servers_response.end()) {+        server_start_id = GetServerID(all_servers_.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of servers = "" << all_servers_.size() << std::endl;+  }++  // Get sockets that belongs to servers+  // Store sockets for dumping data+  void GetSocketsOfServers() {+    for (const auto& _server : all_servers_) {+      std::cout << ""Server "" << GetServerID(_server) << "" listen_socket: "";+      for (const auto& _socket : _server.listen_socket()) {+        int64_t so_id = _socket.socket_id();+        std::cout << so_id << "" "";+        if (CheckID(so_id)) {+          grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+      std::cout << std::endl;+    }+  }++  // Get all top channels, keep querying until getting all+  // Store channels for dumping data+  // No need to check id repeating for top channels+  void GetTopChannelsRPC() {+    int64_t channel_start_id = 0;+    while (true) {+      GetTopChannelsRequest get_top_channels_request;+      GetTopChannelsResponse get_top_channels_response;+      ClientContext get_top_channels_context;+      get_top_channels_context.set_deadline(+          grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+      get_top_channels_request.set_start_channel_id(channel_start_id);+      Status status = channelz_stub_->GetTopChannels(+          &get_top_channels_context, get_top_channels_request,+          &get_top_channels_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR,+                ""GetTopChannels RPC with ""+                ""GetTopChannelsRequest.channel_start_id=%d failed: %s"",+                int(channel_start_id),+                get_top_channels_context.debug_error_string().c_str());+        GPR_ASSERT(0);+      }+      for (const auto& _topchannel : get_top_channels_response.channel()) {+        top_channels_.push_back(_topchannel);+        all_channels_.push_back(_topchannel);+        StoreChannelInJson(_topchannel);+      }+      if (!get_top_channels_response.end()) {+        channel_start_id = GetChannelID(top_channels_.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of top channels = "" << top_channels_.size()+              << std::endl;+  }++  // layer traverse for each top channel+  void TraverseTopChannels() {+    for (const auto& _topchannel : top_channels_) {+      int tree_depth = 0;+      std::queue<grpc::channelz::v1::Channel> channel_queue;+      std::queue<grpc::channelz::v1::Subchannel> subchannel_queue;+      std::cout << ""Tree depth = "" << tree_depth << std::endl;+      GetChannelDescedence(_topchannel, channel_queue, subchannel_queue);+      while (!channel_queue.empty() || !subchannel_queue.empty()) {+        ++tree_depth;+        std::cout << ""Tree depth = "" << tree_depth << std::endl;+        int ch_q_size = channel_queue.size();+        int subch_q_size = subchannel_queue.size();+        for (int i = 0; i < ch_q_size; ++i) {+          grpc::channelz::v1::Channel ch = channel_queue.front();+          channel_queue.pop();+          GetChannelDescedence(ch, channel_queue, subchannel_queue);+        }+        for (int i = 0; i < subch_q_size; ++i) {+          grpc::channelz::v1::Subchannel subch = subchannel_queue.front();+          subchannel_queue.pop();+          GetSubchannelDescedence(subch, channel_queue, subchannel_queue);+        }+      }+    }+  }++  // dump data of all entities to stdout+  void DumpStdout() {+    std::string data_str;+    for (const auto& _channel : all_channels_) {+      std::cout << ""channel "" << GetChannelID(_channel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_channel.data(), &data_str);","This is because channelz entity data is using `google.protobuf.Timestamp`, which is defined here: https://github.com/protocolbuffers/protobuf/blob/master/src/google/protobuf/timestamp.proto#L136. `TextFormat::PrintToString` just convert a protobuf message to a string, and thus we get this unintuitive time. `std::localtime` can do the conversion, but not sure how to handle it with protobuf message. So added a TODO.  ",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/23710,483122279,2020-09-03T16:53:28Z,src/core/lib/iomgr/exec_ctx.h,"@@ -346,8 +352,7 @@ class ApplicationCallbackExecCtx {     functor->internal_success = is_success;     functor->internal_next = nullptr; -    auto* ctx = reinterpret_cast<ApplicationCallbackExecCtx*>(-        gpr_tls_get(&callback_exec_ctx_));+    auto* ctx = Get();","I know this is unrelated, but as long as you're cleaning up, can you specify a type instead of auto here? I think this piece of old code got in when I was in an ""auto all the things phase,"" before I was paying attention to that detail of the style guide.",X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/23710,483123939,2020-09-03T16:56:02Z,src/core/lib/surface/init.cc,"@@ -208,18 +211,44 @@ void grpc_shutdown_internal(void* /*ignored*/) {   grpc_shutdown_internal_locked(); } +static bool can_shutdown_run_synchronously() {+  // Can't do if it's running on the internal callback thread.+  grpc_core::ApplicationCallbackExecCtx* acec =+      grpc_core::ApplicationCallbackExecCtx::Get();+  if (acec != nullptr &&+      ((acec->Flags() & GRPC_APP_CALLBACK_EXEC_CTX_FLAG_IS_INTERNAL_THREAD) !=+       0)) {+    return false;+  }+  // Can't do if it's running on the iomgr poller thread.+  if (grpc_iomgr_is_any_background_poller_thread()) {","Why isn't this just lumped into the previous if condition? shutdown isn't time-sensitive so the if order shouldn't matter, just let it be whatever is readable.And then that always brings up - why have `if (x) { return false; } else { return true; }` which is basically like saying `return !x` - which maybe means remove this function altogether since it's only called from one place and just making it part of the if condition where it is called.",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/23710,483124467,2020-09-03T16:56:58Z,src/core/lib/surface/init.cc,"@@ -208,18 +211,44 @@ void grpc_shutdown_internal(void* /*ignored*/) {   grpc_shutdown_internal_locked(); } +static bool can_shutdown_run_synchronously() {+  // Can't do if it's running on the internal callback thread.+  grpc_core::ApplicationCallbackExecCtx* acec =+      grpc_core::ApplicationCallbackExecCtx::Get();+  if (acec != nullptr &&+      ((acec->Flags() & GRPC_APP_CALLBACK_EXEC_CTX_FLAG_IS_INTERNAL_THREAD) !=+       0)) {+    return false;+  }+  // Can't do if it's running on the iomgr poller thread.+  if (grpc_iomgr_is_any_background_poller_thread()) {+    return false;+  }+  // Otherwise, can.+  return true;+}+ void grpc_shutdown(void) {   GRPC_API_TRACE(""grpc_shutdown(void)"", 0, ());   grpc_core::MutexLock lock(&g_init_mu);+   if (--g_initializations == 0) {-    g_initializations++;-    g_shutting_down = true;-    // spawn a detached thread to do the actual clean up in case we are-    // currently in an executor thread.-    grpc_core::Thread cleanup_thread(-        ""grpc_shutdown"", grpc_shutdown_internal, nullptr, nullptr,-        grpc_core::Thread::Options().set_joinable(false).set_tracked(false));-    cleanup_thread.Start();+    if (can_shutdown_run_synchronously()) {","Yeah, seeing this code here makes me strongly repeat what I wrote above. Just write out all the if conditions here as to whether or not we can shutdown synchronously. You can put the comments here itself so that anyone changing shutdown in the future doesn't have to navigate the code to understand this.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/23710,483205088,2020-09-03T19:25:39Z,src/core/lib/surface/init.cc,"@@ -208,18 +211,44 @@ void grpc_shutdown_internal(void* /*ignored*/) {   grpc_shutdown_internal_locked(); } +static bool can_shutdown_run_synchronously() {+  // Can't do if it's running on the internal callback thread.+  grpc_core::ApplicationCallbackExecCtx* acec =+      grpc_core::ApplicationCallbackExecCtx::Get();+  if (acec != nullptr &&+      ((acec->Flags() & GRPC_APP_CALLBACK_EXEC_CTX_FLAG_IS_INTERNAL_THREAD) !=+       0)) {+    return false;+  }+  // Can't do if it's running on the iomgr poller thread.+  if (grpc_iomgr_is_any_background_poller_thread()) {","This was done deliberately done to expose how it check if it's possible to call sync shutdown. Given the small number of conditions, they could be combined into one return statement.",
67390330,mohanli-ml,https://api.github.com/repos/grpc/grpc/pulls/23860,484008974,2020-09-06T01:51:49Z,test/cpp/util/channelz_sampler.cc,"@@ -0,0 +1,530 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */+#include <unistd.h>++#include <cstdlib>+#include <fstream>+#include <iostream>+#include <memory>+#include <ostream>+#include <queue>+#include <string>++#include ""absl/strings/str_format.h""+#include ""gflags/gflags.h""+#include ""google/protobuf/text_format.h""+#include ""grpc/grpc.h""+#include ""grpc/support/port_platform.h""+#include ""grpcpp/channel.h""+#include ""grpcpp/client_context.h""+#include ""grpcpp/create_channel.h""+#include ""grpcpp/ext/channelz_service_plugin.h""+#include ""grpcpp/grpcpp.h""+#include ""grpcpp/security/credentials.h""+#include ""grpcpp/security/server_credentials.h""+#include ""grpcpp/server.h""+#include ""grpcpp/server_builder.h""+#include ""grpcpp/server_context.h""+#include ""src/core/lib/json/json.h""+#include ""src/cpp/server/channelz/channelz_service.h""+#include ""src/proto/grpc/channelz/channelz.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/cpp/util/test_config.h""+#include ""test/cpp/util/test_credentials_provider.h""++DEFINE_string(server_address, """", ""channelz server address"");+DEFINE_string(custom_credentials_type, """", ""custom credentials type"");+DEFINE_int64(sampling_times, 1, ""number of sampling"");+DEFINE_int64(sampling_interval_seconds, 0, ""sampling interval in seconds"");+DEFINE_string(output_json, """", ""output filename in json format"");++namespace {+using grpc::ClientContext;+using grpc::Status;+using grpc::channelz::v1::GetChannelRequest;+using grpc::channelz::v1::GetChannelResponse;+using grpc::channelz::v1::GetServerRequest;+using grpc::channelz::v1::GetServerResponse;+using grpc::channelz::v1::GetServerSocketsRequest;+using grpc::channelz::v1::GetServerSocketsResponse;+using grpc::channelz::v1::GetServersRequest;+using grpc::channelz::v1::GetServersResponse;+using grpc::channelz::v1::GetSocketRequest;+using grpc::channelz::v1::GetSocketResponse;+using grpc::channelz::v1::GetSubchannelRequest;+using grpc::channelz::v1::GetSubchannelResponse;+using grpc::channelz::v1::GetTopChannelsRequest;+using grpc::channelz::v1::GetTopChannelsResponse;+}  // namespace++class ChannelzSampler final {+ public:+  // Get server_id of a server+  int64_t GetServerID(const grpc::channelz::v1::Server& server) {+    return server.ref().server_id();+  }++  // Get channel_id of a channel+  inline int64_t GetChannelID(const grpc::channelz::v1::Channel& channel) {+    return channel.ref().channel_id();+  }++  // Get subchannel_id of a subchannel+  inline int64_t GetSubchannelID(+      const grpc::channelz::v1::Subchannel& subchannel) {+    return subchannel.ref().subchannel_id();+  }++  // Get socket_id of a socket+  inline int64_t GetSocketID(const grpc::channelz::v1::Socket& socket) {+    return socket.ref().socket_id();+  }++  // Get a channel based on channel_id+  grpc::channelz::v1::Channel GetChannelRPC(int64_t channel_id) {+    GetChannelRequest get_channel_request;+    get_channel_request.set_channel_id(channel_id);+    GetChannelResponse get_channel_response;+    ClientContext get_channel_context;+    get_channel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetChannel(+        &get_channel_context, get_channel_request, &get_channel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetChannelRPC failed: %s"",+              get_channel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_channel_response.channel();+  }++  // Get a subchannel based on subchannel_id+  grpc::channelz::v1::Subchannel GetSubchannelRPC(int64_t subchannel_id) {+    GetSubchannelRequest get_subchannel_request;+    get_subchannel_request.set_subchannel_id(subchannel_id);+    GetSubchannelResponse get_subchannel_response;+    ClientContext get_subchannel_context;+    get_subchannel_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSubchannel(&get_subchannel_context,+                                                  get_subchannel_request,+                                                  &get_subchannel_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSubchannelRPC failed: %s"",+              get_subchannel_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_subchannel_response.subchannel();+  }++  // get a socket based on socket_id+  grpc::channelz::v1::Socket GetSocketRPC(int64_t socket_id) {+    GetSocketRequest get_socket_request;+    get_socket_request.set_socket_id(socket_id);+    GetSocketResponse get_socket_response;+    ClientContext get_socket_context;+    get_socket_context.set_deadline(+        grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+    Status status = channelz_stub_->GetSocket(+        &get_socket_context, get_socket_request, &get_socket_response);+    if (!status.ok()) {+      gpr_log(GPR_ERROR, ""GetSocketRPC failed: %s"",+              get_socket_context.debug_error_string().c_str());+      GPR_ASSERT(0);+    }+    return get_socket_response.socket();+  }++  // get the descedent channels/subchannels/sockets of a channel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetChannelDescedence(+      const grpc::channelz::v1::Channel& channel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Channel "" << GetChannelID(channel) << "" descendence - "";+    if (channel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : channel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (channel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : channel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (channel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : channel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // get the descedent channels/subchannels/sockets of a subchannel+  // push descedent channels/subchannels to queue for layer traverse+  // store descedent channels/subchannels/sockets for dumping data+  void GetSubchannelDescedence(+      grpc::channelz::v1::Subchannel& subchannel,+      std::queue<grpc::channelz::v1::Channel>& channel_queue,+      std::queue<grpc::channelz::v1::Subchannel>& subchannel_queue) {+    std::cout << ""    Subchannel "" << GetSubchannelID(subchannel)+              << "" descendence - "";+    if (subchannel.channel_ref_size() > 0) {+      std::cout << ""channel: "";+      for (const auto& _channelref : subchannel.channel_ref()) {+        int64_t ch_id = _channelref.channel_id();+        std::cout << ch_id << "" "";+        grpc::channelz::v1::Channel ch = GetChannelRPC(ch_id);+        channel_queue.push(ch);+        if (CheckID(ch_id)) {+          all_channels_.push_back(ch);+          StoreChannelInJson(ch);+        }+      }+    }+    if (subchannel.subchannel_ref_size() > 0) {+      std::cout << ""subchannel: "";+      for (const auto& _subchannelref : subchannel.subchannel_ref()) {+        int64_t subch_id = _subchannelref.subchannel_id();+        std::cout << subch_id << "" "";+        grpc::channelz::v1::Subchannel subch = GetSubchannelRPC(subch_id);+        subchannel_queue.push(subch);+        if (CheckID(subch_id)) {+          all_subchannels_.push_back(subch);+          StoreSubchannelInJson(subch);+        }+      }+    }+    if (subchannel.socket_ref_size() > 0) {+      std::cout << ""socket: "";+      for (const auto& _socketref : subchannel.socket_ref()) {+        int64_t so_id = _socketref.socket_id();+        std::cout << so_id << "" "";+        grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+        if (CheckID(so_id)) {+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+    }+    std::cout << std::endl;+  }++  // Set up the channelz sampler client+  // Initialize json as an array+  void Setup(const std::string& custom_credentials_type,+             const std::string& server_address) {+    json_ = grpc_core::Json::Array();+    rpc_timeout_seconds_ = 20;+    grpc::ChannelArguments channel_args;+    std::shared_ptr<grpc::ChannelCredentials> channel_creds =+        grpc::testing::GetCredentialsProvider()->GetChannelCredentials(+            custom_credentials_type, &channel_args);+    std::shared_ptr<grpc::Channel> channel =+        CreateChannel(server_address, channel_creds);+    channelz_stub_ = grpc::channelz::v1::Channelz::NewStub(channel);+  }++  // Get all servers, keep querying until getting all+  // Store servers for dumping data+  // Need to check id repeating for servers+  void GetServersRPC() {+    int64_t server_start_id = 0;+    while (true) {+      GetServersRequest get_servers_request;+      GetServersResponse get_servers_response;+      ClientContext get_servers_context;+      get_servers_context.set_deadline(+          grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+      get_servers_request.set_start_server_id(server_start_id);+      Status status = channelz_stub_->GetServers(+          &get_servers_context, get_servers_request, &get_servers_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR,+                ""GetServers RPC with GetServersRequest.server_start_id=%d ""+                ""failed: %s"",+                int(server_start_id),+                get_servers_context.debug_error_string().c_str());+        GPR_ASSERT(0);+      }+      for (const auto& _server : get_servers_response.server()) {+        all_servers_.push_back(_server);+        StoreServerInJson(_server);+      }+      if (!get_servers_response.end()) {+        server_start_id = GetServerID(all_servers_.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of servers = "" << all_servers_.size() << std::endl;+  }++  // Get sockets that belongs to servers+  // Store sockets for dumping data+  void GetSocketsOfServers() {+    for (const auto& _server : all_servers_) {+      std::cout << ""Server "" << GetServerID(_server) << "" listen_socket: "";+      for (const auto& _socket : _server.listen_socket()) {+        int64_t so_id = _socket.socket_id();+        std::cout << so_id << "" "";+        if (CheckID(so_id)) {+          grpc::channelz::v1::Socket so = GetSocketRPC(so_id);+          all_sockets_.push_back(so);+          StoreSocketInJson(so);+        }+      }+      std::cout << std::endl;+    }+  }++  // Get all top channels, keep querying until getting all+  // Store channels for dumping data+  // No need to check id repeating for top channels+  void GetTopChannelsRPC() {+    int64_t channel_start_id = 0;+    while (true) {+      GetTopChannelsRequest get_top_channels_request;+      GetTopChannelsResponse get_top_channels_response;+      ClientContext get_top_channels_context;+      get_top_channels_context.set_deadline(+          grpc_timeout_seconds_to_deadline(rpc_timeout_seconds_));+      get_top_channels_request.set_start_channel_id(channel_start_id);+      Status status = channelz_stub_->GetTopChannels(+          &get_top_channels_context, get_top_channels_request,+          &get_top_channels_response);+      if (!status.ok()) {+        gpr_log(GPR_ERROR,+                ""GetTopChannels RPC with ""+                ""GetTopChannelsRequest.channel_start_id=%d failed: %s"",+                int(channel_start_id),+                get_top_channels_context.debug_error_string().c_str());+        GPR_ASSERT(0);+      }+      for (const auto& _topchannel : get_top_channels_response.channel()) {+        top_channels_.push_back(_topchannel);+        all_channels_.push_back(_topchannel);+        StoreChannelInJson(_topchannel);+      }+      if (!get_top_channels_response.end()) {+        channel_start_id = GetChannelID(top_channels_.back()) + 1;+      } else {+        break;+      }+    }+    std::cout << ""Number of top channels = "" << top_channels_.size()+              << std::endl;+  }++  // layer traverse for each top channel+  void TraverseTopChannels() {+    for (const auto& _topchannel : top_channels_) {+      int tree_depth = 0;+      std::queue<grpc::channelz::v1::Channel> channel_queue;+      std::queue<grpc::channelz::v1::Subchannel> subchannel_queue;+      std::cout << ""Tree depth = "" << tree_depth << std::endl;+      GetChannelDescedence(_topchannel, channel_queue, subchannel_queue);+      while (!channel_queue.empty() || !subchannel_queue.empty()) {+        ++tree_depth;+        std::cout << ""Tree depth = "" << tree_depth << std::endl;+        int ch_q_size = channel_queue.size();+        int subch_q_size = subchannel_queue.size();+        for (int i = 0; i < ch_q_size; ++i) {+          grpc::channelz::v1::Channel ch = channel_queue.front();+          channel_queue.pop();+          GetChannelDescedence(ch, channel_queue, subchannel_queue);+        }+        for (int i = 0; i < subch_q_size; ++i) {+          grpc::channelz::v1::Subchannel subch = subchannel_queue.front();+          subchannel_queue.pop();+          GetSubchannelDescedence(subch, channel_queue, subchannel_queue);+        }+      }+    }+  }++  // dump data of all entities to stdout+  void DumpStdout() {+    std::string data_str;+    for (const auto& _channel : all_channels_) {+      std::cout << ""channel "" << GetChannelID(_channel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_channel.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _subchannel : all_subchannels_) {+      std::cout << ""subchannel "" << GetSubchannelID(_subchannel)+                << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_subchannel.data(),+                                                    &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _server : all_servers_) {+      std::cout << ""server "" << GetServerID(_server) << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_server.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+    for (const auto& _socket : all_sockets_) {+      std::cout << ""socket "" << GetSocketID(_socket) << "" data:"" << std::endl;+      ::google::protobuf::TextFormat::PrintToString(_socket.data(), &data_str);+      printf(""%s"", data_str.c_str());+    }+  }++  // Store a channel in Json+  void StoreChannelInJson(const grpc::channelz::v1::Channel& channel) {+    std::string id = grpc::to_string(GetChannelID(channel));+    std::string type = ""Channel"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(channel.data(), &description);+    StoreEntityInJson(id, type, description);+  }++  // Store a subchannel in Json+  void StoreSubchannelInJson(const grpc::channelz::v1::Subchannel& subchannel) {+    std::string id = grpc::to_string(GetSubchannelID(subchannel));+    std::string type = ""Subchannel"";+    std::string description;+    ::google::protobuf::TextFormat::PrintToString(subchannel.data(),+                                                  &description);+    StoreEntityInJson(id, type, description);","Sorry still a bit confused here. `description` should already be a JSON string instead of a raw string since it is updated by `::google::protobuf::TextFormat::PrintToString(const Message & message, std::string * output)` before `StoreEntityInJson`. (https://developers.google.com/protocol-buffers/docs/reference/cpp/google.protobuf.text_format). We could use `google::protobuf::util::MessageToJsonString(const Message & message, std::string * output, const JsonOptions & options)`, but looks like the two functions are taking similar parameters?",
1471472,wenbozhu,https://api.github.com/repos/grpc/grpc/pulls/24083,485907669,2020-09-09T20:37:52Z,src/core/ext/transport/cronet/transport/cronet_status.h,"@@ -0,0 +1,59 @@+/*+ *+ * Copyright 2016 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_EXT_TRANSPORT_CRONET_TRANSPORT_CRONET_STATUS_H+#define GRPC_CORE_EXT_TRANSPORT_CRONET_TRANSPORT_CRONET_STATUS_H++/*  HTTP/2 error codes are mapped to the following cronet net error codes */",could you check if there is any similar implementation for Java?,
64815511,yulin-liang,https://api.github.com/repos/grpc/grpc/pulls/24083,485931248,2020-09-09T21:26:13Z,src/core/ext/transport/cronet/transport/cronet_status.h,"@@ -0,0 +1,59 @@+/*+ *+ * Copyright 2016 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_EXT_TRANSPORT_CRONET_TRANSPORT_CRONET_STATUS_H+#define GRPC_CORE_EXT_TRANSPORT_CRONET_TRANSPORT_CRONET_STATUS_H++/*  HTTP/2 error codes are mapped to the following cronet net error codes */",gRPC-Java has the similar implementation. The `Cronet error` will be passed to the application layer as a part of `gRPC error`. Here's the code: https://github.com/grpc/grpc-java/blob/master/cronet/src/main/java/io/grpc/cronet/CronetClientStream.java#L555-L561,
64815511,yulin-liang,https://api.github.com/repos/grpc/grpc/pulls/24083,486500025,2020-09-10T17:05:47Z,src/core/ext/transport/cronet/transport/cronet_status.h,"@@ -0,0 +1,59 @@+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_EXT_TRANSPORT_CRONET_TRANSPORT_CRONET_STATUS_H+#define GRPC_CORE_EXT_TRANSPORT_CRONET_TRANSPORT_CRONET_STATUS_H++/*  HTTP/2 error codes are mapped to the following cronet net error codes */+enum cronet_status_code {+  CRONET_STATUS_SUCCESS = 0,+  CRONET_STATUS_ILLEGAL_ARGUMENT = -100,+  CRONET_STATUS_ILLEGAL_ARGUMENT_STORAGE_PATH_MUST_EXIST = -101,+  CRONET_STATUS_ILLEGAL_ARGUMENT_INVALID_PIN = -102,+  CRONET_STATUS_ILLEGAL_ARGUMENT_INVALID_HOSTNAME = -103,+  CRONET_STATUS_ILLEGAL_ARGUMENT_INVALID_HTTP_METHOD = -104,+  CRONET_STATUS_ILLEGAL_ARGUMENT_INVALID_HTTP_HEADER = -105,+  CRONET_STATUS_ILLEGAL_STATE = -200,",Looks like -106 is not in the [Cronet_RESULT enums](https://chromium.googlesource.com/chromium/src/+/master/components/cronet/native/generated/cronet.idl_c.h)(From line 71 to 103). Have you ran into this error code with Cronet transport?,
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/24114,489056136,2020-09-15T23:02:42Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -1697,13 +1642,12 @@ ChannelData::ChannelData(grpc_channel_element_args* args, grpc_error** error)   // Get default service config   const char* service_config_json = grpc_channel_arg_get_string(       grpc_channel_args_find(args->channel_args, GRPC_ARG_SERVICE_CONFIG));-  if (service_config_json != nullptr) {-    *error = GRPC_ERROR_NONE;-    default_service_config_ = ServiceConfig::Create(service_config_json, error);-    if (*error != GRPC_ERROR_NONE) {-      default_service_config_.reset();-      return;-    }+  if (service_config_json == nullptr) service_config_json = ""{}"";","what happens if initially the resolver does not provide a service config, but later provides an incorrect service config?In that case, are we to continue using the previously used ""no service config"" or are we to go into transient failure?Also, please add a comment around the semantics of `default_service_config_` that no default service config provided as an arg means an empty service config being used as the default",
2030323,JunTaoLuo,https://api.github.com/repos/grpc/grpc/pulls/22916,489083669,2020-09-16T00:01:08Z,src/csharp/Grpc.Tools/ProtoCompile.cs,"@@ -375,60 +375,48 @@ protected override bool ValidateParameters()         static readonly Encoding s_utf8WithoutBom = new UTF8Encoding(false);         protected override Encoding ResponseFileEncoding => s_utf8WithoutBom; -        // Protoc takes one argument per line from the response file, and does not-        // require any quoting whatsoever. Otherwise, this is similar to the-        // standard CommandLineBuilder-        class ProtocResponseFileBuilder+        class ProtocCommandLineBuilder : CommandLineBuilder         {-            StringBuilder _data = new StringBuilder(1000);-            public override string ToString() => _data.ToString();--            // If 'value' is not empty, append '--name=value\n'.-            public void AddSwitchMaybe(string name, string value)+            // If 'value' is not empty, append '--name=value'.+            public void AppendSwitchUnquotedIfNotNullOrEmpty(string name, string value)             {                 if (!string.IsNullOrEmpty(value))                 {-                    _data.Append(""--"").Append(name).Append(""="")-                         .Append(value).Append('\n');+                    AppendSwitchIfNotNull($""--{name}="", value);                 }             }              // Add switch with the 'values' separated by commas, for options.-            public void AddSwitchMaybe(string name, string[] values)+            public void AppendSwitchIfNotNullOrEmpty(string name, string[] values)             {                 if (values?.Length > 0)                 {-                    _data.Append(""--"").Append(name).Append(""="")-                         .Append(string.Join("","", values)).Append('\n');+                    AppendSwitchIfNotNull($""--{name}="", values, "","");                 }             }+        } -            // Add a positional argument to the file data.-            public void AddArg(string arg)-            {-                _data.Append(arg).Append('\n');-            }-        };--        // Called by the base ToolTask to get response file contents.-        protected override string GenerateResponseFileCommands()+        // Called by the base ToolTask to get the command line.+        protected override string GenerateCommandLineCommands()         {-            var cmd = new ProtocResponseFileBuilder();-            cmd.AddSwitchMaybe(Generator + ""_out"", TrimEndSlash(OutputDir));-            cmd.AddSwitchMaybe(Generator + ""_opt"", OutputOptions);-            cmd.AddSwitchMaybe(""plugin=protoc-gen-grpc"", GrpcPluginExe);-            cmd.AddSwitchMaybe(""grpc_out"", TrimEndSlash(GrpcOutputDir));-            cmd.AddSwitchMaybe(""grpc_opt"", GrpcOutputOptions);+            var cmd = new ProtocCommandLineBuilder();","I'm concerned about the second point here since we have had issues with long paths. Even in the case where there are a few ProtoPath specified, if each contains a long file path, the long path can cause issues. I think it's better to use response files and fix the issue in protoc instead. If we must accept this workaround, I would add an option to choose between response files and command line with a default to use response files.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/24193,492392829,2020-09-21T23:00:23Z,tools/distrib/gen_compilation_database.py,"@@ -0,0 +1,130 @@+#!/usr/bin/env python3++# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This is based on the script on the Envoy project+# https://github.com/envoyproxy/envoy/blob/master/tools/gen_compilation_database.py++import argparse+import glob+import json+import logging+import os+import re+import shlex+import subprocess+from pathlib import Path++RE_INCLUDE_SYSTEM = re.compile(""\s*-I\s+/usr/[^ ]+"")+++# This method is equivalent to https://github.com/grailbio/bazel-compilation-database/blob/master/generate.sh+def generateCompilationDatabase(args):+    # We need to download all remote outputs for generated source code. This option lives here to override those+    # specified in bazelrc.+    bazel_options = shlex.split(os.environ.get(""BAZEL_BUILD_OPTIONS"", """")) + [+        ""--config=compdb"",+        ""--remote_download_outputs=all"",+    ]++    subprocess.check_call([""bazel"", ""build""] + bazel_options + [+        ""--aspects=@bazel_compdb//:aspects.bzl%compilation_database_aspect"",+        ""--output_groups=compdb_files,header_files""+    ] + args.bazel_targets)++    execroot = subprocess.check_output([""bazel"", ""info"", ""execution_root""] ++                                       bazel_options).decode().strip()++    compdb = []+    for compdb_file in Path(execroot).glob(""**/*.compile_commands.json""):+        compdb.extend(+            json.loads(+                ""["" ++                compdb_file.read_text().replace(""__EXEC_ROOT__"", execroot) ++                ""]""))+    return compdb+++def isHeader(filename):+    for ext in ("".h"", "".hh"", "".hpp"", "".hxx""):+        if filename.endswith(ext):+            return True+    return False+++def isCompileTarget(target, args):+    filename = target[""file""]+    if not args.include_headers and isHeader(filename):+        return False+    if not args.include_genfiles:+        if filename.startswith(""bazel-out/""):+            return False+    if not args.include_external:+        if filename.startswith(""external/""):+            return False+    return True+++def modifyCompileCommand(target, args):+    cc, options = target[""command""].split("" "", 1)++    # Workaround for bazel added C++11 options, those doesn't affect build itself but+    # clang-tidy will misinterpret them.+    options = options.replace(""-std=c++0x "", """")+    options = options.replace(""-std=c++11 "", """")++    if args.vscode:+        # Visual Studio Code doesn't seem to like ""-iquote"". Replace it with+        # old-style ""-I"".+        options = options.replace(""-iquote "", ""-I "")++    if args.ignore_system_headers:+        if target[""file""] == ""test/cpp/end2end/xds_end2end_test.cc"":",Oops my test code got in accidentally.,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/24205,493036977,2020-09-22T21:13:08Z,src/cpp/server/load_reporter/load_reporter_async_service_impl.cc,"@@ -358,7 +358,7 @@ void LoadReporterAsyncServiceImpl::ReportLoadHandler::Shutdown( }  void LoadReporterAsyncServiceImpl::ReportLoadHandler::OnFinishDone(-    std::shared_ptr<ReportLoadHandler> self, bool ok) {+    const std::shared_ptr<ReportLoadHandler>& self, bool ok) {",The one holding the last ref is `CallableTag` so technically this would work but I like to make this function consistent with other handlers in the class. So I'm rather reverting this and adding NOLINT comment to the function.,
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/24224,494311249,2020-09-24T13:21:28Z,src/core/lib/gprpp/dual_ref_counted.h,"@@ -0,0 +1,342 @@+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#ifndef GRPC_CORE_LIB_GPRPP_DUAL_REF_COUNTED_H+#define GRPC_CORE_LIB_GPRPP_DUAL_REF_COUNTED_H++#include <grpc/support/port_platform.h>++#include <grpc/support/atm.h>+#include <grpc/support/log.h>+#include <grpc/support/sync.h>++#include <atomic>+#include <cassert>+#include <cinttypes>++#include ""src/core/lib/debug/trace.h""+#include ""src/core/lib/gprpp/atomic.h""+#include ""src/core/lib/gprpp/debug_location.h""+#include ""src/core/lib/gprpp/orphanable.h""+#include ""src/core/lib/gprpp/ref_counted_ptr.h""++namespace grpc_core {++// DualRefCounted is an interface for reference-counted objects with two+// classes of refs: strong refs (usually just called ""refs"") and weak refs.+// Each class of refs can be incremented and decremented independently.+//+// Objects start with 1 strong ref and 0 weak refs at instantiation.+// When the strong refcount reaches 0, the object's Orphan() method is called.+// When the weak refcount reaches 0, the object is destroyed.+//+// This will be used by CRTP (curiously-recurring template pattern), e.g.:+//   class MyClass : public RefCounted<MyClass> { ... };+template <typename Child>+class DualRefCounted : public Orphanable {+ public:+  virtual ~DualRefCounted() = default;++  RefCountedPtr<Child> Ref() GRPC_MUST_USE_RESULT {+    IncrementRefCount();+    return RefCountedPtr<Child>(static_cast<Child*>(this));+  }++  RefCountedPtr<Child> Ref(const DebugLocation& location,+                           const char* reason) GRPC_MUST_USE_RESULT {+    IncrementRefCount(location, reason);+    return RefCountedPtr<Child>(static_cast<Child*>(this));+  }++  void Unref() {+#ifndef NDEBUG+    // Grab a copy of the trace flag before the atomic change, since we+    // can't safely access it afterwards if we're going to be freed.+    auto* trace_flag = trace_flag_;+#endif+    // Convert strong ref to weak ref.+    const uint64_t prev_ref_pair =","Any reason for the const in this method? I'm ok with their presence, just curious since we don't usually follow that pattern in local code.",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/24224,494351707,2020-09-24T14:10:30Z,test/core/gprpp/ref_counted_ptr_test.cc,"@@ -250,6 +255,263 @@ TEST(RefCountedPtr, CanPassSubclassToFunctionExpectingSubclass) {   FunctionTakingSubclass(p); } +//+// WeakRefCountedPtr<> tests+//++class Bar : public DualRefCounted<Bar> {+ public:+  Bar() : value_(0) {}++  explicit Bar(int value) : value_(value) {}++  ~Bar() { GPR_ASSERT(shutting_down_); }++  void Orphan() override { shutting_down_ = true; }++  int value() const { return value_; }++ private:+  int value_;+  bool shutting_down_ = false;+};++TEST(WeakRefCountedPtr, DefaultConstructor) { WeakRefCountedPtr<Bar> bar; }++TEST(WeakRefCountedPtr, ExplicitConstructorEmpty) {+  WeakRefCountedPtr<Bar> bar(nullptr);+}++TEST(WeakRefCountedPtr, ExplicitConstructor) {+  RefCountedPtr<Bar> bar_strong(new Bar());+  bar_strong->WeakRef().release();+  WeakRefCountedPtr<Bar> bar(bar_strong.get());+}++TEST(WeakRefCountedPtr, MoveConstructor) {+  RefCountedPtr<Bar> bar_strong(new Bar());+  WeakRefCountedPtr<Bar> bar = bar_strong->WeakRef();+  WeakRefCountedPtr<Bar> bar2(std::move(bar));+  EXPECT_EQ(nullptr, bar.get());","I know that you've defined the move constructor in a specific way, but let me suggest that that's an implementation detail. The semantics of std::move is that the object will be in a valid but unspecified state. So, I don't think it's ok to compare its value to nullptr after a move in general - that becomes a test of the implementation rather than a test of the concept of movability working.",
869251,renkelvin,https://api.github.com/repos/grpc/grpc/pulls/24208,494484829,2020-09-24T17:20:10Z,src/core/lib/security/credentials/external/external_account_credentials.cc,"@@ -0,0 +1,291 @@+//+// Copyright 2019 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//+#include <grpc/support/port_platform.h>++#include ""src/core/lib/security/credentials/external/external_account_credentials.h""++#include ""absl/strings/str_format.h""+#include ""absl/strings/str_join.h""+#include ""absl/time/clock.h""+#include ""absl/time/time.h""++#include ""src/core/lib/http/parser.h""+#include ""src/core/lib/security/util/json_util.h""+#include ""src/core/lib/slice/b64.h""++#define EXTERNAL_ACCOUNT_CREDENTIALS_GRANT_TYPE \+  ""urn:ietf:params:oauth:grant-type:token-exchange""+#define EXTERNAL_ACCOUNT_CREDENTIALS_REQUESTED_TOKEN_TYPE \+  ""urn:ietf:params:oauth:token-type:access_token""+#define GOOGLE_CLOUD_PLATFORM_DEFAULT_SCOPE \+  ""https://www.googleapis.com/auth/cloud-platform""++namespace grpc_core {+namespace experimental {++ExternalAccountCredentials::ExternalAccountCredentials(+    ExternalAccountCredentialsOptions options, std::vector<std::string> scopes)+    : options_(std::move(options)) {+  if (scopes.empty()) {+    scopes.push_back(GOOGLE_CLOUD_PLATFORM_DEFAULT_SCOPE);+  }+  scopes_ = scopes;","I think technically, it can be moved but I'm not sure if it's necessary.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24224,494559313,2020-09-24T19:26:23Z,src/core/lib/gprpp/dual_ref_counted.h,"@@ -0,0 +1,342 @@+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#ifndef GRPC_CORE_LIB_GPRPP_DUAL_REF_COUNTED_H+#define GRPC_CORE_LIB_GPRPP_DUAL_REF_COUNTED_H++#include <grpc/support/port_platform.h>++#include <grpc/support/atm.h>+#include <grpc/support/log.h>+#include <grpc/support/sync.h>++#include <atomic>+#include <cassert>+#include <cinttypes>++#include ""src/core/lib/debug/trace.h""+#include ""src/core/lib/gprpp/atomic.h""+#include ""src/core/lib/gprpp/debug_location.h""+#include ""src/core/lib/gprpp/orphanable.h""+#include ""src/core/lib/gprpp/ref_counted_ptr.h""++namespace grpc_core {++// DualRefCounted is an interface for reference-counted objects with two+// classes of refs: strong refs (usually just called ""refs"") and weak refs.+// Each class of refs can be incremented and decremented independently.+//+// Objects start with 1 strong ref and 0 weak refs at instantiation.+// When the strong refcount reaches 0, the object's Orphan() method is called.+// When the weak refcount reaches 0, the object is destroyed.+//+// This will be used by CRTP (curiously-recurring template pattern), e.g.:+//   class MyClass : public RefCounted<MyClass> { ... };+template <typename Child>+class DualRefCounted : public Orphanable {+ public:+  virtual ~DualRefCounted() = default;++  RefCountedPtr<Child> Ref() GRPC_MUST_USE_RESULT {+    IncrementRefCount();+    return RefCountedPtr<Child>(static_cast<Child*>(this));+  }++  RefCountedPtr<Child> Ref(const DebugLocation& location,+                           const char* reason) GRPC_MUST_USE_RESULT {+    IncrementRefCount(location, reason);+    return RefCountedPtr<Child>(static_cast<Child*>(this));+  }++  void Unref() {+#ifndef NDEBUG+    // Grab a copy of the trace flag before the atomic change, since we+    // can't safely access it afterwards if we're going to be freed.","This is something I copied from `RefCounted<>`.  In that case, we do need to grab the tracer before we decrement the atomic, because even if the current thread does not decrease the refcount to 0, it is no longer holding a ref once it decrements the atomic, which means that some other thread can come along and decrement to 0 and free before this thread gets to look at the `tracer_` field.But in this case, I think you're right that this is not needed, because the atomic change here is going to result in us still holding a weak ref, so we can be sure that no other thread will delete the object out from under us.  So I've removed it.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24224,494561119,2020-09-24T19:29:48Z,src/core/lib/gprpp/dual_ref_counted.h,"@@ -0,0 +1,342 @@+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#ifndef GRPC_CORE_LIB_GPRPP_DUAL_REF_COUNTED_H+#define GRPC_CORE_LIB_GPRPP_DUAL_REF_COUNTED_H++#include <grpc/support/port_platform.h>++#include <grpc/support/atm.h>+#include <grpc/support/log.h>+#include <grpc/support/sync.h>++#include <atomic>+#include <cassert>+#include <cinttypes>++#include ""src/core/lib/debug/trace.h""+#include ""src/core/lib/gprpp/atomic.h""+#include ""src/core/lib/gprpp/debug_location.h""+#include ""src/core/lib/gprpp/orphanable.h""+#include ""src/core/lib/gprpp/ref_counted_ptr.h""++namespace grpc_core {++// DualRefCounted is an interface for reference-counted objects with two+// classes of refs: strong refs (usually just called ""refs"") and weak refs.+// Each class of refs can be incremented and decremented independently.+//+// Objects start with 1 strong ref and 0 weak refs at instantiation.+// When the strong refcount reaches 0, the object's Orphan() method is called.+// When the weak refcount reaches 0, the object is destroyed.+//+// This will be used by CRTP (curiously-recurring template pattern), e.g.:+//   class MyClass : public RefCounted<MyClass> { ... };+template <typename Child>+class DualRefCounted : public Orphanable {","This seems like it doesn't bring a huge amount of benefit, since it's hard to image how this would be misused.  But I also don't see any harm.However, when I tried this, it generated a ton of compiler errors, which seem to stem from this:```/usr/bin/../lib/gcc/x86_64-linux-gnu/9/../../../../include/c++/9/type_traits:1302:66: error: incomplete type 'grpc_core::testing::(anonymous namespace)::Foo' used in type trait expression    : public integral_constant<bool, __is_base_of(_Base, _Derived)>                                                                 ^./src/core/lib/gprpp/dual_ref_counted.h:55:22: note: in instantiation of template class 'std::is_base_of<grpc_core::DualRefCounted<grpc_core::testing::(anonymous namespace)::Foo>, grpc_core::testing::(anonymous namespace)::Foo>' requested here  static_assert(std::is_base_of<DualRefCounted<Child>, Child>::value,                     ^test/core/gprpp/dual_ref_counted_test.cc:30:20: note: in instantiation of template class 'grpc_core::DualRefCounted<grpc_core::testing::(anonymous namespace)::Foo>' requested hereclass Foo : public DualRefCounted<Foo> {                   ^test/core/gprpp/dual_ref_counted_test.cc:30:7: note: definition of 'grpc_core::testing::(anonymous namespace)::Foo' is not complete until the closing '}'class Foo : public DualRefCounted<Foo> {      ^```So I don't think this is going to work anyway.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24224,494565760,2020-09-24T19:38:56Z,test/core/gprpp/ref_counted_ptr_test.cc,"@@ -250,6 +255,263 @@ TEST(RefCountedPtr, CanPassSubclassToFunctionExpectingSubclass) {   FunctionTakingSubclass(p); } +//+// WeakRefCountedPtr<> tests+//++class Bar : public DualRefCounted<Bar> {+ public:+  Bar() : value_(0) {}++  explicit Bar(int value) : value_(value) {}++  ~Bar() { GPR_ASSERT(shutting_down_); }++  void Orphan() override { shutting_down_ = true; }++  int value() const { return value_; }++ private:+  int value_;+  bool shutting_down_ = false;+};++TEST(WeakRefCountedPtr, DefaultConstructor) { WeakRefCountedPtr<Bar> bar; }++TEST(WeakRefCountedPtr, ExplicitConstructorEmpty) {+  WeakRefCountedPtr<Bar> bar(nullptr);+}++TEST(WeakRefCountedPtr, ExplicitConstructor) {+  RefCountedPtr<Bar> bar_strong(new Bar());+  bar_strong->WeakRef().release();+  WeakRefCountedPtr<Bar> bar(bar_strong.get());+}++TEST(WeakRefCountedPtr, MoveConstructor) {+  RefCountedPtr<Bar> bar_strong(new Bar());+  WeakRefCountedPtr<Bar> bar = bar_strong->WeakRef();+  WeakRefCountedPtr<Bar> bar2(std::move(bar));+  EXPECT_EQ(nullptr, bar.get());","I think that for a smart pointer, it's totally reasonable to assume that `std::move()` will reset the value to null.  Even if it's not guaranteed by the spec, there's basically no other implementation that makes sense, because you'd have to maintain a separate data member to indicate validity.",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/24224,494786040,2020-09-25T06:58:34Z,test/core/gprpp/ref_counted_ptr_test.cc,"@@ -250,6 +255,263 @@ TEST(RefCountedPtr, CanPassSubclassToFunctionExpectingSubclass) {   FunctionTakingSubclass(p); } +//+// WeakRefCountedPtr<> tests+//++class Bar : public DualRefCounted<Bar> {+ public:+  Bar() : value_(0) {}++  explicit Bar(int value) : value_(value) {}++  ~Bar() { GPR_ASSERT(shutting_down_); }++  void Orphan() override { shutting_down_ = true; }++  int value() const { return value_; }++ private:+  int value_;+  bool shutting_down_ = false;+};++TEST(WeakRefCountedPtr, DefaultConstructor) { WeakRefCountedPtr<Bar> bar; }++TEST(WeakRefCountedPtr, ExplicitConstructorEmpty) {+  WeakRefCountedPtr<Bar> bar(nullptr);+}++TEST(WeakRefCountedPtr, ExplicitConstructor) {+  RefCountedPtr<Bar> bar_strong(new Bar());+  bar_strong->WeakRef().release();+  WeakRefCountedPtr<Bar> bar(bar_strong.get());+}++TEST(WeakRefCountedPtr, MoveConstructor) {+  RefCountedPtr<Bar> bar_strong(new Bar());+  WeakRefCountedPtr<Bar> bar = bar_strong->WeakRef();+  WeakRefCountedPtr<Bar> bar2(std::move(bar));+  EXPECT_EQ(nullptr, bar.get());","Looking over things, I'll agree with you that this is the case with existing smart pointers; both unique_ptr and shared_ptr guarantee that the other will be empty after move assignment..... However, and here is where things get ugly, it turns out that clang-tidy doesn't like this usage pattern. I looked and saw that our internal clang-tidy has marked warnings against the analogous use in RefCountedPtr. Perhaps some sort of NOLINT tag needs to be applied somewhere to protect against this?",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/24274,497859735,2020-09-30T23:42:28Z,setup.py,"@@ -164,24 +168,33 @@  def check_linker_need_libatomic():     """"""Test if linker on system needs libatomic.""""""-    code_test = (b'#include <atomic>\n' +-                 b'int main() { return std::atomic<int64_t>{}; }')-    cpp_test = subprocess.Popen(['c++', '-x', 'c++', '-std=c++11', '-'],-                                stdin=PIPE,-                                stdout=PIPE,-                                stderr=PIPE)-    cpp_test.communicate(input=code_test)-    if cpp_test.returncode == 0:-        return False-    # Double-check to see if -latomic actually can solve the problem.-    # https://github.com/grpc/grpc/issues/22491-    cpp_test = subprocess.Popen(-        ['c++', '-x', 'c++', '-std=c++11', '-latomic', '-'],-        stdin=PIPE,-        stdout=PIPE,-        stderr=PIPE)-    cpp_test.communicate(input=code_test)-    return cpp_test.returncode == 0+    compiler = ccompiler.new_compiler()+    distutils.sysconfig.customize_compiler(compiler)","From my code reading, `customize_compiler` is the most important function here to choose compilers from environment variables such as `CC` or `CXX`, which is inevitable to support cross-platform toolchains. How about implementing the code fetching `CXX` variable to find the right compiler instead?",
28025951,HannahShiSFB,https://api.github.com/repos/grpc/grpc/pulls/24085,498551221,2020-10-01T22:56:30Z,tools/run_tests/run_tests_matrix.py,"@@ -180,15 +180,6 @@ def _create_test_jobs(extra_args=[], inner_jobs=_DEFAULT_INNER_JOBS):                                 ['--report_multi_target'],                                 inner_jobs=inner_jobs) -    # supported on linux only",It's included later at https://github.com/HannahShiSFB/grpc/blob/60b5d06ec1c069e96dc7163aad3412754716ff56/tools/run_tests/run_tests_matrix.py#L233,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/24289,499064569,2020-10-02T21:49:04Z,src/python/grpcio_tests/tests_aio/unit/init_test.py,"@@ -14,38 +14,26 @@ import logging import unittest -import grpc--from grpc.experimental import aio-from tests_aio.unit._test_server import start_test_server from tests_aio.unit._test_base import AioTestBase -from tests.unit import resources--_PRIVATE_KEY = resources.private_key()-_CERTIFICATE_CHAIN = resources.certificate_chain()-_TEST_ROOT_CERTIFICATES = resources.test_root_certificates() +class TestInit(AioTestBase): -class TestChannel(AioTestBase):+    async def test_grpc(self):+        import grpc  # pylint: disable=wrong-import-position+        channel = grpc.aio.insecure_channel('dummy')+        self.assertIsInstance(channel, grpc.aio.Channel) -    async def test_insecure_channel(self):-        server_target, _ = await start_test_server()  # pylint: disable=unused-variable+    async def test_grpc_dot_aio(self):+        import grpc.aio  # pylint: disable=wrong-import-position+        channel = grpc.aio.insecure_channel('dummy')+        self.assertIsInstance(channel, grpc.aio.Channel) -        channel = aio.insecure_channel(server_target)+    async def test_aio_from_grpc(self):+        from grpc import aio  # pylint: disable=wrong-import-position+        channel = aio.insecure_channel('dummy')         self.assertIsInstance(channel, aio.Channel) -    async def test_secure_channel(self):","The secure channel case is well tested in [secure_call_test.py](https://github.com/grpc/grpc/blob/master/src/python/grpcio_tests/tests_aio/unit/secure_call_test.py), and the insecure channel case is tested in almost all other ""unit"" tests. I think this file wasn't in a good shape as you can see the conflict between the file name and the test class name. But it has a good name for actual module initialization tests.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24193,503201468,2020-10-12T10:34:38Z,tools/distrib/clang_tidy_code.sh,"@@ -21,6 +21,22 @@ set -ex cd $(dirname $0)/../.. REPO_ROOT=$(pwd) +# grep targets with manual tag, which is not included in a result of bazel build using ...+# let's get a list of them using query command and pass it to gen_compilation_database.py+export MANUAL_TARGETS=$(bazel query 'attr(""tags"", ""manual"", tests(//test/cpp/...))' | grep -v _on_ios)++# generate a clang compilation database+tools/distrib/gen_compilation_database.py \+  --include_headers \+  --ignore_system_headers \+  --dedup_targets \+  ""//:*"" \","I wish there was a better way of identifying the targets we want to include in compdb (I'd like something that's zero-maintenance), but I'm assuming you already explored that option and could not come up with anything better (but still simple).Perhaps add a TODO that mentions what would ideally be the set of targets we'd like to list here (is it ""all C/C++ code in this repo""?), and that the current list of targets is the best approximation of that we could get.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/24193,504115191,2020-10-13T16:58:06Z,tools/run_tests/sanity/sanity_tests.yaml,"@@ -24,6 +24,9 @@ - script: tools/distrib/check_pytype.sh - script: tools/distrib/clang_format_code.sh - script: tools/distrib/clang_tidy_code.sh+  # ClangTidy needs to run exclusively because it runs bazel making changes+  # and it needs many CPUs to parse all source files to be diagnosed.+  cpu_cost: 1000",Oh I figured this out. This is still required because clangtidy needs files under the bazel output directory because the compilation database is built from bazel and it builds a new source directory view in that directory. (This is why it can access third_party files without having third_party sources in the repo) But this view can be removed or changed by another bazel invocation so it should run exclusively. I updated the comment.,
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/24408,506748077,2020-10-16T22:30:20Z,src/core/ext/xds/certificate_provider_store.h,"@@ -40,20 +40,26 @@ class CertificateProviderStore {     RefCountedPtr<CertificateProviderFactory::Config> config;   }; +  // Maps plugin instance (opaque) name to plugin defition.   typedef std::map<std::string, PluginDefinition> PluginDefinitionMap;    CertificateProviderStore(PluginDefinitionMap plugin_config_map)       : plugin_config_map_(std::move(plugin_config_map)) {} -  // If a provider corresponding to the config is found, a raw pointer to the-  // grpc_tls_certificate_provider in the map is returned. If no provider is-  // found for a key, a new provider is created. The CertificateProviderStore-  // maintains a ref to the grpc_tls_certificate_provider for its entire+  // If a certificate provider corresponding to the instance name \a key is+  // found, a ref to the grpc_tls_certificate_provider is returned. If no+  // provider is found for the key, a new provider is created from the plugin+  // definition map.+  // Returns nullptr on failure to get or create a new certificate provider.+  // If a certificate provider is created, the CertificateProviderStore+  // maintains a ref to the created grpc_tls_certificate_provider for its entire","To do that, we would also need a release method on the store to release the ref, once the two-level distributor no longer needs that certificate provider instance. I'm not sure it's that useful though.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/24455,507927977,2020-10-19T17:30:18Z,examples/python/route_guide/run_codegen.sh,"@@ -1,3 +1,5 @@+#!/bin/bash","LOL. I tried to change it to non-zero at one point, and our CI failed. Our test protos compilation triggers protoc error, but I failed to trace the root cause. So, I guess it's a tech debt never got paid.I meant to turn on ""xtrace"" mode, not error on non-zero exit code.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/24380,508724702,2020-10-20T17:52:00Z,src/csharp/Grpc.Core.Xamarin/Grpc.Core.Xamarin.csproj,"@@ -0,0 +1,61 @@+﻿<Project Sdk=""Microsoft.NET.Sdk"">++  <Import Project=""..\Grpc.Core\Common.csproj.include"" />++  <PropertyGroup>+    <Authors>The gRPC Authors</Authors>+    <Copyright>Copyright 2015 The gRPC Authors</Copyright>+    <Description>Xamarin support for gRPC C#. Note that the gRPC C# support for the Xamarin platform is considered experimental.+    This package contains the native grpc_csharp_ext libraries that are needed to run gRPC C# on mobile platforms (Android, iOS).+    WARNING: Versions of Grpc.Core.Xamarin and Grpc.Core dependencies being used in your project must always match exactly, otherwise","Can we not shift this requirement from a comment to a package dependency constraint? For example, generating this file from a template, which takes a dependency on the `Grpc.Core` nuget of the same version?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24380,508748886,2020-10-20T18:30:52Z,src/csharp/Grpc.Core.Xamarin/Grpc.Core.Xamarin.csproj,"@@ -0,0 +1,61 @@+﻿<Project Sdk=""Microsoft.NET.Sdk"">++  <Import Project=""..\Grpc.Core\Common.csproj.include"" />++  <PropertyGroup>+    <Authors>The gRPC Authors</Authors>+    <Copyright>Copyright 2015 The gRPC Authors</Copyright>+    <Description>Xamarin support for gRPC C#. Note that the gRPC C# support for the Xamarin platform is considered experimental.+    This package contains the native grpc_csharp_ext libraries that are needed to run gRPC C# on mobile platforms (Android, iOS).+    WARNING: Versions of Grpc.Core.Xamarin and Grpc.Core dependencies being used in your project must always match exactly, otherwise","We could add this ""exact version"" constraint in a .nuspec file (which is the ""old"" way of specifying the metadata for a nuget package), but for consistency reasons, I'd like to build the nuget  from a .csproj (same as all our other nugets).The .csproj's `<ProjectReference>` unfortunately doesn't allow specifying the need for exact version match and introducing .nuspec is messy / clumsy (as the csproj and nuspec don't really mix that well). ",
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/24435,508812272,2020-10-20T20:19:08Z,src/proto/grpc/testing/messages.proto,"@@ -212,3 +212,41 @@ message LoadBalancerStatsResponse {   int32 num_failures = 2;   map<string, RpcsByPeer> rpcs_by_method = 3; }++// Request for retrieving a test client's real time stats.+message LoadBalancerRealTimeStatsRequest {}++// Real-time stats for RPCs sent by a test client.+message LoadBalancerRealTimeStatsResponse {+  // The real-time total number of RPCs issued.+  int32 num_rpcs_started = 1;+  // The real-time total number of RPCs completed successfully for each peer.+  map<string, int32> num_rpcs_succeeded_by_peer = 2;+  // The real-time total number of RPCs failed.+  int32 num_rpcs_failed = 3;+}++// Configurations for a test client.+message ClientConfigureRequest {++  // Type of RPCs to send.+  enum RpcType {+    EMPTY_CALL = 0;+    UNARY_CALL = 1;+  }++  // Metadata to be attached for the given type of RPCs.+  message Metadata {+    RpcType type = 1;+    string key = 2;+    string value = 3;+  }++  // The types of RPCs the client sends.+  repeated RpcType types = 1;","How do these two fields work?RPC with `types[0]` will send `metadata[0]`?Make another message for `{type, metadata}`?",
503812,voidzcy,https://api.github.com/repos/grpc/grpc/pulls/24435,508818083,2020-10-20T20:28:03Z,src/proto/grpc/testing/messages.proto,"@@ -212,3 +212,41 @@ message LoadBalancerStatsResponse {   int32 num_failures = 2;   map<string, RpcsByPeer> rpcs_by_method = 3; }++// Request for retrieving a test client's real time stats.+message LoadBalancerRealTimeStatsRequest {}++// Real-time stats for RPCs sent by a test client.+message LoadBalancerRealTimeStatsResponse {+  // The real-time total number of RPCs issued.+  int32 num_rpcs_started = 1;+  // The real-time total number of RPCs completed successfully for each peer.+  map<string, int32> num_rpcs_succeeded_by_peer = 2;+  // The real-time total number of RPCs failed.+  int32 num_rpcs_failed = 3;+}++// Configurations for a test client.+message ClientConfigureRequest {++  // Type of RPCs to send.+  enum RpcType {+    EMPTY_CALL = 0;+    UNARY_CALL = 1;+  }++  // Metadata to be attached for the given type of RPCs.+  message Metadata {+    RpcType type = 1;+    string key = 2;+    string value = 3;+  }++  // The types of RPCs the client sends.+  repeated RpcType types = 1;","The field `types` and `metadata` in `ClientConfigureRequest` are orthogonal, equivalent to what we have for command line options today. Basically, `types` specifies the kinds of RPCs the test client will send and `metadata` contains a list of metadata to be attached to the kind of RPCs as specified inside it. For example, if we have `types = [UNARY, EMPTY]` and `metadata = [ {UNARY, 'key1', 'value1'}, {UNARY, 'key2', 'value2'} ]`, then the test client will send two kinds of RPCs and only the unary RPCs will have two metadata pairs.",
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/24435,508819878,2020-10-20T20:31:27Z,src/proto/grpc/testing/messages.proto,"@@ -212,3 +212,41 @@ message LoadBalancerStatsResponse {   int32 num_failures = 2;   map<string, RpcsByPeer> rpcs_by_method = 3; }++// Request for retrieving a test client's real time stats.+message LoadBalancerRealTimeStatsRequest {}++// Real-time stats for RPCs sent by a test client.+message LoadBalancerRealTimeStatsResponse {+  // The real-time total number of RPCs issued.+  int32 num_rpcs_started = 1;+  // The real-time total number of RPCs completed successfully for each peer.+  map<string, int32> num_rpcs_succeeded_by_peer = 2;+  // The real-time total number of RPCs failed.+  int32 num_rpcs_failed = 3;+}++// Configurations for a test client.+message ClientConfigureRequest {++  // Type of RPCs to send.+  enum RpcType {+    EMPTY_CALL = 0;+    UNARY_CALL = 1;+  }++  // Metadata to be attached for the given type of RPCs.+  message Metadata {+    RpcType type = 1;+    string key = 2;+    string value = 3;+  }++  // The types of RPCs the client sends.+  repeated RpcType types = 1;","Oh, right. I missed that `message Metadata` also have `type` in it.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/24478,508820583,2020-10-20T20:32:46Z,tools/distrib/python/grpcio_tools/BUILD.bazel,"@@ -37,12 +38,20 @@ pyx_library(     deps = ["":protoc_lib""], ) +internal_copied_filegroup(+    name = ""well_known_protos"",+    srcs = [""@com_google_protobuf//:well_known_protos""],+    dest = ""grpc_tools/_proto/"",+    strip_prefix = ""src/"",+)","It hasn't mattered in the past because `grpcio-tools` is only a library under Bazel, whereas the wheel is mostly used as a  CLI utility. To be honest, I thought that the well-known types were added into the descriptor DB and linked in without us vendoring a copy of the protos, but it turns out my manual test was affected by a cached copy of the `_proto/` directory on my filesystem.",
503812,voidzcy,https://api.github.com/repos/grpc/grpc/pulls/24435,508852860,2020-10-20T21:32:11Z,src/proto/grpc/testing/test.proto,"@@ -85,8 +85,21 @@ service LoadBalancerStatsService {       returns (LoadBalancerStatsResponse) {} } +// A service used to obtain real-time stats for verifying LB behavior.+service LoadBalancerRealTimeStatsService {+  // Gets the real-time stats for RPCs sent by a test client.+  rpc GetClientRealTimeStats(LoadBalancerRealTimeStatsRequest)","Yeah, we can put it into the existing `LoadBalancerStatsService`.Regarding the naming, doesn't ""real-time stats"" mean what's being recorded with respect to time as the x-axis? 😄 Or it could be named as `GetClientAccumulatedStats` if you think that's better.",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/24503,508912630,2020-10-21T00:05:57Z,test/cpp/end2end/BUILD,"@@ -81,6 +81,26 @@ grpc_cc_test(     ], ) +grpc_cc_test(+    name = ""async_end2end_no_xds_test"",+    srcs = [""async_end2end_test.cc""],",">This is a fairly heavyweight test. Is there a smaller test we can use instead, since all we really care about here is that the build works?We would ideally want something like `public_headers_must_be_c89.c` to make sure everything works since simply compiling :grpc++ does not fail without the fix.>Also, is there any reasonable way to verify that the xDS code is actually not being included here? I guess if nothing else, we can compare the binary sizes of the test built with xDS and the same test built without it. But maybe we can even run something to check for xDS symbols in the binary?I've been trying to figure out a good way to test this myself.I actually was looking into a way to pass `grpc_no_xds` as a flag from this test instead of using the underlying `:grpc++_no_xds` target directly. Bazel seems to have something called transitions but they do not want to support `--define` for transitions https://docs.bazel.build/versions/master/skylark/config.html#unsupported-native-optionsSeems like we should be changing the dependency on --define=grpc_no_xds=true to a custom build setting - https://docs.bazel.build/versions/master/skylark/config.html#users-defined-build-settings",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/24187,509003308,2020-10-21T05:44:06Z,test/cpp/util/proto_file_parser.h,"@@ -26,6 +26,12 @@ #include ""test/cpp/util/config_grpc_cli.h"" #include ""test/cpp/util/proto_reflection_descriptor_database.h"" +#if defined(_WIN32) && !defined(__CYGWIN__)+#define PATH_SEPARATOR "";""","Let's prefix with GRPC_CLI to avoid polluting global namespace, or maybe make it a static class member?",
17011,jskeet,https://api.github.com/repos/grpc/grpc/pulls/24380,509084870,2020-10-21T08:24:30Z,src/csharp/Grpc.Core.Xamarin/Grpc.Core.Xamarin.csproj,"@@ -0,0 +1,61 @@+﻿<Project Sdk=""Microsoft.NET.Sdk"">++  <Import Project=""..\Grpc.Core\Common.csproj.include"" />++  <PropertyGroup>+    <Authors>The gRPC Authors</Authors>+    <Copyright>Copyright 2015 The gRPC Authors</Copyright>+    <Description>Xamarin support for gRPC C#. Note that the gRPC C# support for the Xamarin platform is considered experimental.+    This package contains the native grpc_csharp_ext libraries that are needed to run gRPC C# on mobile platforms (Android, iOS).+    WARNING: Versions of Grpc.Core.Xamarin and Grpc.Core dependencies being used in your project must always match exactly, otherwise","Can we not specify a PackageReference with a constraint of (say, for 2.34) `[2.34.0, 2.35.0)`, maybe in some conditional way so we could just use the ProjectReference when building locally? I understand that would require the Grpc.Core package to be published (e.g. to a local nuget source) before building this one, but it would achieve the desired effect. I do agree with avoiding bringing NuSpec into it.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24380,509211066,2020-10-21T11:47:31Z,src/csharp/Grpc.Core/Grpc.Core.csproj,"@@ -39,18 +39,10 @@       <PackagePath>runtimes/osx/native/libgrpc_csharp_ext.x64.dylib</PackagePath>       <Pack>true</Pack>     </Content>-    <Content Include=""..\nativelibs\csharp_ext_macos_x86\libgrpc_csharp_ext.dylib"">-      <PackagePath>runtimes/osx/native/libgrpc_csharp_ext.x86.dylib</PackagePath>-      <Pack>true</Pack>-    </Content>     <Content Include=""..\nativelibs\csharp_ext_linux_x64\libgrpc_csharp_ext.so"">       <PackagePath>runtimes/linux/native/libgrpc_csharp_ext.x64.so</PackagePath>       <Pack>true</Pack>     </Content>-    <Content Include=""..\nativelibs\csharp_ext_linux_x86\libgrpc_csharp_ext.so"">","yes, x86 versions for linux and macos will just go away. We could have a separate package, but coincidentally, we might lose the ability to build the x86 macos artifact on our CI soon anyway (the OS X version that our CI kokoro provides doesn't allow you to build x86 binaries anymore, unless you explicitly opt in to using out of deprecated mac images) and removing x86 mac artifact altogether would make that problem go away, so I opted for simplicity here and decided to just remove the artifacts. More on removal of x86 linux and macos artifacts is here: https://github.com/grpc/grpc/pull/24378",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24503,509404664,2020-10-21T15:52:39Z,test/cpp/end2end/BUILD,"@@ -81,6 +81,26 @@ grpc_cc_test(     ], ) +grpc_cc_test(+    name = ""async_end2end_no_xds_test"",+    srcs = [""async_end2end_test.cc""],","I'm open to any reasonable option, as long as it works both internally and externally.  The build rules are fairly different in both environments, and I had to jump through a bunch of hoops to get it working right when I added support for ""--define=grpc_no_xds=true"".",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24500,509678029,2020-10-21T20:41:34Z,doc/naming.md,"@@ -34,12 +34,16 @@ Most gRPC implementations support the following URI schemes:     resolver does not support this, but the c-ares based resolver     supports specifying this in the form ""IP:port"".) -- `unix:path` or `unix://absolute_path` -- Unix domain sockets (Unix systems only)+- `unix:path`, `unix://absolute_path`, or `unix:@abstract_path` -- Unix domain sockets (Unix systems only)   - `path` indicates the location of the desired socket.   - In the first form, the path may be relative or absolute; in the     second form, the path must be absolute (i.e., there will actually be     three slashes, two prior to the path and another to begin the     absolute path).+  - In the third form, `abstract_path` indicates a name in the abstract namespace:+     - The name has no connection with filesystem pathnames.+     - No permissions will apply to the socket - any process/user may access the socket.+     - `abstract_unix_socket` must be a URI compatible string.","Formally speaking, I'm not sure what ""URI compatible string"" means.  Is this just saying that the path needs to be percent-encoded if it contains any characters that are not allowed in a URI path?  If so, I don't think this is actually different than any other parameter, so it doesn't need to be explicitly stated.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24500,509703956,2020-10-21T21:15:33Z,test/core/end2end/fixtures/h2_uds.cc,"@@ -46,8 +46,7 @@ struct fullstack_fixture_data {  static int unique = 1; -static grpc_end2end_test_fixture chttp2_create_fixture_fullstack(-    grpc_channel_args* /*client_args*/, grpc_channel_args* /*server_args*/) {+static grpc_end2end_test_fixture chttp2_create_fixture_fullstack_base(const std::string& addr) {","Please take the parameter by value, so that the caller can use `std::move()`.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24500,509706579,2020-10-21T21:18:41Z,test/core/end2end/fixtures/h2_uds.cc,"@@ -61,6 +60,20 @@ static grpc_end2end_test_fixture chttp2_create_fixture_fullstack(   return f; } +static grpc_end2end_test_fixture chttp2_create_fixture_fullstack(+    grpc_channel_args* /*client_args*/, grpc_channel_args* /*server_args*/) {+  const std::string localaddr = absl::StrFormat(""unix:/tmp/grpc_fullstack_test.%d.%d"",+                                               getpid(), unique++);+  return chttp2_create_fixture_fullstack_base(localaddr);",Please use `std::move()` for the parameter.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24500,509729934,2020-10-21T21:49:36Z,src/core/lib/iomgr/parse_address.cc,"@@ -56,6 +56,15 @@ bool grpc_parse_unix(const grpc_uri* uri,   if (path_len == maxlen) return false;   un->sun_family = AF_UNIX;   strcpy(un->sun_path, uri->path);+  if (un->sun_path[0] == '@') {","One side-effect of this is that it will basically make it impossible to specify a relative path for a non-abstract socket that begins with an `@` -- anyone who has such a configuration will break, and they would need to work around it by saying something like `./@...`.I think such a case is unlikely in practice, but I'm not crazy about imposing this kind of limitation in principle.  This seems like a fairly ugly hack.I've looked around for how other tools and languages deal with this.  It looks like the `@`-prefix approach [is used in golang](https://github.com/golang/go/blob/b2a8317b31d652b3ee293a313269b8290bcdf96c/src/syscall/syscall_linux.go#L393), and I've found [one java library that uses it](https://github.com/kohlschutter/junixsocket/blob/4d3b2af23fb8d1f011dbf7ae01cd51f2d5986b19/junixsocket-demo/src/main/java/org/newsclub/net/unix/demo/DemoHelper.java#L161), but [netty appears to use a leading nul byte](https://github.com/netty/netty/blob/7d971a78a0869171948bb479b3c114d5e772ad45/transport-native-epoll/src/test/java/io/netty/channel/epoll/EpollAbstractDomainSocketEchoTest.java#L26), and [it looks like python does too](https://stackoverflow.com/questions/30818410/can-not-connect-to-an-abstract-unix-socket-in-python).  So it's not clear to me that the `@`-prefix is a universal convention.Given the lack of a strong convention here, I am not inclined to do it this way.  A cleaner alternative might be to indicate use of an abstract socket via a query param in the `unix:` URI.I'd like to get feedback from @menghanl and @ejona86 on this, in terms of how this would work in grpc-go and grpc-java.",
26072277,dfawley,https://api.github.com/repos/grpc/grpc/pulls/24500,509799422,2020-10-21T23:51:15Z,src/core/lib/iomgr/parse_address.cc,"@@ -56,6 +56,15 @@ bool grpc_parse_unix(const grpc_uri* uri,   if (path_len == maxlen) return false;   un->sun_family = AF_UNIX;   strcpy(un->sun_path, uri->path);+  if (un->sun_path[0] == '@') {","> Query params would have the same problem, since ? is a valid character on the unix filesystem.If we are supporting query parameters, a `?` in the path would need to be percent-encoded.> The most robust thing I can think of would be to indicate it as a separate scheme, i.e. unix-abstract:pathFWIW, we can't change Go's behavior at this point, as we'd be breaking anyone relying upon `unix:@abstract` to work.  We could support an _explicit_ `unix-abstract` scheme if we needed to (we'd just prefix with an `@` ourselves), but I would rather not do this.",
1058384,Capstan,https://api.github.com/repos/grpc/grpc/pulls/24506,510384364,2020-10-22T18:51:40Z,tools/interop_matrix/patches/csharp_v1.0.1/git_repo.patch,"@@ -5,13 +5,6 @@ index c880e42ba1..70ef9596bb 160000 @@ -1 +1 @@ -Subproject commit c880e42ba1c8032d4cdde2aba0541d8a9d9fa2e9 +Subproject commit 70ef9596bbcc11353b9bb8d4e91478694dd21439-diff --git a/third_party/gflags b/third_party/gflags-index 05b155ff59..30dbc81fb5 160000---- a/third_party/gflags-+++ b/third_party/gflags-@@ -1 +1 @@--Subproject commit 05b155ff59114735ec8cd089f669c4c3d8f59029-+Subproject commit 30dbc81fb5ffdc98ea9b14b1918bfe4e8779b26e","While this is no longer part of this PR, I have reverted it from that commit.",
1058384,Capstan,https://api.github.com/repos/grpc/grpc/pulls/24506,510394520,2020-10-22T19:09:01Z,test/cpp/util/test_config_cc.cc,"@@ -16,21 +16,15 @@  *  */ -#include <gflags/gflags.h>+#include ""absl/flags/parse.h"" #include ""test/cpp/util/test_config.h"" -// In some distros, gflags is in the namespace google, and in some others,-// in gflags. This hack is enabling us to find both.-namespace google {}-namespace gflags {}-using namespace google;-using namespace gflags;- namespace grpc { namespace testing { +// TODO(Capstan): remove `remove_flags` void InitTest(int* argc, char*** argv, bool remove_flags) {-  ParseCommandLineFlags(argc, argv, remove_flags);","I have not audited the 17 or so places that set `remove_flags` to `true` to see if they really needed that or not, FTR. Abseil's variant will not edit them in situ, but instead return a `std::vector<char *>` of the reduced command line ([docs](https://abseil.io/docs/cpp/guides/flags#parsing-flags-during-startup)).I could also change this function to apply the resulting edits based on `remove_flags`, and put off the audit or larger refactoring for another day.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/24471,510399194,2020-10-22T19:17:44Z,tools/run_tests/python_utils/upload_rbe_results.py,"@@ -287,7 +287,8 @@ def _get_resultstore_data(api_key, invocation_id):      if not args.skip_upload:         # BigQuery sometimes fails with large uploads, so batch 1,000 rows at a time.-        for i in range((len(bq_rows) / 1000) + 1):-            _upload_results_to_bq(bq_rows[i * 1000:(i + 1) * 1000])+        MAX_ROWS = 1000","Yes, when it's divisible by 1000, it will have _upload_results_to_bq called with an empty list.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/24270,511048848,2020-10-23T17:57:09Z,tools/bazel.rc,"@@ -61,17 +58,17 @@ build:tsan --copt=-fsanitize=thread build:tsan --copt=-fno-omit-frame-pointer build:tsan --copt=-DGPR_NO_DIRECT_SYSCALLS build:tsan --copt=-DGRPC_TSAN-# TODO(jtattermusch): ideally we would set --copt=-DTHREAD_SANITIZER (used by absl)","Abseil doesn't use `DTHREAD_SANITIZER` anymore ([link](https://github.com/abseil/abseil-cpp/blob/d1de75bf540f091b4dfc860713d556e578c0f158/absl/base/config.h#L690)). Instead, it starts to detect it using a feature.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24233,511925414,2020-10-26T12:34:16Z,src/csharp/Grpc.Core/Grpc.Core.csproj,"@@ -36,19 +36,19 @@   <ItemGroup>     <EmbeddedResource Include=""..\..\..\etc\roots.pem"" />     <Content Include=""..\nativelibs\csharp_ext_macos_x64\libgrpc_csharp_ext.dylib"">-      <PackagePath>runtimes/osx/native/libgrpc_csharp_ext.x64.dylib</PackagePath>+      <PackagePath>runtimes/osx-x64/native/libgrpc_csharp_ext.x64.dylib</PackagePath>","Changing this file is not enough, as there is other logic that relies on knowing these exact paths.E.g. https://github.com/grpc/grpc/blob/9936d7785cdf41c13974497423c7d87bfbbd4054/src/csharp/Grpc.Core/build/net45/Grpc.Core.targets (which is required for running on legacy .NET frameworks other than Core CLR).andhttps://github.com/grpc/grpc/blob/9936d7785cdf41c13974497423c7d87bfbbd4054/src/csharp/Grpc.Core/Internal/NativeExtension.cs#L83",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24570,512316839,2020-10-26T22:54:01Z,src/core/ext/filters/deadline/deadline_filter.cc,"@@ -69,12 +69,13 @@ static void timer_callback(void* arg, grpc_error* error) {         GRPC_ERROR_CREATE_FROM_STATIC_STRING(""Deadline Exceeded""),         GRPC_ERROR_INT_GRPC_STATUS, GRPC_STATUS_DEADLINE_EXCEEDED);     deadline_state->call_combiner->Cancel(GRPC_ERROR_REF(error));-    GRPC_CLOSURE_INIT(&deadline_state->timer_callback,-                      send_cancel_op_in_call_combiner, elem,-                      grpc_schedule_on_exec_ctx);-    GRPC_CALL_COMBINER_START(deadline_state->call_combiner,-                             &deadline_state->timer_callback, error,-                             ""deadline exceeded -- sending cancel_stream op"");+    // The inlined closure might still be in use (holding a pending call), so","And if this is a problem, why are we not also running into the same problem when `deadline_timer->timer_callback` is used inside of `send_cancel_op_in_call_combiner()`?Depending on what the problem really is (I'd really like to fully understand that before we agree on *any* fix), I think #24575 might be a more general approach.  Can one of you please try that out with the absl change and see if it fixes the problem?",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/24570,512323114,2020-10-26T23:10:54Z,src/core/ext/filters/deadline/deadline_filter.cc,"@@ -69,12 +69,13 @@ static void timer_callback(void* arg, grpc_error* error) {         GRPC_ERROR_CREATE_FROM_STATIC_STRING(""Deadline Exceeded""),         GRPC_ERROR_INT_GRPC_STATUS, GRPC_STATUS_DEADLINE_EXCEEDED);     deadline_state->call_combiner->Cancel(GRPC_ERROR_REF(error));-    GRPC_CLOSURE_INIT(&deadline_state->timer_callback,-                      send_cancel_op_in_call_combiner, elem,-                      grpc_schedule_on_exec_ctx);-    GRPC_CALL_COMBINER_START(deadline_state->call_combiner,-                             &deadline_state->timer_callback, error,-                             ""deadline exceeded -- sending cancel_stream op"");+    // The inlined closure might still be in use (holding a pending call), so","Yes, that is the path I saw too for this bug. Based on the logs at https://github.com/grpc/grpc/issues/24549, we do see that the race is occurring at the access of `deadline_state->closure` in this function `timer_callback()` while the closure is still stuck in the ExecCtx, and it looks like this is the only access outside the call combiner.After patching this in, @donnadionne verified that the data race no longer exists. I was also verifying the other uses of `timer_callback` and the one inside `send_cancel_op_in_call_combiner()` also seemed doubtful, and it does seem to be the case that the initial closure can still be pending even when `send_cancel_op_in_call_combiner()` is invoked. Updated.@donnadionne can you verify that this is indeed the sequence of events that we are running into?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24570,512329713,2020-10-26T23:30:55Z,src/core/ext/filters/deadline/deadline_filter.cc,"@@ -69,12 +69,13 @@ static void timer_callback(void* arg, grpc_error* error) {         GRPC_ERROR_CREATE_FROM_STATIC_STRING(""Deadline Exceeded""),         GRPC_ERROR_INT_GRPC_STATUS, GRPC_STATUS_DEADLINE_EXCEEDED);     deadline_state->call_combiner->Cancel(GRPC_ERROR_REF(error));-    GRPC_CLOSURE_INIT(&deadline_state->timer_callback,-                      send_cancel_op_in_call_combiner, elem,-                      grpc_schedule_on_exec_ctx);-    GRPC_CALL_COMBINER_START(deadline_state->call_combiner,-                             &deadline_state->timer_callback, error,-                             ""deadline exceeded -- sending cancel_stream op"");+    // The inlined closure might still be in use (holding a pending call), so","It's not clear to me that the logs in #24549 provide enough data to indicate exactly what's happening here.  They do show that the closure is in the `ExecCtx` queue, but they don't show how it got there.  How do we know this is from `start_timer_if_needed()` as opposed to some other code path?  For example, it could also be the case that the timer itself has fired and has invoked the callback already, but the `ExecCtx` has offloaded it to another thread for some reason.To be clear, I'm not saying that I think that's likely; I actually do think that the `start_timer_if_needed()` code-path is the most likely culprit.  I'm just saying that I want to get proof instead of making an assumption.  Othewise we don't know for sure that we've really solved the problem.Can we maybe add some logging to the deadline filter to verify the sequence of events here?",
10470658,donnadionne,https://api.github.com/repos/grpc/grpc/pulls/24570,512439299,2020-10-27T06:17:30Z,src/core/ext/filters/deadline/deadline_filter.cc,"@@ -69,12 +69,13 @@ static void timer_callback(void* arg, grpc_error* error) {         GRPC_ERROR_CREATE_FROM_STATIC_STRING(""Deadline Exceeded""),         GRPC_ERROR_INT_GRPC_STATUS, GRPC_STATUS_DEADLINE_EXCEEDED);     deadline_state->call_combiner->Cancel(GRPC_ERROR_REF(error));-    GRPC_CLOSURE_INIT(&deadline_state->timer_callback,-                      send_cancel_op_in_call_combiner, elem,-                      grpc_schedule_on_exec_ctx);-    GRPC_CALL_COMBINER_START(deadline_state->call_combiner,-                             &deadline_state->timer_callback, error,-                             ""deadline exceeded -- sending cancel_stream op"");+    // The inlined closure might still be in use (holding a pending call), so","I added logs here:```@@ -96,6 +115,7 @@ static void start_timer_if_needed(grpc_call_element* elem,       // Note: We do not start the timer if there is already a timer       return;     case GRPC_DEADLINE_STATE_FINISHED:+      gpr_log(GPR_INFO, ""DONNAAA GRPC_DEADLINE_STATE_FINISHED"");       deadline_state->timer_state = GRPC_DEADLINE_STATE_PENDING;       // If we've already created and destroyed a timer, we always create a       // new closure: we have no other guarantee that the inlined closure is@@ -104,12 +124,23 @@ static void start_timer_if_needed(grpc_call_element* elem,           GRPC_CLOSURE_CREATE(timer_callback, elem, grpc_schedule_on_exec_ctx);       break;     case GRPC_DEADLINE_STATE_INITIAL:+      gpr_log(GPR_INFO, ""DONNAAA GRPC_DEADLINE_STATE_INITIAL"");       deadline_state->timer_state = GRPC_DEADLINE_STATE_PENDING;       closure =           GRPC_CLOSURE_INIT(&deadline_state->timer_callback, timer_callback,                             elem, grpc_schedule_on_exec_ctx);       break;   }+      void *stack[50];+      int size = absl::GetStackTrace(stack, 48, 2);+      for (int i = 0; i < size; ++i) {+        char out[256];+        if (absl::Symbolize(stack[i], out, 256)) {+          gpr_log(GPR_INFO, ""donna stack trace:%s and %s"", stack[i], out);+        } else {+          gpr_log(GPR_INFO, ""donna stack trace:%s"", stack[i]);+        }+      }```Please see test logs here:https://source.cloud.google.com/results/invocations/27839f78-5173-432f-bd75-dbd80de666d0/targets/%2F%2Ftest%2Fcpp%2Fend2end:xds_end2end_test@poller%3Depoll1/logI know we went into bothcase GRPC_DEADLINE_STATE_FINISHED: andcase GRPC_DEADLINE_STATE_INITIAL:",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23866,512531606,2020-10-27T09:23:35Z,tools/bazel.rc,"@@ -2,6 +2,7 @@  build --client_env=CC=clang build --copt=-DGRPC_BAZEL_BUILD+build --copt=-DABSL_FORCE_THREAD_IDENTITY_MODE=2","as users have pointed out previously, for compilation flags that are required for a successful and working build, the `bazel.rc` file is not a good place. The issue is that when you're using gRPC from your own project, you won't automatically get the options that are declared in bazel.rc (unless you manually take them and add them to own your project's bazel.rc).So if `-DABSL_FORCE_THREAD_IDENTITY_MODE=2` is actually required (it seems that it is), it needs to be defined elsewhere (To be fair some of the other compilation options defined here should also move).I THINK the right approach is to use ""config_setting"" here. https://github.com/grpc/grpc/blob/7fccdd7385386eda004d1ccf91c3aea41832b161/BUILD#L40FTR An example where users have run into a very similar problem (it ended up being solved in a different way, as GRPC_BAZEL_BUILD is currently not really required for building grpc library) is here:https://github.com/bazelbuild/bazel/issues/4341#issuecomment-574707231CC @gnossen ",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24500,512860835,2020-10-27T16:52:33Z,src/core/lib/iomgr/parse_address.cc,"@@ -56,6 +56,15 @@ bool grpc_parse_unix(const grpc_uri* uri,   if (path_len == maxlen) return false;   un->sun_family = AF_UNIX;   strcpy(un->sun_path, uri->path);+  if (un->sun_path[0] == '@') {","Your current behavior is broken.  I think you should prioritize this change.  The longer you wait before fixing this, the harder it will be.In any case, for this particular issue, you could special-case `%00` at the start of a `unix:` URI without solving the more general problem.",
26072277,dfawley,https://api.github.com/repos/grpc/grpc/pulls/24500,512894854,2020-10-27T17:37:48Z,src/core/lib/iomgr/parse_address.cc,"@@ -56,6 +56,15 @@ bool grpc_parse_unix(const grpc_uri* uri,   if (path_len == maxlen) return false;   un->sun_family = AF_UNIX;   strcpy(un->sun_path, uri->path);+  if (un->sun_path[0] == '@') {","> Your current behavior is broken. I think you should prioritize this change. The longer you wait before fixing this, the harder it will be.We've *loooong* since reached critical mass on this.  We can't make such a widespread behavior change without doing it in a new API.  On the plus side we also can implement some other gRPC-standard-but-Go-missed-the-memo behavior like: ""DNS as default scheme"" and ""channel begins in IDLE state"".  https://github.com/grpc/grpc-go/issues/1786Back to the issue at hand, it looks like we have 3 options:1. Support leading-`@` in C and Java2. New scheme in all languages: `unix-abstract` (e.g.)3. `%00` as a special case for `unix` scheme in Go, and implement abstract sockets in C/Java if they don't ""Just Work"" with this encoding",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/24500,512993292,2020-10-27T20:00:04Z,src/core/lib/iomgr/parse_address.cc,"@@ -56,6 +56,15 @@ bool grpc_parse_unix(const grpc_uri* uri,   if (path_len == maxlen) return false;   un->sun_family = AF_UNIX;   strcpy(un->sun_path, uri->path);+  if (un->sun_path[0] == '@') {","Prior art. Null prefix:* [Linux](https://www.man7.org/linux/man-pages/man7/unix.7.html)* [Netty](https://github.com/netty/netty/blob/7d971a78a0869171948bb479b3c114d5e772ad45/transport-native-epoll/src/test/java/io/netty/channel/epoll/EpollAbstractDomainSocketEchoTest.java#L26)* [Python](https://docs.python.org/3/library/socket.html)* [junixsocket](https://javadoc.io/doc/com.kohlschutter.junixsocket/junixsocket-common/latest/org.newsclub.net.unix/org/newsclub/net/unix/AFUNIXSocketAddress.html) (there is also a convenience factory method that prepends the \0) (mainly including it because it was miscategorized before)@-prefix:* [/proc/net/unix](https://man7.org/linux/man-pages/man5/proc.5.html)* netstat (for display only; probably due to /proc/net/unix)* [Go](https://github.com/golang/go/blob/333e90448a0e55f2e1161853caecf3d30ef3a74a/src/syscall/syscall_linux.go#L426-L427) ([more](https://github.com/golang/go/blob/333e90448a0e55f2e1161853caecf3d30ef3a74a/src/syscall/syscall_linux.go#L504-L517)). May be incorrectly categorized. Does not appear to be publicly documented, in fact, [goes against ""the address must be a file system path""](https://golang.org/pkg/net/#Dial).Separate scheme/api:* [socat](https://manpages.debian.org/testing/socat/socat.1.en.html) (UNIX-LISTEN:/some/path vs ABSTRACT-LISTEN:somename)* [dbus](https://dbus.freedesktop.org/doc/dbus-specification.html#addresses) (unix:path=/some/path vs unix:abstract=somename)* curl ([--abstract-unix-socket vs --unix-socket](https://curl.haxx.se/docs/manpage.html)) ([CURLOPT_ABSTRACT_UNIX_SOCKET](https://curl.haxx.se/libcurl/c/CURLOPT_ABSTRACT_UNIX_SOCKET.html) vs [CURLOPT_UNIX_SOCKET_PATH](https://curl.haxx.se/libcurl/c/CURLOPT_UNIX_SOCKET_PATH.html))It appears abstract socket names are fully opaque and can be arbitrary binary, even additional nulls. I don't know if there's any conventions to encode them using the current locale (e.g., UTF-8) or whether they are left uninterpreted (raw bytes). It seems there's a mix in their usage. For example, Python accepts both bytes and strings, but returns the address as bytes. It also seems it doesn't need to be null-terminated.junixsocket does not use @-prefix. [The referenced code](https://github.com/kohlschutter/junixsocket/blob/4d3b2af23fb8d1f011dbf7ae01cd51f2d5986b19/junixsocket-demo/src/main/java/org/newsclub/net/unix/demo/DemoHelper.java#L161) was just an example. That doesn't make it useless, but greatly diminishes its persuasion power.I think I'm preferring using a separate scheme. The null-prefix seems like a low-level syscall API encoding and not something that should be user-visible. I'm surprised @-prefix is not more widespread, because it seems users assume ""that's the way it is;"" I have to assume that is due to netstat's influence. A separate scheme would also be useful as we may want to process the URI as bytes instead of Unicode.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24500,513021236,2020-10-27T20:50:28Z,src/core/lib/iomgr/parse_address.cc,"@@ -56,6 +56,15 @@ bool grpc_parse_unix(const grpc_uri* uri,   if (path_len == maxlen) return false;   un->sun_family = AF_UNIX;   strcpy(un->sun_path, uri->path);+  if (un->sun_path[0] == '@') {","Yeah, whatever we do here has to be RFC-3986-compliant, so it will need to be percent-encoded.I'd still prefer to trigger this with `%00` with the existing `unix` scheme.  That seems to be the most common API from Eric's list, and it's most consistent with how the underlying syscall API actually works.  (I'd argue that that was a really bad design choice for the syscall API, but that ship has sailed.)Doug, is there any reason you can't special-case using percent-encoding for the entire path when the path starts with `%00`?",
26072277,dfawley,https://api.github.com/repos/grpc/grpc/pulls/24500,513028184,2020-10-27T21:01:13Z,src/core/lib/iomgr/parse_address.cc,"@@ -56,6 +56,15 @@ bool grpc_parse_unix(const grpc_uri* uri,   if (path_len == maxlen) return false;   un->sun_family = AF_UNIX;   strcpy(un->sun_path, uri->path);+  if (un->sun_path[0] == '@') {","> Doug, is there any reason you can't special-case using percent-encoding for the entire path when the path starts with %00?No, if we decide on `unix`-plus-NIL-prefix and the path starts with `%00`, we could just as easily %-decode the whole string.For the record I'm fine with any of the 3 options, with no strong feelings about any of them.  They all have similar pros and cons.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/23866,513145859,2020-10-28T02:42:03Z,tools/bazel.rc,"@@ -2,6 +2,7 @@  build --client_env=CC=clang build --copt=-DGRPC_BAZEL_BUILD+build --copt=-DABSL_FORCE_THREAD_IDENTITY_MODE=2","Thanks, Jan! I didn't think that this PR was ready for the review but it seems that I pressed the review button :)`ABSL_FORCE_THREAD_IDENTITY_MODE` is temporary tested by the advice from b/170119606#comment3 and I don't want to add that option if possible. (Ideally I want Abseil to determine whether it should be used by some magic) But If I have to, I will add it to the config_setting. Thanks!",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/24500,513560801,2020-10-28T15:55:53Z,src/core/lib/iomgr/parse_address.cc,"@@ -56,6 +56,15 @@ bool grpc_parse_unix(const grpc_uri* uri,   if (path_len == maxlen) return false;   un->sun_family = AF_UNIX;   strcpy(un->sun_path, uri->path);+  if (un->sun_path[0] == '@') {","> Okay, then I vote for %00 prefix with the existing unix scheme, and assuming the entire string is percent-encoded.So it sounds like two votes for a separate scheme, one vote for %00 prefix. I can live with any of the options, but %00 doesn't get my vote.> Target strings are not supposed to be treated as raw bytesAny bytes values that are not allowed in that part of the URI would need to be percent encoded. That was not actually the point.Percent-encoded bytes are generally UTF-8, and some URI implementations may convert them automatically to a Unicode character. But the actual encoding is not defined by URI itself. The point I was making is we may need to process those percent-encoded bytes as bytes and make sure they round-trip as bytes. For example, in Java they will become UTF-16 and bad things will happen. Even in a UTF-8 based language you can't iterate over the runes or check for invalid UTF-8 sequences.I think C and Go could support raw bytes fairly easily. Java would be harder because strings are UTF-16. It would be easy to update Netty to support bytes. I'm fine with only supporting strings. Although I did notice one abstract socket on my system that appears to be padded with nulls: `@var/run/nvidia-xdriver-673466ff@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@`. Most likely that is a bug.We should be careful to _not_ null-terminate these strings though; they are length-defined.> Okay, then I vote for %00 prefix with the existing unix scheme, and assuming the entire string is percent-encoded.No, we can't assume the entire string is percent-encoded. ASCII letters and numbers will be unencoded. It will actually be quite normal for the entire rest of the string to not need any percent encoding. It seems to be a case where ""it's binary but commonly treated like a string.""",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/24543,513765496,2020-10-28T21:15:32Z,src/boringssl/gen_build_yaml.py,"@@ -47,7 +47,7 @@ def __init__(self, sources):     def WriteFiles(self, files):         test_binaries = ['ssl_test', 'crypto_test']         asm_outputs = {-            key: value for key, value in files.items() if any(+            key: value for key, value in list(files.items()) if any(",I think `list(X.items())` should be `X.items()` when it's being used in the `for-in`.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/24610,514571938,2020-10-29T21:14:46Z,setup.py,"@@ -106,8 +106,9 @@     'License :: OSI Approved :: Apache Software License', ] -BUILD_WITH_BORING_SSL_ASM = os.environ.get('GRPC_BUILD_WITH_BORING_SSL_ASM',-                                           True)+# Environment variable to turn off BoringSSL ASM optimization.+BUILD_WITHOUT_BORING_SSL_ASM = os.environ.get(",Yeah. We should have kept the same key name pattern. But we should not break it as well.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24350,514978461,2020-10-30T09:46:49Z,src/python/grpcio_tests/tests/qps/worker_server.py,"@@ -91,13 +92,14 @@ def _create_server(self, config):             raise Exception('Unsupported server type {}'.format(                 config.server_type)) +        server_port = config.port if self._server_port is None else self._server_port","This is actually a different behavior than in grpc-java.In java, ServerConfig.port takes precedencehttps://github.com/grpc/grpc-java/blob/59528d8efec465326b071ffc6f3f2220bcf641c9/benchmarks/src/main/java/io/grpc/benchmarks/driver/LoadWorker.java#L136",
1627021,EraYaN,https://api.github.com/repos/grpc/grpc/pulls/24233,515140863,2020-10-30T14:32:31Z,src/csharp/Grpc.Core/Grpc.Core.csproj,"@@ -36,19 +36,19 @@   <ItemGroup>     <EmbeddedResource Include=""..\..\..\etc\roots.pem"" />     <Content Include=""..\nativelibs\csharp_ext_macos_x64\libgrpc_csharp_ext.dylib"">-      <PackagePath>runtimes/osx/native/libgrpc_csharp_ext.x64.dylib</PackagePath>+      <PackagePath>runtimes/osx-x64/native/libgrpc_csharp_ext.x64.dylib</PackagePath>","Alright I have addressed these issues I think. For the core platforms (and xamarin too) the project should probably rely in the ""smart"" SDK for finding the correct native binary instead of creating the list itself. The runtime seems to be able handle the search paths (with all the fallback RIDs etc) and native library extensions (.so, .dll, dylib) by itself. I'm not sure how to access that logic with the way the native symbols are loaded currently, but it might be worth exploring.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24613,515160482,2020-10-30T14:57:28Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -4348,48 +4347,48 @@ TEST_P(LdsRdsTest, XdsRoutingApplyXdsTimeout) {     balancers_[0]->ads_service()->SetLdsResource(listener);   }   // Test grpc_timeout_header_max of 1.5 seconds applied-  auto t0 = system_clock::now();+  gpr_timespec tolerance = gpr_time_from_millis(kToleranceMillis, GPR_TIMESPAN);+  gpr_timespec est_timeout_time = gpr_time_add(+      gpr_now(GPR_CLOCK_REALTIME),+      gpr_time_from_millis(+          kTimeoutGrpcTimeoutHeaderMaxSecond * 1000 + kTimeoutMillis,+          GPR_TIMESPAN));   CheckRpcSendFailure(1,                       RpcOptions()                           .set_rpc_service(SERVICE_ECHO1)                           .set_rpc_method(METHOD_ECHO1)                           .set_wait_for_ready(true)                           .set_timeout_ms(kTimeoutApplicationSecond * 1000),                       StatusCode::DEADLINE_EXCEEDED);-  auto ellapsed_nano_seconds =-      std::chrono::duration_cast<std::chrono::nanoseconds>(system_clock::now() --                                                           t0);-  EXPECT_GT(ellapsed_nano_seconds.count(),-            kTimeoutGrpcTimeoutHeaderMaxSecond * 1000000000 + kTimeoutNano);-  EXPECT_LT(ellapsed_nano_seconds.count(),-            kTimeoutMaxStreamDurationSecond * 1000000000);+  gpr_timespec timeout_time = gpr_now(GPR_CLOCK_REALTIME);+  GPR_ASSERT(gpr_time_similar(est_timeout_time, timeout_time, tolerance));","This allows the two times to be off in either direction.  I think that's more permissive than what we want for this test, and I'm concerned that this may be masking the real problem, considering that the failure mode we saw previously was that the timeout happened too early.  I think the test should allow the timeout to take a little longer, but I don't think it should ever happen sooner than expected.  The test should fail if that occurs.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24613,515161386,2020-10-30T14:58:50Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -4245,9 +4245,8 @@ TEST_P(LdsRdsTest, XdsRoutingClusterUpdateClustersWithPickingDelays) { }  TEST_P(LdsRdsTest, XdsRoutingApplyXdsTimeout) {-  // TODO(https://github.com/grpc/grpc/issues/24549): TSAN won't work here.-  if (BuiltUnderAsan() || BuiltUnderTsan()) return;-+  const int64_t kToleranceMillis = 100;+  const int64_t kTimeoutMillis = 500;   const int64_t kTimeoutNano = 500000000;","Please define `kTimeoutNano` as a function of `kTimeoutMillis`, so that the two can't accidentally wind up set to different values in the future.",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/24490,515388476,2020-10-30T21:21:26Z,include/grpcpp/impl/codegen/async_unary_call.h,"@@ -66,26 +72,133 @@ class ClientAsyncResponseReaderInterface { };  namespace internal {-template <class R>-class ClientAsyncResponseReaderFactory {++class ClientAsyncResponseReaderHelper {  public:   /// Start a call and write the request out if \a start is set.   /// \a tag will be notified on \a cq when the call has been started (i.e.   /// intitial metadata sent) and \a request has been written out.   /// If \a start is not set, the actual call must be initiated by StartCall   /// Note that \a context will be used to fill in custom initial metadata-  /// used to send to the server when starting the call.+  /// used to send to the server when starting the call. Optionally pass in a+  /// base class for request and response types so that the internal functions+  /// and structs can be templated based on that, allowing reuse across RPCs+  /// (e.g., MessageLite for protobuf). Since constructors can't have an+  /// explicit template parameter, the last argument is a bogus to the","I actuallly meant ""bogus"" but didn't give a noun after that. I am changing it to ""extraneous parameter"".",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/24490,515388894,2020-10-30T21:22:39Z,include/grpcpp/impl/codegen/async_unary_call.h,"@@ -134,61 +244,36 @@ class ClientAsyncResponseReader final   ///   - the \a ClientContext associated with this call is updated with   ///     possible initial and trailing metadata sent from the server.   void Finish(R* msg, ::grpc::Status* status, void* tag) override {-    GPR_CODEGEN_ASSERT(started_);-    if (initial_metadata_read_) {-      finish_buf.set_output_tag(tag);-      finish_buf.RecvMessage(msg);-      finish_buf.AllowNoMessage();-      finish_buf.ClientRecvStatus(context_, status);-      call_.PerformOps(&finish_buf);-    } else {-      single_buf.set_output_tag(tag);-      single_buf.RecvInitialMetadata(context_);-      single_buf.RecvMessage(msg);-      single_buf.AllowNoMessage();-      single_buf.ClientRecvStatus(context_, status);-      call_.PerformOps(&single_buf);-    }+    GPR_CODEGEN_DEBUG_ASSERT(started_);+    finish_(context_, &call_, initial_metadata_read_, single_buf_, &finish_buf_,+            static_cast<void*>(msg), status, tag);   }   private:-  friend class internal::ClientAsyncResponseReaderFactory<R>;+  friend class internal::ClientAsyncResponseReaderHelper;   ::grpc::ClientContext* const context_;   ::grpc::internal::Call call_;-  bool started_;+  bool started_ = false;","No. It used to be initialized from a parameter to the constructor, but the new constructor no longer has that parameter.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24622,515435796,2020-10-31T00:53:32Z,src/core/lib/gprpp/stat.h,"@@ -33,6 +33,11 @@ namespace grpc_core { // StatusCode::kInternal will be returned. absl::Status GetFileModificationTime(const char* filename, time_t* timestamp); +// A simple helper function that will combine |path1| and |path2| with the file+// separator, and return the new path. It doesn't check the validity of the new+// path.+std::string PathJoin(std::string path1, std::string path2);","The parameters here should both be `absl::string_view`, since this function does not need to take ownership.",
1209285,codeblooded,https://api.github.com/repos/grpc/grpc/pulls/24350,516159125,2020-11-02T18:00:56Z,src/python/grpcio_tests/tests/qps/worker_server.py,"@@ -91,13 +92,14 @@ def _create_server(self, config):             raise Exception('Unsupported server type {}'.format(                 config.server_type)) +        server_port = config.port if self._server_port is None else self._server_port","@jtattermusch I do not think this behavior is consistent across languages. I based this change off of the C++ worker. In my understanding, it lacks the second condition which checks that the port is unspecified in the scenario. In essence, it overrides when the flag is set:https://github.com/grpc/grpc/blob/04045e227359ae23b5b31fb9e7d14e3b79d3e373/test/cpp/qps/qps_worker.cc#L237-L239",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/24622,516159139,2020-11-02T18:00:58Z,test/core/gprpp/stat_test.cc,"@@ -31,6 +31,9 @@ #include ""src/core/lib/iomgr/load_file.h"" #include ""test/core/util/test_config.h"" +#define SERVER_KEY_CERT_DIR ""src/core/tsi/test_creds""","I might be wrong, but I was under the impression that these tests using credential paths were only run in non-Windows platforms. For example, [this test](https://github.com/grpc/grpc/blob/master/test/core/security/BUILD#L250-L260) also uses the forward slash credential paths, but without tags of ""no windows"". Did I understand this correctly?",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/24613,516175147,2020-11-02T18:29:59Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -4348,48 +4346,64 @@ TEST_P(LdsRdsTest, XdsRoutingApplyXdsTimeout) {     balancers_[0]->ads_service()->SetLdsResource(listener);   }   // Test grpc_timeout_header_max of 1.5 seconds applied-  auto t0 = system_clock::now();+  gpr_timespec est_timeout_time = gpr_time_add(+      gpr_now(GPR_CLOCK_MONOTONIC),+      gpr_time_from_millis(+          kTimeoutGrpcTimeoutHeaderMaxSecond * 1000 + kTimeoutMillis,+          GPR_TIMESPAN));+  gpr_timespec est_upperbound =+      gpr_time_add(gpr_now(GPR_CLOCK_MONOTONIC),+                   gpr_time_from_millis(+                       kTimeoutMaxStreamDurationSecond * 1000 + kTimeoutMillis,+                       GPR_TIMESPAN));   CheckRpcSendFailure(1,                       RpcOptions()                           .set_rpc_service(SERVICE_ECHO1)                           .set_rpc_method(METHOD_ECHO1)                           .set_wait_for_ready(true)                           .set_timeout_ms(kTimeoutApplicationSecond * 1000),",`set_timeout_ms` will call `grpc_timeout_milliseconds_to_deadline` which measures `now` again and potentially uses `grpc_test_slowdown_factor`. This could be a potential problem when running it under sanitizers. (But this doesn't explain why it's done sooner)Another angle I'm thinking of is type conversion between long and double happening a couple of times along the way. Not sure which one is problematic but it can introduce some error.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24500,516934008,2020-11-03T20:22:39Z,src/core/lib/iomgr/resolve_address_posix.cc,"@@ -77,6 +73,14 @@ static grpc_error* posix_blocking_resolve_address(     port = default_port;   } +  if (host.compare(""unix"") == 0) {+    return grpc_resolve_unix_domain_address(port.c_str(), addresses);+  }++  if (host.compare(""unix-abstract"") == 0) {","I don't think this is the right place to do this. The code we have above this for ""unix:"" URIs is a really ugly hack that I've been meaning to get rid of. And even though I haven't yet gotten around to doing that, I don't want to repeat the existing mistake here.The iomgr name resolution code is used both on the client and on the server, but the ability for it to understand ""unix:"" or ""unix-abstract:"" names is only needed on the server side, because the client side only uses this code when it already knows it's a DNS name -- if you specify a ""unix:"" or ""unix-abstract:"" URI on the client side, it selects a different resolver that does not call the iomgr name resolution code in the first place.So instead of adding this hack here, we should instead move this check into the server code, which is the only place where it's actually needed.  I think the right place to do that is here:https://github.com/grpc/grpc/blob/fa334f322fe3969b103c2292dfa79b6759e3ab72/src/core/ext/transport/chttp2/server/chttp2_server.cc#L281That code should be changed to do something like this:```grpc_error* error = GRPC_ERROR_NONE;if (absl::StartsWith(addr, ""unix:"")) {  error = grpc_resolve_unix_domain_address(addr + sizeof(""unix:""), &resolved);} else if (absl::StartsWith(addr, ""unix-abstract:"")) {  error = grpc_resolve_unix_abstract_domain_address(addr + sizeof(""unix-abstract:""), &resolved);} else {  error = grpc_blocking_resolve_address(addr, ""https"", &resolved);}```Then we can get rid of both of the special cases here.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24500,517418333,2020-11-04T15:16:05Z,src/core/lib/iomgr/unix_sockets_posix.h,"@@ -29,13 +29,15 @@  #include ""src/core/lib/iomgr/resolve_address.h"" +#include ""absl/strings/string_view.h""+ void grpc_create_socketpair_if_unix(int sv[2]);  grpc_error* grpc_resolve_unix_domain_address(     const char* name, grpc_resolved_addresses** addresses);  grpc_error* grpc_resolve_unix_abstract_domain_address(-    const char* name, grpc_resolved_addresses** addresses);+    const absl::string_view name, grpc_resolved_addresses** addresses);","This parameter doesn't need to be `const`, since it's being passed in by value.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24500,517434363,2020-11-04T15:38:06Z,src/core/lib/iomgr/parse_address.h,"@@ -52,6 +54,15 @@ bool grpc_parse_ipv4_hostport(const char* hostport, grpc_resolved_address* addr, bool grpc_parse_ipv6_hostport(const char* hostport, grpc_resolved_address* addr,                               bool log_errors); +/** Populate \a resolved_addr to be a unix socket at |path| */+grpc_error* grpc_fill_unix_path(const absl::string_view path,","The `absl::string_view` parameter should not be `const`, since it's being passed in by value.Same for the next function.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24500,517610316,2020-11-04T20:24:56Z,src/core/lib/iomgr/parse_address.h,"@@ -52,6 +54,15 @@ bool grpc_parse_ipv4_hostport(const char* hostport, grpc_resolved_address* addr, bool grpc_parse_ipv6_hostport(const char* hostport, grpc_resolved_address* addr,                               bool log_errors); +/** Populate \a resolved_addr to be a unix socket at |path| */+grpc_error* grpc_fill_unix_path(const absl::string_view path,",I don't see the value of doing that for a parameter.  That just imposes constraints on the implementation and doesn't add any benefit to the caller.,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/24622,517680369,2020-11-04T22:57:08Z,test/core/gprpp/stat_test.cc,"@@ -31,6 +31,9 @@ #include ""src/core/lib/iomgr/load_file.h"" #include ""test/core/util/test_config.h"" +#define SERVER_KEY_CERT_DIR ""src/core/tsi/test_creds""","@nicolasnobleHi Nicolas, any ideas on why these paths using forward slashes work on Windows? I thought it could be due to the macros defined in https://github.com/grpc/grpc/blob/master/include/grpc/impl/codegen/port_platform.h, but was not entirely sure of it. Could you please confirm that? Thanks!",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/24696,519011146,2020-11-06T21:21:44Z,tools/run_tests/run_xds_tests.py,"@@ -311,6 +311,63 @@ def get_client_stats(num_rpcs, timeout_sec):             logger.debug('Invoked GetClientStats RPC to %s: %s', host, response)             return response +def get_client_accumulated_stats():+    if CLIENT_HOSTS:+        hosts = CLIENT_HOSTS+    else:+        hosts = ['localhost']+    for host in hosts:+        with grpc.insecure_channel('%s:%d' %+                                   (host, args.stats_port)) as channel:+            stub = test_pb2_grpc.LoadBalancerStatsServiceStub(channel)+            request = messages_pb2.LoadBalancerAccumulatedStatsRequest()+            logger.debug('Invoking GetClientAccumulatedStats RPC to %s:%d:',+                         host, args.stats_port)+            response = stub.GetClientAccumulatedStats(request,+                                                      wait_for_ready=True,+                                                      timeout=_CONNECTION_TIMEOUT_SEC)+            logger.debug('Invoked GetClientAccumulatedStats RPC to %s: %s',+                         host,+                         response)+            return response++def configure_client(rpc_types, metadata):+    if CLIENT_HOSTS:+        hosts = CLIENT_HOSTS+    else:+        hosts = ['localhost']+    for host in hosts:+        with grpc.insecure_channel('%s:%d' %+                                   (host, args.stats_port)) as channel:+            stub = test_pb2_grpc.XdsUpdateClientConfigureServiceStub(channel)+            request = messages_pb2.ClientConfigureRequest()+            for rpc_type in rpc_types:+                if rpc_type not in ['empty_call', 'unary_call']:",Can the rpc_type parameter instead be a list of type `ClientConfigureRequest.RpcType`? Then we wouldn't need this check here or the `if rpc_type == 'empty_call'` conditional in constructing the `request` and `md` protobufs below.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24622,519043936,2020-11-06T22:50:40Z,test/core/gprpp/stat_test.cc,"@@ -31,6 +31,9 @@ #include ""src/core/lib/iomgr/load_file.h"" #include ""test/core/util/test_config.h"" +#define SERVER_KEY_CERT_DIR ""src/core/tsi/test_creds""","I chatted about this with Nico, and he said that apparently forward slashes do actually work on Widows, but they're not really supposed to, so we shouldn't depend on it.(There's apparently some nice support in C++17 for doing this in a cross-platform way, but we are still stuck on C++11, so that doesn't really help us.)How about this: Let's make this API take its input as `std::vector<absl::string_view>`.  That way, we can join as many path components as we want in a single call.  And we can represent hard-coded paths like this as an array:```const char* kServerCertPath[] = { ""src"", ""core"", ""tsi"", ""test_creds"" };```Then we can convert it to a path string like this:```std::vector<absl::string_view>(kServerCertPath, kServerCertPath + GPR_ARRAY_SIZE(kServerCertPath));```And we can pass that into this API to get a string.  WDYT?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/24737,522572629,2020-11-13T02:30:38Z,src/core/README.md,"@@ -1,4 +1,5 @@-# Overview+# Shared C-core library -This directory contains source code for C library (a.k.a the *gRPC C core*) that provides all gRPC's core functionality through a low level API. Libraries in other languages in this repository (C++, C#, Ruby,-Python, PHP, NodeJS, Objective-C) are layered on top of this library.+This library provides all of gRPC's core functionality through a low level API.+gRPC libraries for the other languages supported in this repo, are built on+top of this shared C-core library.","Sorry for being strict. The Core team has tried hard to use just ""Core"" instead of ""C-Core"" because it is no longer written only in C. Also, there are other languages supported by gRPC out side of this repo using gRPC Core (see https://github.com/grpc).",
4140793,chalin,https://api.github.com/repos/grpc/grpc/pulls/24737,522574279,2020-11-13T02:36:47Z,src/core/README.md,"@@ -1,4 +1,5 @@-# Overview+# Shared C-core library -This directory contains source code for C library (a.k.a the *gRPC C core*) that provides all gRPC's core functionality through a low level API. Libraries in other languages in this repository (C++, C#, Ruby,-Python, PHP, NodeJS, Objective-C) are layered on top of this library.+This library provides all of gRPC's core functionality through a low level API.+gRPC libraries for the other languages supported in this repo, are built on+top of this shared C-core library.","No worries, that's what reviewers are meant to do!> The Core team has tried hard to use just ""Core"" instead of ""C-Core"" because it is no longer written only in C.Ok, that makes sense (I'll update that tomorrow).Note that ""C core"" is used in the [repo's top-level README](https://github.com/grpc/grpc/blob/master/README.md); should that be updated too?",
4140793,chalin,https://api.github.com/repos/grpc/grpc/pulls/24737,523186851,2020-11-13T19:48:27Z,src/core/README.md,"@@ -1,4 +1,6 @@-# Overview+# gRPC core library++This shared library provides all of gRPC's core functionality through a low+level API. gRPC libraries for the other languages supported in this repo, are+built on top of this shared core library.","My original goal for this PR was to eliminate the list of languages so as to keep things [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself). Such a change is part of the broader docs work being done in the context of this issue: [_Eliminate duplicate docs across site and repos \#180_](https://github.com/grpc/grpc.io/issues/180).IMHO, if the languages are to be listed, they should be listed in one place only. The most likely place to have that list is the repo's top-level README (which does contain the list, but it isn't explicitly stated in that way -- I'll see if I can improve the situation when I submit a PR for the top-level README).",
4140793,chalin,https://api.github.com/repos/grpc/grpc/pulls/24737,523192726,2020-11-13T20:00:21Z,src/core/README.md,"@@ -1,4 +1,6 @@-# Overview+# gRPC core library","Oh, I think that there might be the confusion between two possible interpretations:1. Core as in ""gRPC Core"", or2. Core as in the ""core library"" among all the libraries made available from this repo.The title used in the heading of this document refers to (2). But yes, I'm eager for @nicolasnoble's input. (Thank's for cc  'ing him.)",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/24756,523298280,2020-11-14T00:23:46Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -1830,6 +1826,41 @@ class XdsEnd2endTest : public ::testing::TestWithParam<TestType> {     return assignment;   } +  void SetListenerAndRouteConfiguration(+      int idx, Listener listener, const RouteConfiguration& route_config) {","(if counting fault injection) building xDS Listener now impacted by 3 parameters: route config, RDS enablement, and filter config. Since the first two is handled in this method, do we plan to accept filter configs in this method as well?",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/24768,524965168,2020-11-17T08:24:37Z,src/csharp/Grpc.Tools/Common.cs,"@@ -66,27 +66,53 @@ static Platform()                 default: Cpu = CpuKind.Unknown; break;             } #else-            // Running under either Mono or full MS framework.-            Os = OsKind.Windows;-            if (Type.GetType(""Mono.Runtime"", throwOnError: false) != null)+            // Using the same best-effort detection logic as Grpc.Core/PlatformApis.cs+            var platform = Environment.OSVersion.Platform;+            if (platform == PlatformID.Win32NT || platform == PlatformID.Win32S || platform == PlatformID.Win32Windows)             {-                // Congratulations. We are running under Mono.-                var plat = Environment.OSVersion.Platform;-                if (plat == PlatformID.MacOSX)-                {-                    Os = OsKind.MacOsX;-                }-                else if (plat == PlatformID.Unix || (int)plat == 128)-                {-                    // This is how Mono detects OSX internally.-                    Os = File.Exists(""/usr/lib/libc.dylib"") ? OsKind.MacOsX : OsKind.Linux;-                }+                Os = OsKind.Windows;+            }+            else if (platform == PlatformID.Unix && GetUname() == ""Darwin"")+            {+                Os = OsKind.MacOsX;+            }+            else+            {+                Os = OsKind.Linux;             }              // Hope we are not building on ARM under Xamarin!             Cpu = Environment.Is64BitProcess ? CpuKind.X64 : CpuKind.X86; #endif         }++        [DllImport(""libc"")]+        static extern int uname(IntPtr buf);++        // This code is copied from Grpc.Core/PlatformApis.cs+        static string GetUname()+        {+            var buffer = Marshal.AllocHGlobal(8192);+            try+            {+                if (uname(buffer) == 0)+                {+                    return Marshal.PtrToStringAnsi(buffer);+                }+                return string.Empty;+            }+            catch+            {",nit: it seems like the exceptions shouldn't happen under any normal circumstances. Can we log an error message here with details about the error?Same idea for the fallthrough to `return string.Empty;` in the try block above,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24768,524975638,2020-11-17T08:42:00Z,src/csharp/Grpc.Tools/Common.cs,"@@ -66,27 +66,53 @@ static Platform()                 default: Cpu = CpuKind.Unknown; break;             } #else-            // Running under either Mono or full MS framework.-            Os = OsKind.Windows;-            if (Type.GetType(""Mono.Runtime"", throwOnError: false) != null)+            // Using the same best-effort detection logic as Grpc.Core/PlatformApis.cs+            var platform = Environment.OSVersion.Platform;+            if (platform == PlatformID.Win32NT || platform == PlatformID.Win32S || platform == PlatformID.Win32Windows)             {-                // Congratulations. We are running under Mono.-                var plat = Environment.OSVersion.Platform;-                if (plat == PlatformID.MacOSX)-                {-                    Os = OsKind.MacOsX;-                }-                else if (plat == PlatformID.Unix || (int)plat == 128)-                {-                    // This is how Mono detects OSX internally.-                    Os = File.Exists(""/usr/lib/libc.dylib"") ? OsKind.MacOsX : OsKind.Linux;-                }+                Os = OsKind.Windows;+            }+            else if (platform == PlatformID.Unix && GetUname() == ""Darwin"")+            {+                Os = OsKind.MacOsX;+            }+            else+            {+                Os = OsKind.Linux;             }              // Hope we are not building on ARM under Xamarin!             Cpu = Environment.Is64BitProcess ? CpuKind.X64 : CpuKind.X86; #endif         }++        [DllImport(""libc"")]+        static extern int uname(IntPtr buf);++        // This code is copied from Grpc.Core/PlatformApis.cs+        static string GetUname()+        {+            var buffer = Marshal.AllocHGlobal(8192);+            try+            {+                if (uname(buffer) == 0)+                {+                    return Marshal.PtrToStringAnsi(buffer);+                }+                return string.Empty;+            }+            catch+            {","The exception can happen if the `[DllImport(""libc"")]` of uname fails, which I think can happen on Linux, depending on the distro or depending on whether the libc-dev package is installed. So I think we need to keep the catch block silent.on linux, `platform == PlatformID.Unix` will be also true so we will run this check on linux too (to differentiate between linux and mac).",
503812,voidzcy,https://api.github.com/repos/grpc/grpc/pulls/24587,525651332,2020-11-18T02:14:59Z,test/cpp/interop/xds_interop_client.cc,"@@ -47,7 +48,7 @@ ABSL_FLAG(int32_t, num_channels, 1, ""Number of channels.""); ABSL_FLAG(bool, print_response, false, ""Write RPC response to stdout.""); ABSL_FLAG(int32_t, qps, 1, ""Qps per channel.""); // TODO(Capstan): Consider using absl::Duration-ABSL_FLAG(int32_t, rpc_timeout_sec, 30, ""Per RPC timeout seconds."");+ABSL_FLAG(int32_t, rpc_timeout_sec, INT_MAX, ""Per RPC timeout seconds."");","It's not a good idea to set INT_MAX as the default deadline. It makes infrastructure failures slow/hard to be detected, especially in warmup. We may want to make it dynamically configurable through the `Configure()` RPC. I believe the timeout test would need it as well.",
503812,voidzcy,https://api.github.com/repos/grpc/grpc/pulls/24587,526336581,2020-11-18T18:46:30Z,test/cpp/interop/xds_interop_client.cc,"@@ -154,17 +205,23 @@ class TestClient {     {       std::lock_guard<std::mutex> lk(mu);       saved_request_id = ++global_request_id;+      ++global_request_id_by_method[""UNARY_CALL""];     }     std::chrono::system_clock::time_point deadline =         std::chrono::system_clock::now() +         std::chrono::seconds(absl::GetFlag(FLAGS_rpc_timeout_sec));     AsyncClientCall* call = new AsyncClientCall;-    call->context.set_deadline(deadline);     for (const auto& data : metadata) {       call->context.AddMetadata(data.first, data.second);+      // TODO@donnadionne: move deadlinet to separate proto.",nit: s/deadlinet/deadline/g. But whatever...,
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/24791,527189650,2020-11-19T20:50:36Z,tools/doxygen/Doxyfile.php,"@@ -817,7 +817,90 @@ src/php/lib/Grpc/ServerCredentials.php \ src/php/lib/Grpc/ServerStreamingCall.php \ src/php/lib/Grpc/Timeval.php \-src/php/lib/Grpc/UnaryCall.php+src/php/lib/Grpc/UnaryCall.php \+src/php/vendor/doctrine/instantiator/CONTRIBUTING.md \",You probably need to clean up your composer temporary directory before running `generate_projects.sh`.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24732,528500730,2020-11-23T07:05:40Z,tools/internal_ci/windows/grpc_build_artifacts.cfg,"@@ -16,7 +16,7 @@  # Location of the continuous shell script in repository. build_file: ""grpc/tools/internal_ci/windows/grpc_build_artifacts.bat""-timeout_mins: 120+timeout_mins: 180",I understand this might be necessary but I really really don't like the fact that building the artifacts takes such a long time as it indirectly harms our productivity.No action needed here but I just wanted to point out this is in a sense technical debt we should be removing at some point.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24732,528501494,2020-11-23T07:08:28Z,tools/internal_ci/windows/grpc_run_tests_matrix.bat,"@@ -17,6 +17,11 @@ cd /d %~dp0\..\..\..  call tools/internal_ci/helper_scripts/prepare_build_windows.bat || exit /b 1 +@rem Only install Python interpreters if we are running Python tests",shouldn't this be in prepare_build_windows.bat? (it's exatly the same idea as prepare_build_macos_rc: https://github.com/grpc/grpc/blob/e36246bc9d65bc241f0b26671590b2b25f10df72/tools/internal_ci/helper_scripts/prepare_build_macos_rc#L85),
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/24732,528886653,2020-11-23T17:45:39Z,tools/internal_ci/windows/grpc_build_artifacts.cfg,"@@ -16,7 +16,7 @@  # Location of the continuous shell script in repository. build_file: ""grpc/tools/internal_ci/windows/grpc_build_artifacts.bat""-timeout_mins: 120+timeout_mins: 180","The time for build artifacts are inflating. I agree the time is getting longer, and further investigation into the new Windows image is needed to nail down why the same job took longer time.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/24818,529029749,2020-11-23T22:13:16Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -781,6 +781,39 @@ cdef CallbackFailureHandler SERVER_SHUTDOWN_FAILURE_HANDLER = CallbackFailureHan     InternalError)  +cdef class _ConcurrentRpcLimiter:++    def __cinit__(self, int maximum_concurrent_rpcs, object loop):+        if maximum_concurrent_rpcs <= 0:+            raise ValueError(""maximum_concurrent_rpcs should be a postive integer"")+        self._maximum_concurrent_rpcs = maximum_concurrent_rpcs+        self._active_rpcs = 0+        self._active_rpcs_condition = asyncio.Condition()+        self._loop = loop++    async def check_before_request_call(self):+        await self._active_rpcs_condition.acquire()+        try:+            predicate = lambda: self._active_rpcs < self._maximum_concurrent_rpcs+            await self._active_rpcs_condition.wait_for(predicate)+            self._active_rpcs += 1+        finally:+            self._active_rpcs_condition.release()++    def decrease_once_finished(self, object rpc_task):+        def decrease_active_rpcs_count(_):+            async def decrease_active_rpcs_count_with_lock():",I'm not seeing a data dependency from `decrease_active_rpcs_count_with_lock` to `decrease_active_rpcs_count`. Can this function be defined in `decrease_once_finished` instead? I think that will save us from having to create a new function scope each time `decrease_active_rpcs_count` is called.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/24818,529033293,2020-11-23T22:20:36Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -781,6 +781,39 @@ cdef CallbackFailureHandler SERVER_SHUTDOWN_FAILURE_HANDLER = CallbackFailureHan     InternalError)  +cdef class _ConcurrentRpcLimiter:++    def __cinit__(self, int maximum_concurrent_rpcs, object loop):+        if maximum_concurrent_rpcs <= 0:+            raise ValueError(""maximum_concurrent_rpcs should be a postive integer"")+        self._maximum_concurrent_rpcs = maximum_concurrent_rpcs+        self._active_rpcs = 0+        self._active_rpcs_condition = asyncio.Condition()+        self._loop = loop++    async def check_before_request_call(self):+        await self._active_rpcs_condition.acquire()+        try:+            predicate = lambda: self._active_rpcs < self._maximum_concurrent_rpcs+            await self._active_rpcs_condition.wait_for(predicate)+            self._active_rpcs += 1+        finally:+            self._active_rpcs_condition.release()++    def decrease_once_finished(self, object rpc_task):+        def decrease_active_rpcs_count(_):+            async def decrease_active_rpcs_count_with_lock():+                await self._active_rpcs_condition.acquire()+                try:+                    self._active_rpcs -= 1+                    self._active_rpcs_condition.notify()","I was wondering about coroutine starvation here.There doesn't seem to be an API guarantee on the order in which waiting coroutines are woken up by a call to `notify`. If any arbitrary ordering is valid, then it's possible for a waiting coroutine to never make progress. Based on the [`asyncio.Condition` implementation](https://github.com/python/cpython/blob/79d2e62c008446fbbc6f264bb8a30e2d38b6ff58/Lib/asyncio/locks.py#L337), it looks like this shouldn't happen in practice.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24822,529329268,2020-11-24T09:16:28Z,tools/dockerfile/interoptest/grpc_interop_go/build_interop.sh,"@@ -16,18 +16,13 @@ # Builds Go interop server and client in a base image. set -e -# Clone just the grpc-go source code without any dependencies.-# We are cloning from a local git repo that contains the right revision-# to test instead of using ""go get"" to download from Github directly.-git clone --recursive /var/local/jenkins/grpc-go src/google.golang.org/grpc--# Get all gRPC Go dependencies-(cd src/google.golang.org/grpc && make deps && make testdeps)- # copy service account keys if available cp -r /var/local/jenkins/service_account $HOME || true -# Build the interop client and server-(cd src/google.golang.org/grpc/interop/client && go install)-(cd src/google.golang.org/grpc/interop/server && go install)-  +if [[ -z ""$GOPATH"" ]]; then+    GOPATH=""$(go env GOPATH)""+fi++# Download the interop server and client at head.+GO111MODULE=on go get google.golang.org/grpc/interop/client@HEAD \","I'm not an expert on go build system, but it seems that by doing this you're always getting go interop client from https://github.com/grpc/grpc-go's  HEAD of master branch - which is wrong.The interop tests are designed in a way to allows testing arbitrary version of grpc-go (e.g. in case you wanted to test an unsubmitted change locally).On the CI, it works like this:- the CI job checks out the ""right"" version of the grpc-go repository (which in this case is upstream/master, but that's just a coincidence) into a local directory and makes it available to the docker container via a mounted volume (/var/local/jenkins/)- the interop job then tests that exact version of grpc-go by building it inside the docker container.This setup allows you to run run_interop_tests.py locally and run interop tests on your local working copy.All the languages are setup this way and there is no reason for go not to follow the same pattern.So it's fine to change the go interop build, but the source code being tested needs to come from /var/local/jenkins/grpc-go. Also let's keep the comment that explains what's being done.",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/24793,530661351,2020-11-25T21:54:58Z,src/core/lib/gpr/log.cc,"@@ -30,10 +30,17 @@  GPR_GLOBAL_CONFIG_DEFINE_STRING(grpc_verbosity, ""ERROR"",                                 ""Default gRPC logging verbosity"")+GPR_GLOBAL_CONFIG_DEFINE_STRING(grpc_stacktrace_minloglevel, """",+                                ""Messages logged at the same or higher level ""+                                ""than this will print stacktrace"")++static constexpr gpr_atm GPR_LOG_SEVERITY_UNSET = GPR_LOG_SEVERITY_ERROR + 10;+static constexpr gpr_atm GPR_LOG_SEVERITY_NONE = GPR_LOG_SEVERITY_ERROR + 11;","I wonder whether this should be made an option for logs too along with DEBUG, INFO and ERROR so that people that don't want logs can set grpc_verbosity to none",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24826,532658440,2020-11-30T14:57:39Z,src/core/lib/iomgr/load_file.h,"@@ -27,9 +27,9 @@  #include ""src/core/lib/iomgr/error.h"" -/* Loads the content of a file into a slice. add_null_terminator will add+/* Loads the content of a file into an output. add_null_terminator will add","I think this should still say ""a slice"" -- it's talking about the type, not the name of the output parameter.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24826,532660602,2020-11-30T15:00:27Z,src/core/lib/surface/call.cc,"@@ -637,11 +637,10 @@ static void execute_batch_in_call_combiner(void* arg, grpc_error* /*ignored*/) {  // start_batch_closure points to a caller-allocated closure to be used // for entering the call combiner.-static void execute_batch(grpc_call* call,-                          grpc_transport_stream_op_batch* batch,+static void execute_batch(grpc_call* call, grpc_transport_stream_op_batch* op,","The parameter here should be called `batch`, not `op`.  Let's fix the declaration instead.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24826,532662120,2020-11-30T15:02:34Z,src/core/lib/transport/transport.cc,"@@ -177,33 +177,31 @@ grpc_endpoint* grpc_transport_get_endpoint(grpc_transport* transport) { // though it lives in lib, it handles transport stream ops sure // it's grpc_transport_stream_op_batch_finish_with_failure void grpc_transport_stream_op_batch_finish_with_failure(-    grpc_transport_stream_op_batch* batch, grpc_error* error,+    grpc_transport_stream_op_batch* op, grpc_error* error,","The parameter here should be called `batch`, not `op`.  Let's fix the declaration instead.",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/24834,532954638,2020-11-30T22:50:04Z,src/core/ext/xds/xds_certificate_provider.h,"@@ -70,12 +82,22 @@ class XdsCertificateProvider : public grpc_tls_certificate_provider {       grpc_tls_certificate_distributor* identity_cert_distributor);    Mutex mu_;+  // Use a separate mutex for san_matchers_ to avoid deadlocks since+  // san_matchers_ needs to be accessed when a handshake is being done and we+  // run into a possible deadlock scenario if using the same mutex. The mutex+  // deadlock cycle is formed as -+  // WatchStatusCallback() -> SetKeyMaterials() ->+  // TlsChannelSecurityConnector::TlsChannelCertificateWatcher::OnCertificatesChanged()+  // -> HandshakeManager::Add() -> SecurityHandshaker::DoHandshake() ->+  // subject_alternative_names_matchers()+  Mutex san_matchers_mu_;   bool watching_root_certs_ = false;   bool watching_identity_certs_ = false;   std::string root_cert_name_;   std::string identity_cert_name_;   RefCountedPtr<grpc_tls_certificate_distributor> root_cert_distributor_;   RefCountedPtr<grpc_tls_certificate_distributor> identity_cert_distributor_;+  std::vector<std::string> san_matchers_;","Would it work if we move this into a commonly shared class by both xDS and non-xDS cases, like `FileWatcherCertificateProvider` or `grpc_tls_credentials_options`?  I am asking because we might also want to support a list of allowed SAN names in the future. It seems better if we only have one `san_matchers_` cached somewhere.But of course, if that doesn't fit with xDS structure, then I guess we are also fine with having another `san_matchers_` cached in `FileWatcherCertificateProvider` or `grpc_tls_credentials_options`.",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/24834,532959998,2020-11-30T23:02:33Z,src/core/lib/security/credentials/xds/xds_credentials.cc,"@@ -31,11 +34,101 @@ constexpr const char XdsCredentials::kCredentialsTypeXds[];  namespace { -int ServerAuthCheckSchedule(void* /* config_user_data */,+// Based on+// https://github.com/grpc/grpc-java/blob/ca12e7a339add0ef48202fb72434b9dc0df41756/xds/src/main/java/io/grpc/xds/internal/sds/trust/SdsX509TrustManager.java#L62+bool VerifySingleSubjectAlterativeName(+    const std::string& subject_alternative_name, const std::string& matcher) {+  if (subject_alternative_name.empty() ||+      absl::StartsWith(subject_alternative_name, ""."")) {+    // Illegal pattern/domain name+    return false;+  }+  if (matcher.empty() || absl::StartsWith(matcher, ""."")) {+    // Illegal domain name+    return false;+  }+  // Normalize sanToVerify and pattern by turning them into absolute domain+  // names if they are not yet absolute. This is needed because server+  // certificates do not normally contain absolute names or patterns, but they+  // should be treated as absolute. At the same time, any sanToVerify presented+  // to this method should also be treated as absolute for the purposes of+  // matching to the server certificate.+  std::string normalized_san =+      absl::EndsWith(subject_alternative_name, ""."")+          ? subject_alternative_name+          : absl::StrCat(subject_alternative_name, ""."");+  std::string normalized_matcher =+      absl::EndsWith(matcher, ""."") ? matcher : absl::StrCat(matcher, ""."");+  absl::AsciiStrToLower(&normalized_san);+  absl::AsciiStrToLower(&normalized_matcher);+  if (!absl::StrContains(normalized_san, ""*"")) {+    return normalized_san == normalized_matcher;+  }+  // WILDCARD PATTERN RULES:+  // 1. Asterisk (*) is only permitted in the left-most domain name label and+  // must be the+  //    only character in that label (i.e., must match the whole left-most+  //    label). For example, *.example.com is permitted, while *a.example.com,+  //    a*.example.com, a*b.example.com, a.*.example.com are not permitted.+  // 2. Asterisk (*) cannot match across domain name labels.+  //    For example, *.example.com matches test.example.com but does not match+  //    sub.test.example.com.+  // 3. Wildcard patterns for single-label domain names are not permitted.+  if (!absl::StartsWith(normalized_san, ""*."")) {+    // Asterisk (*) is only permitted in the left-most domain name label and+    // must be the only character in that label+    return false;+  }+  if (normalized_san == ""*."") {+    // Wildcard pattern for single-label domain name -- not permitted.+    return false;+  }+  std::string suffix = normalized_san.substr(1);+  if (absl::StrContains(suffix, ""*"")) {+    // Asterisk (*) is not permitted in the suffix+    return false;+  }+  if (!absl::EndsWith(normalized_matcher, suffix)) return false;+  int suffix_start_index = normalized_matcher.length() - suffix.length();+  // Asterisk matching across domain labels is not permitted.+  return suffix_start_index <= 0 /* should not happen */ ||+         normalized_matcher.find_last_of('.', suffix_start_index - 1) ==+             std::string::npos;+}++bool VerifySubjectAlternativeNames(const char* const* subject_alternative_names,","As discussed offline, it would be great if we can move this function to a common-shared file(like `tls_utils.h`), so that other classes can reuse it in the future.",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/24834,532973168,2020-11-30T23:34:40Z,src/core/lib/security/security_connector/tls/tls_security_connector.cc,"@@ -240,6 +240,31 @@ void TlsChannelSecurityConnector::check_peer(                 : check_arg_->peer_cert_full_chain;         gpr_free(peer_pem_chain);       }+      // Fill in the subject alternative names+      std::vector<char*> subject_alternative_names;","I think the logic would be simplified if we can use `std::string` instead of `char*` here. Is it because we have `char**` in `grpc_tls_server_authorization_check_arg`?My next planned large item is to improve the custom verification. Do you think it would be a good idea if we make `grpc_tls_server_authorization_check_arg` an internal core class instead of a struct exposed in `grpc_security.h`? It now contains many irrelevant fields. If we make it an internal class, we will need some helper functions like `get_target_name_from_arg`, `get_peer_cert_from_arg`, `get_san_names_from_arg` in `grpc_security.h` to get the information we really want. But the changes here could be simplified by using `std::string`, since here we don't use the core API. WDYT? You can add a TODO(for cleaning up the code) for me if that makes sense to you.  ",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24860,533503698,2020-12-01T15:31:10Z,tools/run_tests/artifacts/artifact_targets.py,"@@ -348,7 +348,6 @@ def targets():         ProtocArtifact('linux', 'x64'),         ProtocArtifact('linux', 'x86'),         ProtocArtifact('macos', 'x64'),-        ProtocArtifact('macos', 'x86'),",Removing these should be enough to make the nuget package build pass?https://github.com/grpc/grpc/blob/2dc98b1e4cfa433140bc1c7af0a99b718720331a/src/csharp/Grpc.Tools/Grpc.Tools.csproj#L72https://github.com/grpc/grpc/blob/2dc98b1e4cfa433140bc1c7af0a99b718720331a/src/csharp/Grpc.Tools/Grpc.Tools.csproj#L65,
394885,soheilhy,https://api.github.com/repos/grpc/grpc/pulls/24864,533571304,2020-12-01T16:58:43Z,src/cpp/server/server_context.cc,"@@ -174,33 +174,40 @@ void ServerContextBase::CompletionOp::FillOps(internal::Call* call) { }  bool ServerContextBase::CompletionOp::FinalizeResult(void** tag, bool* status) {-  // Decide whether to call the cancel callback within the lock-  bool call_cancel;+  // Decide whether to do the unref or call the cancel callback within the lock+  bool do_unref = false;+  bool has_tag = false;+  bool call_cancel = false;    {     grpc_core::MutexLock lock(&mu_);     if (done_intercepting_) {       // We are done intercepting.-      bool has_tag = has_tag_;+      has_tag = has_tag_;       if (has_tag) {         *tag = tag_;       }-      Unref();-      return has_tag;-    }-    finalized_ = true;+      // Release the lock before unreffing as Unref may delete this object+      do_unref = true;+    } else {+      finalized_ = true;++      // If for some reason the incoming status is false, mark that as a+      // cancellation.+      // TODO(vjpai): does this ever happen?+      if (!*status) {+        cancelled_ = 1;+      } -    // If for some reason the incoming status is false, mark that as a-    // cancellation.-    // TODO(vjpai): does this ever happen?-    if (!*status) {-      cancelled_ = 1;+      call_cancel = (cancelled_ != 0);+      // Release the lock since we may call a callback and interceptors.     }--    call_cancel = (cancelled_ != 0);-    // Release the lock since we may call a callback and interceptors.   } +  if (do_unref) {+    Unref();","nit: Internally, we usually add a comment here to avoid similar issues in the future:```// `Unref()` can delete `this`. So, do not access `this` afterwards.```",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/24813,535441891,2020-12-03T17:35:45Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -277,8 +280,7 @@ class ChannelData {   void UpdateStateAndPickerLocked(",This function name is now ambiguous since we don't know which lock is being held. Can I suggest a comment above the name saying which lock must be held before entering the function? Or a commented ABSL_GUARDED_BY annotation (which I think we still can't use but which could at least be helpful for readability as a comment),X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/24813,535442126,2020-12-03T17:36:01Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -277,8 +280,7 @@ class ChannelData {   void UpdateStateAndPickerLocked(       grpc_connectivity_state state, const absl::Status& status,       const char* reason,-      std::unique_ptr<LoadBalancingPolicy::SubchannelPicker> picker,-      grpc_error* resolver_transient_failure_error = GRPC_ERROR_NONE);+      std::unique_ptr<LoadBalancingPolicy::SubchannelPicker> picker);    void UpdateServiceConfigInControlPlaneLocked(",I guess technically this function name is ambiguous too.,X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/24813,535448868,2020-12-03T17:46:08Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -277,8 +280,7 @@ class ChannelData {   void UpdateStateAndPickerLocked(","I guess it was already ambiguous since there were already other gpr_mu and Mutex in the struct, so this really needs a detailed comment.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24813,535472451,2020-12-03T18:18:35Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -2062,6 +2059,83 @@ void ChannelData::DestroyResolverAndLbPolicyLocked() {   } } +void ChannelData::UpdateStateAndPickerLocked(+    grpc_connectivity_state state, const absl::Status& status,+    const char* reason,+    std::unique_ptr<LoadBalancingPolicy::SubchannelPicker> picker) {+  // Clean the control plane when entering IDLE.+  if (picker == nullptr || state == GRPC_CHANNEL_SHUTDOWN) {+    health_check_service_name_.reset();+    saved_service_config_.reset();+    saved_config_selector_.reset();+  }+  // Update connectivity state.+  state_tracker_.SetState(state, status, reason);+  if (channelz_node_ != nullptr) {+    channelz_node_->SetConnectivityState(state);+    channelz_node_->AddTraceEvent(+        channelz::ChannelTrace::Severity::Info,+        grpc_slice_from_static_string(+            channelz::ChannelNode::GetChannelConnectivityStateChangeString(+                state)));+  }+  // Grab data plane lock to do subchannel updates and update the picker.+  //+  // Note that we want to minimize the work done while holding the data+  // plane lock, to keep the critical section small.  So, for all of the+  // objects that we might wind up unreffing here, we actually hold onto+  // the refs until after we release the lock, and then unref them at+  // that point.  This includes the following:+  // - refs to subchannel wrappers in the keys of pending_subchannel_updates_+  // - ref stored in retry_throttle_data_+  // - ref stored in service_config_+  // - ref stored in config_selector_+  // - ownership of the existing picker in picker_+  RefCountedPtr<ServerRetryThrottleData> retry_throttle_data_to_unref;+  RefCountedPtr<ServiceConfig> service_config_to_unref;+  RefCountedPtr<ConfigSelector> config_selector_to_unref;+  {+    MutexLock lock(&data_plane_mu_);","No code every holds `data_plane_mu_` and `resolution_mu_` at the same time, so there's no need to define ordering.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/24861,535483457,2020-12-03T18:33:52Z,tools/run_tests/run_tests.py,"@@ -355,7 +355,7 @@ def test_specs(self):                             tests = subprocess.check_output(                                 [binary, '--benchmark_list_tests'],                                 stderr=fnull)-                        for line in tests.split('\n'):+                        for line in tests.decode().split('\n'):","[The default is ""UTF-8""](https://docs.python.org/3/library/stdtypes.html#bytes.decode). This should be fine.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24892,536298190,2020-12-04T18:34:10Z,src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc,"@@ -515,7 +515,13 @@ static bool inner_resolve_as_ip_literal_locked(     *port = default_port;   }   grpc_resolved_address addr;-  *hostport = grpc_core::JoinHostPort(*host, atoi(port->c_str()));+  int port_number = atoi(port->c_str());+  if (strcmp(port->c_str(), ""http"") == 0) {","@apolcyn Is there a portable way to look up the port name in /etc/services?The native DNS resolver code also [hard-codes these values](https://github.com/grpc/grpc/blob/61af6e7f4f25804a0140de96cd5a1983ec6e16a7/src/core/lib/iomgr/resolve_address_posix.cc#L88), but it looks like that code only kicks in if `getaddrinfo()` doesn't understand the port name, so I assume it checks /etc/services first.  It would be nice to get the same behavior here if we can.",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/24834,536474694,2020-12-05T01:31:38Z,src/core/lib/security/security_connector/tls/tls_security_connector.cc,"@@ -240,6 +240,31 @@ void TlsChannelSecurityConnector::check_peer(                 : check_arg_->peer_cert_full_chain;         gpr_free(peer_pem_chain);       }+      // Fill in the subject alternative names+      std::vector<char*> subject_alternative_names;","looks like reviewable did not post the response here.I think the logic would be simplified if we can use std::string instead of char* here. Is it because we have char** in grpc_tls_server_authorization_check_arg?That is correct. If we can use std::string, the logic would get simpler.My next planned large item is to improve the custom verification. Do you think it would be a good idea if we make grpc_tls_server_authorization_check_arg an internal core class instead of a struct exposed in grpc_security.h? It now contains many irrelevant fields.Are you sure we can use an internal Core class from the wrapped C++ layer? What if we want to use this from other languages too?Added a TODO anyway",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/24911,537777825,2020-12-07T19:37:12Z,templates/grpc.gemspec.template,"@@ -34,7 +34,7 @@     s.require_paths = %w( src/ruby/lib src/ruby/bin src/ruby/pb )     s.platform      = Gem::Platform::RUBY -    s.add_dependency 'google-protobuf', '~> 3.13'+    s.add_dependency 'google-protobuf', '~> ${settings.protobuf_short_version}'","@gnossen which doc are you referring to?If we list a protobuf patch release version here, then this gemspec will only permit the picked up google-protobuf dependency's patch version to change, i.e. the minor version will be pinned. But if we only list the minor version number, then the minor version number will be allowed to move.Maybe `protobuf_short_version` should be called `protobuf_minor_version` ?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/24755,538825241,2020-12-08T21:36:21Z,examples/python/auth/async_customized_auth_client.py,"@@ -0,0 +1,101 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Client of the Python AsyncIO example of customizing authentication mechanism.""""""++import argparse+import asyncio+import logging++import grpc++import _credentials++helloworld_pb2, helloworld_pb2_grpc = grpc.protos_and_services(+    ""helloworld.proto"")++_LOGGER = logging.getLogger(__name__)+_LOGGER.setLevel(logging.INFO)++_SERVER_ADDR_TEMPLATE = 'localhost:%d'+_SIGNATURE_HEADER_KEY = 'x-signature'+++class AuthGateway(grpc.AuthMetadataPlugin):++    def __call__(self, context: grpc.AuthMetadataContext,+                 callback: grpc.AuthMetadataPluginCallback) -> None:+        """"""Implements authentication by passing metadata to a callback.++        Implementations of this method must not block.++        Args:+          context: An AuthMetadataContext providing information on the RPC that+            the plugin is being called to authenticate.+          callback: An AuthMetadataPluginCallback to be invoked either+            synchronously or asynchronously.+        """"""+        # Example AuthMetadataContext object:+        # AuthMetadataContext(+        #     service_url=u'https://localhost:50051/helloworld.Greeter',+        #     method_name=u'SayHello')+        signature = context.method_name[::-1]+        callback(((_SIGNATURE_HEADER_KEY, signature),), None)+++def create_client_channel(addr: str) -> grpc.aio.Channel:+    # Call credential object will be invoked for every single RPC+    call_credentials = grpc.metadata_call_credentials(AuthGateway(),+                                                      name='auth gateway')+    # Channel credential will be valid for the entire channel+    channel_credential = grpc.ssl_channel_credentials(+        _credentials.ROOT_CERTIFICATE)+    # Combining channel credentials and call credentials together+    composite_credentials = grpc.composite_channel_credentials(+        channel_credential,+        call_credentials,+    )+    channel = grpc.aio.secure_channel(addr, composite_credentials)+    return channel+++async def send_rpc(channel: grpc.aio.Channel) -> helloworld_pb2.HelloReply:+    stub = helloworld_pb2_grpc.GreeterStub(channel)+    request = helloworld_pb2.HelloRequest(name='you')+    try:+        response = await stub.SayHello(request)+    except grpc.RpcError as rpc_error:+        _LOGGER.error('Received error: %s', rpc_error)+        return rpc_error+    else:+        _LOGGER.info('Received message: %s', response)+        return response+++async def main() -> None:+    parser = argparse.ArgumentParser()+    parser.add_argument('--port',+                        nargs='?',+                        type=int,+                        default=50051,+                        help='the address of server')+    args = parser.parse_args()++    channel = create_client_channel(_SERVER_ADDR_TEMPLATE % args.port)","The API to make a function usable for `async with` is [`contextlib.asynccontextmanager`](https://docs.python.org/3/library/contextlib.html#contextlib.asynccontextmanager), but only available in 3.7. Or we have to make it a class with `__aenter__` and `__aexit__`. I hope our examples can support 3.6, as our minimum supported version suggests. I can try out other styles, if you have thoughts.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/24755,538831731,2020-12-08T21:47:26Z,examples/python/auth/helloworld.proto,"@@ -0,0 +1,38 @@+// Copyright 2015 gRPC authors.","As I recall, Piper was the only thing that choked on a symlink the last time we tried it. If CI fails, feel free to change it back. But if it works, I definitely think it's worth reducing the duplication.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24949,539718783,2020-12-09T23:14:08Z,src/core/ext/xds/xds_client.cc,"@@ -66,6 +66,7 @@ namespace grpc_core {  TraceFlag grpc_xds_client_trace(false, ""xds_client"");+TraceFlag grpc_xds_client_ref_count_trace(false, ""xds_client_ref_count"");","Please change ""ref_count"" to ""refcount"", since this will allow this tracer to be enabled/disabled all at once with the other ref-count tracers.  For context, see:https://github.com/grpc/grpc/blob/0dd3f7e65b4938de736ccd0326aa26523df4649c/src/core/lib/debug/trace.cc#L51(In https://github.com/grpc/grpc/blob/master/doc/environment_variables.md, we document the fact that you can enable all refcount tracers with ""refcount"".)",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/24955,540399030,2020-12-10T18:26:33Z,src/core/tsi/ssl_transport_security.cc,"@@ -910,33 +910,31 @@ static tsi_result tsi_set_min_and_max_tls_versions(     return TSI_INVALID_ARGUMENT;   } #if OPENSSL_VERSION_NUMBER >= 0x10100000-  // Set the min TLS version of the SSL context.+  // Set the min TLS version of the SSL context if using OpenSSL version","To confirm, are you proposing the following code?```#if OPENSSL_VERSION_NUMBER >= 0x10100000#if defined(TLS1_3_VERSION)  switch (min_tls_version) {    case tsi_tls_version::TSI_TLS1_2:      SSL_CTX_set_min_proto_version(ssl_context, TLS1_2_VERSION);      break;    case tsi_tls_version::TSI_TLS1_3:      SSL_CTX_set_min_proto_version(ssl_context, TLS1_3_VERSION);      break;    default:      // Some logging.      break;  }  switch (max_tls_version) {    case tsi_tls_version::TSI_TLS1_2:      SSL_CTX_set_max_proto_version(ssl_context, TLS1_2_VERSION);      break;    case tsi_tls_version::TSI_TLS1_3:      SSL_CTX_set_max_proto_version(ssl_context, TLS1_3_VERSION);      break    default:      // Some logging.      break;  }#endif#endif```If so, that's more than just a readability change - that's a functional change. Consider the case when `OPENSSL_VERSION_NUMBER >= 0x10100000` and `TLS1_3_VERSION` is **not** defined. Then, this method is a no-op, so there are no restrictions on the minimum TLS version. And since we've instantiated the `SSL_CTX` using `TLS_method` rather than `TLSv1_2_method` (see [here](https://github.com/grpc/grpc/blob/master/src/core/tsi/ssl_transport_security.cc#L1902)), we have opened ourselves up to a downgrade attack where the peer could try to negotiate TLS  1.1 or worse.Please correct me if I misunderstood your proposal. :)@davidben to keep me honest on the fact that we must set a min TLS version if we use `TLS_method`.",
109690,davidben,https://api.github.com/repos/grpc/grpc/pulls/24955,540518369,2020-12-10T21:43:29Z,src/core/tsi/ssl_transport_security.cc,"@@ -910,33 +910,31 @@ static tsi_result tsi_set_min_and_max_tls_versions(     return TSI_INVALID_ARGUMENT;   } #if OPENSSL_VERSION_NUMBER >= 0x10100000-  // Set the min TLS version of the SSL context.+  // Set the min TLS version of the SSL context if using OpenSSL version","If you do not set a minimum version with `TLS_method`, it'll use the library's default. That default, for BoringSSL, is currently TLS 1.0. So, yeah, if your aim is to reject TLS 1.0 and 1.1, you need to set a minimum version.This PR is a little odd though. So, if I understand right, the issue is that OpenSSLs which lack TLS 1.3 will break when you set the maximum to TSI_TLS1_3, rather than silently going down to TLS 1.2. But this also loses the TSI_FAILED_PRECONDITION logic, which seems valuable because if someone uses a totally random constant, you'd like for the library to fail.It seems to me the desired logic should be something like:```Leave this block unchanged. If the caller requested TLS 1.3 and the underlyinglibrary is incapable of providing TLS 1.3, the connection should fail, rather thansilently allowing TLS 1.2.  // Set the min TLS version of the SSL context.  switch (min_tls_version) {    case tsi_tls_version::TSI_TLS1_2:      SSL_CTX_set_min_proto_version(ssl_context, TLS1_2_VERSION);      break;// If the library does not support TLS 1.3, and the caller requested a minimum// of TLS 1.3, return an error. The caller's request cannot be satisfied.#if defined(TLS1_3_VERSION)    case tsi_tls_version::TSI_TLS1_3:      SSL_CTX_set_min_proto_version(ssl_context, TLS1_3_VERSION);      break;#endif    default:      gpr_log(GPR_INFO, ""TLS version is not supported."");      return TSI_FAILED_PRECONDITION;  }Change this block so if the library doesn't support TLS 1.3, set a maximum ofTLS 1.2, which is still compatible with what they requested.  // Set the max TLS version of the SSL context.  switch (max_tls_version) {    case tsi_tls_version::TSI_TLS1_2:      SSL_CTX_set_max_proto_version(ssl_context, TLS1_2_VERSION);      break;    case tsi_tls_version::TSI_TLS1_3:#if defined(TLS1_3_VERSION)      SSL_CTX_set_max_proto_version(ssl_context, TLS1_3_VERSION);#else      // The library doesn't support TLS 1.3, so set a maximum of      // TLS 1.2 instead.      SSL_CTX_set_max_proto_version(ssl_context, TLS1_2_VERSION);#endif      break;    default:      gpr_log(GPR_INFO, ""TLS version is not supported."");      return TSI_FAILED_PRECONDITION;  }```(There may be a cleaner way to express this.)",
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/24955,540709988,2020-12-11T05:58:47Z,src/core/tsi/ssl_transport_security.cc,"@@ -910,33 +910,31 @@ static tsi_result tsi_set_min_and_max_tls_versions(     return TSI_INVALID_ARGUMENT;   } #if OPENSSL_VERSION_NUMBER >= 0x10100000-  // Set the min TLS version of the SSL context.+  // Set the min TLS version of the SSL context if using OpenSSL version","Thanks for the suggestion David! I've added it in the newest commit.Some context: A user with LibreSSL encountered a failure due to this method: the gRPC stack requested min=1.2, max=1.3, and LibreSSL satisfied `OPENSSL_VERSION_NUMBER >= 0x10100000` but did not have `TLS1_3_VERSION` defined. As I understand it, LibreSSL is [actively working on adopting OpenSSL's TLS 1.3 semantics](https://github.com/libressl-portable/portable/issues/228#issuecomment-736544468), but this fix is needed in the meantime.To avoid this kind of problem in the future, @veblush will add e2e tests against different SSL libraries (see #24960).",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/24955,541348849,2020-12-11T22:10:44Z,src/core/tsi/ssl_transport_security.cc,"@@ -924,16 +930,21 @@ static tsi_result tsi_set_min_and_max_tls_versions(       gpr_log(GPR_INFO, ""TLS version is not supported."");       return TSI_FAILED_PRECONDITION;   }+   // Set the max TLS version of the SSL context.   switch (max_tls_version) {     case tsi_tls_version::TSI_TLS1_2:       SSL_CTX_set_max_proto_version(ssl_context, TLS1_2_VERSION);       break;-#if defined(TLS1_3_VERSION)     case tsi_tls_version::TSI_TLS1_3:+#if defined(TLS1_3_VERSION)       SSL_CTX_set_max_proto_version(ssl_context, TLS1_3_VERSION);-      break;+#else+      // If the libraary does not support TLS 1.3, then set the max TLS version",nit. s/libraary/library,
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/24955,541352347,2020-12-11T22:14:46Z,src/core/tsi/ssl_transport_security.cc,"@@ -910,33 +910,31 @@ static tsi_result tsi_set_min_and_max_tls_versions(     return TSI_INVALID_ARGUMENT;   } #if OPENSSL_VERSION_NUMBER >= 0x10100000-  // Set the min TLS version of the SSL context.+  // Set the min TLS version of the SSL context if using OpenSSL version",Thanks @davidben for suggestion! The new logic makes sense to me.The reason we introduced min/max TLS version was mainly for testing. Since TLS 1.2 and TLS 1.3 behave differently. We do not have customer request for only wanting a specific version of TLS.,
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/24956,541417034,2020-12-11T23:37:43Z,src/core/ext/xds/xds_server_config_fetcher.cc,"@@ -0,0 +1,125 @@+//+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/xds/xds_client.h""+#include ""src/core/lib/surface/api_trace.h""+#include ""src/core/lib/surface/server.h""++namespace grpc_core {+namespace {++class XdsServerConfigFetcher : public grpc_server_config_fetcher {+ public:+  XdsServerConfigFetcher() {+    grpc_error* error = GRPC_ERROR_NONE;+    xds_client_ = XdsClient::GetOrCreate(&error);+    if (error != GRPC_ERROR_NONE) {+      gpr_log(GPR_ERROR, ""Failed to create xds client: %s"",+              grpc_error_string(error));+    }+  }++  void StartWatch(std::string listening_address,+                  std::unique_ptr<grpc_server_config_fetcher::WatcherInterface>+                      watcher) override {+    auto listener_watcher = absl::make_unique<ListenerWatcher>(watcher.get());+    auto* listener_watcher_ptr = listener_watcher.get();+    // TODO(yashykt): Get the resource name id from bootstrap+    xds_client_->WatchListenerData(+        absl::StrCat(""grpc/server?udpa.resource.listening_address="",+                     listening_address),+        std::move(listener_watcher));+    MutexLock lock(&mu_);+    auto& watcher_state = watchers_[watcher.get()];+    watcher_state.listening_address = listening_address;+    watcher_state.watcher = std::move(watcher);+    watcher_state.listener_watcher = listener_watcher_ptr;+  }++  void CancelWatch(+      grpc_server_config_fetcher::WatcherInterface* watcher) override {+    MutexLock lock(&mu_);+    auto it = watchers_.find(watcher);+    if (it != watchers_.end()) {+      // Cancel the watch on the listener before erasing+      xds_client_->CancelListenerDataWatch(it->second.listening_address,+                                           it->second.listener_watcher,+                                           false /* delay_unsubscription */);+    }+    watchers_.erase(watcher);+  }++  // Return the interested parties from the xds client so that it can be polled.+  grpc_pollset_set* interested_parties() override {+    return xds_client_->interested_parties();+  }++ private:+  class ListenerWatcher : public XdsClient::ListenerWatcherInterface {+   public:+    explicit ListenerWatcher(+        grpc_server_config_fetcher::WatcherInterface* server_config_watcher)+        : server_config_watcher_(server_config_watcher) {}++    void OnListenerChanged(XdsApi::LdsUpdate listener) override {+      // TODO(yashykt): Construct channel args according to received update+      server_config_watcher_->UpdateConfig(nullptr);+    }++    void OnError(grpc_error* error) override {+      gpr_log(GPR_ERROR, ""ListenerWatcher:%p XdsClient reports error: %s"", this,+              grpc_error_string(error));+      GRPC_ERROR_UNREF(error);+      // TODO(yashykt): We might want to bubble this error to the application.+    }++    void OnResourceDoesNotExist() override {","I can't respond on reviewable.io due to the overbroad oauth permissions it requires.@markdroth wrote:> We should probably discuss how we should handle this case for the gRPC server. On the client, we report TRANSIENT_FAILURE in this case. The analogous thing here might be to stop listening, but that seems harder to implement.The analogous thing would be to go back to the waiting state we use during start(). That would mean a graceful shutdown: stop listen and issue GOAWAYs (although for Go the listening would continue, just stop accepting). Existing RPCs can continue, because xDS configuration has already been applied to them.Go and Java could gracefully stop the server without much trouble. But at least Java would have trouble starting it back up again for mundane reasons. I had some future API plans that would make that easier though.",
26072277,dfawley,https://api.github.com/repos/grpc/grpc/pulls/24956,541419339,2020-12-11T23:40:55Z,src/core/ext/xds/xds_server_config_fetcher.cc,"@@ -0,0 +1,125 @@+//+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/xds/xds_client.h""+#include ""src/core/lib/surface/api_trace.h""+#include ""src/core/lib/surface/server.h""++namespace grpc_core {+namespace {++class XdsServerConfigFetcher : public grpc_server_config_fetcher {+ public:+  XdsServerConfigFetcher() {+    grpc_error* error = GRPC_ERROR_NONE;+    xds_client_ = XdsClient::GetOrCreate(&error);+    if (error != GRPC_ERROR_NONE) {+      gpr_log(GPR_ERROR, ""Failed to create xds client: %s"",+              grpc_error_string(error));+    }+  }++  void StartWatch(std::string listening_address,+                  std::unique_ptr<grpc_server_config_fetcher::WatcherInterface>+                      watcher) override {+    auto listener_watcher = absl::make_unique<ListenerWatcher>(watcher.get());+    auto* listener_watcher_ptr = listener_watcher.get();+    // TODO(yashykt): Get the resource name id from bootstrap+    xds_client_->WatchListenerData(+        absl::StrCat(""grpc/server?udpa.resource.listening_address="",+                     listening_address),+        std::move(listener_watcher));+    MutexLock lock(&mu_);+    auto& watcher_state = watchers_[watcher.get()];+    watcher_state.listening_address = listening_address;+    watcher_state.watcher = std::move(watcher);+    watcher_state.listener_watcher = listener_watcher_ptr;+  }++  void CancelWatch(+      grpc_server_config_fetcher::WatcherInterface* watcher) override {+    MutexLock lock(&mu_);+    auto it = watchers_.find(watcher);+    if (it != watchers_.end()) {+      // Cancel the watch on the listener before erasing+      xds_client_->CancelListenerDataWatch(it->second.listening_address,+                                           it->second.listener_watcher,+                                           false /* delay_unsubscription */);+    }+    watchers_.erase(watcher);+  }++  // Return the interested parties from the xds client so that it can be polled.+  grpc_pollset_set* interested_parties() override {+    return xds_client_->interested_parties();+  }++ private:+  class ListenerWatcher : public XdsClient::ListenerWatcherInterface {+   public:+    explicit ListenerWatcher(+        grpc_server_config_fetcher::WatcherInterface* server_config_watcher)+        : server_config_watcher_(server_config_watcher) {}++    void OnListenerChanged(XdsApi::LdsUpdate listener) override {+      // TODO(yashykt): Construct channel args according to received update+      server_config_watcher_->UpdateConfig(nullptr);+    }++    void OnError(grpc_error* error) override {+      gpr_log(GPR_ERROR, ""ListenerWatcher:%p XdsClient reports error: %s"", this,+              grpc_error_string(error));+      GRPC_ERROR_UNREF(error);+      // TODO(yashykt): We might want to bubble this error to the application.+    }++    void OnResourceDoesNotExist() override {","> although for Go the listening would continue, just stop accepting > Go and Java could gracefully stop the server without much trouble.The ~first thing we do in `GracefulStop` in Go is stop listening:https://github.com/grpc/grpc-go/blob/c638ab8ccda6cefc15eb9a25357fc98b018ede37/server.go#L1677-L1679And we have no way to re-start serving currently, either.We'd have to have a way to GracefulStop without closing the listener, and a way to re-start.  Neither of these is trivial, but would not be insurmountable either.",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/24956,541426541,2020-12-11T23:50:03Z,src/core/ext/xds/xds_server_config_fetcher.cc,"@@ -0,0 +1,125 @@+//+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/xds/xds_client.h""+#include ""src/core/lib/surface/api_trace.h""+#include ""src/core/lib/surface/server.h""++namespace grpc_core {+namespace {++class XdsServerConfigFetcher : public grpc_server_config_fetcher {+ public:+  XdsServerConfigFetcher() {+    grpc_error* error = GRPC_ERROR_NONE;+    xds_client_ = XdsClient::GetOrCreate(&error);+    if (error != GRPC_ERROR_NONE) {+      gpr_log(GPR_ERROR, ""Failed to create xds client: %s"",+              grpc_error_string(error));+    }+  }++  void StartWatch(std::string listening_address,+                  std::unique_ptr<grpc_server_config_fetcher::WatcherInterface>+                      watcher) override {+    auto listener_watcher = absl::make_unique<ListenerWatcher>(watcher.get());+    auto* listener_watcher_ptr = listener_watcher.get();+    // TODO(yashykt): Get the resource name id from bootstrap+    xds_client_->WatchListenerData(+        absl::StrCat(""grpc/server?udpa.resource.listening_address="",+                     listening_address),+        std::move(listener_watcher));+    MutexLock lock(&mu_);+    auto& watcher_state = watchers_[watcher.get()];+    watcher_state.listening_address = listening_address;+    watcher_state.watcher = std::move(watcher);+    watcher_state.listener_watcher = listener_watcher_ptr;+  }++  void CancelWatch(+      grpc_server_config_fetcher::WatcherInterface* watcher) override {+    MutexLock lock(&mu_);+    auto it = watchers_.find(watcher);+    if (it != watchers_.end()) {+      // Cancel the watch on the listener before erasing+      xds_client_->CancelListenerDataWatch(it->second.listening_address,+                                           it->second.listener_watcher,+                                           false /* delay_unsubscription */);+    }+    watchers_.erase(watcher);+  }++  // Return the interested parties from the xds client so that it can be polled.+  grpc_pollset_set* interested_parties() override {+    return xds_client_->interested_parties();+  }++ private:+  class ListenerWatcher : public XdsClient::ListenerWatcherInterface {+   public:+    explicit ListenerWatcher(+        grpc_server_config_fetcher::WatcherInterface* server_config_watcher)+        : server_config_watcher_(server_config_watcher) {}++    void OnListenerChanged(XdsApi::LdsUpdate listener) override {+      // TODO(yashykt): Construct channel args according to received update+      server_config_watcher_->UpdateConfig(nullptr);+    }++    void OnError(grpc_error* error) override {+      gpr_log(GPR_ERROR, ""ListenerWatcher:%p XdsClient reports error: %s"", this,+              grpc_error_string(error));+      GRPC_ERROR_UNREF(error);+      // TODO(yashykt): We might want to bubble this error to the application.+    }++    void OnResourceDoesNotExist() override {","Ugh. I thought you would just stop Accept()ing, since you don't ""own"" that listener. But I guess that is impossible as the Accept() is non-interruptable.Preventing Close() is ""easy"", as you can wrap the provided Listener (yes, I know wrapping can lead to interesting details). But Accept() wouldn't return until at least one more connection arrived. I guess you could just have Accept() continue sitting there, and if a connection comes just close it.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24953,542260705,2020-12-14T10:07:29Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -1010,6 +1010,14 @@ def _detect_and_print_issues(build_yaml_like):     bazel_rules.update(         _extract_rules_from_bazel_xml(_bazel_query_xml_tree(query))) +# TODO(veblush): Remove this workaround once upb is supported well+# for both Bazel and non-Bazel (https://github.com/grpc/grpc/issues/24904)+google_api_upbdefs_rule = bazel_rules[""//:google_api_upbdefs""]","Since in this hack you are manually adding a descriptor.upbdefs.c and descriptor.updefs.h depedency for a specific target `//:google_api_upbdefs`, it also means that you'd have to add   this hack for all the libraries that depend on `upb_lib_descriptor` and it seems that that's something that's hard to maintain and also something that over time people will run into (and they won't be able to solve this themselves as this whole issue is quite cryptic).I don't know of a better solution right now (I'd have to spend some time thinking about it and experimenting), but at least you could:- introduce a separate function (e.g. `_inject_ubp_descriptor_dependency`) with a proper description that explains why this hack is being used. I think this hack being applied at the top level is an invitation for folks to add more similar hacks (and I don't like that). Also the function should make it explicit that there are some libraries (currently it's just `//:google_api_upbdefs` but it might be more of them in the future) where the upb descriptor.proto dependency needs to be added manually. If there is a reason why only `:google_api_upbdefs` will ever need the upb_lib_descriptor dependency, please include that reasoning in the comment in said function.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24953,542282252,2020-12-14T10:39:48Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -1010,6 +1010,14 @@ def _detect_and_print_issues(build_yaml_like):     bazel_rules.update(         _extract_rules_from_bazel_xml(_bazel_query_xml_tree(query))) +# TODO(veblush): Remove this workaround once upb is supported well+# for both Bazel and non-Bazel (https://github.com/grpc/grpc/issues/24904)+google_api_upbdefs_rule = bazel_rules[""//:google_api_upbdefs""]",Idea: Maybe there's some way to adjust the logic from https://github.com/grpc/grpc/pull/24925/files to fix this without actually requiring to manually list the libraries that depend on descriptor.updefs.c and descriptor.upbdefs.h?,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/24953,542284461,2020-12-14T10:43:15Z,BUILD,"@@ -2712,7 +2712,7 @@ grpc_cc_library(     ],     external_deps = [         ""upb_lib"",-        ""upb_lib_descriptor"",+        ""upb_lib_descriptor_reflection"",","I'm puzzled by this: Can you elaborate a bit on why replacing `upb_lib_descriptor` with `upb_lib_descriptor_reflection` does anything when the conflict we're seeing is ""undeclared dependency on descriptor.upbdefs.h""? (it seems like the _reflection version of the library would also depdend on descriptor.upbdefs.h and descriptor.upbdefs.c)",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/24956,542595438,2020-12-14T18:06:39Z,src/core/ext/xds/xds_server_config_fetcher.cc,"@@ -0,0 +1,125 @@+//+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/xds/xds_client.h""+#include ""src/core/lib/surface/api_trace.h""+#include ""src/core/lib/surface/server.h""++namespace grpc_core {+namespace {++class XdsServerConfigFetcher : public grpc_server_config_fetcher {+ public:+  XdsServerConfigFetcher() {+    grpc_error* error = GRPC_ERROR_NONE;+    xds_client_ = XdsClient::GetOrCreate(&error);+    if (error != GRPC_ERROR_NONE) {+      gpr_log(GPR_ERROR, ""Failed to create xds client: %s"",+              grpc_error_string(error));+    }+  }++  void StartWatch(std::string listening_address,+                  std::unique_ptr<grpc_server_config_fetcher::WatcherInterface>+                      watcher) override {+    auto listener_watcher = absl::make_unique<ListenerWatcher>(watcher.get());+    auto* listener_watcher_ptr = listener_watcher.get();+    // TODO(yashykt): Get the resource name id from bootstrap+    xds_client_->WatchListenerData(+        absl::StrCat(""grpc/server?udpa.resource.listening_address="",+                     listening_address),+        std::move(listener_watcher));+    MutexLock lock(&mu_);+    auto& watcher_state = watchers_[watcher.get()];+    watcher_state.listening_address = listening_address;+    watcher_state.watcher = std::move(watcher);+    watcher_state.listener_watcher = listener_watcher_ptr;+  }++  void CancelWatch(+      grpc_server_config_fetcher::WatcherInterface* watcher) override {+    MutexLock lock(&mu_);+    auto it = watchers_.find(watcher);+    if (it != watchers_.end()) {+      // Cancel the watch on the listener before erasing+      xds_client_->CancelListenerDataWatch(it->second.listening_address,+                                           it->second.listener_watcher,+                                           false /* delay_unsubscription */);+    }+    watchers_.erase(watcher);+  }++  // Return the interested parties from the xds client so that it can be polled.+  grpc_pollset_set* interested_parties() override {+    return xds_client_->interested_parties();+  }++ private:+  class ListenerWatcher : public XdsClient::ListenerWatcherInterface {+   public:+    explicit ListenerWatcher(+        grpc_server_config_fetcher::WatcherInterface* server_config_watcher)+        : server_config_watcher_(server_config_watcher) {}++    void OnListenerChanged(XdsApi::LdsUpdate listener) override {+      // TODO(yashykt): Construct channel args according to received update+      server_config_watcher_->UpdateConfig(nullptr);+    }++    void OnError(grpc_error* error) override {+      gpr_log(GPR_ERROR, ""ListenerWatcher:%p XdsClient reports error: %s"", this,+              grpc_error_string(error));+      GRPC_ERROR_UNREF(error);+      // TODO(yashykt): We might want to bubble this error to the application.+    }++    void OnResourceDoesNotExist() override {","> So can we agree to go with that approach?Accept+close SGTM, but it isn't a complete solution. We also have to shut down existing connections. It seems like Go could do this semi-easily by making listener.Close() a noop and starting a graceful shutdown. In Java I could do a hack where a credential shuts down the connection, or we add a way to clone server configuration, or we add a large new Listener API I've been brainstorming for a few years. I'm not wild about those options. I imagine it would be awkward for C as well.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24956,542646506,2020-12-14T18:57:30Z,src/core/ext/xds/xds_server_config_fetcher.cc,"@@ -0,0 +1,125 @@+//+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/xds/xds_client.h""+#include ""src/core/lib/surface/api_trace.h""+#include ""src/core/lib/surface/server.h""++namespace grpc_core {+namespace {++class XdsServerConfigFetcher : public grpc_server_config_fetcher {+ public:+  XdsServerConfigFetcher() {+    grpc_error* error = GRPC_ERROR_NONE;+    xds_client_ = XdsClient::GetOrCreate(&error);+    if (error != GRPC_ERROR_NONE) {+      gpr_log(GPR_ERROR, ""Failed to create xds client: %s"",+              grpc_error_string(error));+    }+  }++  void StartWatch(std::string listening_address,+                  std::unique_ptr<grpc_server_config_fetcher::WatcherInterface>+                      watcher) override {+    auto listener_watcher = absl::make_unique<ListenerWatcher>(watcher.get());+    auto* listener_watcher_ptr = listener_watcher.get();+    // TODO(yashykt): Get the resource name id from bootstrap+    xds_client_->WatchListenerData(+        absl::StrCat(""grpc/server?udpa.resource.listening_address="",+                     listening_address),+        std::move(listener_watcher));+    MutexLock lock(&mu_);+    auto& watcher_state = watchers_[watcher.get()];+    watcher_state.listening_address = listening_address;+    watcher_state.watcher = std::move(watcher);+    watcher_state.listener_watcher = listener_watcher_ptr;+  }++  void CancelWatch(+      grpc_server_config_fetcher::WatcherInterface* watcher) override {+    MutexLock lock(&mu_);+    auto it = watchers_.find(watcher);+    if (it != watchers_.end()) {+      // Cancel the watch on the listener before erasing+      xds_client_->CancelListenerDataWatch(it->second.listening_address,+                                           it->second.listener_watcher,+                                           false /* delay_unsubscription */);+    }+    watchers_.erase(watcher);+  }++  // Return the interested parties from the xds client so that it can be polled.+  grpc_pollset_set* interested_parties() override {+    return xds_client_->interested_parties();+  }++ private:+  class ListenerWatcher : public XdsClient::ListenerWatcherInterface {+   public:+    explicit ListenerWatcher(+        grpc_server_config_fetcher::WatcherInterface* server_config_watcher)+        : server_config_watcher_(server_config_watcher) {}++    void OnListenerChanged(XdsApi::LdsUpdate listener) override {+      // TODO(yashykt): Construct channel args according to received update+      server_config_watcher_->UpdateConfig(nullptr);+    }++    void OnError(grpc_error* error) override {+      gpr_log(GPR_ERROR, ""ListenerWatcher:%p XdsClient reports error: %s"", this,+              grpc_error_string(error));+      GRPC_ERROR_UNREF(error);+      // TODO(yashykt): We might want to bubble this error to the application.+    }++    void OnResourceDoesNotExist() override {","Yeah, it is probably a little awkward for C-core too, but I think it's doable.  I suspect that we can have the `Chttp2Listener` keep a list of active connections and then send down a transport op that closes those connections.   @yashykt, what do you think?",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/24956,542759741,2020-12-14T20:41:25Z,src/core/ext/xds/xds_server_config_fetcher.cc,"@@ -0,0 +1,125 @@+//+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/xds/xds_client.h""+#include ""src/core/lib/surface/api_trace.h""+#include ""src/core/lib/surface/server.h""++namespace grpc_core {+namespace {++class XdsServerConfigFetcher : public grpc_server_config_fetcher {+ public:+  XdsServerConfigFetcher() {+    grpc_error* error = GRPC_ERROR_NONE;+    xds_client_ = XdsClient::GetOrCreate(&error);+    if (error != GRPC_ERROR_NONE) {+      gpr_log(GPR_ERROR, ""Failed to create xds client: %s"",+              grpc_error_string(error));+    }+  }++  void StartWatch(std::string listening_address,+                  std::unique_ptr<grpc_server_config_fetcher::WatcherInterface>+                      watcher) override {+    auto listener_watcher = absl::make_unique<ListenerWatcher>(watcher.get());+    auto* listener_watcher_ptr = listener_watcher.get();+    // TODO(yashykt): Get the resource name id from bootstrap+    xds_client_->WatchListenerData(+        absl::StrCat(""grpc/server?udpa.resource.listening_address="",+                     listening_address),+        std::move(listener_watcher));+    MutexLock lock(&mu_);+    auto& watcher_state = watchers_[watcher.get()];+    watcher_state.listening_address = listening_address;+    watcher_state.watcher = std::move(watcher);+    watcher_state.listener_watcher = listener_watcher_ptr;+  }++  void CancelWatch(+      grpc_server_config_fetcher::WatcherInterface* watcher) override {+    MutexLock lock(&mu_);+    auto it = watchers_.find(watcher);+    if (it != watchers_.end()) {+      // Cancel the watch on the listener before erasing+      xds_client_->CancelListenerDataWatch(it->second.listening_address,+                                           it->second.listener_watcher,+                                           false /* delay_unsubscription */);+    }+    watchers_.erase(watcher);+  }++  // Return the interested parties from the xds client so that it can be polled.+  grpc_pollset_set* interested_parties() override {+    return xds_client_->interested_parties();+  }++ private:+  class ListenerWatcher : public XdsClient::ListenerWatcherInterface {+   public:+    explicit ListenerWatcher(+        grpc_server_config_fetcher::WatcherInterface* server_config_watcher)+        : server_config_watcher_(server_config_watcher) {}++    void OnListenerChanged(XdsApi::LdsUpdate listener) override {+      // TODO(yashykt): Construct channel args according to received update+      server_config_watcher_->UpdateConfig(nullptr);+    }++    void OnError(grpc_error* error) override {+      gpr_log(GPR_ERROR, ""ListenerWatcher:%p XdsClient reports error: %s"", this,+              grpc_error_string(error));+      GRPC_ERROR_UNREF(error);+      // TODO(yashykt): We might want to bubble this error to the application.+    }++    void OnResourceDoesNotExist() override {","Actually, for Java I see a way we can fairly easily disable builder mutation after XdsServer creation, which would allow us to re-create the server. So XdsServer could just shut down the grpc server when the listener is deleted and re-create it when listener exists again. There'd be some non-trivial synchronization to implement Server interface, but it would be self-contained.So I think there are two options on the table for Java at the moment: credential hack and server shutdown+recreation.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/24988,542916383,2020-12-14T23:17:10Z,src/core/lib/security/credentials/external/url_external_account_credentials.cc,"@@ -121,7 +123,18 @@ void UrlExternalAccountCredentials::RetrieveSubjectToken(   grpc_httpcli_request request;   memset(&request, 0, sizeof(grpc_httpcli_request));   request.host = const_cast<char*>(url_.authority().c_str());-  request.http.path = gpr_strdup(url_.path().c_str());+  if (url_.query_parameter_pairs().empty()) {+    request.http.path = gpr_strdup(url_.path().c_str());+  } else {+    std::vector<std::string> query_vector;+    for (const URI::QueryParam& query_kv : url_.query_parameter_pairs()) {+      query_vector.emplace_back(+          absl::StrCat(query_kv.key, ""="", query_kv.value));","This will suffer from percent-encoding problems. See for example: https://github.com/grpc/grpc/blob/master/test/core/uri/uri_parser_test.cc#L151-L153. A percent-decoded query value, which is what the URI class provides, can legitimately contain the character '&'. Without re-encoding it to ""%26"", the resulting path here won't be correct.I'm not sure how many places we're reconstructing a URI string piece by piece, but it may be worth having a more generic way to re-encode URI components.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/24983,543018092,2020-12-15T03:28:28Z,tools/run_tests/xds_test_driver/framework/infrastructure/gcp/compute.py,"@@ -0,0 +1,336 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import enum+import logging+from typing import Optional, Dict, Any++import dataclasses+import googleapiclient.errors+from googleapiclient import discovery+import retrying++from framework.infrastructure import gcp++logger = logging.getLogger(__name__)+++class ComputeV1(gcp.api.GcpProjectApiResource):+    # todo(sergiitk): move someplace better+    _WAIT_FOR_BACKEND_SEC = 1200+    _WAIT_FOR_OPERATION_SEC = 1200+    _GCP_API_RETRIES = 5++    @dataclasses.dataclass(frozen=True)+    class GcpResource:+        name: str+        url: str++    @dataclasses.dataclass(frozen=True)+    class ZonalGcpResource(GcpResource):+        zone: str++    def __init__(self, api_manager: gcp.api.GcpApiManager, project: str):+        super().__init__(api_manager.compute('v1'), project)++    class HealthCheckProtocol(enum.Enum):+        TCP = enum.auto()++    class BackendServiceProtocol(enum.Enum):+        HTTP2 = enum.auto()+        GRPC = enum.auto()++    def create_health_check_tcp(self, name,+                                use_serving_port=False) -> GcpResource:+        health_check_settings = {}+        if use_serving_port:+            health_check_settings['portSpecification'] = 'USE_SERVING_PORT'++        return self._insert_resource(self.api.healthChecks(), {+            'name': name,+            'type': 'TCP',+            'tcpHealthCheck': health_check_settings,+        })++    def delete_health_check(self, name):+        self._delete_resource(self.api.healthChecks(), healthCheck=name)++    def create_backend_service_traffic_director(+            self,+            name: str,+            health_check: GcpResource,+            protocol: Optional[BackendServiceProtocol] = None) -> GcpResource:+        if not isinstance(protocol, self.BackendServiceProtocol):+            raise TypeError(f'Unexpected Backend Service protocol: {protocol}')+        return self._insert_resource(+            self.api.backendServices(),+            {+                'name': name,+                'loadBalancingScheme':+                    'INTERNAL_SELF_MANAGED',  # Traffic Director+                'healthChecks': [health_check.url],+                'protocol': protocol.name,+            })++    def get_backend_service_traffic_director(self, name: str) -> GcpResource:+        return self._get_resource(self.api.backendServices(),+                                  backendService=name)++    def patch_backend_service(self, backend_service, body, **kwargs):+        self._patch_resource(collection=self.api.backendServices(),+                             backendService=backend_service.name,+                             body=body,+                             **kwargs)++    def backend_service_add_backends(self, backend_service, backends):+        backend_list = [{+            'group': backend.url,+            'balancingMode': 'RATE',+            'maxRatePerEndpoint': 5+        } for backend in backends]++        self._patch_resource(collection=self.api.backendServices(),+                             body={'backends': backend_list},+                             backendService=backend_service.name)++    def backend_service_remove_all_backends(self, backend_service):+        self._patch_resource(collection=self.api.backendServices(),+                             body={'backends': []},+                             backendService=backend_service.name)++    def delete_backend_service(self, name):+        self._delete_resource(self.api.backendServices(), backendService=name)++    def create_url_map(+            self,+            name: str,+            matcher_name: str,+            src_hosts,+            dst_default_backend_service: GcpResource,+            dst_host_rule_match_backend_service: Optional[GcpResource] = None,+    ) -> GcpResource:+        if dst_host_rule_match_backend_service is None:+            dst_host_rule_match_backend_service = dst_default_backend_service+        return self._insert_resource(+            self.api.urlMaps(), {+                'name':+                    name,+                'defaultService':+                    dst_default_backend_service.url,+                'hostRules': [{+                    'hosts': src_hosts,+                    'pathMatcher': matcher_name,+                }],+                'pathMatchers': [{+                    'name': matcher_name,+                    'defaultService': dst_host_rule_match_backend_service.url,+                }],+            })++    def delete_url_map(self, name):+        self._delete_resource(self.api.urlMaps(), urlMap=name)++    def create_target_grpc_proxy(+            self,+            name: str,+            url_map: GcpResource,+    ) -> GcpResource:+        return self._insert_resource(self.api.targetGrpcProxies(), {+            'name': name,+            'url_map': url_map.url,+            'validate_for_proxyless': True,+        })++    def delete_target_grpc_proxy(self, name):+        self._delete_resource(self.api.targetGrpcProxies(),+                              targetGrpcProxy=name)++    def create_target_http_proxy(+            self,+            name: str,+            url_map: GcpResource,+    ) -> GcpResource:+        return self._insert_resource(self.api.targetHttpProxies(), {+            'name': name,+            'url_map': url_map.url,+        })++    def delete_target_http_proxy(self, name):+        self._delete_resource(self.api.targetHttpProxies(),+                              targetHttpProxy=name)++    def create_forwarding_rule(+            self,+            name: str,+            src_port: int,+            target_proxy: GcpResource,+            network_url: str,+    ) -> GcpResource:+        return self._insert_resource(+            self.api.globalForwardingRules(),+            {+                'name': name,+                'loadBalancingScheme':+                    'INTERNAL_SELF_MANAGED',  # Traffic Director+                'portRange': src_port,+                'IPAddress': '0.0.0.0',+                'network': network_url,+                'target': target_proxy.url,+            })++    def delete_forwarding_rule(self, name):+        self._delete_resource(self.api.globalForwardingRules(),+                              forwardingRule=name)++    @staticmethod+    def _network_endpoint_group_not_ready(neg):+        return not neg or neg.get('size', 0) == 0++    def wait_for_network_endpoint_group(self, name, zone):++        @retrying.retry(retry_on_result=self._network_endpoint_group_not_ready,+                        stop_max_delay=60 * 1000,+                        wait_fixed=2 * 1000)+        def _wait_for_network_endpoint_group_ready():+            try:+                neg = self.get_network_endpoint_group(name, zone)+                logger.debug(+                    'Waiting for endpoints: NEG %s in zone %s, '+                    'current count %s', neg['name'], zone, neg.get('size'))+            except googleapiclient.errors.HttpError as error:+                # noinspection PyProtectedMember+                reason = error._get_reason()+                logger.debug('Retrying NEG load, got %s, details %s',+                             error.resp.status, reason)+                raise+            return neg++        network_endpoint_group = _wait_for_network_endpoint_group_ready()+        # @todo(sergiitk): dataclass+        return self.ZonalGcpResource(network_endpoint_group['name'],+                                     network_endpoint_group['selfLink'], zone)++    def get_network_endpoint_group(self, name, zone):+        neg = self.api.networkEndpointGroups().get(project=self.project,+                                                   networkEndpointGroup=name,+                                                   zone=zone).execute()+        # @todo(sergiitk): dataclass+        return neg++    def wait_for_backends_healthy_status(+            self,+            backend_service,+            backends,+            timeout_sec=_WAIT_FOR_BACKEND_SEC,+            wait_sec=4,+    ):+        pending = set(backends)++        @retrying.retry(retry_on_result=lambda result: not result,+                        stop_max_delay=timeout_sec * 1000,+                        wait_fixed=wait_sec * 1000)","Was not clear to me that `wait_fixed` is in milliseconds, which made this look like a _really_ long wait. Replace 1000 with a named constant?",X
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/24983,543022849,2020-12-15T03:43:21Z,tools/run_tests/xds_test_driver/framework/rpc/__init__.py,"@@ -0,0 +1,95 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging+import re+from typing import Optional, ClassVar, Dict++import grpc+from google.protobuf import json_format+import google.protobuf.message++logger = logging.getLogger(__name__)++# Type aliases+Message = google.protobuf.message.Message+++class GrpcClientHelper:+    channel: grpc.Channel+    DEFAULT_CONNECTION_TIMEOUT_SEC = 60+    DEFAULT_WAIT_FOR_READY_SEC = 60++    def __init__(self, channel: grpc.Channel, stub_class: ClassVar):+        self.channel = channel+        self.stub = stub_class(channel)+        # For better logging+        self.service_name = re.sub('Stub$', '', self.stub.__class__.__name__)++    def call_unary_when_channel_ready(+            self,+            *,+            rpc: str,+            req: Message,+            wait_for_ready_sec: Optional[int] = DEFAULT_WAIT_FOR_READY_SEC,+            connection_timeout_sec: Optional[+                int] = DEFAULT_CONNECTION_TIMEOUT_SEC) -> Message:+        if wait_for_ready_sec is None:+            wait_for_ready_sec = self.DEFAULT_WAIT_FOR_READY_SEC+        if connection_timeout_sec is None:+            connection_timeout_sec = self.DEFAULT_CONNECTION_TIMEOUT_SEC++        timeout_sec = wait_for_ready_sec + connection_timeout_sec+        rpc_callable: grpc.UnaryUnaryMultiCallable = getattr(self.stub, rpc)++        call_kwargs = dict(wait_for_ready=True, timeout=timeout_sec)+        self._log_debug(rpc, req, call_kwargs)+        return rpc_callable(req, **call_kwargs)++    def _log_debug(self, rpc, req, call_kwargs):+        logger.debug('RPC %s.%s(request=%s(%r), %s)', self.service_name, rpc,+                     req.__class__.__name__, json_format.MessageToDict(req),+                     ', '.join({f'{k}={v}' for k, v in call_kwargs.items()}))+++class GrpcApp:","I'm not sure of python best practices here, so it might be fine as is, but it seems confusing to me that some classes are defined in the __init__.py file yet others merit (why?) their own file. ",X
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/24983,543023645,2020-12-15T03:45:57Z,tools/run_tests/xds_test_driver/framework/test_app/server_app.py,"@@ -0,0 +1,252 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import functools+import logging+from typing import Optional++from framework.infrastructure import k8s+import framework.rpc+from framework.rpc import grpc_channelz+from framework.test_app import base_runner++logger = logging.getLogger(__name__)++# Type aliases+ChannelzServiceClient = grpc_channelz.ChannelzServiceClient+++class XdsTestServer(framework.rpc.GrpcApp):","A top-level comment on most/each class would be really helpful in trying to understand what's going on here. For example, we have `XdsTestServer`s that are gRPC servers - this seems to not be that, rather a wrapper around a channel to the server? Or a wrapper around some-k8s-thing?",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/24983,543029268,2020-12-15T04:03:42Z,tools/run_tests/xds_test_driver/framework/xds_k8s_testcase.py,"@@ -0,0 +1,391 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import enum+import hashlib+import logging+from typing import Tuple++from absl import flags+from absl.testing import absltest++from framework import xds_flags+from framework import xds_k8s_flags+from framework.infrastructure import k8s+from framework.infrastructure import gcp+from framework.infrastructure import traffic_director+from framework.rpc import grpc_channelz+from framework.test_app import client_app+from framework.test_app import server_app++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_flags)+flags.adopt_module_key_flags(xds_k8s_flags)++# Type aliases+XdsTestServer = server_app.XdsTestServer+XdsTestClient = client_app.XdsTestClient+++class XdsKubernetesTestCase(absltest.TestCase):+    k8s_api_manager: k8s.KubernetesApiManager+    gcp_api_manager: gcp.api.GcpApiManager++    def __init__(self, *args, **kwargs):+        super().__init__(*args, **kwargs)++    @classmethod+    def setUpClass(cls):+        # GCP+        cls.project: str = xds_flags.PROJECT.value+        cls.network: str = xds_flags.NETWORK.value+        cls.gcp_service_account: str = xds_k8s_flags.GCP_SERVICE_ACCOUNT.value+        cls.td_bootstrap_image = xds_k8s_flags.TD_BOOTSTRAP_IMAGE.value++        # Base namespace+        # todo(sergiitk): generate for each test+        cls.namespace: str = xds_flags.NAMESPACE.value++        # Test server+        cls.server_image = xds_k8s_flags.SERVER_IMAGE.value+        cls.server_name = xds_flags.SERVER_NAME.value+        cls.server_port = xds_flags.SERVER_PORT.value+        cls.server_xds_host = xds_flags.SERVER_NAME.value+        cls.server_xds_port = xds_flags.SERVER_XDS_PORT.value++        # Test client+        cls.client_image = xds_k8s_flags.CLIENT_IMAGE.value+        cls.client_name = xds_flags.CLIENT_NAME.value+        cls.client_port = xds_flags.CLIENT_PORT.value+        cls.client_port_forwarding = xds_k8s_flags.CLIENT_PORT_FORWARDING.value++        # Resource managers+        cls.k8s_api_manager = k8s.KubernetesApiManager(+            xds_k8s_flags.KUBE_CONTEXT.value)+        cls.gcp_api_manager = gcp.api.GcpApiManager()++    def setUp(self):+        # todo(sergiitk): generate for each test+        self.server_namespace = self.namespace+        self.client_namespace = self.namespace++        # Init this in child class+        self.server_runner = None","Maybe a Java centric approach, but this would seem less error-prone to define `getServer/ClientRunner` methods instead, and have the child class override those methods.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/24983,543031049,2020-12-15T04:09:19Z,tools/run_tests/xds_test_driver/README.md,"@@ -0,0 +1,74 @@+# xDS Kubernetes Interop Tests++Proxyless Security Mesh Interop Tests executed on Kubernetes. Work in progress.","This still says work in progress. (which might be fine, but we should expand on that somewhat).In general, given the time constraints and the massive size of this PR, any review is likely to miss some items. What I think we need here in the readme are:1) A clear description of how this interoperates (or does not) with the existing `run_xds_tests.py`, including if there are any plans to deprecate that script and rely on the contents of `xds_test_driver/` exclusively (it _looks_ like much of the functionality is included in this PR, but I'm not clear on if this PR can run non-k8s tests at this point). Coming into this fresh, it would seem that run_xds_tests.py and the xds_test_driver/ directory are related, when currently they are separate things.2) A plan for how to move this from ""experimental/work in progress"" to code that is fully-reviewed and complete. Do we want to put this in a `./run_tests/xds_test_driver/alpha/` directory for now to indicate the current status? I'm not sure that's a great approach - what will github's UI show when the files are relocated in a subsequent PR? - so suggestions welcome.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/24983,543033621,2020-12-15T04:17:15Z,tools/run_tests/xds_test_driver/framework/rpc/__init__.py,"@@ -0,0 +1,95 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging+import re+from typing import Optional, ClassVar, Dict++import grpc+from google.protobuf import json_format+import google.protobuf.message++logger = logging.getLogger(__name__)++# Type aliases+Message = google.protobuf.message.Message+++class GrpcClientHelper:+    channel: grpc.Channel+    DEFAULT_CONNECTION_TIMEOUT_SEC = 60+    DEFAULT_WAIT_FOR_READY_SEC = 60++    def __init__(self, channel: grpc.Channel, stub_class: ClassVar):+        self.channel = channel+        self.stub = stub_class(channel)+        # For better logging+        self.service_name = re.sub('Stub$', '', self.stub.__class__.__name__)++    def call_unary_when_channel_ready(","I don't understand the intent of this helper method. It seems to just add wait_for_ready + a timeout to the call, but those are already args for grpc.UnaryUnaryMultiCallable. As far as I can tell this isn't actually doing anything to wait for the channel to be ready, e.g., querying the channel state - it's just adding two ""fake"" concepts (wait for ready sec, connection timeout sec) to make a deadline. (I realize some of this may be a port from the existing run_xds_tests.py).I think having the helper is fine, but if what I wrote above is correct, it seems misnamed?I'm also not sure what about this class justifies it being a class and extended elsewhere, versus just a helper method that can be invoked without inheritance, given that the main point of this class is just to set a deadline on an RPC?",X
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/24983,543034246,2020-12-15T04:19:14Z,tools/run_tests/xds_test_driver/framework/rpc/__init__.py,"@@ -0,0 +1,95 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging+import re+from typing import Optional, ClassVar, Dict++import grpc+from google.protobuf import json_format+import google.protobuf.message++logger = logging.getLogger(__name__)++# Type aliases+Message = google.protobuf.message.Message+++class GrpcClientHelper:+    channel: grpc.Channel+    DEFAULT_CONNECTION_TIMEOUT_SEC = 60+    DEFAULT_WAIT_FOR_READY_SEC = 60++    def __init__(self, channel: grpc.Channel, stub_class: ClassVar):+        self.channel = channel+        self.stub = stub_class(channel)+        # For better logging+        self.service_name = re.sub('Stub$', '', self.stub.__class__.__name__)",This looks unfortunate - how bad is the log output without this?,
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/24895,543507171,2020-12-15T16:44:27Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -115,11 +113,17 @@ namespace { // ChannelData definition // +class LoadBalancedCall;+ class ChannelData {  public:-  struct QueuedCall {+  struct ResolverQueuedCall {     grpc_call_element* elem;-    QueuedCall* next = nullptr;+    ResolverQueuedCall* next = nullptr;","Is the continued use of intrusive lists important for performance in this case, or is this ready to get moved toward an STL forward_list or something which would allow range-based for? I think that's a decision for a follow-on PR but wanted to put it out there for consideration so that it doesn't get forgotten.",