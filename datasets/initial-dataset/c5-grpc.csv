user_id,user_login,pull_request_url,comment_id,created_at,path,diff_hunk,content,code_smell
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25001,543531458,2020-12-15T17:14:36Z,WORKSPACE,"@@ -18,23 +18,15 @@ register_toolchains(     ""//third_party/toolchains/bazel_0.26.0_rbe_windows:cc-toolchain-x64_windows"", ) -load(""@bazel_toolchains//rules/exec_properties:exec_properties.bzl"", ""create_exec_properties_dict"", ""custom_exec_properties"", ""merge_dicts"")+load(""@bazel_toolchains//rules/exec_properties:exec_properties.bzl"", ""create_rbe_exec_properties_dict"", ""custom_exec_properties"")  custom_exec_properties(     name = ""grpc_custom_exec_properties"",     constants = {-        ""LARGE_MACHINE"": merge_dicts(-            create_exec_properties_dict(),-            # TODO(jtattermusch): specifying 'labels = {""abc"": ""xyz""}' in create_exec_properties_dict-            # is not possible without https://github.com/bazelbuild/bazel-toolchains/pull/748-            # and currently the toolchain we're using is too old for that. To be able to select worker-            # pools through labels, we use a workaround and populate the corresponding label values-            # manually (see create_exec_properties_dict logic for how labels get transformed)-            # Remove this workaround once we transition to a new-enough bazel toolchain.-            # The next line corresponds to 'labels = {""os"": ""ubuntu"", ""machine_size"": ""large""}'-            {-                ""label:os"": ""ubuntu"",-                ""label:machine_size"": ""large"",+        ""LARGE_MACHINE"": create_rbe_exec_properties_dict(","@gnossen  depending on what the bazel distribtests say, we might want to reintroduce parts of the workaround  that was used previously (conditionally based on the bazel version used?).It would be good to get some data on what bazel versions we can continue to support.",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/24895,543541670,2020-12-15T17:28:44Z,src/core/ext/filters/client_channel/subchannel.cc,"@@ -142,8 +133,7 @@ size_t ConnectedSubchannel::GetInitialCallSizeEstimate( RefCountedPtr<SubchannelCall> SubchannelCall::Create(Args args,                                                      grpc_error** error) {   const size_t allocation_size =-      args.connected_subchannel->GetInitialCallSizeEstimate(-          args.parent_data_size);+      args.connected_subchannel->GetInitialCallSizeEstimate();",I know that the actual parent data size allocation is now done in a different arena allocation; does that affect the arena allocation efficiency or memory usage at all?,
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/24988,543612341,2020-12-15T19:06:47Z,src/core/lib/security/credentials/external/url_external_account_credentials.cc,"@@ -120,8 +114,19 @@ void UrlExternalAccountCredentials::RetrieveSubjectToken(   cb_ = cb;   grpc_httpcli_request request;   memset(&request, 0, sizeof(grpc_httpcli_request));-  request.host = const_cast<char*>(url_.authority().c_str());-  request.http.path = gpr_strdup(url_.path().c_str());+  absl::StatusOr<URI> tmp_url = URI::Parse(url_);+  if (!tmp_url.ok()) {+    FinishRetrieveSubjectToken(+        """", GRPC_ERROR_CREATE_FROM_COPIED_STRING(+                absl::StrFormat(""Invalid credential source url."").c_str()));+    return;+  }+  URI url_object = *tmp_url;+  request.host = const_cast<char*>(url_object.authority().c_str());",I believe this creates an unnecessary copy of the URI. You can treat a StatusOr similar to how you would a smart pointer. `request.host = const_cast<char*>(tmp_url->authority().c_str());`See https://abseil.io/tips/181.,X
869251,renkelvin,https://api.github.com/repos/grpc/grpc/pulls/24988,543662455,2020-12-15T20:25:58Z,src/core/lib/security/credentials/external/url_external_account_credentials.cc,"@@ -120,8 +114,19 @@ void UrlExternalAccountCredentials::RetrieveSubjectToken(   cb_ = cb;   grpc_httpcli_request request;   memset(&request, 0, sizeof(grpc_httpcli_request));-  request.host = const_cast<char*>(url_.authority().c_str());-  request.http.path = gpr_strdup(url_.path().c_str());+  absl::StatusOr<URI> tmp_url = URI::Parse(url_);+  if (!tmp_url.ok()) {+    FinishRetrieveSubjectToken(+        """", GRPC_ERROR_CREATE_FROM_COPIED_STRING(+                absl::StrFormat(""Invalid credential source url."").c_str()));+    return;+  }+  URI url_object = *tmp_url;+  request.host = const_cast<char*>(url_object.authority().c_str());+  std::vector<std::string> v =+      absl::StrSplit(url_.c_str(), absl::MaxSplits('/', 3));","For url creds, it's assumed that the url has a static format of `<scheme>://<authority>/<path>`. It's very unlikely to change but it seems too big an assumption given the cases you provide.I'm trying to use the class `URI` to parse the url, while there might be some issue. Please see my comments in the other thread.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/24993,543664109,2020-12-15T20:28:49Z,src/python/grpcio/grpc/_runtime_protos.py,"@@ -15,6 +15,48 @@ import sys  _REQUIRED_SYMBOLS = (""_protos"", ""_services"", ""_protos_and_services"")+_MINIMUM_VERSION = (3, 5, 0)+++def _has_runtime_proto_symbols(mod):+    return all(hasattr(mod, sym) for sym in _REQUIRED_SYMBOLS)+++def _is_grpc_tools_importable():+    try:+        import grpc_tools  # pylint: disable=unused-import+        return True+    except ImportError as e:+        # NOTE: It's possible that we're encountering a transitive ImportError, so+        # we check for that and re-raise if so.+        if ""grpc_tools"" not in e.args[0]:+            raise+        return False+++def _call_with_lazy_import(fn_name, version_fn, uninstalled_fn, protobuf_path):+    """"""Calls one of the three functions, lazily importing grpc_tools.++    Args:+      fn_name: The name of the function to import from grpc_tools.protoc.+      version_fn: A function to call in case the Python version is insufficient.+      uninstalled_fn: A function to call in case grpc_tools is not installed.+      protobuf_path: The path to import.++    Returns:+      The appropriate module object.+    """"""+    if sys.version_info < _MINIMUM_VERSION:+        return version_fn(protobuf_path)+    else:+        if not _is_grpc_tools_importable():+            return uninstalled_fn(protobuf_path)+        import grpc_tools.protoc+        if _has_runtime_proto_symbols(grpc_tools.protoc):+            fn = getattr(grpc_tools.protoc, fn_name)+            return fn(protobuf_path)+        else:+            return uninstalled_fn(protobuf_path)","No real reason. The argument is ignored, but I left it there for consistency of call pattern with the actual function. Is there some advantage to *not* passing this argument I'm not seeing? All I can think of is a small speed boost, but this is an error case in an import, so that argument doesn't seem particularly compelling to me.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/24993,543668355,2020-12-15T20:36:06Z,src/python/grpcio/grpc/_runtime_protos.py,"@@ -15,6 +15,48 @@ import sys  _REQUIRED_SYMBOLS = (""_protos"", ""_services"", ""_protos_and_services"")+_MINIMUM_VERSION = (3, 5, 0)+++def _has_runtime_proto_symbols(mod):+    return all(hasattr(mod, sym) for sym in _REQUIRED_SYMBOLS)+++def _is_grpc_tools_importable():+    try:+        import grpc_tools  # pylint: disable=unused-import+        return True+    except ImportError as e:+        # NOTE: It's possible that we're encountering a transitive ImportError, so+        # we check for that and re-raise if so.+        if ""grpc_tools"" not in e.args[0]:+            raise+        return False+++def _call_with_lazy_import(fn_name, version_fn, uninstalled_fn, protobuf_path):+    """"""Calls one of the three functions, lazily importing grpc_tools.++    Args:+      fn_name: The name of the function to import from grpc_tools.protoc.+      version_fn: A function to call in case the Python version is insufficient.+      uninstalled_fn: A function to call in case grpc_tools is not installed.+      protobuf_path: The path to import.++    Returns:+      The appropriate module object.+    """"""+    if sys.version_info < _MINIMUM_VERSION:+        return version_fn(protobuf_path)+    else:+        if not _is_grpc_tools_importable():+            return uninstalled_fn(protobuf_path)+        import grpc_tools.protoc+        if _has_runtime_proto_symbols(grpc_tools.protoc):+            fn = getattr(grpc_tools.protoc, fn_name)+            return fn(protobuf_path)+        else:+            return uninstalled_fn(protobuf_path)","No, isn't fine.It wasn't used, so I thought your intention was to log it, like ""Error while generating for file [...]:"". Otherwise, we could consider make those exception raising functions taking `unused_args`, `unused_kwargs`.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/24983,543733912,2020-12-15T22:29:29Z,tools/run_tests/xds_test_driver/framework/test_app/base_runner.py,"@@ -0,0 +1,245 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import contextlib+import logging+import pathlib+from typing import Optional++import mako.template+import yaml++from framework.infrastructure import k8s++logger = logging.getLogger(__name__)+++class RunnerError(Exception):+    """"""Error running app""""""+++TEMPLATE_DIR = '../../kubernetes-manifests'+++class KubernetesBaseRunner:++    def __init__(self,+                 k8s_namespace,+                 namespace_template=None,+                 reuse_namespace=False):+        # Kubernetes namespaced resources manager+        self.k8s_namespace: k8s.KubernetesNamespace = k8s_namespace+        self.reuse_namespace = reuse_namespace+        self.namespace_template = namespace_template or 'namespace.yaml'++        # Mutable state+        self.namespace: Optional[k8s.V1Namespace] = None++    def run(self, **kwargs):+        if self.reuse_namespace:+            self.namespace = self._reuse_namespace()+        if not self.namespace:+            self.namespace = self._create_namespace(+                self.namespace_template, namespace_name=self.k8s_namespace.name)++    def cleanup(self, *, force=False):+        if (self.namespace and not self.reuse_namespace) or force:+            self._delete_namespace()+            self.namespace = None++    @staticmethod+    def _render_template(template_file, **kwargs):+        template = mako.template.Template(filename=str(template_file))+        return template.render(**kwargs)++    @staticmethod+    def _manifests_from_yaml_file(yaml_file):+        # Parse yaml","You're right - all these comments are from times when all this was one large function. Now that they're refactored into self-containing units, they make little sense.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/24983,543752033,2020-12-15T23:05:30Z,tools/run_tests/xds_test_driver/framework/infrastructure/gcp/compute.py,"@@ -0,0 +1,336 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import enum+import logging+from typing import Optional, Dict, Any++import dataclasses+import googleapiclient.errors+from googleapiclient import discovery+import retrying++from framework.infrastructure import gcp++logger = logging.getLogger(__name__)+++class ComputeV1(gcp.api.GcpProjectApiResource):+    # todo(sergiitk): move someplace better+    _WAIT_FOR_BACKEND_SEC = 1200+    _WAIT_FOR_OPERATION_SEC = 1200+    _GCP_API_RETRIES = 5++    @dataclasses.dataclass(frozen=True)+    class GcpResource:+        name: str+        url: str++    @dataclasses.dataclass(frozen=True)+    class ZonalGcpResource(GcpResource):+        zone: str++    def __init__(self, api_manager: gcp.api.GcpApiManager, project: str):+        super().__init__(api_manager.compute('v1'), project)++    class HealthCheckProtocol(enum.Enum):+        TCP = enum.auto()++    class BackendServiceProtocol(enum.Enum):+        HTTP2 = enum.auto()+        GRPC = enum.auto()++    def create_health_check_tcp(self, name,+                                use_serving_port=False) -> GcpResource:+        health_check_settings = {}+        if use_serving_port:+            health_check_settings['portSpecification'] = 'USE_SERVING_PORT'++        return self._insert_resource(self.api.healthChecks(), {+            'name': name,+            'type': 'TCP',+            'tcpHealthCheck': health_check_settings,+        })++    def delete_health_check(self, name):+        self._delete_resource(self.api.healthChecks(), healthCheck=name)++    def create_backend_service_traffic_director(+            self,+            name: str,+            health_check: GcpResource,+            protocol: Optional[BackendServiceProtocol] = None) -> GcpResource:+        if not isinstance(protocol, self.BackendServiceProtocol):+            raise TypeError(f'Unexpected Backend Service protocol: {protocol}')+        return self._insert_resource(+            self.api.backendServices(),+            {+                'name': name,+                'loadBalancingScheme':+                    'INTERNAL_SELF_MANAGED',  # Traffic Director+                'healthChecks': [health_check.url],+                'protocol': protocol.name,+            })++    def get_backend_service_traffic_director(self, name: str) -> GcpResource:+        return self._get_resource(self.api.backendServices(),+                                  backendService=name)++    def patch_backend_service(self, backend_service, body, **kwargs):+        self._patch_resource(collection=self.api.backendServices(),+                             backendService=backend_service.name,+                             body=body,+                             **kwargs)++    def backend_service_add_backends(self, backend_service, backends):+        backend_list = [{+            'group': backend.url,+            'balancingMode': 'RATE',+            'maxRatePerEndpoint': 5+        } for backend in backends]++        self._patch_resource(collection=self.api.backendServices(),+                             body={'backends': backend_list},+                             backendService=backend_service.name)++    def backend_service_remove_all_backends(self, backend_service):+        self._patch_resource(collection=self.api.backendServices(),+                             body={'backends': []},+                             backendService=backend_service.name)++    def delete_backend_service(self, name):+        self._delete_resource(self.api.backendServices(), backendService=name)++    def create_url_map(+            self,+            name: str,+            matcher_name: str,+            src_hosts,+            dst_default_backend_service: GcpResource,+            dst_host_rule_match_backend_service: Optional[GcpResource] = None,+    ) -> GcpResource:+        if dst_host_rule_match_backend_service is None:+            dst_host_rule_match_backend_service = dst_default_backend_service+        return self._insert_resource(+            self.api.urlMaps(), {+                'name':+                    name,+                'defaultService':+                    dst_default_backend_service.url,+                'hostRules': [{+                    'hosts': src_hosts,+                    'pathMatcher': matcher_name,+                }],+                'pathMatchers': [{+                    'name': matcher_name,+                    'defaultService': dst_host_rule_match_backend_service.url,+                }],+            })++    def delete_url_map(self, name):+        self._delete_resource(self.api.urlMaps(), urlMap=name)++    def create_target_grpc_proxy(+            self,+            name: str,+            url_map: GcpResource,+    ) -> GcpResource:+        return self._insert_resource(self.api.targetGrpcProxies(), {+            'name': name,+            'url_map': url_map.url,+            'validate_for_proxyless': True,+        })++    def delete_target_grpc_proxy(self, name):+        self._delete_resource(self.api.targetGrpcProxies(),+                              targetGrpcProxy=name)++    def create_target_http_proxy(+            self,+            name: str,+            url_map: GcpResource,+    ) -> GcpResource:+        return self._insert_resource(self.api.targetHttpProxies(), {+            'name': name,+            'url_map': url_map.url,+        })++    def delete_target_http_proxy(self, name):+        self._delete_resource(self.api.targetHttpProxies(),+                              targetHttpProxy=name)++    def create_forwarding_rule(+            self,+            name: str,+            src_port: int,+            target_proxy: GcpResource,+            network_url: str,+    ) -> GcpResource:+        return self._insert_resource(+            self.api.globalForwardingRules(),+            {+                'name': name,+                'loadBalancingScheme':+                    'INTERNAL_SELF_MANAGED',  # Traffic Director+                'portRange': src_port,+                'IPAddress': '0.0.0.0',+                'network': network_url,+                'target': target_proxy.url,+            })++    def delete_forwarding_rule(self, name):+        self._delete_resource(self.api.globalForwardingRules(),+                              forwardingRule=name)++    @staticmethod+    def _network_endpoint_group_not_ready(neg):+        return not neg or neg.get('size', 0) == 0++    def wait_for_network_endpoint_group(self, name, zone):++        @retrying.retry(retry_on_result=self._network_endpoint_group_not_ready,+                        stop_max_delay=60 * 1000,+                        wait_fixed=2 * 1000)+        def _wait_for_network_endpoint_group_ready():+            try:+                neg = self.get_network_endpoint_group(name, zone)+                logger.debug(+                    'Waiting for endpoints: NEG %s in zone %s, '+                    'current count %s', neg['name'], zone, neg.get('size'))+            except googleapiclient.errors.HttpError as error:+                # noinspection PyProtectedMember+                reason = error._get_reason()+                logger.debug('Retrying NEG load, got %s, details %s',+                             error.resp.status, reason)+                raise+            return neg++        network_endpoint_group = _wait_for_network_endpoint_group_ready()+        # @todo(sergiitk): dataclass+        return self.ZonalGcpResource(network_endpoint_group['name'],+                                     network_endpoint_group['selfLink'], zone)++    def get_network_endpoint_group(self, name, zone):+        neg = self.api.networkEndpointGroups().get(project=self.project,+                                                   networkEndpointGroup=name,+                                                   zone=zone).execute()+        # @todo(sergiitk): dataclass+        return neg++    def wait_for_backends_healthy_status(+            self,+            backend_service,+            backends,+            timeout_sec=_WAIT_FOR_BACKEND_SEC,+            wait_sec=4,+    ):+        pending = set(backends)++        @retrying.retry(retry_on_result=lambda result: not result,","The reason is I started with `retyring`, then wasn't happy with how it difficult it made provide timeout/wait variables. I don't like having all these nested methods.Then at the very last moment I found out that `tenacity` is a thing and it superseded `retrying`, which is now is abandoned.> It originates from a fork of retrying which is sadly no longer maintained. Tenacity isn’t api compatible with retrying but adds significant new functionality and fixes a number of longstanding bugs.> — https://tenacity.readthedocs.io/en/latest/index.htmlWould it be ok with you if I replaced retrying with tenacity everywhere in the following iteration?",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/24983,543753998,2020-12-15T23:09:41Z,tools/run_tests/xds_test_driver/framework/rpc/grpc_testing.py,"@@ -0,0 +1,47 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+from typing import Optional++import grpc++import framework.rpc+from src.proto.grpc.testing import test_pb2_grpc+from src.proto.grpc.testing import messages_pb2++# Type aliases+LoadBalancerStatsRequest = messages_pb2.LoadBalancerStatsRequest+LoadBalancerStatsResponse = messages_pb2.LoadBalancerStatsResponse+++class LoadBalancerStatsServiceClient(framework.rpc.GrpcClientHelper):","Heh, I've struggled with this name. This is supposed to contain helpers for GRPC services defined here in `grpc.testing` [test.proto](https://github.com/grpc/grpc/blob/master/src/proto/grpc/testing/test.proto). So many testing/tests, I'm not sure what's a good name for this. Would you recommend one? Or maybe should I make it more clear by adding file docstring?",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/24983,543757252,2020-12-15T23:16:59Z,tools/run_tests/xds_test_driver/framework/rpc/__init__.py,"@@ -0,0 +1,95 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging+import re+from typing import Optional, ClassVar, Dict++import grpc+from google.protobuf import json_format+import google.protobuf.message++logger = logging.getLogger(__name__)++# Type aliases+Message = google.protobuf.message.Message+++class GrpcClientHelper:+    channel: grpc.Channel+    DEFAULT_CONNECTION_TIMEOUT_SEC = 60+    DEFAULT_WAIT_FOR_READY_SEC = 60++    def __init__(self, channel: grpc.Channel, stub_class: ClassVar):+        self.channel = channel+        self.stub = stub_class(channel)+        # For better logging+        self.service_name = re.sub('Stub$', '', self.stub.__class__.__name__)++    def call_unary_when_channel_ready(+            self,+            *,+            rpc: str,+            req: Message,+            wait_for_ready_sec: Optional[int] = DEFAULT_WAIT_FOR_READY_SEC,+            connection_timeout_sec: Optional[+                int] = DEFAULT_CONNECTION_TIMEOUT_SEC) -> Message:+        if wait_for_ready_sec is None:+            wait_for_ready_sec = self.DEFAULT_WAIT_FOR_READY_SEC+        if connection_timeout_sec is None:+            connection_timeout_sec = self.DEFAULT_CONNECTION_TIMEOUT_SEC++        timeout_sec = wait_for_ready_sec + connection_timeout_sec+        rpc_callable: grpc.UnaryUnaryMultiCallable = getattr(self.stub, rpc)++        call_kwargs = dict(wait_for_ready=True, timeout=timeout_sec)+        self._log_debug(rpc, req, call_kwargs)+        return rpc_callable(req, **call_kwargs)++    def _log_debug(self, rpc, req, call_kwargs):+        logger.debug('RPC %s.%s(request=%s(%r), %s)', self.service_name, rpc,+                     req.__class__.__name__, json_format.MessageToDict(req),+                     ', '.join({f'{k}={v}' for k, v in call_kwargs.items()}))+++class GrpcApp:","Was on a fence here. As you've noticed, in other places with base classes, I've moved base classes to their own files, see `infrastructure/test_app/base_runner.py`, `infrastructure/gcp/api.py`. My main reason for this is absl.logging only logs filename, without a module. This made it confusing, because all `__init__.py` were logged with the same file name. Maybe not the best argument.Where would you prefer base classes? Init or own files?",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/24983,543760797,2020-12-15T23:24:48Z,tools/run_tests/xds_test_driver/framework/test_app/server_app.py,"@@ -0,0 +1,252 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import functools+import logging+from typing import Optional++from framework.infrastructure import k8s+import framework.rpc+from framework.rpc import grpc_channelz+from framework.test_app import base_runner++logger = logging.getLogger(__name__)++# Type aliases+ChannelzServiceClient = grpc_channelz.ChannelzServiceClient+++class XdsTestServer(framework.rpc.GrpcApp):","This is absolutely, 100%, needs to be changed. `XdsTestServer` supposed to represent services implemented in [XdsTestServer](https://github.com/grpc/grpc-java/blob/master/interop-testing/src/main/java/io/grpc/testing/integration/XdsTestServer.java) component of the test app. Same goes for the client. This is not coupled to their k8s-runners and must be separated to different files.This just an artifact of early development, where I wasn't yet sure which components I'll have at the end, and what's a good way to structure them.However, there's only so much time to refactor/regroup things, so I had to postpone this restructuring.@ericgribkoff  Would it be ok with you if I made this change in the next iteration? Or would you like to see it in this PR?",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/24983,543763889,2020-12-15T23:32:04Z,tools/run_tests/xds_test_driver/framework/rpc/__init__.py,"@@ -0,0 +1,95 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging+import re+from typing import Optional, ClassVar, Dict++import grpc+from google.protobuf import json_format+import google.protobuf.message++logger = logging.getLogger(__name__)++# Type aliases+Message = google.protobuf.message.Message+++class GrpcClientHelper:+    channel: grpc.Channel+    DEFAULT_CONNECTION_TIMEOUT_SEC = 60+    DEFAULT_WAIT_FOR_READY_SEC = 60++    def __init__(self, channel: grpc.Channel, stub_class: ClassVar):+        self.channel = channel+        self.stub = stub_class(channel)+        # For better logging+        self.service_name = re.sub('Stub$', '', self.stub.__class__.__name__)","Not bad at all - just with this small change they RPC logs look like method calls - much nicer:`RPC LoadBalancerStatsService.GetClientStats(request=LoadBalancerStatsRequest({'numRpcs': 100, 'timeoutSec': 1200}), timeout=1260, wait_for_ready=True)`I wish there was a better way to get service name from the stub. Did I miss anything abvious?",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/24983,543785526,2020-12-16T00:25:23Z,tools/run_tests/xds_test_driver/README.md,"@@ -0,0 +1,74 @@+# xDS Kubernetes Interop Tests++Proxyless Security Mesh Interop Tests executed on Kubernetes. Work in progress.","That makes sense. Happy to discuss over GVC (I know it's late for your timezone/early for me right now, so not sure if that will line up). I think putting a note here in the readme that (ideally) lists at least a couple of the concrete things needed (kind of like a high-level set of TODOs) before the ""work in progress"" note here can be removed would address the first point from my earlier comment. If there's not an easy to produce list (maybe just ""address all inline TODOs?""), just something to the effect of what you said in your reply here would be sufficient too. I'll note that my main concern is not that you will need to make changes to this breaking other people's work, but rather that you may inevitably get drawn into other high-priority work and this will remain as-is until something breaks, at which point the structure here will no longer be fresh in anyone's mind.Regarding distinguishing between this and run_xds_tests.py - how about just renaming this directory to `xds_k8s_test_driver`? Then we can at least easily point to the difference between this PR and the existing run_xds_tests.py setups.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/24983,543786046,2020-12-16T00:26:48Z,tools/run_tests/xds_test_driver/framework/infrastructure/gcp/compute.py,"@@ -0,0 +1,336 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import enum+import logging+from typing import Optional, Dict, Any++import dataclasses+import googleapiclient.errors+from googleapiclient import discovery+import retrying++from framework.infrastructure import gcp++logger = logging.getLogger(__name__)+++class ComputeV1(gcp.api.GcpProjectApiResource):+    # todo(sergiitk): move someplace better+    _WAIT_FOR_BACKEND_SEC = 1200+    _WAIT_FOR_OPERATION_SEC = 1200+    _GCP_API_RETRIES = 5++    @dataclasses.dataclass(frozen=True)+    class GcpResource:+        name: str+        url: str++    @dataclasses.dataclass(frozen=True)+    class ZonalGcpResource(GcpResource):+        zone: str++    def __init__(self, api_manager: gcp.api.GcpApiManager, project: str):+        super().__init__(api_manager.compute('v1'), project)++    class HealthCheckProtocol(enum.Enum):+        TCP = enum.auto()++    class BackendServiceProtocol(enum.Enum):+        HTTP2 = enum.auto()+        GRPC = enum.auto()++    def create_health_check_tcp(self, name,+                                use_serving_port=False) -> GcpResource:+        health_check_settings = {}+        if use_serving_port:+            health_check_settings['portSpecification'] = 'USE_SERVING_PORT'++        return self._insert_resource(self.api.healthChecks(), {+            'name': name,+            'type': 'TCP',+            'tcpHealthCheck': health_check_settings,+        })++    def delete_health_check(self, name):+        self._delete_resource(self.api.healthChecks(), healthCheck=name)++    def create_backend_service_traffic_director(+            self,+            name: str,+            health_check: GcpResource,+            protocol: Optional[BackendServiceProtocol] = None) -> GcpResource:+        if not isinstance(protocol, self.BackendServiceProtocol):+            raise TypeError(f'Unexpected Backend Service protocol: {protocol}')+        return self._insert_resource(+            self.api.backendServices(),+            {+                'name': name,+                'loadBalancingScheme':+                    'INTERNAL_SELF_MANAGED',  # Traffic Director+                'healthChecks': [health_check.url],+                'protocol': protocol.name,+            })++    def get_backend_service_traffic_director(self, name: str) -> GcpResource:+        return self._get_resource(self.api.backendServices(),+                                  backendService=name)++    def patch_backend_service(self, backend_service, body, **kwargs):+        self._patch_resource(collection=self.api.backendServices(),+                             backendService=backend_service.name,+                             body=body,+                             **kwargs)++    def backend_service_add_backends(self, backend_service, backends):+        backend_list = [{+            'group': backend.url,+            'balancingMode': 'RATE',+            'maxRatePerEndpoint': 5+        } for backend in backends]++        self._patch_resource(collection=self.api.backendServices(),+                             body={'backends': backend_list},+                             backendService=backend_service.name)++    def backend_service_remove_all_backends(self, backend_service):+        self._patch_resource(collection=self.api.backendServices(),+                             body={'backends': []},+                             backendService=backend_service.name)++    def delete_backend_service(self, name):+        self._delete_resource(self.api.backendServices(), backendService=name)++    def create_url_map(+            self,+            name: str,+            matcher_name: str,+            src_hosts,+            dst_default_backend_service: GcpResource,+            dst_host_rule_match_backend_service: Optional[GcpResource] = None,+    ) -> GcpResource:+        if dst_host_rule_match_backend_service is None:+            dst_host_rule_match_backend_service = dst_default_backend_service+        return self._insert_resource(+            self.api.urlMaps(), {+                'name':+                    name,+                'defaultService':+                    dst_default_backend_service.url,+                'hostRules': [{+                    'hosts': src_hosts,+                    'pathMatcher': matcher_name,+                }],+                'pathMatchers': [{+                    'name': matcher_name,+                    'defaultService': dst_host_rule_match_backend_service.url,+                }],+            })++    def delete_url_map(self, name):+        self._delete_resource(self.api.urlMaps(), urlMap=name)++    def create_target_grpc_proxy(+            self,+            name: str,+            url_map: GcpResource,+    ) -> GcpResource:+        return self._insert_resource(self.api.targetGrpcProxies(), {+            'name': name,+            'url_map': url_map.url,+            'validate_for_proxyless': True,+        })++    def delete_target_grpc_proxy(self, name):+        self._delete_resource(self.api.targetGrpcProxies(),+                              targetGrpcProxy=name)++    def create_target_http_proxy(+            self,+            name: str,+            url_map: GcpResource,+    ) -> GcpResource:+        return self._insert_resource(self.api.targetHttpProxies(), {+            'name': name,+            'url_map': url_map.url,+        })++    def delete_target_http_proxy(self, name):+        self._delete_resource(self.api.targetHttpProxies(),+                              targetHttpProxy=name)++    def create_forwarding_rule(+            self,+            name: str,+            src_port: int,+            target_proxy: GcpResource,+            network_url: str,+    ) -> GcpResource:+        return self._insert_resource(+            self.api.globalForwardingRules(),+            {+                'name': name,+                'loadBalancingScheme':+                    'INTERNAL_SELF_MANAGED',  # Traffic Director+                'portRange': src_port,+                'IPAddress': '0.0.0.0',+                'network': network_url,+                'target': target_proxy.url,+            })++    def delete_forwarding_rule(self, name):+        self._delete_resource(self.api.globalForwardingRules(),+                              forwardingRule=name)++    @staticmethod+    def _network_endpoint_group_not_ready(neg):+        return not neg or neg.get('size', 0) == 0++    def wait_for_network_endpoint_group(self, name, zone):++        @retrying.retry(retry_on_result=self._network_endpoint_group_not_ready,+                        stop_max_delay=60 * 1000,+                        wait_fixed=2 * 1000)+        def _wait_for_network_endpoint_group_ready():+            try:+                neg = self.get_network_endpoint_group(name, zone)+                logger.debug(+                    'Waiting for endpoints: NEG %s in zone %s, '+                    'current count %s', neg['name'], zone, neg.get('size'))+            except googleapiclient.errors.HttpError as error:+                # noinspection PyProtectedMember+                reason = error._get_reason()+                logger.debug('Retrying NEG load, got %s, details %s',+                             error.resp.status, reason)+                raise+            return neg++        network_endpoint_group = _wait_for_network_endpoint_group_ready()+        # @todo(sergiitk): dataclass+        return self.ZonalGcpResource(network_endpoint_group['name'],+                                     network_endpoint_group['selfLink'], zone)++    def get_network_endpoint_group(self, name, zone):+        neg = self.api.networkEndpointGroups().get(project=self.project,+                                                   networkEndpointGroup=name,+                                                   zone=zone).execute()+        # @todo(sergiitk): dataclass+        return neg++    def wait_for_backends_healthy_status(+            self,+            backend_service,+            backends,+            timeout_sec=_WAIT_FOR_BACKEND_SEC,+            wait_sec=4,+    ):+        pending = set(backends)++        @retrying.retry(retry_on_result=lambda result: not result,","That makes sense. Maybe a top-level TODO in the readme about replacing retrying with tenacity? But yeah, this can wait - I hadn't heard of either library before, and in looking at tenacity's docs, I didn't see immediately that it was a fork until I realized the usage examples for both libraries were remarkably similar.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/24983,543787091,2020-12-16T00:29:20Z,tools/run_tests/xds_test_driver/framework/rpc/__init__.py,"@@ -0,0 +1,95 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging+import re+from typing import Optional, ClassVar, Dict++import grpc+from google.protobuf import json_format+import google.protobuf.message++logger = logging.getLogger(__name__)++# Type aliases+Message = google.protobuf.message.Message+++class GrpcClientHelper:+    channel: grpc.Channel+    DEFAULT_CONNECTION_TIMEOUT_SEC = 60+    DEFAULT_WAIT_FOR_READY_SEC = 60++    def __init__(self, channel: grpc.Channel, stub_class: ClassVar):+        self.channel = channel+        self.stub = stub_class(channel)+        # For better logging+        self.service_name = re.sub('Stub$', '', self.stub.__class__.__name__)","Nothing obvious. I am suspecting that maybe this class shouldn't be needing to extract the name from the stub, but as far as doing that goes, this looks fine.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/24983,543792926,2020-12-16T00:44:28Z,tools/run_tests/xds_test_driver/framework/rpc/__init__.py,"@@ -0,0 +1,95 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging+import re+from typing import Optional, ClassVar, Dict++import grpc+from google.protobuf import json_format+import google.protobuf.message++logger = logging.getLogger(__name__)++# Type aliases+Message = google.protobuf.message.Message+++class GrpcClientHelper:+    channel: grpc.Channel+    DEFAULT_CONNECTION_TIMEOUT_SEC = 60+    DEFAULT_WAIT_FOR_READY_SEC = 60++    def __init__(self, channel: grpc.Channel, stub_class: ClassVar):+        self.channel = channel+        self.stub = stub_class(channel)+        # For better logging+        self.service_name = re.sub('Stub$', '', self.stub.__class__.__name__)++    def call_unary_when_channel_ready(","Sure, but also fine with leaving structure as-is (I am far from authoritative as far as Python style/structure goes) and just renaming to `call_unary_with_deadline`.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/24996,544532765,2020-12-16T18:37:42Z,test/cpp/interop/xds_interop_client.cc,"@@ -345,6 +347,18 @@ class TestClient {     std::unique_ptr<ClientAsyncResponseReader<SimpleResponse>>         simple_response_reader;   };+  static bool RpcStatusCheckSuccess(AsyncClientCall* call) {+    // Determine RPC success based on expected status.+    if (absl::GetFlag(FLAGS_expect_status) == ""OK"" && call->status.ok()) {","Please use `status_code_from_string()` here.https://github.com/grpc/grpc/blob/64f04e4d2fad514c56bfd3a9145f6403c3697281/src/core/lib/channel/status_util.h#L31Then you can just do something like this:```grpc_status_code code;GPR_ASSERT(grpc_status_code_from_string(    absl::GetFlag(FLAGS_expect_status).c_str(), &code));return code == call->status.error_code();```",
26072277,dfawley,https://api.github.com/repos/grpc/grpc/pulls/24956,544639177,2020-12-16T21:34:47Z,src/core/ext/xds/xds_server_config_fetcher.cc,"@@ -0,0 +1,125 @@+//+//+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/xds/xds_client.h""+#include ""src/core/lib/surface/api_trace.h""+#include ""src/core/lib/surface/server.h""++namespace grpc_core {+namespace {++class XdsServerConfigFetcher : public grpc_server_config_fetcher {+ public:+  XdsServerConfigFetcher() {+    grpc_error* error = GRPC_ERROR_NONE;+    xds_client_ = XdsClient::GetOrCreate(&error);+    if (error != GRPC_ERROR_NONE) {+      gpr_log(GPR_ERROR, ""Failed to create xds client: %s"",+              grpc_error_string(error));+    }+  }++  void StartWatch(std::string listening_address,+                  std::unique_ptr<grpc_server_config_fetcher::WatcherInterface>+                      watcher) override {+    auto listener_watcher = absl::make_unique<ListenerWatcher>(watcher.get());+    auto* listener_watcher_ptr = listener_watcher.get();+    // TODO(yashykt): Get the resource name id from bootstrap+    xds_client_->WatchListenerData(+        absl::StrCat(""grpc/server?udpa.resource.listening_address="",+                     listening_address),+        std::move(listener_watcher));+    MutexLock lock(&mu_);+    auto& watcher_state = watchers_[watcher.get()];+    watcher_state.listening_address = listening_address;+    watcher_state.watcher = std::move(watcher);+    watcher_state.listener_watcher = listener_watcher_ptr;+  }++  void CancelWatch(+      grpc_server_config_fetcher::WatcherInterface* watcher) override {+    MutexLock lock(&mu_);+    auto it = watchers_.find(watcher);+    if (it != watchers_.end()) {+      // Cancel the watch on the listener before erasing+      xds_client_->CancelListenerDataWatch(it->second.listening_address,+                                           it->second.listener_watcher,+                                           false /* delay_unsubscription */);+    }+    watchers_.erase(watcher);+  }++  // Return the interested parties from the xds client so that it can be polled.+  grpc_pollset_set* interested_parties() override {+    return xds_client_->interested_parties();+  }++ private:+  class ListenerWatcher : public XdsClient::ListenerWatcherInterface {+   public:+    explicit ListenerWatcher(+        grpc_server_config_fetcher::WatcherInterface* server_config_watcher)+        : server_config_watcher_(server_config_watcher) {}++    void OnListenerChanged(XdsApi::LdsUpdate listener) override {+      // TODO(yashykt): Construct channel args according to received update+      server_config_watcher_->UpdateConfig(nullptr);+    }++    void OnError(grpc_error* error) override {+      gpr_log(GPR_ERROR, ""ListenerWatcher:%p XdsClient reports error: %s"", this,+              grpc_error_string(error));+      GRPC_ERROR_UNREF(error);+      // TODO(yashykt): We might want to bubble this error to the application.+    }++    void OnResourceDoesNotExist() override {","cc @easwars as FYI and in case I missed anything that might make this difficult.Yes, that SGTM.  We already wrap the listener so making its Close() not close the user's listener when the grpc server is shutdown (until the xds-grpc-server is shutdown) is easy enough.At that point, we would immediately create a new gRPC server and make the listener wrapper insta-close new connections until the LDS resource returns.",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/25028,545397591,2020-12-17T20:55:18Z,tools/run_tests/run_xds_tests.py,"@@ -1282,6 +1282,9 @@ def test_circuit_breaking(gcp, original_backend_service, instance_group,         logger.info('UNARY_CALL reached stable state after increase (%d)',                     extra_backend_service_max_requests)         logger.info('success')+        configure_client([","I propose this change just as a nice-to have. This is the end of the `test_circuit_breaking` function in the Python test runner after the test case is successful. I propose we call `Configure()` once more to the test client to reset it to be only sending `UnaryCall` with empty metadata at this stage. The test runner will eventually kill the test client, but in the meantime, since the last `Configure()` call is with `rpc-behavior: keep-open`, in this particular instance the Ruby test client will keep opening new threads to send those RPCs. It takes a few minutes for the cleanup to happen so in those few minutes, the Ruby test client could have easily started tens of thousands of new threads. It did not cause any problem, per se, as the tests will still pass as those RPCs are irrelevant and the test client will be killed eventually. So this is just a nice-to-have.",
503812,voidzcy,https://api.github.com/repos/grpc/grpc/pulls/25028,545456129,2020-12-17T22:52:46Z,src/ruby/pb/test/xds_client.rb,"@@ -71,6 +88,29 @@ def create_stub(opts)   ) end +class ConfigureTarget < Grpc::Testing::XdsUpdateClientConfigureService::Service+  include Grpc::Testing++  def configure(req, _call)+    $rpcs_to_send = req['types'];","We'd want updates for `$rpcs_to_send` and `$metadata_to_send` to be _atomic_. That is, the thread running `run_test_loop` should see them consistently, not interleave with this `configure` call (e.g., `$rpcs_to_send` is updated but `$metadata_to_send` is not).In Java, we created a data structure ([`RpcConfig`](https://github.com/grpc/grpc-java/blob/499694e9da1982f5a23ae6ea8b1fa8b63f63a2c1/interop-testing/src/main/java/io/grpc/testing/integration/XdsTestClient.java#L508)) to hold `rpcs_to_send` and `metadata_to_send`. Each time this configure() RPC is invoked, it creates a new `RpcConfig` instance to replace the global reference.In C-core, it is similar, but just slightly different representations (It uses [a queue of configs](https://github.com/grpc/grpc/blob/e33849682c410452596ebf008a2b984e388e4f99/test/cpp/interop/xds_interop_client.cc#L411-L419), each of which is the configuration for one RPC type).",X
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/25028,545494750,2020-12-18T00:30:01Z,tools/internal_ci/linux/grpc_xds_ruby_test_in_docker.sh,"@@ -60,11 +60,11 @@ touch ""$TOOLS_DIR""/src/proto/grpc/health/v1/__init__.py  (cd src/ruby && bundle && rake compile) -GRPC_VERBOSITY=debug GRPC_TRACE=xds_client,xds_resolver,xds_cluster_manager_lb,cds_lb,xds_cluster_resolver_lb,priority_lb,xds_cluster_impl_lb,weighted_target_lb ""$PYTHON"" \+GRPC_VERBOSITY=debug GRPC_TRACE=xds_client,xds_resolver,xds_cluster_manager_lb,cds_lb,priority_lb,xds_cluster_impl_lb,weighted_target_lb ""$PYTHON"" \","Reverted. For some reason I thought that tag was not valid seeing some errors in the logs, but I no longer see those.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25026,546882299,2020-12-21T19:12:48Z,src/core/lib/security/credentials/google_default/google_default_credentials.cc,"@@ -80,21 +81,22 @@ grpc_google_default_channel_credentials::create_security_connector(     grpc_core::RefCountedPtr<grpc_call_credentials> call_creds,     const char* target, const grpc_channel_args* args,     grpc_channel_args** new_args) {-  bool is_grpclb_load_balancer = grpc_channel_arg_get_bool(-      grpc_channel_args_find(args, GRPC_ARG_ADDRESS_IS_GRPCLB_LOAD_BALANCER),-      false);-  bool is_backend_from_grpclb_load_balancer = grpc_channel_arg_get_bool(-      grpc_channel_args_find(-          args, GRPC_ARG_ADDRESS_IS_BACKEND_FROM_GRPCLB_LOAD_BALANCER),-      false);-  bool use_alts =-      is_grpclb_load_balancer || is_backend_from_grpclb_load_balancer;+  const bool is_grpclb_load_balancer = grpc_channel_args_find_bool(+      args, GRPC_ARG_ADDRESS_IS_GRPCLB_LOAD_BALANCER, false);+  const bool is_backend_from_grpclb_load_balancer = grpc_channel_args_find_bool(+      args, GRPC_ARG_ADDRESS_IS_BACKEND_FROM_GRPCLB_LOAD_BALANCER, false);+  const char* xds_cluster =+      grpc_channel_args_find_string(args, GRPC_ARG_XDS_CLUSTER_NAME);+  const bool is_xds_cfe_cluster =+      xds_cluster != nullptr && strcmp(xds_cluster, ""google_cfe"") == 0;+  const bool use_alts = is_grpclb_load_balancer ||+                        is_backend_from_grpclb_load_balancer ||+                        is_xds_cfe_cluster;","Doh!  You're right, I had this condition backwards.  Fixed now.Thanks for catching this!  When we set up the C2P interop tests, we'll need to make sure we cover this case.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/24814,546890936,2020-12-21T19:33:30Z,src/core/lib/security/credentials/google_default/google_default_credentials.cc,"@@ -267,6 +269,14 @@ static grpc_error* create_default_creds_from_path(     goto end;   } +  /* Finally try an external account credentials.*/+  result = grpc_core::ExternalAccountCredentials::Create(json, {}, &error);","One (minor) potential issue is that this creates an implicit requirement that external account credentials remain to be attempted last.So if a new mechanism is added, it would need to be added before this.Is it worth adding an ""external account credentials is valid"" API to determine whether or not we should attempt to parse the JSON config as an external account config, similar to the `grpc_auth_refresh_token_is_valid` and `grpc_auth_json_key_is_valid` above ?",X
869251,renkelvin,https://api.github.com/repos/grpc/grpc/pulls/24814,546896510,2020-12-21T19:46:11Z,src/core/lib/security/credentials/google_default/google_default_credentials.cc,"@@ -267,6 +269,14 @@ static grpc_error* create_default_creds_from_path(     goto end;   } +  /* Finally try an external account credentials.*/+  result = grpc_core::ExternalAccountCredentials::Create(json, {}, &error);","> One (minor) potential issue is that this creates an implicit requirement that external account credentials remain to be attempted last.I think the order shouldn't matter since the credentials type is determined by the `type` field of the credentials json file. Do I miss anything here?> Is it worth adding an ""external account credentials is valid"" API to determine whether or not we should attempt to parse the JSON config as an external account config, similar to the `grpc_auth_refresh_token_is_valid` and `grpc_auth_json_key_is_valid` above?`grpc_auth_refresh_token_is_valid` and `grpc_auth_json_key_is_valid` seem to be more C-style. `grpc_core::ExternalAccountCredentials::Create()` method here will early error out if the json input is invalid. So there shouldn't be efficiency issues.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/24668,546920536,2020-12-21T20:43:47Z,examples/python/async_streaming/server.py,"@@ -0,0 +1,89 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import logging+import time+from concurrent.futures import ThreadPoolExecutor+from typing import Iterable++import grpc+from google.protobuf.json_format import MessageToJson++import phone_pb2+import phone_pb2_grpc+++def create_state_response(call_state: phone_pb2.CallState.State+                         ) -> phone_pb2.StreamCallResponse:+    response = phone_pb2.StreamCallResponse()+    response.call_state.state = call_state+    return response+++class Phone(phone_pb2_grpc.PhoneServicer):++    def __init__(self):+        self._id_counter = 0++    def _create_call_session(self) -> phone_pb2.CallInfo:+        call_info = phone_pb2.CallInfo()+        call_info.session_id = str(self._id_counter)+        self._id_counter += 1",This is not threadsafe. [It resolves to multiple Python opcodes.](https://godbolt.org/z/9vxYjq),
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/24668,546960990,2020-12-21T22:28:09Z,examples/python/async_streaming/README.md,"@@ -0,0 +1,42 @@+# gRPC Python Non-Blocking Streaming RPC Client Example++The goal of this example is to demonstrate how to handle streaming responses without blocking the current thread.","Added more description about the API pattern, the usage of examples, and why async streaming is needed.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/25068,549789494,2020-12-29T17:36:31Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/traffic_director.py,"@@ -335,9 +326,6 @@ def setup_client_security(self,      def cleanup(self, *, force=False):         # Cleanup in the reverse order of creation-        # TODO(sergiitk): remove next line once proxy deletion is not dependent-        # upon proxy type.-        self.target_proxy_is_http = True","I think we should keep the option of using HTTP target proxy, just in case. There's a TODO to get rid of the flag though:https://github.com/grpc/grpc/blob/657b7c49de07e1d4462d15555266a57bf7cf0916/tools/run_tests/xds_k8s_test_driver/framework/infrastructure/traffic_director.py#L69-L70",X
14166415,sanjaypujare,https://api.github.com/repos/grpc/grpc/pulls/25079,550711009,2021-01-01T00:26:40Z,tools/run_tests/xds_k8s_test_driver/tests/security_test.py,"@@ -119,38 +122,63 @@ def test_mtls_error(self):                                    client_tls=True,                                    client_mtls=False) -        # Create the routing rule map+        # Create the routing rule map.         self.td.setup_routing_rule_map_for_grpc(self.server_xds_host,                                                 self.server_xds_port)-        # Wait for backends healthy after url map is created+        # Now that TD setup is complete, Backend Service can be populated+        # with healthy backends (NEGs).         self.td.wait_for_backends_healthy_status() -        # Start the client.+        # Start the client, but don't wait for it to report a healthy channel.         test_client: _XdsTestClient = self.startSecureTestClient(             test_server, wait_for_active_server_channel=False) -        # With negative tests we can't be absolutely certain expected-        # failure state is not caused by something else.-        # To mitigate for this, we repeat the checks a few times in case-        # the channel eventually stabilizes and RPCs pass.-        # TODO(sergiitk): use tenacity retryer, move nums to constants-        wait_sec = 10-        checks = 3-        for check in range(1, checks + 1):-            self.assertMtlsErrorSetup(test_client)-            self.assertFailedRpcs(test_client)-            if check != checks:-                logger.info(-                    'Check %s successful, waiting %s sec before the next check',-                    check, wait_sec)-                time.sleep(wait_sec)--    @absltest.skip(SKIP_REASON)+        self.assertClientCannotReachServerRepeatedly(test_client)+        logger.info(+            ""[SUCCESS] Client's connectivity state is consistent with a mTLS ""+            ""error caused by not presenting mTLS certificate to the server."")+     def test_server_authz_error(self):         """"""Negative test: AuthZ error.          Client does not authorize server because of mismatched SAN name.+        The order of operations is the same as in `test_mtls_error`.         """"""+        # Create backend service+        self.td.setup_backend_for_grpc()++        # Start server and attach its NEGs to the backend service, but+        # until they become healthy.+        test_server: _XdsTestServer = self.startSecureTestServer()+        self.setupServerBackends(wait_for_healthy_status=False)++        # Regular TLS setup, but with client policy configured using+        # intentionality incorrect server_namespace.+        self.td.setup_server_security(server_namespace=self.server_namespace,","I traced these calls to see where `server_namespace` is used on the server security side. I noticed it is used in the ECS matcher labels. So presumably the Node metadata also contains a ""app: {server_namespace}-{server_name}"" entry. This wasn't part of the original test spec (although it might not be a bad idea). Just curious - what purpose does it serve?",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/25079,552081878,2021-01-05T17:29:57Z,tools/run_tests/xds_k8s_test_driver/tests/security_test.py,"@@ -119,38 +122,63 @@ def test_mtls_error(self):                                    client_tls=True,                                    client_mtls=False) -        # Create the routing rule map+        # Create the routing rule map.         self.td.setup_routing_rule_map_for_grpc(self.server_xds_host,                                                 self.server_xds_port)-        # Wait for backends healthy after url map is created+        # Now that TD setup is complete, Backend Service can be populated+        # with healthy backends (NEGs).         self.td.wait_for_backends_healthy_status() -        # Start the client.+        # Start the client, but don't wait for it to report a healthy channel.         test_client: _XdsTestClient = self.startSecureTestClient(             test_server, wait_for_active_server_channel=False) -        # With negative tests we can't be absolutely certain expected-        # failure state is not caused by something else.-        # To mitigate for this, we repeat the checks a few times in case-        # the channel eventually stabilizes and RPCs pass.-        # TODO(sergiitk): use tenacity retryer, move nums to constants-        wait_sec = 10-        checks = 3-        for check in range(1, checks + 1):-            self.assertMtlsErrorSetup(test_client)-            self.assertFailedRpcs(test_client)-            if check != checks:-                logger.info(-                    'Check %s successful, waiting %s sec before the next check',-                    check, wait_sec)-                time.sleep(wait_sec)--    @absltest.skip(SKIP_REASON)+        self.assertClientCannotReachServerRepeatedly(test_client)+        logger.info(+            ""[SUCCESS] Client's connectivity state is consistent with a mTLS ""+            ""error caused by not presenting mTLS certificate to the server."")+     def test_server_authz_error(self):         """"""Negative test: AuthZ error.          Client does not authorize server because of mismatched SAN name.+        The order of operations is the same as in `test_mtls_error`.         """"""+        # Create backend service+        self.td.setup_backend_for_grpc()++        # Start server and attach its NEGs to the backend service, but+        # until they become healthy.+        test_server: _XdsTestServer = self.startSecureTestServer()+        self.setupServerBackends(wait_for_healthy_status=False)++        # Regular TLS setup, but with client policy configured using+        # intentionality incorrect server_namespace.+        self.td.setup_server_security(server_namespace=self.server_namespace,+                                      server_name=self.server_name,+                                      server_port=self.server_port,+                                      tls=True,+                                      mtls=False)+        incorrect_namespace = f'incorrect-namespace-{uuid.uuid4().hex}'+        self.td.setup_client_security(server_namespace=incorrect_namespace,+                                      server_name=self.server_name,+                                      tls=True,+                                      mtls=False)++        # Create the routing rule map.+        self.td.setup_routing_rule_map_for_grpc(self.server_xds_host,+                                                self.server_xds_port)+        # Now that TD setup is complete, Backend Service can be populated+        # with healthy backends (NEGs).+        self.td.wait_for_backends_healthy_status()","Yes, it's on my list. Thought it's out of scope of this change.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25076,552099294,2021-01-05T18:00:21Z,examples/python/route_guide/asyncio_route_guide_client.py,"@@ -0,0 +1,130 @@+# Copyright 2020 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""The Python AsyncIO implementation of the gRPC route guide client.""""""++import asyncio+import random+import logging+from typing import List, Iterable++import grpc++import route_guide_pb2+import route_guide_pb2_grpc+import route_guide_resources+++def make_route_note(message: str, latitude: int,+                    longitude: int) -> route_guide_pb2.RouteNote:+    return route_guide_pb2.RouteNote(+        message=message,+        location=route_guide_pb2.Point(latitude=latitude, longitude=longitude))+++# Performs an unary call+async def guide_get_one_feature(stub: route_guide_pb2_grpc.RouteGuideStub,+                                point: route_guide_pb2.Point) -> None:+    feature = await stub.GetFeature(point)+    if not feature.location:+        print(""Server returned incomplete feature"")+        return++    if feature.name:+        print(""Feature called %s at %s"" % (feature.name, feature.location))+    else:+        print(""Found no feature at %s"" % feature.location)+++async def guide_get_feature(stub: route_guide_pb2_grpc.RouteGuideStub) -> None:+    await guide_get_one_feature(+        stub, route_guide_pb2.Point(latitude=409146138, longitude=-746188906))+    await guide_get_one_feature(stub,+                                route_guide_pb2.Point(latitude=0, longitude=0))+++# Performs a server-streaming call+async def guide_list_features(stub: route_guide_pb2_grpc.RouteGuideStub+                             ) -> None:+    rectangle = route_guide_pb2.Rectangle(+        lo=route_guide_pb2.Point(latitude=400000000, longitude=-750000000),+        hi=route_guide_pb2.Point(latitude=420000000, longitude=-730000000))+    print(""Looking for features between 40, -75 and 42, -73"")++    features = stub.ListFeatures(rectangle)++    async for feature in features:+        print(""Feature called %s at %s"" % (feature.name, feature.location))+++def generate_route(feature_list: List[route_guide_pb2.Feature]+                  ) -> Iterable[route_guide_pb2.Point]:+    for _ in range(0, 10):+        random_feature = feature_list[random.randint(0, len(feature_list) - 1)]",NIt: [`random.choice`?](https://docs.python.org/3/library/random.html#random.choice),
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25070,552215074,2021-01-05T21:38:51Z,Rakefile,"@@ -131,28 +130,17 @@ task 'gem:native' do         ""invoked on macos with ruby #{RUBY_VERSION}. The ruby macos artifact "" \         ""build should be running on ruby 2.5.""     end-    system ""rake cross native gem RUBY_CC_VERSION=#{ruby_cc_versions} V=#{verbose} GRPC_CONFIG=#{grpc_config}""+    system ""rake cross native gem RUBY_CC_VERSION=3.0.0:2.7.0:2.6.0:2.5.0:2.4.0:2.3.0 V=#{verbose} GRPC_CONFIG=#{grpc_config}""   else     Rake::Task['dlls'].execute-    ['x86-mingw32', 'x64-mingw32'].each do |plat|+    ['x86-mingw32', 'x64-mingw32', 'x86_64-linux', 'x86-linux'].each do |plat|       run_rake_compiler plat, <<-EOT         gem update --system --no-document && \         bundle && \         rake native:#{plat} pkg/#{spec.full_name}-#{plat}.gem pkg/#{spec.full_name}.gem \-          RUBY_CC_VERSION=#{ruby_cc_versions} V=#{verbose} GRPC_CONFIG=#{grpc_config}-      EOT-    end-    # Truncate grpc_c.*.ruby files because they're for Windows only.-    File.truncate('grpc_c.32.ruby', 0)-    File.truncate('grpc_c.64.ruby', 0)-    ['x86_64-linux', 'x86-linux'].each do |plat|-      run_rake_compiler plat,  <<-EOT-        gem update --system --no-document && \-        bundle && \-        rake native:#{plat} pkg/#{spec.full_name}-#{plat}.gem pkg/#{spec.full_name}.gem \-          RUBY_CC_VERSION=#{ruby_cc_versions} V=#{verbose} GRPC_CONFIG=#{grpc_config} &&-        sudo chmod -R a+rw pkg &&-        patchelf_gem.sh pkg/#{spec.full_name}-#{plat}.gem","I can't see what we need `patchelf_gem.sh` for now, though.Here are the dynamic dependencies of the x86-64 pre-compiled linux ruby gems now:```~/grpc/pkg$ find grpc-1.35.0.dev-x86_64-linux -name grpc_c.so -exec bash -c ""echo {} && ldd {}"" \;grpc-1.35.0.dev-x86_64-linux/src/ruby/lib/grpc/2.3/grpc_c.so	linux-vdso.so.1 (0x00007ffe5ab59000)	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f236363b000)	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f2363636000)	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f23634b3000)	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f23632f2000)	/lib64/ld-linux-x86-64.so.2 (0x00007f2363dd8000)grpc-1.35.0.dev-x86_64-linux/src/ruby/lib/grpc/2.7/grpc_c.so	linux-vdso.so.1 (0x00007ffe4f14d000)	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f5f9ccc7000)	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f5f9cb06000)	/lib64/ld-linux-x86-64.so.2 (0x00007f5f9d5c6000)grpc-1.35.0.dev-x86_64-linux/src/ruby/lib/grpc/2.6/grpc_c.so	linux-vdso.so.1 (0x00007ffee61ee000)	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fa3c049e000)	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fa3c02dd000)	/lib64/ld-linux-x86-64.so.2 (0x00007fa3c0d9d000)grpc-1.35.0.dev-x86_64-linux/src/ruby/lib/grpc/2.5/grpc_c.so	linux-vdso.so.1 (0x00007fffddeec000)	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f7e2967c000)	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f7e29677000)	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f7e294f4000)	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f7e29333000)	/lib64/ld-linux-x86-64.so.2 (0x00007f7e29e19000)grpc-1.35.0.dev-x86_64-linux/src/ruby/lib/grpc/2.4/grpc_c.so	linux-vdso.so.1 (0x00007ffc3a919000)	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fbd27850000)	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fbd2784b000)	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fbd276c8000)	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fbd27507000)	/lib64/ld-linux-x86-64.so.2 (0x00007fbd27fed000)```... note how there is no `libcrypt` dependency, which if I'm correct [is the purpose of patchelf_gem.sh](https://github.com/grpc/grpc/blob/393687c3b7faba823648e020b73fcfcba6a757f9/third_party/rake-compiler-dock/build/patchelf_gem.sh#L20) ?Also note that [manylinux14 appears to have removed libcrypt from their list of permitted dynamic libraries](https://www.python.org/dev/peps/pep-0599/) -- ""This list is identical to the externally-provided libraries originally allowed for manylinux2010, with one exception: libcrypt.so.1 was removed due to being deprecated in Fedora 30.""",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25069,552240444,2021-01-05T22:36:30Z,test/cpp/naming/resolver_component_tests_runner.py,"@@ -606,7 +637,8 @@ def _quit_on_signal(signum, _frame):   '--enable_srv_queries', 'True',   '--enable_txt_queries', 'True',   '--inject_broken_nameserver_list', 'False',-  '--local_dns_server_address', '127.0.0.1:%d' % args.dns_server_port])+  '--local_dns_server_address', '127.0.0.1:%d' % args.dns_server_port+  ] + filter(lambda x: x, args.extra_args.split(',')))",They are already auto-generated -- `test/cpp/naming/resolver_component_tests_runner.py` is generated from `templates/test/cpp/naming/resolver_component_tests_defs.include`,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/25070,552255080,2021-01-05T23:15:31Z,Rakefile,"@@ -131,28 +130,17 @@ task 'gem:native' do         ""invoked on macos with ruby #{RUBY_VERSION}. The ruby macos artifact "" \         ""build should be running on ruby 2.5.""     end-    system ""rake cross native gem RUBY_CC_VERSION=#{ruby_cc_versions} V=#{verbose} GRPC_CONFIG=#{grpc_config}""+    system ""rake cross native gem RUBY_CC_VERSION=3.0.0:2.7.0:2.6.0:2.5.0:2.4.0:2.3.0 V=#{verbose} GRPC_CONFIG=#{grpc_config}""   else     Rake::Task['dlls'].execute-    ['x86-mingw32', 'x64-mingw32'].each do |plat|+    ['x86-mingw32', 'x64-mingw32', 'x86_64-linux', 'x86-linux'].each do |plat|       run_rake_compiler plat, <<-EOT         gem update --system --no-document && \         bundle && \         rake native:#{plat} pkg/#{spec.full_name}-#{plat}.gem pkg/#{spec.full_name}.gem \-          RUBY_CC_VERSION=#{ruby_cc_versions} V=#{verbose} GRPC_CONFIG=#{grpc_config}-      EOT-    end-    # Truncate grpc_c.*.ruby files because they're for Windows only.-    File.truncate('grpc_c.32.ruby', 0)-    File.truncate('grpc_c.64.ruby', 0)-    ['x86_64-linux', 'x86-linux'].each do |plat|-      run_rake_compiler plat,  <<-EOT-        gem update --system --no-document && \-        bundle && \-        rake native:#{plat} pkg/#{spec.full_name}-#{plat}.gem pkg/#{spec.full_name}.gem \-          RUBY_CC_VERSION=#{ruby_cc_versions} V=#{verbose} GRPC_CONFIG=#{grpc_config} &&-        sudo chmod -R a+rw pkg &&-        patchelf_gem.sh pkg/#{spec.full_name}-#{plat}.gem",Oh mnaylinux2014 somehow managed to remove that dependency. Actually it was only observed from ruby build case so this could be skipped if the grpc so files start not to have it. Thanks!,
176234,larskanis,https://api.github.com/repos/grpc/grpc/pulls/25070,552447168,2021-01-06T09:00:21Z,Rakefile,"@@ -131,28 +130,17 @@ task 'gem:native' do         ""invoked on macos with ruby #{RUBY_VERSION}. The ruby macos artifact "" \         ""build should be running on ruby 2.5.""     end-    system ""rake cross native gem RUBY_CC_VERSION=#{ruby_cc_versions} V=#{verbose} GRPC_CONFIG=#{grpc_config}""+    system ""rake cross native gem RUBY_CC_VERSION=3.0.0:2.7.0:2.6.0:2.5.0:2.4.0:2.3.0 V=#{verbose} GRPC_CONFIG=#{grpc_config}""   else     Rake::Task['dlls'].execute-    ['x86-mingw32', 'x64-mingw32'].each do |plat|+    ['x86-mingw32', 'x64-mingw32', 'x86_64-linux', 'x86-linux'].each do |plat|       run_rake_compiler plat, <<-EOT         gem update --system --no-document && \         bundle && \         rake native:#{plat} pkg/#{spec.full_name}-#{plat}.gem pkg/#{spec.full_name}.gem \-          RUBY_CC_VERSION=#{ruby_cc_versions} V=#{verbose} GRPC_CONFIG=#{grpc_config}-      EOT-    end-    # Truncate grpc_c.*.ruby files because they're for Windows only.-    File.truncate('grpc_c.32.ruby', 0)-    File.truncate('grpc_c.64.ruby', 0)-    ['x86_64-linux', 'x86-linux'].each do |plat|-      run_rake_compiler plat,  <<-EOT-        gem update --system --no-document && \-        bundle && \-        rake native:#{plat} pkg/#{spec.full_name}-#{plat}.gem pkg/#{spec.full_name}.gem \-          RUBY_CC_VERSION=#{ruby_cc_versions} V=#{verbose} GRPC_CONFIG=#{grpc_config} &&-        sudo chmod -R a+rw pkg &&-        patchelf_gem.sh pkg/#{spec.full_name}-#{plat}.gem",I remove the unnecessary dependency to `libcrypt.so.2` [in rake-compiler-dock](https://github.com/rake-compiler/rake-compiler-dock/blob/9c6e35e91661a98335a0ecddd329f6c96a8a0a70/Dockerfile.mri.erb#L210-L211).,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/24961,552917800,2021-01-06T19:26:07Z,bazel/grpc_extra_deps.bzl,"@@ -35,6 +35,6 @@ def grpc_extra_deps():     go_rules_dependencies()     go_register_toolchains() -    apple_rules_dependencies()+    apple_rules_dependencies(ignore_version_differences)","Nit: Supply this as a keyword argument, rather than a positional argument. It's not likely, but it's possible that someone in the `rules_apple` repo adds a new keyword argument to the *beginning* of the list rather than the end. That would technically break API, but it wouldn't be unprecedented.",X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/25122,554277831,2021-01-09T02:28:26Z,src/core/lib/security/authorization/matchers.h,"@@ -0,0 +1,127 @@++// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_MATCHERS_H+#define GRPC_CORE_LIB_SECURITY_AUTHORIZATION_MATCHERS_H++#include <grpc/support/port_platform.h>++#include <memory>+#include <string>++#include ""absl/strings/string_view.h""+#include ""absl/types/optional.h""++#include ""re2/re2.h""++namespace grpc_core {++class StringMatcher {+ public:+  enum class StringMatcherType {+    EXACT,       // value stored in string_matcher_ field+    PREFIX,      // value stored in string_matcher_ field+    SUFFIX,      // value stored in string_matcher_ field+    SAFE_REGEX,  // pattern stored in regex_matcher_ field+    CONTAINS,    // value stored in string_matcher_ field+  };++  StringMatcher() = default;+  StringMatcher(StringMatcherType type, const std::string& matcher,+                bool ignore_case = false,+                bool use_ignore_case_in_regex = false);++  StringMatcher(const StringMatcher& other);+  StringMatcher& operator=(const StringMatcher& other);+  bool operator==(const StringMatcher& other) const;++  bool Match(absl::string_view value) const;++  std::string ToString() const;++  StringMatcherType type() const { return type_; }++  // Valid for EXACT, PREFIX, SUFFIX and CONTAINS+  const std::string& string_matcher() const { return string_matcher_; }++  // Valid for SAFE_REGEX+  RE2* regex_matcher() const { return regex_matcher_.get(); }++  static_assert(static_cast<int>(StringMatcherType::EXACT) == 0 &&+                    static_cast<int>(StringMatcherType::PREFIX) == 1 &&+                    static_cast<int>(StringMatcherType::SUFFIX) == 2 &&+                    static_cast<int>(StringMatcherType::SAFE_REGEX) == 3 &&+                    static_cast<int>(StringMatcherType::CONTAINS) == 4,+                ""Wrong StringMatcherType index."");++ private:+  StringMatcherType type_ = StringMatcherType::EXACT;+  std::string string_matcher_;+  std::unique_ptr<RE2> regex_matcher_;+  bool ignore_case_ = false;+};++class HeaderMatcher {+ public:+  enum class HeaderMatcherType {+    EXACT,       // value stored in StringMatcher field+    PREFIX,      // value stored in StringMatcher field+    SUFFIX,      // value stored in StringMatcher field+    SAFE_REGEX,  // value stored in StringMatcher field+    CONTAINS,    // value stored in StringMatcher field+    RANGE,       // uses range_start and range_end fields+    PRESENT,     // uses present_match field+  };++  HeaderMatcher() = default;+  HeaderMatcher(const std::string& name, HeaderMatcherType type,+                const std::string& matcher, int64_t range_start = 0,+                int64_t range_end = 0, bool present_match = false,+                bool invert_match = false);++  HeaderMatcher(const HeaderMatcher& other);+  HeaderMatcher& operator=(const HeaderMatcher& other);+  bool operator==(const HeaderMatcher& other) const;++  HeaderMatcherType type() const { return type_; }+  const std::string& name() const { return name_; }+  // Valid for EXACT, PREFIX, SUFFIX, SAFE_REGEX and CONTAINS+  const StringMatcher* matcher() const { return matcher_.get(); }++  bool Match(const absl::optional<absl::string_view>& value) const;++  std::string ToString() const;++  static_assert(static_cast<int>(HeaderMatcherType::EXACT) == 0 &&+                    static_cast<int>(HeaderMatcherType::PREFIX) == 1 &&+                    static_cast<int>(HeaderMatcherType::SUFFIX) == 2 &&+                    static_cast<int>(HeaderMatcherType::SAFE_REGEX) == 3 &&+                    static_cast<int>(HeaderMatcherType::CONTAINS) == 4,+                ""Wrong HeaderMatcherType index."");++ private:+  std::string name_;+  HeaderMatcherType type_ = HeaderMatcherType::EXACT;+  std::unique_ptr<StringMatcher> matcher_ = nullptr;+  int64_t range_start_;+  int64_t range_end_;+  bool present_match_;","@markdroth Envoy only looks at the type, whether it is present_match specifier. It does not care about the value if it is true/false.https://github.com/envoyproxy/envoy/blob/e80b8eec292382f28ac0428229ce165d12ddeb13/source/common/http/header_utility.cc#L136xDS code considers the value. https://github.com/grpc/grpc/blob/91c4cbd51b5edf62a2612e857853891f8f817bd9/src/core/ext/filters/client_channel/resolver/xds/xds_resolver.cc#L427I updated the code to use the value.  But shouldn't we follow Envoy behavior? WDYT?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25122,555401014,2021-01-11T23:18:09Z,src/core/lib/security/authorization/matchers.h,"@@ -0,0 +1,127 @@++// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_SECURITY_AUTHORIZATION_MATCHERS_H+#define GRPC_CORE_LIB_SECURITY_AUTHORIZATION_MATCHERS_H++#include <grpc/support/port_platform.h>++#include <memory>+#include <string>++#include ""absl/strings/string_view.h""+#include ""absl/types/optional.h""++#include ""re2/re2.h""++namespace grpc_core {++class StringMatcher {+ public:+  enum class StringMatcherType {+    EXACT,       // value stored in string_matcher_ field+    PREFIX,      // value stored in string_matcher_ field+    SUFFIX,      // value stored in string_matcher_ field+    SAFE_REGEX,  // pattern stored in regex_matcher_ field+    CONTAINS,    // value stored in string_matcher_ field+  };++  StringMatcher() = default;+  StringMatcher(StringMatcherType type, const std::string& matcher,+                bool ignore_case = false,+                bool use_ignore_case_in_regex = false);++  StringMatcher(const StringMatcher& other);+  StringMatcher& operator=(const StringMatcher& other);+  bool operator==(const StringMatcher& other) const;++  bool Match(absl::string_view value) const;++  std::string ToString() const;++  StringMatcherType type() const { return type_; }++  // Valid for EXACT, PREFIX, SUFFIX and CONTAINS+  const std::string& string_matcher() const { return string_matcher_; }++  // Valid for SAFE_REGEX+  RE2* regex_matcher() const { return regex_matcher_.get(); }++  static_assert(static_cast<int>(StringMatcherType::EXACT) == 0 &&+                    static_cast<int>(StringMatcherType::PREFIX) == 1 &&+                    static_cast<int>(StringMatcherType::SUFFIX) == 2 &&+                    static_cast<int>(StringMatcherType::SAFE_REGEX) == 3 &&+                    static_cast<int>(StringMatcherType::CONTAINS) == 4,+                ""Wrong StringMatcherType index."");++ private:+  StringMatcherType type_ = StringMatcherType::EXACT;+  std::string string_matcher_;+  std::unique_ptr<RE2> regex_matcher_;+  bool ignore_case_ = false;+};++class HeaderMatcher {+ public:+  enum class HeaderMatcherType {+    EXACT,       // value stored in StringMatcher field+    PREFIX,      // value stored in StringMatcher field+    SUFFIX,      // value stored in StringMatcher field+    SAFE_REGEX,  // value stored in StringMatcher field+    CONTAINS,    // value stored in StringMatcher field+    RANGE,       // uses range_start and range_end fields+    PRESENT,     // uses present_match field+  };++  HeaderMatcher() = default;+  HeaderMatcher(const std::string& name, HeaderMatcherType type,+                const std::string& matcher, int64_t range_start = 0,+                int64_t range_end = 0, bool present_match = false,+                bool invert_match = false);++  HeaderMatcher(const HeaderMatcher& other);+  HeaderMatcher& operator=(const HeaderMatcher& other);+  bool operator==(const HeaderMatcher& other) const;++  HeaderMatcherType type() const { return type_; }+  const std::string& name() const { return name_; }+  // Valid for EXACT, PREFIX, SUFFIX, SAFE_REGEX and CONTAINS+  const StringMatcher* matcher() const { return matcher_.get(); }++  bool Match(const absl::optional<absl::string_view>& value) const;++  std::string ToString() const;++  static_assert(static_cast<int>(HeaderMatcherType::EXACT) == 0 &&+                    static_cast<int>(HeaderMatcherType::PREFIX) == 1 &&+                    static_cast<int>(HeaderMatcherType::SUFFIX) == 2 &&+                    static_cast<int>(HeaderMatcherType::SAFE_REGEX) == 3 &&+                    static_cast<int>(HeaderMatcherType::CONTAINS) == 4,+                ""Wrong HeaderMatcherType index."");++ private:+  std::string name_;+  HeaderMatcherType type_ = HeaderMatcherType::EXACT;+  std::unique_ptr<StringMatcher> matcher_ = nullptr;+  int64_t range_start_;+  int64_t range_end_;+  bool present_match_;","Thanks for pointing this out!I'm actually very surprised that Envoy doesn't actually look at the value of this field.  This means that Envoy interprets this field the same way, regardless of whether the value is true or false.  @htuch, is this behavior intentional?The comment in the proto file doesn't make this clear:https://github.com/envoyproxy/envoy/blob/748a9009e840c4f8da3a4021aa33f81d68712d8f/api/envoy/config/route/v3/route_components.proto#L1857The field is a boolean, so we assumed that if the value was false, that meant to match when the field is *not* present.  That's how we implemented it in C-core, and @voidzcy tells me that grpc-java did the same thing.  @menghanl can tell us how grpc-go works.We should probably figure out how this is supposed to work and then change all of our implementations to do the same thing.",X
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/25107,556117866,2021-01-12T21:49:24Z,tools/internal_ci/linux/grpc_xds_bazel_test_in_docker.sh,"@@ -76,3 +77,4 @@ GRPC_VERBOSITY=debug GRPC_TRACE=xds_client,xds_resolver,xds_cluster_manager_lb,c     --client_cmd='bazel-bin/test/cpp/interop/xds_interop_client --server=xds:///{server_uri} --stats_port={stats_port} --qps={qps} {fail_on_failed_rpc} \       {rpcs_to_send} \       {metadata_to_send}'+!! --xds_v3_support",This is unfortunate. A single invocation of `run_xds_tests.py` can already take 1+ hr. Doubling that is not going to be fun. We either need separate Kokoro job for v3 tests (I don't see any drawbacks to this?) or push the v2/v3 logic into `run_xds_tests.py` (we already accommodate running multiple clients in parallel for a single `run_xds_test.py` invocation for our staging tests).,X
35056280,srini100,https://api.github.com/repos/grpc/grpc/pulls/25141,556201924,2021-01-13T01:21:52Z,doc/xds-test-descriptions.md,"@@ -463,6 +463,42 @@ Test driver asserts: 1.  All backends in the primary locality receive at least 1 RPC. 1.  No backends in the secondary locality receive RPCs. ++### load_based_failover++This test verifies that traffic is partially diverted to a secondary locality+when the QPS is greater than the configured RPS in the priority locality.++Client parameters:++1.  --num_channels=1+1.  --qps=100++Load balancer configuration:++1.  The primary MIG with 2 backends in the same zone as the client+1.  The secondary MIG with 2 backends in a different zone++Test driver asserts:++1.  All backends in the primary locality receive at least 1 RPC.+1.  No backends in the secondary locality receive RPCs.++The test driver sets `balancingMode` is `RATE`, and `maxRate` to 80 in the primary locality.++Test driver asserts:",The test should run long enough to see a change in the EDS assignments. At least 4x of the load reporting interval?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25142,556212071,2021-01-13T01:53:38Z,src/python/grpcio_tests/tests_py3_only/interop/xds_interop_client.py,"@@ -254,15 +354,24 @@ def _run(args: argparse.Namespace, methods: Sequence[str],     logger.info(""Starting python xDS Interop Client."")     global _global_server  # pylint: disable=global-statement     method_handles = []-    for method in methods:-        method_handles.append(-            _MethodHandle(method, per_method_metadata.get(method, []),-                          args.num_channels, args.qps, args.server,-                          args.rpc_timeout_sec, args.print_response))+    channel_configs = {}+    for method in _SUPPORTED_METHODS:+        if method in methods:+            qps = args.qps+        else:+            qps = 0",Do we want to raise an exception here?,
503812,voidzcy,https://api.github.com/repos/grpc/grpc/pulls/25142,556224427,2021-01-13T02:33:14Z,src/python/grpcio_tests/tests_py3_only/interop/xds_interop_client.py,"@@ -254,15 +354,24 @@ def _run(args: argparse.Namespace, methods: Sequence[str],     logger.info(""Starting python xDS Interop Client."")     global _global_server  # pylint: disable=global-statement     method_handles = []-    for method in methods:-        method_handles.append(-            _MethodHandle(method, per_method_metadata.get(method, []),-                          args.num_channels, args.qps, args.server,-                          args.rpc_timeout_sec, args.print_response))+    channel_configs = {}+    for method in _SUPPORTED_METHODS:+        if method in methods:+            qps = args.qps+        else:+            qps = 0","C-core, Ruby and Java put an assertion. It's a nice-to-have thing. Not a big issue, more for preventing bugs.",
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/25141,556748614,2021-01-13T18:42:27Z,doc/xds-test-descriptions.md,"@@ -463,6 +463,42 @@ Test driver asserts: 1.  All backends in the primary locality receive at least 1 RPC. 1.  No backends in the secondary locality receive RPCs. ++### load_based_failover++This test verifies that traffic is partially diverted to a secondary locality+when the QPS is greater than the configured RPS in the priority locality.++Client parameters:++1.  --num_channels=1+1.  --qps=100++Load balancer configuration:++1.  The primary MIG with 2 backends in the same zone as the client+1.  The secondary MIG with 2 backends in a different zone++Test driver asserts:++1.  All backends in the primary locality receive at least 1 RPC.+1.  No backends in the secondary locality receive RPCs.++The test driver sets `balancingMode` is `RATE`, and `maxRate` to 80 in the primary locality.++Test driver asserts:++1.  All backends in the primary locality receive at least 1 RPC.+1.  All backends in the secondary locality receive at least 1 RPC.++The test driver set `maxRate` to 120 in the primary locality.",It doesn't seems to be necessary to me. This is 20% more than the QPS client is sending. Should be big enough for the traffic to recover to only the primary locality.,
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/25141,556751124,2021-01-13T18:46:39Z,doc/xds-test-descriptions.md,"@@ -463,6 +463,42 @@ Test driver asserts: 1.  All backends in the primary locality receive at least 1 RPC. 1.  No backends in the secondary locality receive RPCs. ++### load_based_failover++This test verifies that traffic is partially diverted to a secondary locality+when the QPS is greater than the configured RPS in the priority locality.++Client parameters:++1.  --num_channels=1+1.  --qps=100++Load balancer configuration:++1.  The primary MIG with 2 backends in the same zone as the client+1.  The secondary MIG with 2 backends in a different zone++Test driver asserts:++1.  All backends in the primary locality receive at least 1 RPC.+1.  No backends in the secondary locality receive RPCs.++The test driver sets `balancingMode` is `RATE`, and `maxRate` to 80 in the primary locality.++Test driver asserts:","The reporting interval is 10 seconds. The current timeout for each check is 360 seconds. Far more than 4x.As long as TD doesn't change the reporting interval (to more than 60 seconds, which is too long IMO), we should be safe here.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25142,556757786,2021-01-13T18:57:57Z,src/python/grpcio_tests/tests_py3_only/interop/xds_interop_client.py,"@@ -254,15 +354,24 @@ def _run(args: argparse.Namespace, methods: Sequence[str],     logger.info(""Starting python xDS Interop Client."")     global _global_server  # pylint: disable=global-statement     method_handles = []-    for method in methods:-        method_handles.append(-            _MethodHandle(method, per_method_metadata.get(method, []),-                          args.num_channels, args.qps, args.server,-                          args.rpc_timeout_sec, args.print_response))+    channel_configs = {}+    for method in _SUPPORTED_METHODS:+        if method in methods:+            qps = args.qps+        else:+            qps = 0","I don't understand why there would be an assertion here. The intent is to create threads for all RPC types from the beginning of the process, but to set the QPS rate for those not yet configured to 0.This is not an exceptional codepath. In fact, by default, it will be run every time the process starts up.Note that we're iterating over `_SUPPORTED_METHODS` here.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/25134,556950270,2021-01-13T23:43:47Z,src/cpp/ext/filters/census/context.cc,"@@ -28,8 +28,9 @@ using ::opencensus::tags::TagMap; using ::opencensus::trace::Span; using ::opencensus::trace::SpanContext; -void GenerateServerContext(absl::string_view tracing, absl::string_view stats,-                           absl::string_view primary_role,+void GenerateServerContext(absl::string_view tracing,+                           absl::string_view /*stats*/,+                           absl::string_view /*primary_role*/,",Can these ~arguments~ parameters be deleted?,
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/25151,557578638,2021-01-14T17:46:24Z,tools/run_tests/run_xds_tests.py,"@@ -1004,6 +1004,10 @@ def test_path_matching(gcp, original_backend_service, instance_group,                 if compare_expected_instances(stats, expected_instances):                     logger.info(""success"")                     break+                elif i == retry_count - 1:","Use return instead of break above, and then you can raise unconditionally after the for loop exits? That would avoid this if statement and make it cleared what triggers the failure",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/25150,557582233,2021-01-14T17:52:18Z,src/python/grpcio_tests/tests_py3_only/interop/xds_interop_client.py,"@@ -150,8 +150,8 @@ def GetClientStats(         return response      def GetClientAccumulatedStats(-            self, request: messages_pb2.LoadBalancerAccumulatedStatsRequest,-            context: grpc.ServicerContext+        self, request: messages_pb2.LoadBalancerAccumulatedStatsRequest,","Good idea, I've removed it from this PR. From the yapf upgrade yesterday, I'm guessing we may expect more formatting changes as we go along.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22082,557674171,2021-01-14T20:19:20Z,src/core/lib/security/security_connector/local/local_security_connector.cc,"@@ -70,39 +77,19 @@ void local_check_peer(tsi_peer peer, grpc_endpoint* ep,                       grpc_core::RefCountedPtr<grpc_auth_context>* auth_context,                       grpc_closure* on_peer_checked,                       grpc_local_connect_type type) {-  int fd = grpc_endpoint_get_fd(ep);-  grpc_resolved_address resolved_addr;-  memset(&resolved_addr, 0, sizeof(resolved_addr));-  resolved_addr.len = GRPC_MAX_SOCKADDR_SIZE;   bool is_endpoint_local = false;-  if (getsockname(fd, reinterpret_cast<grpc_sockaddr*>(resolved_addr.addr),-                  &resolved_addr.len) == 0) {-    grpc_resolved_address addr_normalized;-    grpc_resolved_address* addr =-        grpc_sockaddr_is_v4mapped(&resolved_addr, &addr_normalized)-            ? &addr_normalized-            : &resolved_addr;-    grpc_sockaddr* sock_addr = reinterpret_cast<grpc_sockaddr*>(&addr->addr);-    // UDS-    if (type == UDS && grpc_is_unix_socket(addr)) {++  char* peer_address = ep->vtable->get_peer(ep);+  if (type == UDS && strncmp(k_unix, peer_address, n_unix) == 0) {","Instead of doing string parsing here, I suggest converting the peer address URI into a `grpc_resolved_address` using something like this:https://github.com/grpc/grpc/blob/eb0b2067321502d1376dd7ba8d5d345865528601/src/core/ext/filters/client_channel/subchannel.cc#L912Then you can basically use the existing code to check the value of the `grpc_resolved_address`.",X
4779759,dapengzhang0,https://api.github.com/repos/grpc/grpc/pulls/25107,557791204,2021-01-15T00:42:03Z,tools/internal_ci/linux/grpc_xds_bazel_test_in_docker.sh,"@@ -76,3 +77,4 @@ GRPC_VERBOSITY=debug GRPC_TRACE=xds_client,xds_resolver,xds_cluster_manager_lb,c     --client_cmd='bazel-bin/test/cpp/interop/xds_interop_client --server=xds:///{server_uri} --stats_port={stats_port} --qps={qps} {fail_on_failed_rpc} \       {rpcs_to_send} \       {metadata_to_send}'+!! --xds_v3_support",I agree this approach takes too long.  Will try other approaches.,
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/25107,558051655,2021-01-15T08:42:57Z,tools/run_tests/run_xds_tests.py,"@@ -96,7 +97,11 @@ def parse_port_range(port_arg):   argp = argparse.ArgumentParser(description='Run xDS interop tests on GCP')-argp.add_argument('--project_id', help='GCP project id')+argp.add_argument('--project_id', default='grpc-testing', help='GCP project id')+argp.add_argument(+    '--project_num',","I don't think that getting the project number requires a new authentication token. All of our REST API calls to GCE already require authentication, which is set up using the google default creds on the VM. The doc page I linked to says that the caller requires IAM permission `resourcemanager.projects.get` on the project - I think our VMs already have this. I just ssh'ed into one and ran the snippet from the documentation page I linked:```resource_manager = googleapiclient.discovery.build('cloudresourcemanager', 'v1') project_request = resource_manager.projects().get(projectId='grpc_testing')project_response = project_request.execute()>>> pprint(project_response){'createTime': ..., 'projectId': 'grpc-testing', 'projectNumber': '830293263384'}```And it worked as-is. So you would just need to construct the parallel `resource_manager` object analogous to how we construct a `compute` accessor here: https://github.com/grpc/grpc/blob/master/tools/run_tests/run_xds_tests.py#L2009 (we already have a dependency on googleapiclient for use with the GCE rest API).If the above doesn't work for some reason, then setting both by default until all callers are updated, and then removing the default sounds reasonable. But given the number of calls I think it would be easier to fetch programmatically (there could totally be problems I'm not seeing with authentication, as I don't have a complete grasp of the permissions needed vs how they're setup by Kokoro, but just going by a quick manual trial, it seems we already have the necessary permissions in our CI VMs)",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22082,558446738,2021-01-15T17:12:24Z,src/core/lib/security/security_connector/local/local_security_connector.cc,"@@ -66,45 +69,24 @@ grpc_core::RefCountedPtr<grpc_auth_context> local_auth_context_create(   return ctx; } +static void uri_to_sockaddr(const char* uri_str, grpc_resolved_address* addr) {","Since this now exists in two places, how about putting this in the iomgr parse_address library?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/22082,558450182,2021-01-15T17:18:16Z,src/core/lib/security/security_connector/local/local_security_connector.cc,"@@ -66,45 +69,24 @@ grpc_core::RefCountedPtr<grpc_auth_context> local_auth_context_create(   return ctx; } +static void uri_to_sockaddr(const char* uri_str, grpc_resolved_address* addr) {",Please change the first parameter to `absl::string_view`.  This avoids the need to allocate a copy of the string in the caller just to ensure that it's nul-terminated.,
4779759,dapengzhang0,https://api.github.com/repos/grpc/grpc/pulls/25107,558450651,2021-01-15T17:19:04Z,tools/run_tests/run_xds_tests.py,"@@ -96,7 +97,11 @@ def parse_port_range(port_arg):   argp = argparse.ArgumentParser(description='Run xDS interop tests on GCP')-argp.add_argument('--project_id', help='GCP project id')+argp.add_argument('--project_id', default='grpc-testing', help='GCP project id')+argp.add_argument(+    '--project_num',","I see, the authentication is not a problem using the snippet. I think getting project number from id programmatically is still more complex than providing both project number and id in the command line args.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/22082,558524757,2021-01-15T19:05:45Z,src/core/lib/security/security_connector/local/local_security_connector.cc,"@@ -66,45 +69,24 @@ grpc_core::RefCountedPtr<grpc_auth_context> local_auth_context_create(   return ctx; } +static void uri_to_sockaddr(const char* uri_str, grpc_resolved_address* addr) {",Moved to parse_address library.,
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/25154,558565859,2021-01-15T20:06:23Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -5442,16 +5443,36 @@ class XdsSecurityTest : public BasicTest {       StartBackend(0);       ResetBackendCounters();       if (test_expects_failure) {-        if (!SendRpc().ok()) break;+        Status status = SendRpc();+        if (status.ok()) {+          gpr_log(GPR_ERROR, ""RPC succeeded. Failure expected. Trying again."");+          continue;+        }       } else {         WaitForBackend(0);-        if (SendRpc().ok() &&-            backends_[0]->backend_service()->request_count() == 1UL &&-            backends_[0]->backend_service()->last_peer_identity() ==-                expected_authenticated_identity) {-          break;+        Status status = SendRpc();+        if (!status.ok()) {+          gpr_log(GPR_ERROR, ""RPC failed. code=%d message=%s Trying again."",+                  status.error_code(), status.error_message().c_str());+          continue;+        }+        if (backends_[0]->backend_service()->last_peer_identity() !=+            expected_authenticated_identity) {+          gpr_log(+              GPR_ERROR,+              ""Expected client identity does not match. (actual) %s vs ""+              ""(expected) %s Trying again."",+              absl::StrJoin(+                  backends_[0]->backend_service()->last_peer_identity(), "","")+                  .c_str(),+              absl::StrJoin(expected_authenticated_identity, "","").c_str());+          // It is possible that the new subchannel list has not been subbed in+          // yet. It will be subbed in once the subchannel connects.+          gpr_sleep_until(grpc_timeout_milliseconds_to_deadline(2000));","As discussed, I removed this sleep and instead removed the backends restarting code while increasing the number of retries. Since, we are no longer restarting backends, there is no longer a need to change the channel arguments either.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25200,561047313,2021-01-20T15:20:41Z,third_party/README.md,"@@ -48,6 +48,14 @@ Usually the process is  Updating some dependencies requires extra care. +### Updating third_party/abseil-cpp++- Two additional steps should be done before running `generate_projects.sh` above.",Sg. Instructions are useful. But it would be nice to be able to get rid of these extra steps - how hard would it be now that our Makefile has ben shrinked significantly and the extract_metadata_from_bazel.py script has stabilized?,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25200,561049612,2021-01-20T15:23:39Z,third_party/README.md,"@@ -48,6 +48,14 @@ Usually the process is  Updating some dependencies requires extra care. +### Updating third_party/abseil-cpp++- Two additional steps should be done before running `generate_projects.sh` above.","Whoa, just looked at https://github.com/grpc/grpc/blob/master/src/abseil-cpp/preprocessed_builds.yaml.gen.py - VERY complicated (and looks like it does similar sort of magic that's already done by extract_metadata_from_bazel.py)Maybe this would be a nice opportunity for simplification.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25130,561380241,2021-01-20T23:37:33Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -3057,6 +3070,32 @@ void CallData::InjectRecvInitialMetadataReadyForConfigSelectorCommitCallback(       &recv_initial_metadata_ready_; } +void CallData::RecvTrailingMetadataReadyForAdditionalErrorContext(","I don't think it's necessary to intercept the recv_trailing_metadata op to do this.  Instead, you can just modify the error here:https://github.com/grpc/grpc/blob/5b5e3f84a195929a1774f1dc69e6e8c05caa7817/src/core/ext/filters/client_channel/client_channel.cc#L2934",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25200,561761019,2021-01-21T10:24:24Z,third_party/ABSEIL_MANUAL.md,"@@ -0,0 +1,30 @@+# Abseil in gRPC++This document explains how to use Abseil throughout gRPC. Note that this isn't+supposed to explain general usage of Abseil.++## The version of Abseil++gRPC is inteded to use the LTS versions of Abseil only because it simplfies+dependency management. Abseil is being distributed via package distribution+systems such as vcpkg and cocoapods. If gRPC depends on the certain version","Btw, this is basically a absl-specific clarification of https://github.com/grpc/grpc/tree/master/third_party#guidelines-on-updating-submodules:*IMPORTANT: whenever possible, try to only update to a stable release of a library (= not to master / random commit). Depending on unreleased revisions makes gRPC installation harder for users, as it forces them to always build the dependency from source and prevents them from using more convenient installation channels (linux packages, package managers etc.)*",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25130,562139185,2021-01-21T19:23:38Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -3057,6 +3070,32 @@ void CallData::InjectRecvInitialMetadataReadyForConfigSelectorCommitCallback(       &recv_initial_metadata_ready_; } +void CallData::RecvTrailingMetadataReadyForAdditionalErrorContext(+    void* arg, grpc_error* error) {+  auto* self = static_cast<CallData*>(arg);+  GRPC_ERROR_REF(error);+  if (error != GRPC_ERROR_NONE) {+    if (self->dynamic_call_ == nullptr) {+      error = grpc_error_set_int(+          error, GRPC_ERROR_INT_OCCURRED_WHILE_AWAITING_NAME_RESOLUTION, true);","I was going to comment this in https://github.com/grpc/grpc/pull/25158, but I don't think it's necessary to pack everything into the error message string, i.e. I don't think it's necessary to avoid reliance on `debug_error_string()`.I know for sure that Ruby, Python, and C# all include the full JSON error string from https://github.com/grpc/grpc/blob/9eae29f8e988f7eb15989410e5cb2745a059a895/include/grpc/impl/codegen/grpc_types.h#L676 (the equivalent of `debug_error_string()` in their RPC exceptions.I'm not sure about PHP and Node, but we could probably modify them to include it in their RPC exceptions/errors they don't already.For C++ users, I know that at least the application code which initially motivated this actually logs `debug_error_string()` along with the RPC error status, whenever an RPC fails. So, I believe at least some number of C++ users  aware that the more verbose but detailed error message lies within `debug_error_string()`.Overall, I'm on the side of encouraging use of `debug_error_string()`, rather than putting more and more details into the error message string (which can remain as a brief description of the error).",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25130,562181996,2021-01-21T20:38:54Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -3057,6 +3070,32 @@ void CallData::InjectRecvInitialMetadataReadyForConfigSelectorCommitCallback(       &recv_initial_metadata_ready_; } +void CallData::RecvTrailingMetadataReadyForAdditionalErrorContext(","In the case where the resolver returns an error and the call is not wait_for_ready, we're already using the resolver error to fail the call:https://github.com/grpc/grpc/blob/5b5e3f84a195929a1774f1dc69e6e8c05caa7817/src/core/ext/filters/client_channel/client_channel.cc#L3139If you want to augment that error message, go for it, but since we already have a place in the code where we're generating this error, we might as well just change it to include what we want instead of taking a separate callback to change it again later.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25130,562187050,2021-01-21T20:47:54Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -3057,6 +3070,32 @@ void CallData::InjectRecvInitialMetadataReadyForConfigSelectorCommitCallback(       &recv_initial_metadata_ready_; } +void CallData::RecvTrailingMetadataReadyForAdditionalErrorContext(+    void* arg, grpc_error* error) {+  auto* self = static_cast<CallData*>(arg);+  GRPC_ERROR_REF(error);+  if (error != GRPC_ERROR_NONE) {+    if (self->dynamic_call_ == nullptr) {+      error = grpc_error_set_int(+          error, GRPC_ERROR_INT_OCCURRED_WHILE_AWAITING_NAME_RESOLUTION, true);","One problem I have with `debug_error_string()` is that it's basically exposing internal implementation details that may change.  Note that the current format of this field is based on the existing implementation of `grpc_error`.  However, that type actually causes us a lot of problems, and I would like to see us completely eliminate that type in favor of `absl::Status`.  I expect that we'll wind up adding a lot of the same info as a payload on the `absl::Status` object, but the format may not be exactly the same.  @veblush took a stretch OKR this quarter to try to come up with a design for this.  None of our public APIs should be guaranteeing that this format or its contents will be a stable part of our API.Another problem with `debug_error_string()` is that if we shove all the really useful info in there, then the normal error message field in the RPC status is basically useless.  I think that instead, we should be working to make the normal error message field contain all of the data that the user needs in the vast majority of cases, so that it becomes very rare for them to actually need to look at `debug_error_string()`.  This is consistent with how Java and Go handle their error messages (I don't think they even have an equivalent of `debug_error_string()`), and it was the intent behind #22883.",X
503812,voidzcy,https://api.github.com/repos/grpc/grpc/pulls/25193,562201450,2021-01-21T21:15:16Z,test/cpp/interop/xds_interop_client.cc,"@@ -265,9 +291,11 @@ class TestClient {     }     std::chrono::system_clock::time_point deadline =         std::chrono::system_clock::now() +-        std::chrono::seconds(absl::GetFlag(FLAGS_rpc_timeout_sec));+        std::chrono::seconds(config.timeout_sec != 0","Since this PR includes the change for the proto, which includes the change to `ClientConfigureRequest` by adding the `timeout_sec` field. What about adding the timeout value to the exiting usage of `ClientConfigureRequest` in the circuit breaking test driver? That is, we will always respect the `timeout_sec` from `RpcConfig`. Then we can eliminate the two special handling for timeout_sec here and [line 240](https://github.com/grpc/grpc/blob/2cc1abde097e49699887a79f59e1fed767d56f36/test/cpp/interop/xds_interop_client.cc#L240).",X
503812,voidzcy,https://api.github.com/repos/grpc/grpc/pulls/25193,562204914,2021-01-21T21:21:32Z,test/cpp/interop/xds_interop_client.cc,"@@ -265,9 +291,11 @@ class TestClient {     }     std::chrono::system_clock::time_point deadline =         std::chrono::system_clock::now() +-        std::chrono::seconds(absl::GetFlag(FLAGS_rpc_timeout_sec));+        std::chrono::seconds(config.timeout_sec != 0","Well, [Go](https://github.com/grpc/grpc-go/blob/7f2581f910fc21497091c4109b56d310276fc943/interop/xds/client/client.go#L390) is still using the command line option rpcTimeout as the default. But I guess one major reason is because existing uses in the test driver does not set it. So anyway, the test driver needs to make the change first before client implementations can do it.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25215,562269688,2021-01-21T23:41:45Z,src/core/ext/filters/client_channel/resolver/google_c2p/google_c2p_resolver.cc,"@@ -0,0 +1,360 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/filters/client_channel/resolver_registry.h""+#include ""src/core/ext/xds/xds_client.h""+#include ""src/core/lib/gpr/env.h""+#include ""src/core/lib/http/httpcli.h""+#include ""src/core/lib/iomgr/polling_entity.h""+#include ""src/core/lib/security/credentials/alts/check_gcp_environment.h""++namespace grpc_core {++namespace {++class GoogleCloud2ProdResolver : public Resolver {+ public:+  explicit GoogleCloud2ProdResolver(ResolverArgs args);++  void StartLocked() override;+  void RequestReresolutionLocked() override;+  void ResetBackoffLocked() override;+  void ShutdownLocked() override;++ private:+  // Represents an HTTP request to the metadata server.+  class MetadataQuery : public InternallyRefCounted<MetadataQuery> {+   public:+    MetadataQuery(RefCountedPtr<GoogleCloud2ProdResolver> resolver,+                  const char* path, grpc_polling_entity* pollent);+    ~MetadataQuery() override;++    void Orphan() override;++   private:+    static void OnHttpRequestDone(void* arg, grpc_error* error);++    void MaybeCallOnDone(grpc_error* error);++    // If error is not GRPC_ERROR_NONE, then it's not safe to look at response.+    virtual void OnDone(GoogleCloud2ProdResolver* resolver,+                        const grpc_http_response* response,+                        grpc_error* error) = 0;++    RefCountedPtr<GoogleCloud2ProdResolver> resolver_;+    grpc_httpcli_context context_;+    grpc_httpcli_response response_;+    grpc_closure on_done_;+    Atomic<bool> on_done_called_{false};+  };++  // A metadata server query to get the zone.+  class ZoneQuery : public MetadataQuery {+   public:+    ZoneQuery(RefCountedPtr<GoogleCloud2ProdResolver> resolver,+              grpc_polling_entity* pollent);++   private:+    void OnDone(GoogleCloud2ProdResolver* resolver,+                const grpc_http_response* response, grpc_error* error) override;+  };++  // A metadata server query to get the IPv6 address.+  class IPv6Query : public MetadataQuery {+   public:+    IPv6Query(RefCountedPtr<GoogleCloud2ProdResolver> resolver,+              grpc_polling_entity* pollent);++   private:+    void OnDone(GoogleCloud2ProdResolver* resolver,+                const grpc_http_response* response, grpc_error* error) override;+  };++  void ZoneQueryDone(std::string zone);+  void IPv6QueryDone(bool ipv6_supported);+  void StartXdsResolver();++  std::shared_ptr<WorkSerializer> work_serializer_;+  grpc_polling_entity pollent_;+  bool using_dns_ = false;+  OrphanablePtr<Resolver> child_resolver_;++  OrphanablePtr<ZoneQuery> zone_query_;+  absl::optional<std::string> zone_;++  OrphanablePtr<IPv6Query> ipv6_query_;+  absl::optional<bool> supports_ipv6_;+};++//+// GoogleCloud2ProdResolver::MetadataQuery+//++GoogleCloud2ProdResolver::MetadataQuery::MetadataQuery(+    RefCountedPtr<GoogleCloud2ProdResolver> resolver, const char* path,+    grpc_polling_entity* pollent)+    : resolver_(std::move(resolver)) {+  grpc_httpcli_context_init(&context_);+  // Start HTTP request.+  GRPC_CLOSURE_INIT(&on_done_, OnHttpRequestDone, this, nullptr);+  Ref().release();  // Ref held by callback.+  grpc_httpcli_request request;+  memset(&request, 0, sizeof(grpc_httpcli_request));+  grpc_http_header header = {const_cast<char*>(""Metadata-Flavor""),+                             const_cast<char*>(""Google"")};+  request.host = const_cast<char*>(""metadata.google.internal"");+  request.http.path = const_cast<char*>(path);+  request.http.hdr_count = 1;+  request.http.hdrs = &header;+  grpc_resource_quota* resource_quota =+      grpc_resource_quota_create(""c2p_resolver"");+  grpc_httpcli_get(&context_, pollent, resource_quota, &request,+                   ExecCtx::Get()->Now() + 10000,  // 10s timeout+                   &on_done_, &response_);+  grpc_resource_quota_unref_internal(resource_quota);+}++GoogleCloud2ProdResolver::MetadataQuery::~MetadataQuery() {+  grpc_httpcli_context_destroy(&context_);+}++void GoogleCloud2ProdResolver::MetadataQuery::Orphan() {+  // TODO(roth): Once the HTTP client library supports cancellation,+  // use that here.+  MaybeCallOnDone(GRPC_ERROR_CANCELLED);+  Unref();+}++void GoogleCloud2ProdResolver::MetadataQuery::OnHttpRequestDone(+    void* arg, grpc_error* error) {+  auto* self = static_cast<MetadataQuery*>(arg);+  self->MaybeCallOnDone(error);+  grpc_http_response_destroy(&self->response_);","This looks like a potentially use-after-free since we are destroying `response_` here, but in `MaybeCallDone` we just potentially passed it to a closure scheduled on the work serializer.Any reason to not destroy `response_` in the `MetadataQuery`'s dtor?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25215,562764128,2021-01-22T16:49:41Z,src/core/ext/filters/client_channel/resolver/google_c2p/google_c2p_resolver.cc,"@@ -0,0 +1,360 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/filters/client_channel/resolver_registry.h""+#include ""src/core/ext/xds/xds_client.h""+#include ""src/core/lib/gpr/env.h""+#include ""src/core/lib/http/httpcli.h""+#include ""src/core/lib/iomgr/polling_entity.h""+#include ""src/core/lib/security/credentials/alts/check_gcp_environment.h""++namespace grpc_core {++namespace {++class GoogleCloud2ProdResolver : public Resolver {+ public:+  explicit GoogleCloud2ProdResolver(ResolverArgs args);++  void StartLocked() override;+  void RequestReresolutionLocked() override;+  void ResetBackoffLocked() override;+  void ShutdownLocked() override;++ private:+  // Represents an HTTP request to the metadata server.+  class MetadataQuery : public InternallyRefCounted<MetadataQuery> {+   public:+    MetadataQuery(RefCountedPtr<GoogleCloud2ProdResolver> resolver,+                  const char* path, grpc_polling_entity* pollent);+    ~MetadataQuery() override;++    void Orphan() override;++   private:+    static void OnHttpRequestDone(void* arg, grpc_error* error);++    void MaybeCallOnDone(grpc_error* error);++    // If error is not GRPC_ERROR_NONE, then it's not safe to look at response.+    virtual void OnDone(GoogleCloud2ProdResolver* resolver,+                        const grpc_http_response* response,+                        grpc_error* error) = 0;++    RefCountedPtr<GoogleCloud2ProdResolver> resolver_;+    grpc_httpcli_context context_;+    grpc_httpcli_response response_;+    grpc_closure on_done_;+    Atomic<bool> on_done_called_{false};+  };++  // A metadata server query to get the zone.+  class ZoneQuery : public MetadataQuery {+   public:+    ZoneQuery(RefCountedPtr<GoogleCloud2ProdResolver> resolver,+              grpc_polling_entity* pollent);++   private:+    void OnDone(GoogleCloud2ProdResolver* resolver,+                const grpc_http_response* response, grpc_error* error) override;+  };++  // A metadata server query to get the IPv6 address.+  class IPv6Query : public MetadataQuery {+   public:+    IPv6Query(RefCountedPtr<GoogleCloud2ProdResolver> resolver,+              grpc_polling_entity* pollent);++   private:+    void OnDone(GoogleCloud2ProdResolver* resolver,+                const grpc_http_response* response, grpc_error* error) override;+  };++  void ZoneQueryDone(std::string zone);+  void IPv6QueryDone(bool ipv6_supported);+  void StartXdsResolver();++  std::shared_ptr<WorkSerializer> work_serializer_;+  grpc_polling_entity pollent_;+  bool using_dns_ = false;+  OrphanablePtr<Resolver> child_resolver_;++  OrphanablePtr<ZoneQuery> zone_query_;+  absl::optional<std::string> zone_;++  OrphanablePtr<IPv6Query> ipv6_query_;+  absl::optional<bool> supports_ipv6_;+};++//+// GoogleCloud2ProdResolver::MetadataQuery+//++GoogleCloud2ProdResolver::MetadataQuery::MetadataQuery(+    RefCountedPtr<GoogleCloud2ProdResolver> resolver, const char* path,+    grpc_polling_entity* pollent)+    : resolver_(std::move(resolver)) {+  grpc_httpcli_context_init(&context_);+  // Start HTTP request.+  GRPC_CLOSURE_INIT(&on_done_, OnHttpRequestDone, this, nullptr);+  Ref().release();  // Ref held by callback.+  grpc_httpcli_request request;+  memset(&request, 0, sizeof(grpc_httpcli_request));+  grpc_http_header header = {const_cast<char*>(""Metadata-Flavor""),+                             const_cast<char*>(""Google"")};+  request.host = const_cast<char*>(""metadata.google.internal"");+  request.http.path = const_cast<char*>(path);+  request.http.hdr_count = 1;+  request.http.hdrs = &header;+  grpc_resource_quota* resource_quota =+      grpc_resource_quota_create(""c2p_resolver"");+  grpc_httpcli_get(&context_, pollent, resource_quota, &request,+                   ExecCtx::Get()->Now() + 10000,  // 10s timeout+                   &on_done_, &response_);+  grpc_resource_quota_unref_internal(resource_quota);+}++GoogleCloud2ProdResolver::MetadataQuery::~MetadataQuery() {+  grpc_httpcli_context_destroy(&context_);+}++void GoogleCloud2ProdResolver::MetadataQuery::Orphan() {+  // TODO(roth): Once the HTTP client library supports cancellation,+  // use that here.+  MaybeCallOnDone(GRPC_ERROR_CANCELLED);+  Unref();+}++void GoogleCloud2ProdResolver::MetadataQuery::OnHttpRequestDone(+    void* arg, grpc_error* error) {+  auto* self = static_cast<MetadataQuery*>(arg);+  self->MaybeCallOnDone(error);+  grpc_http_response_destroy(&self->response_);+  self->Unref();+}++void GoogleCloud2ProdResolver::MetadataQuery::MaybeCallOnDone(+    grpc_error* error) {+  bool expected = false;+  if (on_done_called_.CompareExchangeStrong(+          &expected, true, MemoryOrder::RELAXED, MemoryOrder::RELAXED)) {+    // Hop back into WorkSerializer.+    Ref().release();  // Ref held by callback.","Actually, I should have reffed it in the caller, because this function should take ownership of it, as per https://github.com/grpc/grpc/blob/master/doc/core/grpc-error.md#ownership-rules.Fixed.",X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/25103,564094364,2021-01-25T22:43:55Z,test/cpp/test/mock_stream_test.cc,"@@ -0,0 +1,60 @@+/*+ *+ * Copyright 2020 the gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpcpp/test/mock_stream.h>+#include <gtest/gtest.h>++#include ""absl/memory/memory.h""++#include ""src/proto/grpc/testing/echo.grpc.pb.h""++using grpc::testing::EchoRequest;+using grpc::testing::EchoResponse;++TEST(MockStreamTest, Basic) {+  auto cr = absl::make_unique<grpc::testing::MockClientReader<EchoResponse>>();+  ASSERT_NE(cr, nullptr);","Yeah, this is basically to make sure that they're not abstract classes. (meaning all methods are mocked) I guess this should be enough.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25130,564925725,2021-01-27T00:11:01Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -3057,6 +3070,32 @@ void CallData::InjectRecvInitialMetadataReadyForConfigSelectorCommitCallback(       &recv_initial_metadata_ready_; } +void CallData::RecvTrailingMetadataReadyForAdditionalErrorContext(+    void* arg, grpc_error* error) {+  auto* self = static_cast<CallData*>(arg);+  GRPC_ERROR_REF(error);+  if (error != GRPC_ERROR_NONE) {+    if (self->dynamic_call_ == nullptr) {+      error = grpc_error_set_int(+          error, GRPC_ERROR_INT_OCCURRED_WHILE_AWAITING_NAME_RESOLUTION, true);","I took a look at how we might add something along the lines of ""happening during name resolution"" to RPC error messages, but one issue I'm having here is that I can't see how to augment this message in a way that will reliably lead to something that is human-readable, while still preserving the information from the original error message.Looking at how the code in https://github.com/grpc/grpc/blob/0a0aae8ee8f8dd1ea0d4a58ae8ac9700f1b82a4a/src/core/lib/transport/error_utils.cc#L48 determines the RPC error message from a `grpc_error*`, it looks complicated to pluck out the error message from the original `grpc_error`, and add ""happened during name resolution"" to it in a way that makes sense to a reader of the final message.On the other hand, if we just dump the full error object as a JSON string, as `debug_error_string()` does, then IMO the result is actually more legible, simply because JSON is a nice way to organize disparate pieces of information which have been added by different subsystems (for example, the client channel's error message augmentation shouldn't stomp on an error message that was generated by the deadline filter or a resolver).If there ends up being a way to add these different pieces of information to the resulting error message in a reliably legible way, then I'm open to use that. Otherwise, for `debug_error_string()` still seems like the most effective thing to leverage.",X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/25103,566278792,2021-01-28T17:33:46Z,test/cpp/test/mock_stream_test.cc,"@@ -0,0 +1,60 @@+/*+ *+ * Copyright 2020 the gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpcpp/test/mock_stream.h>+#include <gtest/gtest.h>++#include ""absl/memory/memory.h""++#include ""src/proto/grpc/testing/echo.grpc.pb.h""++using grpc::testing::EchoRequest;+using grpc::testing::EchoResponse;++TEST(MockStreamTest, Basic) {+  auto cr = absl::make_unique<grpc::testing::MockClientReader<EchoResponse>>();+  ASSERT_NE(cr, nullptr);","I don't think it gives more test coverage, though because it's testing the mock feature of gtest. If the class is ready to be mockable, I think it's enough for this test? This test is designed to make sure those classes are mockable. (Previously those were not)",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25310,567096228,2021-01-29T21:11:55Z,src/core/ext/xds/xds_http_filters.h,"@@ -0,0 +1,111 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#ifndef GRPC_CORE_EXT_XDS_XDS_HTTP_FILTERS_H+#define GRPC_CORE_EXT_XDS_XDS_HTTP_FILTERS_H++#include <grpc/support/port_platform.h>++#include <memory>+#include <string>++#include ""absl/status/statusor.h""+#include ""absl/strings/str_cat.h""+#include ""absl/strings/string_view.h""+#include ""google/protobuf/any.upb.h""+#include ""upb/def.h""++#include <grpc/grpc.h>++#include ""src/core/lib/channel/channel_stack.h""+#include ""src/core/lib/json/json.h""++namespace grpc_core {++extern const char* kXdsHttpRouterFilterConfigName;++class XdsHttpFilterImpl {","Should we add a field for the well-known name of the filter? E.g., ""envoy.fault"" should be used by fault injection filter instead of authz filter.In v2's listener, the filter config can be empty: https://paste.googleplex.com/5060574406770688.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25310,567131088,2021-01-29T22:36:00Z,src/core/ext/xds/xds_http_filters.cc,"@@ -0,0 +1,93 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/xds/xds_http_filters.h""++#include ""envoy/extensions/filters/http/router/v3/router.upbdefs.h""++namespace grpc_core {++const char* kXdsHttpRouterFilterConfigName =+    ""extensions.filters.http.router.v3.Router"";++namespace {++class XdsHttpRouterFilter : public XdsHttpFilterImpl {","In general, I do expect that individual filters will be implemented in their own files.  However, the router filter in particular is special, because it's basically required at the end of the filter chain, so I think it's fine to have it here.Also, though, keep in mind that this class will not include the filter's actual functionality; that will be implemented in a C-core filter.  This class is just the glue necessary to allow a C-core filter to be configured via xDS.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25310,567151949,2021-01-29T23:40:35Z,src/core/ext/xds/xds_http_filters.h,"@@ -0,0 +1,111 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#ifndef GRPC_CORE_EXT_XDS_XDS_HTTP_FILTERS_H+#define GRPC_CORE_EXT_XDS_XDS_HTTP_FILTERS_H++#include <grpc/support/port_platform.h>++#include <memory>+#include <string>++#include ""absl/status/statusor.h""+#include ""absl/strings/str_cat.h""+#include ""absl/strings/string_view.h""+#include ""google/protobuf/any.upb.h""+#include ""upb/def.h""++#include <grpc/grpc.h>++#include ""src/core/lib/channel/channel_stack.h""+#include ""src/core/lib/json/json.h""++namespace grpc_core {++extern const char* kXdsHttpRouterFilterConfigName;++class XdsHttpFilterImpl {+ public:+  struct FilterConfig {+    absl::string_view config_proto_type_name;+    Json config;++    bool operator==(const FilterConfig& other) const {+      return config_proto_type_name == other.config_proto_type_name &&+             config == other.config;+    }+    std::string ToString() const {+      return absl::StrCat(""{config_proto_type_name="", config_proto_type_name,+                          "" config="", config.Dump(), ""}"");+    }+  };++  virtual ~XdsHttpFilterImpl() = default;++  // Filter config protobuf type name.+  virtual absl::string_view config_proto_type_name() const = 0;++  // Loads the proto message into the upb symtab.+  virtual void PopulateSymtab(upb_symtab* symtab) const = 0;++  // Generates a Config from the xDS filter config proto.+  // Used for the top-level config in the HCM HTTP filter list.+  virtual absl::StatusOr<FilterConfig> GenerateFilterConfig(+      upb_strview serialized_xds_config, upb_arena* arena) const = 0;++  // Generates a Config from the xDS filter config proto.+  // Used for the typed_per_filter_config override in VirtualHost and Route.+  virtual absl::StatusOr<FilterConfig> GenerateFilterConfigOverride(+      upb_strview serialized_xds_config, upb_arena* arena) const = 0;++  // C-core channel filter implementation.+  virtual const grpc_channel_filter* channel_filter() const = 0;++  // Modifies channel args that may affect service config parsing (not+  // visible to the channel as a whole).+  // Takes ownership of args.  Caller takes ownership of return value.+  virtual grpc_channel_args* ModifyChannelArgs(grpc_channel_args* args) const {+    return args;+  }++  // Function to convert the Configs into a JSON string to be added to the+  // per-method part of the service config.+  // The hcm_filter_config comes from the HttpConnectionManager config.+  // The filter_config_override comes from the first of the VirtualHost,+  // Route, or ClusterWeight entries that it is found in, or null if+  // there is no override in any of those locations.+  virtual absl::StatusOr<std::string> GenerateServiceConfig(+      const FilterConfig& hcm_filter_config,+      const FilterConfig* filter_config_override) const = 0;","So far, I haven't find a place in Envoy's code base to perform a merge for filter configs. The callback invocation changes the entire FilterConfig object. Keep this method leaves flexibility. Out of curiosity, is there a filter config performs merge?https://github.com/envoyproxy/envoy/blob/1d1b708c7bf6efa02c41d9ce22cbf1e4a1aeec2c/source/extensions/filters/http/fault/config.cc#L14https://github.com/envoyproxy/envoy/blob/1d1b708c7bf6efa02c41d9ce22cbf1e4a1aeec2c/source/common/filter/http/filter_config_discovery_impl.cc#L65",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25310,568039607,2021-02-01T18:19:02Z,src/core/ext/xds/xds_http_filters.h,"@@ -0,0 +1,111 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#ifndef GRPC_CORE_EXT_XDS_XDS_HTTP_FILTERS_H+#define GRPC_CORE_EXT_XDS_XDS_HTTP_FILTERS_H++#include <grpc/support/port_platform.h>++#include <memory>+#include <string>++#include ""absl/status/statusor.h""+#include ""absl/strings/str_cat.h""+#include ""absl/strings/string_view.h""+#include ""google/protobuf/any.upb.h""+#include ""upb/def.h""++#include <grpc/grpc.h>++#include ""src/core/lib/channel/channel_stack.h""+#include ""src/core/lib/json/json.h""++namespace grpc_core {++extern const char* kXdsHttpRouterFilterConfigName;++class XdsHttpFilterImpl {+ public:+  struct FilterConfig {+    absl::string_view config_proto_type_name;+    Json config;++    bool operator==(const FilterConfig& other) const {+      return config_proto_type_name == other.config_proto_type_name &&+             config == other.config;+    }+    std::string ToString() const {+      return absl::StrCat(""{config_proto_type_name="", config_proto_type_name,+                          "" config="", config.Dump(), ""}"");+    }+  };++  virtual ~XdsHttpFilterImpl() = default;++  // Filter config protobuf type name.+  virtual absl::string_view config_proto_type_name() const = 0;++  // Loads the proto message into the upb symtab.+  virtual void PopulateSymtab(upb_symtab* symtab) const = 0;++  // Generates a Config from the xDS filter config proto.+  // Used for the top-level config in the HCM HTTP filter list.+  virtual absl::StatusOr<FilterConfig> GenerateFilterConfig(+      upb_strview serialized_xds_config, upb_arena* arena) const = 0;++  // Generates a Config from the xDS filter config proto.+  // Used for the typed_per_filter_config override in VirtualHost and Route.+  virtual absl::StatusOr<FilterConfig> GenerateFilterConfigOverride(+      upb_strview serialized_xds_config, upb_arena* arena) const = 0;++  // C-core channel filter implementation.+  virtual const grpc_channel_filter* channel_filter() const = 0;++  // Modifies channel args that may affect service config parsing (not+  // visible to the channel as a whole).+  // Takes ownership of args.  Caller takes ownership of return value.+  virtual grpc_channel_args* ModifyChannelArgs(grpc_channel_args* args) const {+    return args;+  }++  // Function to convert the Configs into a JSON string to be added to the+  // per-method part of the service config.+  // The hcm_filter_config comes from the HttpConnectionManager config.+  // The filter_config_override comes from the first of the VirtualHost,+  // Route, or ClusterWeight entries that it is found in, or null if+  // there is no override in any of those locations.+  virtual absl::StatusOr<std::string> GenerateServiceConfig(+      const FilterConfig& hcm_filter_config,+      const FilterConfig* filter_config_override) const = 0;","Envoy doesn't proactively merge the configs that way that gRPC will do.  Instead, it provides an API called `resolveMostSpecificPerFilterConfig()` for individual filters to explicitly request their override config when they want it, which ultimately calls the following code:https://github.com/envoyproxy/envoy/blob/17e815122ff53d0ac6cb2d64cdbf1bfc547bb7e8/source/common/http/utility.cc#L915There's apparently also a `mostSpecificPerFilterConfigTyped()` that does basically the same thing as `resolveMostSpecificPerFilterConfig()`, although the code is a lot harder to follow.",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/25303,568125747,2021-02-01T20:42:16Z,src/php/ext/grpc/channel_credentials.c,"@@ -236,6 +236,47 @@ PHP_METHOD(ChannelCredentials, createInsecure) {   RETURN_NULL(); } +/**+ * Create XDS channel credentials+ * @param ChannelCredentials|null $fallback_creds  The fallback credentials used+ * if the channel target does not have the 'xds:///' scheme or if the xDS+ * control plane does not provide information on how to fetch credentials+ * dynamically.+ * @return ChannelCredentials The xDS channel credentials object+ */+PHP_METHOD(ChannelCredentials, createXds) {+  grpc_channel_credentials* xds_creds = NULL;+  zval* fallback_creds = NULL;+  if (zend_parse_parameters_ex(0,  // ZEND_PARSE_PARAMS_QUIET,+                               ZEND_NUM_ARGS() TSRMLS_CC, ""O!"", &fallback_creds,+                               grpc_ce_channel_credentials) != SUCCESS) {+    zend_throw_exception(spl_ce_InvalidArgumentException,+                         ""createXds expects a fallback credentials"",+                         1 TSRMLS_CC);+    return;+  }+  char* fallback_creds_hash_str = """";+  if (!fallback_creds) {",`fallback_creds` is a required parameter being passed in from PHP userland. So the user would have to have done```$insecure_creds = ChannelCredentials::createInsecure();createXds($insecure_creds);```for `fallback_creds` here to continue to be `NULL`. Would this work?,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/25295,568199638,2021-02-01T22:55:44Z,src/core/lib/gprpp/time_util.cc,"@@ -0,0 +1,81 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include <grpc/support/log.h>++#include ""src/core/lib/gprpp/time_util.h""++namespace grpc_core {++gpr_timespec ToGprTimeSpec(absl::Duration duration) {+  if (duration == absl::InfiniteDuration()) {+    return gpr_inf_future(GPR_TIMESPAN);+  } else if (duration == -absl::InfiniteDuration()) {+    return gpr_inf_past(GPR_TIMESPAN);+  } else {+    int64_t s = absl::IDivDuration(duration, absl::Seconds(1), &duration);+    int64_t n = absl::IDivDuration(duration, absl::Nanoseconds(1), &duration);+    return gpr_time_add(gpr_time_from_seconds(s, GPR_TIMESPAN),+                        gpr_time_from_nanos(n, GPR_TIMESPAN));+  }+}++gpr_timespec ToGprTimeSpec(absl::Time time) {+  if (time == absl::InfiniteFuture()) {+    return gpr_inf_future(GPR_CLOCK_REALTIME);+  } else if (time == absl::InfinitePast()) {+    return gpr_inf_past(GPR_CLOCK_REALTIME);+  } else {+    timespec ts = absl::ToTimespec(time);+    gpr_timespec out;+    out.tv_sec = static_cast<decltype(out.tv_sec)>(ts.tv_sec);+    out.tv_nsec = static_cast<decltype(out.tv_nsec)>(ts.tv_nsec);+    out.clock_type = GPR_CLOCK_REALTIME;",Abseil time only represents real-time which is based on unix-epoch. Other clock types in gRPC are not based on the same base point so they're not technically mapped to absl-time. It has to involve a clock conversion.,X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/25295,568259975,2021-02-02T01:31:01Z,src/core/lib/gprpp/time_util.cc,"@@ -0,0 +1,81 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include <grpc/support/log.h>++#include ""src/core/lib/gprpp/time_util.h""++namespace grpc_core {++gpr_timespec ToGprTimeSpec(absl::Duration duration) {+  if (duration == absl::InfiniteDuration()) {+    return gpr_inf_future(GPR_TIMESPAN);+  } else if (duration == -absl::InfiniteDuration()) {+    return gpr_inf_past(GPR_TIMESPAN);+  } else {+    int64_t s = absl::IDivDuration(duration, absl::Seconds(1), &duration);+    int64_t n = absl::IDivDuration(duration, absl::Nanoseconds(1), &duration);+    return gpr_time_add(gpr_time_from_seconds(s, GPR_TIMESPAN),","For the case where `duration` is `absl::Nanoseconds(-10)`, `s` will be `0` and `n` will be `-10` but `gpr_timespec` will get `-1` for `tv_sec` and `999999990` for `tv_nsec`. It is handled by [this](https://github.com/grpc/grpc/blob/59c38f4e4daf88062b274c7a4f7bbd5db614ef3a/src/core/lib/gpr/time.cc#L79-L88).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25318,568632227,2021-02-02T14:13:32Z,test/distrib/bazel/test_single_bazel_version.sh,"@@ -0,0 +1,45 @@+#!/usr/bin/env bash+# Copyright 2021 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++shopt -s nullglob++if [ ""$#"" != ""1"" ] ; then+    echo ""Must supply bazel version to be tested."" >/dev/stderr+    exit 1+fi++VERSION=""$1""++cd ""$(dirname ""$0"")""/../../..++EXCEPTIONS=(+  # iOS platform fails the analysis phase since there is no toolchain available+  # for it.+  ""//src/objective-c/...""+  ""//third_party/objective_c/...""++  # This could be a legitmate failure due to bitrot.+  ""//src/proto/grpc/testing:test_gen_proto""++  # This appears to be a legitimately broken BUILD file. There's a reference to+  # a non-existent ""link_dynamic_library.sh"".+   ""//third_party/toolchains/bazel_0.26.0_rbe_windows:all""+)+++export OVERRIDE_BAZEL_VERSION=""$VERSION""+printf ' -%s' ""${EXCEPTIONS[@]}"" | xargs bazel build -- //...","nit: in internal blueprints, the excluded targets ale listed with the ""-"" prefix. Maybe skip the unnecessary xargs magic and just do:EXCLUDED_TARGETS = ( ""-//excluded1"" ""-//foo"" ""-//bar"") and just pass the EXCLUDED_TARGETS directly to the bazel command.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25318,568637035,2021-02-02T14:19:32Z,WORKSPACE,"@@ -18,24 +18,14 @@ register_toolchains(     ""//third_party/toolchains/bazel_0.26.0_rbe_windows:cc-toolchain-x64_windows"", ) -load(""@bazel_toolchains//rules/exec_properties:exec_properties.bzl"", ""create_exec_properties_dict"", ""custom_exec_properties"", ""merge_dicts"")+load(""@bazel_toolchains//rules/exec_properties:exec_properties.bzl"", ""create_rbe_exec_properties_dict"", ""create_exec_properties_dict"", ""custom_exec_properties"", ""merge_dicts"")+load(""//third_party/toolchains:backwards_compatibility.bzl"", ""bc_exec_properties"")  custom_exec_properties(     name = ""grpc_custom_exec_properties"",     constants = {-        ""LARGE_MACHINE"": merge_dicts(-            create_exec_properties_dict(),-            # TODO(jtattermusch): specifying 'labels = {""abc"": ""xyz""}' in create_exec_properties_dict-            # is not possible without https://github.com/bazelbuild/bazel-toolchains/pull/748-            # and currently the toolchain we're using is too old for that. To be able to select worker-            # pools through labels, we use a workaround and populate the corresponding label values-            # manually (see create_exec_properties_dict logic for how labels get transformed)-            # Remove this workaround once we transition to a new-enough bazel toolchain.-            # The next line corresponds to 'labels = {""os"": ""ubuntu"", ""machine_size"": ""large""}'-            {-                ""label:os"": ""ubuntu"",-                ""label:machine_size"": ""large"",-            },+        ""LARGE_MACHINE"": bc_exec_properties(+            [{}, {""os"": ""ubuntu"", ""machine_size"": ""large""}],","Ah, I see that corresponds to the parameterless create_exec_properties_dict().",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25318,568637401,2021-02-02T14:20:03Z,WORKSPACE,"@@ -18,24 +18,14 @@ register_toolchains(     ""//third_party/toolchains/bazel_0.26.0_rbe_windows:cc-toolchain-x64_windows"", ) -load(""@bazel_toolchains//rules/exec_properties:exec_properties.bzl"", ""create_exec_properties_dict"", ""custom_exec_properties"", ""merge_dicts"")+load(""@bazel_toolchains//rules/exec_properties:exec_properties.bzl"", ""create_rbe_exec_properties_dict"", ""create_exec_properties_dict"", ""custom_exec_properties"", ""merge_dicts"")","You no longer need to load any of ""create_rbe_exec_properties_dict"", ""create_exec_properties_dict"", ""custom_exec_properties"", ""merge_dicts""?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25318,568645229,2021-02-02T14:29:53Z,third_party/toolchains/backwards_compatibility.bzl,"@@ -0,0 +1,45 @@+# Copyright 2021 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++load(""@bazel_toolchains//rules/exec_properties:exec_properties.bzl"", ""create_exec_properties_dict"", ""create_rbe_exec_properties_dict"",""merge_dicts"")++def bc_exec_properties(exec_properties):","I think this function deserves a comment. Basically, for newer bazel toolchains we should just be invoking `create_rbe_exec_properties_dict(xyz=abc, foo=bar, labels = { ... })` but we cannot do that for backward compatibility. There should be a comment that provides details of what doesn't work on which bazel/toolchain version.https://github.com/grpc/grpc/pull/25001/files#diff-5493ff8e9397811510e780de47c57abb70137f1afe85d1519130dc3679d60ce5R40",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25318,568652444,2021-02-02T14:38:37Z,test/distrib/bazel/run_bazel_distrib_test.sh,"@@ -0,0 +1,31 @@+#!/usr/bin/env bash+# Copyright 2021 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++shopt -s nullglob++cd ""$(dirname ""$0"")""++SUPPORTED_VERSIONS=(+  ""1.2.1""+  ""2.2.0""+  ""3.7.2""+  ""4.0.0""+)++for VERSION in ""${SUPPORTED_VERSIONS[@]}""; do+    ./test_single_bazel_version.sh ""$VERSION""","nit: the output of 4 consecutive builds is going to be difficult to parse by humans.Ideally these would be 4 independent ""tasks"".",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25318,568655531,2021-02-02T14:42:16Z,test/distrib/bazel/test_single_bazel_version.sh,"@@ -0,0 +1,45 @@+#!/usr/bin/env bash+# Copyright 2021 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++shopt -s nullglob++if [ ""$#"" != ""1"" ] ; then+    echo ""Must supply bazel version to be tested."" >/dev/stderr+    exit 1+fi++VERSION=""$1""++cd ""$(dirname ""$0"")""/../../..++EXCEPTIONS=(+  # iOS platform fails the analysis phase since there is no toolchain available+  # for it.+  ""//src/objective-c/...""+  ""//third_party/objective_c/...""++  # This could be a legitmate failure due to bitrot.+  ""//src/proto/grpc/testing:test_gen_proto""++  # This appears to be a legitimately broken BUILD file. There's a reference to+  # a non-existent ""link_dynamic_library.sh"".+   ""//third_party/toolchains/bazel_0.26.0_rbe_windows:all""+)+++export OVERRIDE_BAZEL_VERSION=""$VERSION""+printf ' -%s' ""${EXCEPTIONS[@]}"" | xargs bazel build -- //...","this can be addressed later, but one thing this distribtest doesn't cover is depending upon grpc as a library in a simple ""helloworld"" project. Like this we will know that gRPC builds fine, but not whether it can be safely imported as a dependency.",
148994,clayg,https://api.github.com/repos/grpc/grpc/pulls/25351,570340596,2021-02-04T15:58:46Z,src/python/grpcio/grpc/aio/_call.py,"@@ -383,9 +385,9 @@ def _raise_for_different_style(self, style: _APIStyle):             raise cygrpc.UsageError(_API_STYLE_ERROR)      def cancel(self) -> bool:+        if self._async_request_poller is not None:+            self._async_request_poller.cancel()         if super().cancel():-            if self._async_request_poller is not None:-                self._async_request_poller.cancel()","a number of the other classes/Mixins override cancel to shutdown their background tasks - but they ALL guard the super (which will return False if the underlying cython_call is already done).Makes me wonder if cancel isn't the best way to spell ""you've got all the await you're going to get - time to shutdown anything you started if it's not already done""",
148994,clayg,https://api.github.com/repos/grpc/grpc/pulls/25351,570342214,2021-02-04T16:00:40Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -625,6 +625,27 @@ async def test_call_rpc_error(self):             self.assertTrue(call.done())             self.assertEqual(grpc.StatusCode.UNAVAILABLE, await call.code()) +    async def test_call_rpc_error_with_async_generator(self):+        request = messages_pb2.StreamingOutputCallRequest()++        # We fail at connection, this gets started but never runs+        async def request_iterator():+            for _ in range(_NUM_STREAM_RESPONSES):+                yield request++        async with aio.insecure_channel(UNREACHABLE_TARGET) as channel:+            stub = test_pb2_grpc.TestServiceStub(channel)+            call = stub.StreamingInputCall(request_iterator(),+                                           timeout=_SHORT_TIMEOUT_S)",without this timeout this test is really long (like other in this module),X
148994,clayg,https://api.github.com/repos/grpc/grpc/pulls/25351,570345881,2021-02-04T16:05:25Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -458,6 +470,29 @@ async def test_error_without_raise_in_unary_unary(self):         rpc_error = exception_context.exception         self.assertEqual(grpc.StatusCode.INTERNAL, rpc_error.code()) +    async def test_error_without_raise_in_stream_unary(self):++        async def request_iterator():+            for _ in range(_NUM_STREAM_REQUESTS):+                yield _REQUEST++        call = self._channel.stream_unary(+            _ERROR_WITHOUT_RAISE_IN_STREAM_UNARY)(request_iterator())++        self.assertEqual(grpc.StatusCode.INTERNAL, await call.code())+        self.assertFalse(call.cancelled())+        self.assertTrue(call._async_request_poller.done())++        call = self._channel.stream_unary(+            _ERROR_WITHOUT_RAISE_IN_STREAM_UNARY)(request_iterator())++        with self.assertRaises(aio.AioRpcError) as exception_context:+            await call+        self.assertEqual(grpc.StatusCode.INTERNAL,+                         exception_context.exception.code())+        self.assertFalse(call.cancelled())+        self.assertTrue(call._async_request_poller.done())","What I'd really like to assert is that after my change you don't get THIS:```$ pytest src/python/grpcio_tests/tests_aio/unit/server_test.py::TestServer::test_error_without_raise_in_stream_unary=========================================================================================== test session starts ===========================================================================================platform linux -- Python 3.8.5, pytest-6.2.2, py-1.10.0, pluggy-0.13.1rootdir: /home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio_testscollected 1 item                                                                                                                                                                                          src/python/grpcio_tests/tests_aio/unit/server_test.py .                                                                                                                                             [100%]============================================================================================ warnings summary =============================================================================================tests_aio/unit/server_test.py::TestServer::test_error_without_raise_in_stream_unary  /home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_server.py:53: DeprecationWarning: The loop argument is deprecated since Python 3.8, and scheduled for removal in Python 3.10.    self._server = cygrpc.AioServer(tests_aio/unit/server_test.py::TestServer::test_error_without_raise_in_stream_unarytests_aio/unit/server_test.py::TestServer::test_error_without_raise_in_stream_unary  /home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py:369: DeprecationWarning: The loop argument is deprecated since Python 3.8, and scheduled for removal in Python 3.10.    self._metadata_sent = asyncio.Event(loop=self._loop)-- Docs: https://docs.pytest.org/en/stable/warnings.html====================================================================================== 1 passed, 3 warnings in 0.12s ======================================================================================Task exception was never retrievedfuture: <Task finished name='Task-8' coro=<_StreamRequestMixin._consume_request_iterator() done, defined at /home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py:396> exception=ExecuteBatchError('Failed ""execute_batch"": (<grpc._cython.cygrpc.SendMessageOperation object at 0x7fe678ac59b0>,)') created at /home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py:374>source_traceback: Object created at (most recent call last):  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/bin/pytest"", line 8, in <module>    sys.exit(console_main())  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 185, in console_main    code = main()  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main    ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__    return self._hookexec(self, self.get_hookimpls(), kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec    return self._inner_hookexec(hook, methods, kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>    self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall    res = hook_impl.function(*args)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main    return wrap_session(config, _main)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/main.py"", line 269, in wrap_session    session.exitstatus = doit(config, session) or 0  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/main.py"", line 323, in _main    config.hook.pytest_runtestloop(session=session)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__    return self._hookexec(self, self.get_hookimpls(), kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec    return self._inner_hookexec(hook, methods, kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>    self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall    res = hook_impl.function(*args)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/main.py"", line 348, in pytest_runtestloop    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__    return self._hookexec(self, self.get_hookimpls(), kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec    return self._inner_hookexec(hook, methods, kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>    self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall    res = hook_impl.function(*args)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 109, in pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 126, in runtestprotocol    reports.append(call_and_report(item, ""call"", log))  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 215, in call_and_report    call = call_runtest_hook(item, when, **kwds)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 254, in call_runtest_hook    return CallInfo.from_call(  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 311, in from_call    result: Optional[TResult] = func()  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 255, in <lambda>    lambda: ihook(item=item, **kwds), when=when, reraise=reraise  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__    return self._hookexec(self, self.get_hookimpls(), kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec    return self._inner_hookexec(hook, methods, kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>    self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall    res = hook_impl.function(*args)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 162, in pytest_runtest_call    item.runtest()  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/unittest.py"", line 321, in runtest    self._testcase(result=self)  # type: ignore[arg-type]  File ""/usr/lib/python3.8/unittest/case.py"", line 736, in __call__    return self.run(*args, **kwds)  File ""/usr/lib/python3.8/unittest/case.py"", line 676, in run    self._callTestMethod(testMethod)  File ""/usr/lib/python3.8/unittest/case.py"", line 633, in _callTestMethod    method()  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio_tests/tests_aio/unit/_test_base.py"", line 31, in wrapper    return loop.run_until_complete(f(*args, **kwargs))  File ""/usr/lib/python3.8/asyncio/base_events.py"", line 603, in run_until_complete    self.run_forever()  File ""/usr/lib/python3.8/asyncio/base_events.py"", line 570, in run_forever    self._run_once()  File ""/usr/lib/python3.8/asyncio/base_events.py"", line 1851, in _run_once    handle._run()  File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run    self._context.run(self._callback, *self._args)  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio_tests/tests_aio/unit/server_test.py"", line 486, in test_error_without_raise_in_stream_unary    call = self._channel.stream_unary(  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_channel.py"", line 182, in __call__    call = StreamUnaryCall(request_iterator, deadline, metadata,  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py"", line 576, in __init__    self._init_stream_request_mixin(request_iterator)  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py"", line 374, in _init_stream_request_mixin    self._async_request_poller = self._loop.create_task(Traceback (most recent call last):  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py"", line 403, in _consume_request_iterator    await self._write(request)  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py"", line 429, in _write    await self._cython_call.send_serialized_message(serialized_request)  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"", line 370, in send_serialized_message    await _send_message(self,  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi"", line 147, in _send_message    await execute_batch(grpc_call_wrapper, ops, loop)  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi"", line 98, in execute_batch    await futuregrpc._cython.cygrpc.ExecuteBatchError: Failed ""execute_batch"": (<grpc._cython.cygrpc.SendMessageOperation object at 0x7fe678ac59b0>,)Task exception was never retrievedfuture: <Task finished name='Task-4' coro=<_StreamRequestMixin._consume_request_iterator() done, defined at /home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py:396> exception=ExecuteBatchError('Failed ""execute_batch"": (<grpc._cython.cygrpc.SendMessageOperation object at 0x7fe678ac5d30>,)') created at /home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py:374>source_traceback: Object created at (most recent call last):  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/bin/pytest"", line 8, in <module>    sys.exit(console_main())  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 185, in console_main    code = main()  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/config/__init__.py"", line 162, in main    ret: Union[ExitCode, int] = config.hook.pytest_cmdline_main(  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__    return self._hookexec(self, self.get_hookimpls(), kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec    return self._inner_hookexec(hook, methods, kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>    self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall    res = hook_impl.function(*args)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/main.py"", line 316, in pytest_cmdline_main    return wrap_session(config, _main)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/main.py"", line 269, in wrap_session    session.exitstatus = doit(config, session) or 0  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/main.py"", line 323, in _main    config.hook.pytest_runtestloop(session=session)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__    return self._hookexec(self, self.get_hookimpls(), kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec    return self._inner_hookexec(hook, methods, kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>    self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall    res = hook_impl.function(*args)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/main.py"", line 348, in pytest_runtestloop    item.config.hook.pytest_runtest_protocol(item=item, nextitem=nextitem)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__    return self._hookexec(self, self.get_hookimpls(), kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec    return self._inner_hookexec(hook, methods, kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>    self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall    res = hook_impl.function(*args)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 109, in pytest_runtest_protocol    runtestprotocol(item, nextitem=nextitem)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 126, in runtestprotocol    reports.append(call_and_report(item, ""call"", log))  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 215, in call_and_report    call = call_runtest_hook(item, when, **kwds)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 254, in call_runtest_hook    return CallInfo.from_call(  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 311, in from_call    result: Optional[TResult] = func()  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 255, in <lambda>    lambda: ihook(item=item, **kwds), when=when, reraise=reraise  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/hooks.py"", line 286, in __call__    return self._hookexec(self, self.get_hookimpls(), kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 93, in _hookexec    return self._inner_hookexec(hook, methods, kwargs)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/manager.py"", line 84, in <lambda>    self._inner_hookexec = lambda hook, methods, kwargs: hook.multicall(  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/pluggy/callers.py"", line 187, in _multicall    res = hook_impl.function(*args)  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/runner.py"", line 162, in pytest_runtest_call    item.runtest()  File ""/home/clayg/.local/share/virtualenvs/grpc-test-ywCqvyqB/lib/python3.8/site-packages/_pytest/unittest.py"", line 321, in runtest    self._testcase(result=self)  # type: ignore[arg-type]  File ""/usr/lib/python3.8/unittest/case.py"", line 736, in __call__    return self.run(*args, **kwds)  File ""/usr/lib/python3.8/unittest/case.py"", line 676, in run    self._callTestMethod(testMethod)  File ""/usr/lib/python3.8/unittest/case.py"", line 633, in _callTestMethod    method()  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio_tests/tests_aio/unit/_test_base.py"", line 31, in wrapper    return loop.run_until_complete(f(*args, **kwargs))  File ""/usr/lib/python3.8/asyncio/base_events.py"", line 603, in run_until_complete    self.run_forever()  File ""/usr/lib/python3.8/asyncio/base_events.py"", line 570, in run_forever    self._run_once()  File ""/usr/lib/python3.8/asyncio/base_events.py"", line 1851, in _run_once    handle._run()  File ""/usr/lib/python3.8/asyncio/events.py"", line 81, in _run    self._context.run(self._callback, *self._args)  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio_tests/tests_aio/unit/server_test.py"", line 479, in test_error_without_raise_in_stream_unary    call = self._channel.stream_unary(  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_channel.py"", line 182, in __call__    call = StreamUnaryCall(request_iterator, deadline, metadata,  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py"", line 576, in __init__    self._init_stream_request_mixin(request_iterator)  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py"", line 374, in _init_stream_request_mixin    self._async_request_poller = self._loop.create_task(Traceback (most recent call last):  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py"", line 403, in _consume_request_iterator    await self._write(request)  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py"", line 429, in _write    await self._cython_call.send_serialized_message(serialized_request)  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/call.pyx.pxi"", line 370, in send_serialized_message    await _send_message(self,  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi"", line 147, in _send_message    await execute_batch(grpc_call_wrapper, ops, loop)  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/callback_common.pyx.pxi"", line 98, in execute_batch    await futuregrpc._cython.cygrpc.ExecuteBatchError: Failed ""execute_batch"": (<grpc._cython.cygrpc.SendMessageOperation object at 0x7fe678ac5d30>,)```",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25272,570466983,2021-02-04T18:54:26Z,bazel/grpc_deps.bzl,"@@ -362,6 +363,52 @@ def grpc_deps():             ],         ) +    if ""com_google_googleapis"" not in native.existing_rules():+        http_archive(+            name = ""com_google_googleapis"",+            sha256 = ""a45019af4d3290f02eaeb1ce10990166978c807cb33a9692141a076ba46d1405"",+            strip_prefix = ""googleapis-82944da21578a53b74e547774cf62ed31a05b841"",+            urls = [+                ""https://github.com/googleapis/googleapis/archive/82944da21578a53b74e547774cf62ed31a05b841.tar.gz"",","Before merging, you'll want to upload a mirror for all of these new dependencies to `storage.googleapis.com/grpc-bazel-mirror` to insulate us from potential Github outages. Check out [`update_mirror.sh`](https://github.com/grpc/grpc/blob/master/bazel/update_mirror.sh)",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25272,570470034,2021-02-04T18:59:04Z,third_party/envoy-api.WORKSPACE,"@@ -0,0 +1,16 @@+workspace(name = ""envoy_api"")++load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")++http_archive(","Why was it necessary to patch in our own `WORKSPACE` file? I know there are often dependency issues with these (so I'm fine with keeping it if the alternative is difficult), but when possible, it is preferable to get those issues fixed upstream.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25351,570533699,2021-02-04T20:48:43Z,src/python/grpcio_tests/tests_aio/unit/call_test.py,"@@ -625,6 +625,27 @@ async def test_call_rpc_error(self):             self.assertTrue(call.done())             self.assertEqual(grpc.StatusCode.UNAVAILABLE, await call.code()) +    async def test_call_rpc_error_with_async_generator(self):+        request = messages_pb2.StreamingOutputCallRequest()++        # We fail at connection, this gets started but never runs+        async def request_iterator():+            for _ in range(_NUM_STREAM_RESPONSES):+                yield request++        async with aio.insecure_channel(UNREACHABLE_TARGET) as channel:+            stub = test_pb2_grpc.TestServiceStub(channel)+            call = stub.StreamingInputCall(request_iterator(),+                                           timeout=_SHORT_TIMEOUT_S)+            with self.assertRaises(aio.AioRpcError) as exception_context:+                await call+            self.assertEqual(grpc.StatusCode.DEADLINE_EXCEEDED,+                             exception_context.exception.code())++        self.assertTrue(call.done())+        self.assertEqual(grpc.StatusCode.DEADLINE_EXCEEDED, await call.code())+        self.assertTrue(call._async_request_poller.done())","Maybe you could try to use the test server? So, instead of creating a new channel, you could reuse `self._stub`.You can config the server to delay before responding as following, and you should find other usages within this file:```pythonrequest = messages_pb2.StreamingOutputCallRequest()        request.response_parameters.append(            messages_pb2.ResponseParameters(                size=_RESPONSE_PAYLOAD_SIZE,                interval_us=_INFINITE_INTERVAL_US,            ))```",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25351,570540826,2021-02-04T21:01:59Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -458,6 +470,29 @@ async def test_error_without_raise_in_unary_unary(self):         rpc_error = exception_context.exception         self.assertEqual(grpc.StatusCode.INTERNAL, rpc_error.code()) +    async def test_error_without_raise_in_stream_unary(self):++        async def request_iterator():+            for _ in range(_NUM_STREAM_REQUESTS):+                yield _REQUEST++        call = self._channel.stream_unary(+            _ERROR_WITHOUT_RAISE_IN_STREAM_UNARY)(request_iterator())++        self.assertEqual(grpc.StatusCode.INTERNAL, await call.code())+        self.assertFalse(call.cancelled())+        self.assertTrue(call._async_request_poller.done())++        call = self._channel.stream_unary(+            _ERROR_WITHOUT_RAISE_IN_STREAM_UNARY)(request_iterator())++        with self.assertRaises(aio.AioRpcError) as exception_context:+            await call+        self.assertEqual(grpc.StatusCode.INTERNAL,+                         exception_context.exception.code())+        self.assertFalse(call.cancelled())+        self.assertTrue(call._async_request_poller.done())",What's the reason for duplicate the same test logic here?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,570593431,2021-02-04T22:40:08Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -35,13 +35,26 @@ import xml.etree.ElementTree as ET import os import sys+import re+ import build_cleaner -_ROOT = os.path.abspath(os.path.join(os.path.dirname(sys.argv[0]), '../..'))-os.chdir(_ROOT)+PROJECT_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), "".."",+                            "".."")+os.chdir(PROJECT_ROOT)++EXTERNAL_LIB_LABEL_PATH_MAP = dict(","This code path is specific for OSS buildgen. Internally, for Blaze, the story will be similar to Bazel. Once we use copybara to switch the OSS Envoy labels to internal Envoy labels gRPC will hopefully build.Thanks for mentioning this. This PR definitely needs a cherry-pick.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,570604600,2021-02-04T23:03:31Z,build_handwritten.yaml,"@@ -74,7 +74,7 @@ configs:     LDXX: clang++     compile_the_world: true     test_environ:-      ASAN_OPTIONS: detect_leaks=1:color=always+      ASAN_OPTIONS: detect_leaks=1:color=always:detect_odr_violation=0","You are right, I did introduced 2 versions of `re2`: `com_googlesource_code_re2` and `com_github_google_re2`. (They updated their repo name at some point https://github.com/google/re2/blob/master/WORKSPACE#L7)I merged these two `http_archive` rules. And this flag is no long needed!",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,570658148,2021-02-05T01:24:27Z,third_party/envoy-api.WORKSPACE,"@@ -0,0 +1,16 @@+workspace(name = ""envoy_api"")++load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")++http_archive(","Oops, I missed this comment when scrolling down the change list. We need to patch in a WORKSPACE file because they don't have a default one (https://github.com/envoyproxy/data-plane-api).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25354,570789971,2021-02-05T08:14:45Z,tools/internal_ci/macos/grpc_run_bazel_c_cpp_tests.sh,"@@ -38,6 +38,7 @@ echo ""${BAZEL_INVOCATION_ID}"" >""${KOKORO_ARTIFACTS_DIR}/bazel_invocation_ids"" tools/bazel \   --bazelrc=tools/remote_build/mac.bazelrc \   test \+  --define=use_strict_warning=true \",this is not the right place to add  flags like this.This script only has the arguments that take care of uploading the test results to the resultstore UI and that sets up kokoro integration.This file is where you should set compilation options is here: https://github.com/grpc/grpc/blob/master/tools/remote_build/mac.bazelrc,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25272,571163856,2021-02-05T18:23:27Z,third_party/envoy-api.WORKSPACE,"@@ -0,0 +1,16 @@+workspace(name = ""envoy_api"")++load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")++http_archive(",Wow. That *is* surprising. It looks like they pull their WORKSPACE file in [from somewhere outside the source tree](https://github.com/envoyproxy/data-plane-api/blob/0026887e3204008527e357d47544a8b949bb8940/ci/build_setup.sh#L28).,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,571909588,2021-02-08T09:51:31Z,build_handwritten.yaml,"@@ -217,6 +217,13 @@ defaults:   zlib:     CFLAGS: -fvisibility=hidden     CPPFLAGS: -DHAVE_UNISTD_H+external_library_prefixes:","Our long term goal is to get rid of build_handwritten.yaml file entirely, so over time we should be removing entries from it, rather than adding more.Basically, lots of build_handwritten.yaml's content is ""hacks that didn't fit elsewhere"" and the newly added ""external_library_prefixes"" is not an exception. Another issue with the custom fields in build_handwritten.yaml is that there is no documentation for what they mean and how they should be used (and since these are mostly hacks, documenting them properly is also quite hard).Overall, I don't think we should add anything to build_handwritten.yaml",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,571915835,2021-02-08T10:00:30Z,tools/buildgen/_mako_renderer.py,"@@ -18,53 +18,61 @@ """"""  import getopt+import glob","I think the refactoring/improvements to _mako_renderer should be reviewed as a separate PR.this PR is already pretty complex and like this is unclear if you're actually changing some logic because the codegen changes or if you're just performing cleanup.If you provided a separate PR that does buildgen cleanup (and provided a good description for that PR), things would probably be quite obvious and such PR would be easy to review. As a side effect of that could also review the existing buildgen code from python3's perspective (which is something that's also needed).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,571920586,2021-02-08T10:07:32Z,tools/buildgen/plugins/generate_vsprojects.py,"@@ -25,9 +25,8 @@ def mako_plugin(dictionary):     """"""The exported plugin code for generate_vsprojeccts -  We want to help the work of the visual studio generators.--  """"""+    We want to help the work of the visual studio generators.","Since you're touching the buildgen logic, I think  the ""generate_vsprojects.py"" plugin is no longer needed and can be deleted.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,571922546,2021-02-08T10:10:30Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -35,13 +35,26 @@ import xml.etree.ElementTree as ET import os import sys+import re","note: from all the refactorings you're doing, only the changes in this file `extract_metadata_from_bazel_xml.py` should be included in this PR. Everything else seems like cleanup/refactoring and it doesn't belong here.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,571929892,2021-02-08T10:21:16Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -35,13 +35,26 @@ import xml.etree.ElementTree as ET import os import sys+import re+ import build_cleaner -_ROOT = os.path.abspath(os.path.join(os.path.dirname(sys.argv[0]), '../..'))-os.chdir(_ROOT)+PROJECT_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), "".."",+                            "".."")+os.chdir(PROJECT_ROOT)++EXTERNAL_LIB_LABEL_PATH_MAP = dict(+    envoy_api=""third_party/envoy-api/"",+    com_google_googleapis=""third_party/googleapis/"",+    com_github_cncf_udpa=""third_party/udpa/"",+    com_envoyproxy_protoc_gen_validate=""third_party/protoc-gen-validate/"",+    com_google_protobuf=""third_party/protobuf"",+    opencensus_proto=""third_party/opencensus-proto/src/"")+RE_SUPPORTED_LIB = re.compile(","RE_SUPPORTED_LIB  name is not really descriptive. It provides no explanation of what it's useful for beyond ""it's going to match some libraries we're later going to need"" which is useless for getting any kind of insight.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,571932587,2021-02-08T10:25:26Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -139,10 +154,17 @@ def _extract_nonpublic_headers(bazel_rule): def _extract_sources(bazel_rule):","right now in the ""tools/buildgen/extract_metadata_from_bazel_xml.py"" script, the logic for reading the build metadata from the bazel build and converting it into something we're later going to process contains no ""custom"" (or ""project specific"") logic - it simply extracts the source files, headers etc. Only after we've extracted this general information, we go ahead a postprocess it in a way that's grpc-project specific. So I'd rather not include custom handling logic like ""if RE_SUPPORTED_LIB.match(xyz)"" in general functions like ""_extract_sources"" (there should be nothing custom about ""Gets list of source files from a bazel rule"").",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,571940980,2021-02-08T10:38:15Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -293,8 +314,9 @@ def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):         elif external_dep_name_maybe:             deps.add(external_dep_name_maybe) -        elif bazel_dep.startswith(-                '//external:') or not bazel_dep.startswith('//'):+        elif bazel_dep.startswith('//external:') or (+                not bazel_dep.startswith('//') and+                not RE_SUPPORTED_LIB.match(bazel_dep)):","In this context, the name ""RE_SUPPORTED_LIB"" gives you no useful insight into what is it you're attempting to do.I think I kind of understand the point of what you're trying to do (you are trying to make sure that the .proto files you want actually do get expanded into the list of your source files), but that doesn't really fit the function name ""_expand_intermediate_deps"", it's description and also https://github.com/grpc/grpc/blob/026046d84301999936a759c1779669c04c6ffaaa/tools/buildgen/extract_metadata_from_bazel_xml.py#L351 My first thought is that perhaps instead of trying to hack the .proto extraction logic into the existing _expand_intermediate_deps function, you could just introduce a separate function (e.g ""_expand_proto_files_from_external_deps"") that simply looks for the .proto files we want and adds them where they belong. That's probably a few lines of code and makes the intent clearer.",X
93533,moserware,https://api.github.com/repos/grpc/grpc/pulls/25374,572286142,2021-02-08T18:49:22Z,src/csharp/Grpc.Tools/ProtoCompile.cs,"@@ -278,6 +278,12 @@ public class ProtoCompile : ToolTask         /// </summary>         public string[] OutputOptions { get; set; } +        /// <summary>+        /// Additional options that will be passed directly to protoc.+        /// For example, ""experimental_allow_proto3_optional""+        /// </summary>+        public string[] AdditionalProtocOptions { get; set; }","I'm open to alternative parameter name suggestions. Also, I was unsure on if people should provide the `--` prefix in the options. The current implementation automatically provides the `--` prefix in order to encourage folks to not to forget it.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25330,572311242,2021-02-08T19:28:45Z,src/ruby/ext/grpc/rb_channel_credentials.c,"@@ -138,9 +138,13 @@ static ID id_pem_cert_chain;     ...     creds3 = Credentials.new(pem_root_certs, pem_private_key,                              pem_cert_chain)+    ...+    creds4 = Credentials.new(fallback_creds)","I'm a little worried this API may be confusing to reason about, because we are deciding to create an XDS credentials object or SSL credentials object, based on whether or not the first parameter to this constructor is a credentials object, and returning an object of the exact same type in both cases.Also, this seems different from [Python](https://github.com/grpc/grpc/pull/25365) and [C++](https://github.com/grpc/grpc/blob/c8c80fd9988fb5e508ae2f6b39e7901c2054d4f6/src/core/lib/security/credentials/xds/xds_credentials.h#L35), which both have `XdsCredentials` and `XdsServerCredentials` as explicit types.Instead of this API, what do you think about creating a new class, `XdsChannelCredentials`, which takes a non-optional fallback creds parameter. Meanwhile, this constructor here can be left unchanged.Same comment for the server credentials API and changes made to `ServerCredentials`.cc @sanjaypujare and @stanley-cheung ",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25354,572699332,2021-02-09T08:54:36Z,tools/internal_ci/macos/grpc_run_bazel_c_cpp_tests.sh,"@@ -38,6 +38,7 @@ echo ""${BAZEL_INVOCATION_ID}"" >""${KOKORO_ARTIFACTS_DIR}/bazel_invocation_ids"" tools/bazel \   --bazelrc=tools/remote_build/mac.bazelrc \   test \+  --define=use_strict_warning=true \","That's a misunderstanding. Note that the `mac.bazelrc` file is under `remote_build` directory (which indicates it's only used for RBE-like builds on kokoro).The mac.bazerc file isn't a ""standard"" rc file that gets included in a build automatically, you need to explicitly apply it with `--bazelrc=tools/remote_build/mac.bazelrc"" which is exactly what's being done here: https://github.com/grpc/grpc/blob/6f339894f18e75049a2d17a5ef233ae29555760c/tools/internal_ci/macos/grpc_run_bazel_c_cpp_tests.sh#L39Therefore my previous concern is still valid and the `--define=use_strict_warning=true` flag still needs to be moved to mac.bazelrc.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25394,573965105,2021-02-10T18:19:11Z,src/python/grpcio/grpc/_channel.py,"@@ -95,7 +95,11 @@ class _RPCState(object):     def __init__(self, due, initial_metadata, trailing_metadata, code, details):         self.condition = threading.Condition()         # The cygrpc.OperationType objects representing events due from the RPC's-        # completion queue.+        # completion queue. If an operation is in `due`, it is guaranteed that a+        # `operate()` has been called on a corresponding operation. But the+        # converse is not true. That is -- in the case of failed `operate()`+        # calls, there may briefly be events in `due` that do not correspond to+        # operations submitted to Core.","Maybe we should mention, all read/write to the `self.due` is protected by `self.condition`. There might be transient events, but won't have a race.",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25378,574028974,2021-02-10T19:52:07Z,src/ruby/pb/test/xds_client.rb,"@@ -98,11 +103,30 @@ def create_stub(opts)   ) end +class StatsPerMethod","nit: we can replace:```def rpcs_started  @rpcs_startedenddef result  @resultend```with```attr_reader :rpcs_started, :result```at the top of the class definition",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,574255667,2021-02-11T05:16:33Z,bazel/grpc_deps.bzl,"@@ -362,6 +363,52 @@ def grpc_deps():             ],         ) +    if ""com_google_googleapis"" not in native.existing_rules():+        http_archive(+            name = ""com_google_googleapis"",+            sha256 = ""a45019af4d3290f02eaeb1ce10990166978c807cb33a9692141a076ba46d1405"",+            strip_prefix = ""googleapis-82944da21578a53b74e547774cf62ed31a05b841"",+            urls = [+                ""https://github.com/googleapis/googleapis/archive/82944da21578a53b74e547774cf62ed31a05b841.tar.gz"",+            ],+        )++    # if ""com_googlesource_code_re2"" not in native.existing_rules():+    #     http_archive(+    #         name = ""com_googlesource_code_re2"",+    #         sha256 = ""9f385e146410a8150b6f4cb1a57eab7ec806ced48d427554b1e754877ff26c3e"",+    #         strip_prefix = ""re2-aecba11114cf1fac5497aeb844b6966106de3eb6"",+    #         urls = [+    #             ""https://github.com/google/re2/archive/aecba11114cf1fac5497aeb844b6966106de3eb6.tar.gz"",+    #         ],+    #     )++    if ""bazel_gazelle"" not in native.existing_rules():","Bazel doesn't handle transitive dependencies (see [doc](https://docs.bazel.build/versions/master/external.html#transitive-dependencies)), this is needed by the to build `googleapis`.https://github.com/googleapis/googleapis/blob/master/WORKSPACE#L259",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,574260628,2021-02-11T05:37:42Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -293,8 +314,9 @@ def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):         elif external_dep_name_maybe:             deps.add(external_dep_name_maybe) -        elif bazel_dep.startswith(-                '//external:') or not bazel_dep.startswith('//'):+        elif bazel_dep.startswith('//external:') or (+                not bazel_dep.startswith('//') and+                not RE_SUPPORTED_LIB.match(bazel_dep)):","For the readability issue, added comment:```# This branch skips external dependencies that we don't understand,# or their build metadata is provided elsewhere.```---For the intermediate dependencies, all the intermediate dependencies will collapse eventually. The special logic for ""proto"" extraction is rewrote. As mentioned above, it's inevitable to map repo name back to path prefixes.The only alternative I can think of: we can store our dependencies' information in a JSON/YAML file, so both Bazel and this script can access them.---Besides, the generated build files don't seem to break build systems for our wrapper languages.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,574261857,2021-02-11T05:43:04Z,build_autogenerated.yaml,"@@ -2777,6 +2777,18 @@ libs:   - src/cpp/server/channelz/channelz_service.h   src:   - src/proto/grpc/channelz/channelz.proto+  - third_party/protobuf/src/google/protobuf/any.proto","Removed well known protos from this path.I thought it was beneficial to make well known protos to be dependency-tracked, so I switched them into this new path.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,574262762,2021-02-11T05:46:54Z,tools/buildgen/generate_build_additions.sh,"@@ -31,7 +31,7 @@ gen_build_yaml_dirs=""  \ gen_build_files="""" for gen_build_yaml in $gen_build_yaml_dirs do-  output_file=`mktemp /tmp/genXXXXXX`+  output_file=$(mktemp /tmp/gen_$(echo $gen_build_yaml | tr '/' '_').yaml.XXXXX)","The ""`"" change is for ""shellcheck"". The name change is intended to make the intermediate files semantically meaningful. It tells you whether this YAML file is generated for Abseil or CAres or RE2, etc..",
28025951,HannahShiSFB,https://api.github.com/repos/grpc/grpc/pulls/25330,574278855,2021-02-11T06:48:05Z,src/ruby/ext/grpc/rb_channel_credentials.c,"@@ -138,9 +138,13 @@ static ID id_pem_cert_chain;     ...     creds3 = Credentials.new(pem_root_certs, pem_private_key,                              pem_cert_chain)+    ...+    creds4 = Credentials.new(fallback_creds)",XdsChannelCredentials class and XdsServerCredentials class are added.@apolcyn @stanley-cheung,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25421,574699692,2021-02-11T17:42:41Z,bazel/grpc_deps.bzl,"@@ -311,10 +311,10 @@ def grpc_deps():     if ""envoy_api"" not in native.existing_rules():         http_archive(             name = ""envoy_api"",-            sha256 = ""6b6376fa5a6e03c617ed029b669013ecf88e7f8b42474436343cbf2540b207ae"",-            strip_prefix = ""data-plane-api-9edfeb841b0f8e54816f0b81949f79104072072c"",+            sha256 = ""4423bef0ab15053dca0f723cbdaf4b48ab145e9d8158f02e33028c66fb1d20e0"",+            strip_prefix = ""data-plane-api-18b54850c9b7ba29a4ab67cbd7ed7eab7b0bbdb2"",             urls = [-                ""https://github.com/envoyproxy/data-plane-api/archive/9edfeb841b0f8e54816f0b81949f79104072072c.tar.gz"",+                ""https://github.com/envoyproxy/data-plane-api/archive/18b54850c9b7ba29a4ab67cbd7ed7eab7b0bbdb2.tar.gz"",",I guess last time I forgot to run: https://github.com/grpc/grpc/blob/master/bazel/update_mirror.shCan you use the script to upload the archive to our GCS?,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25330,574760755,2021-02-11T19:15:03Z,src/ruby/ext/grpc/rb_server.c,"@@ -326,7 +327,14 @@ static VALUE grpc_rb_server_add_http2_port(VALUE self, VALUE port,                StringValueCStr(port));     }   } else {-    creds = grpc_rb_get_wrapped_server_credentials(rb_creds);+    if (grpc_rb_is_server_credentials(rb_creds)) {","nit: can we add a TODO comment here that if we create a common parent class for all server-side credentials, then we can have a single method to retrieve the underlying `grpc_server_credentials` object, and avoid the need for this reflection ?",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25330,574761977,2021-02-11T19:17:03Z,src/ruby/ext/grpc/rb_server.c,"@@ -326,7 +327,14 @@ static VALUE grpc_rb_server_add_http2_port(VALUE self, VALUE port,                StringValueCStr(port));     }   } else {-    creds = grpc_rb_get_wrapped_server_credentials(rb_creds);+    if (grpc_rb_is_server_credentials(rb_creds)) {+      creds = grpc_rb_get_wrapped_server_credentials(rb_creds);+    } else if (grpc_rb_is_xds_server_credentials(rb_creds)) {+      creds = grpc_rb_get_wrapped_xds_server_credentials(rb_creds);+    } else {+      rb_raise(rb_eTypeError,+               ""bad creds, want ServerCredentials or XdsServerCredentials"");","nit: can we specify this error a bit more, to something like:`""failed to create server because credentials parameter has an invalid type, want ServerCredentials or XdsServerCredentials""`",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25330,574770510,2021-02-11T19:30:21Z,src/ruby/ext/grpc/rb_xds_channel_credentials.c,"@@ -0,0 +1,212 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <grpc/grpc_security.h>+#include <grpc/support/alloc.h>+#include <grpc/support/log.h>+#include <ruby/ruby.h>+#include <string.h>++#include ""rb_call_credentials.h""+#include ""rb_channel_credentials.h""+#include ""rb_xds_channel_credentials.h""+#include ""rb_grpc.h""+#include ""rb_grpc_imports.generated.h""++/* grpc_rb_cXdsChannelCredentials is the ruby class that proxies+   grpc_channel_credentials. */+static VALUE grpc_rb_cXdsChannelCredentials = Qnil;++/* grpc_rb_xds_channel_credentials wraps a grpc_channel_credentials.  It+ * provides a mark object that is used to hold references to any objects used to+ * create the credentials. */+typedef struct grpc_rb_xds_channel_credentials {+  /* Holder of ruby objects involved in constructing the credentials */+  VALUE mark;++  /* The actual credentials */+  grpc_channel_credentials* wrapped;+} grpc_rb_xds_channel_credentials;++static void grpc_rb_xds_channel_credentials_free_internal(void* p) {+  grpc_rb_xds_channel_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  };+  wrapper = (grpc_rb_xds_channel_credentials*)p;+  grpc_channel_credentials_release(wrapper->wrapped);+  wrapper->wrapped = NULL;++  xfree(p);+}++/* Destroys the credentials instances. */+static void grpc_rb_xds_channel_credentials_free(void* p) {+  grpc_rb_xds_channel_credentials_free_internal(p);+  grpc_ruby_shutdown();+}++/* Protects the mark object from GC */+static void grpc_rb_xds_channel_credentials_mark(void* p) {+  grpc_rb_xds_channel_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  }+  wrapper = (grpc_rb_xds_channel_credentials*)p;++  if (wrapper->mark != Qnil) {+    rb_gc_mark(wrapper->mark);+  }+}++static rb_data_type_t grpc_rb_xds_channel_credentials_data_type = {+    ""grpc_xds_channel_credentials"",+    {grpc_rb_xds_channel_credentials_mark,+     grpc_rb_xds_channel_credentials_free,+     GRPC_RB_MEMSIZE_UNAVAILABLE,+     NULL},+    NULL,+    NULL,+#ifdef RUBY_TYPED_FREE_IMMEDIATELY+    RUBY_TYPED_FREE_IMMEDIATELY+#endif+};++/* Allocates ChannelCredential instances.+   Provides safe initial defaults for the instance fields. */+static VALUE grpc_rb_xds_channel_credentials_alloc(VALUE cls) {+  grpc_ruby_init();+  grpc_rb_xds_channel_credentials* wrapper =+      ALLOC(grpc_rb_xds_channel_credentials);+  wrapper->wrapped = NULL;+  wrapper->mark = Qnil;+  return TypedData_Wrap_Struct(cls, &grpc_rb_xds_channel_credentials_data_type,+                               wrapper);+}++/* Creates a wrapping object for a given channel credentials. This should only+ * be called with grpc_channel_credentials objects that are not already+ * associated with any Ruby object. */+VALUE grpc_rb_xds_wrap_channel_credentials(grpc_channel_credentials* c,+                                       VALUE mark) {+  VALUE rb_wrapper;+  grpc_rb_xds_channel_credentials* wrapper;+  if (c == NULL) {+    return Qnil;+  }+  rb_wrapper =+      grpc_rb_xds_channel_credentials_alloc(grpc_rb_cXdsChannelCredentials);+  TypedData_Get_Struct(rb_wrapper, grpc_rb_xds_channel_credentials,+                       &grpc_rb_xds_channel_credentials_data_type, wrapper);+  wrapper->wrapped = c;+  wrapper->mark = mark;+  return rb_wrapper;+}++/* The attribute used on the mark object to hold the fallback creds. */+static ID id_fallback_creds;++/*+  call-seq:+    fallback_creds: (ChannelCredentials) fallback credentials to create+    XDS credentials+    Initializes Credential instances. */+static VALUE grpc_rb_xds_channel_credentials_init(VALUE self,+                                                  VALUE fallback_creds) {+  grpc_rb_xds_channel_credentials* wrapper = NULL;+  grpc_channel_credentials* grpc_fallback_creds =+      grpc_rb_get_wrapped_channel_credentials(fallback_creds);+  grpc_channel_credentials* creds =+      grpc_xds_credentials_create(grpc_fallback_creds);++  if (creds == NULL) {+    rb_raise(rb_eRuntimeError, ""could not create a credentials, not sure why"");+    return Qnil;+  }++  TypedData_Get_Struct(self, grpc_rb_xds_channel_credentials,+                       &grpc_rb_xds_channel_credentials_data_type, wrapper);+  wrapper->wrapped = creds;++  /* Add the input objects as hidden fields to preserve them. */+  rb_ivar_set(self, id_fallback_creds, fallback_creds);++  return self;+}++static VALUE grpc_rb_xds_channel_credentials_compose(int argc, VALUE* argv,+                                                     VALUE self) {+  grpc_channel_credentials* creds;+  grpc_call_credentials* other;+  grpc_channel_credentials* prev = NULL;+  VALUE mark;+  if (argc == 0) {+    return self;+  }+  mark = rb_ary_new();+  rb_ary_push(mark, self);+  creds = grpc_rb_get_wrapped_xds_channel_credentials(self);+  for (int i = 0; i < argc; i++) {+    rb_ary_push(mark, argv[i]);+    other = grpc_rb_get_wrapped_call_credentials(argv[i]);+    creds = grpc_composite_channel_credentials_create(creds, other, NULL);+    if (prev != NULL) {+      grpc_channel_credentials_release(prev);+    }+    prev = creds;++    if (creds == NULL) {+      rb_raise(rb_eRuntimeError,+               ""Failed to compose channel and call credentials"");+    }+  }+  return grpc_rb_xds_wrap_channel_credentials(creds, mark);+}++void Init_grpc_xds_channel_credentials() {+  grpc_rb_cXdsChannelCredentials = rb_define_class_under(+      grpc_rb_mGrpcCore, ""XdsChannelCredentials"", rb_cObject);++  /* Allocates an object managed by the ruby runtime */+  rb_define_alloc_func(grpc_rb_cXdsChannelCredentials,+                       grpc_rb_xds_channel_credentials_alloc);++  /* Provides a ruby constructor and support for dup/clone. */+  rb_define_method(grpc_rb_cXdsChannelCredentials, ""initialize"",+                   grpc_rb_xds_channel_credentials_init, 1);+  rb_define_method(grpc_rb_cXdsChannelCredentials, ""initialize_copy"",+                   grpc_rb_cannot_init_copy, 1);+  rb_define_method(grpc_rb_cXdsChannelCredentials, ""compose"",+                   grpc_rb_xds_channel_credentials_compose, -1);++  id_fallback_creds = rb_intern(""__fallback_creds"");+}++/* Gets the wrapped grpc_channel_credentials from the ruby wrapper */+grpc_channel_credentials* grpc_rb_get_wrapped_xds_channel_credentials(VALUE v) {+  grpc_rb_xds_channel_credentials* wrapper = NULL;+  Check_TypedStruct(v, &grpc_rb_xds_channel_credentials_data_type);",similar question as the one in `rb_channel_credentials.c` about if this `Check_TypedStruct` call is needed,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25330,574771807,2021-02-11T19:32:32Z,src/ruby/ext/grpc/rb_xds_channel_credentials.c,"@@ -0,0 +1,212 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <grpc/grpc_security.h>+#include <grpc/support/alloc.h>+#include <grpc/support/log.h>+#include <ruby/ruby.h>+#include <string.h>++#include ""rb_call_credentials.h""+#include ""rb_channel_credentials.h""+#include ""rb_xds_channel_credentials.h""+#include ""rb_grpc.h""+#include ""rb_grpc_imports.generated.h""++/* grpc_rb_cXdsChannelCredentials is the ruby class that proxies+   grpc_channel_credentials. */+static VALUE grpc_rb_cXdsChannelCredentials = Qnil;++/* grpc_rb_xds_channel_credentials wraps a grpc_channel_credentials.  It+ * provides a mark object that is used to hold references to any objects used to+ * create the credentials. */+typedef struct grpc_rb_xds_channel_credentials {+  /* Holder of ruby objects involved in constructing the credentials */+  VALUE mark;++  /* The actual credentials */+  grpc_channel_credentials* wrapped;+} grpc_rb_xds_channel_credentials;++static void grpc_rb_xds_channel_credentials_free_internal(void* p) {+  grpc_rb_xds_channel_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  };+  wrapper = (grpc_rb_xds_channel_credentials*)p;+  grpc_channel_credentials_release(wrapper->wrapped);+  wrapper->wrapped = NULL;++  xfree(p);+}++/* Destroys the credentials instances. */+static void grpc_rb_xds_channel_credentials_free(void* p) {+  grpc_rb_xds_channel_credentials_free_internal(p);+  grpc_ruby_shutdown();+}++/* Protects the mark object from GC */+static void grpc_rb_xds_channel_credentials_mark(void* p) {+  grpc_rb_xds_channel_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  }+  wrapper = (grpc_rb_xds_channel_credentials*)p;++  if (wrapper->mark != Qnil) {+    rb_gc_mark(wrapper->mark);+  }+}++static rb_data_type_t grpc_rb_xds_channel_credentials_data_type = {+    ""grpc_xds_channel_credentials"",+    {grpc_rb_xds_channel_credentials_mark,+     grpc_rb_xds_channel_credentials_free,+     GRPC_RB_MEMSIZE_UNAVAILABLE,+     NULL},+    NULL,+    NULL,+#ifdef RUBY_TYPED_FREE_IMMEDIATELY+    RUBY_TYPED_FREE_IMMEDIATELY+#endif+};++/* Allocates ChannelCredential instances.+   Provides safe initial defaults for the instance fields. */+static VALUE grpc_rb_xds_channel_credentials_alloc(VALUE cls) {+  grpc_ruby_init();+  grpc_rb_xds_channel_credentials* wrapper =+      ALLOC(grpc_rb_xds_channel_credentials);+  wrapper->wrapped = NULL;+  wrapper->mark = Qnil;+  return TypedData_Wrap_Struct(cls, &grpc_rb_xds_channel_credentials_data_type,+                               wrapper);+}++/* Creates a wrapping object for a given channel credentials. This should only+ * be called with grpc_channel_credentials objects that are not already+ * associated with any Ruby object. */+VALUE grpc_rb_xds_wrap_channel_credentials(grpc_channel_credentials* c,+                                       VALUE mark) {+  VALUE rb_wrapper;+  grpc_rb_xds_channel_credentials* wrapper;+  if (c == NULL) {+    return Qnil;+  }+  rb_wrapper =+      grpc_rb_xds_channel_credentials_alloc(grpc_rb_cXdsChannelCredentials);+  TypedData_Get_Struct(rb_wrapper, grpc_rb_xds_channel_credentials,+                       &grpc_rb_xds_channel_credentials_data_type, wrapper);+  wrapper->wrapped = c;+  wrapper->mark = mark;+  return rb_wrapper;+}++/* The attribute used on the mark object to hold the fallback creds. */+static ID id_fallback_creds;++/*+  call-seq:+    fallback_creds: (ChannelCredentials) fallback credentials to create+    XDS credentials+    Initializes Credential instances. */+static VALUE grpc_rb_xds_channel_credentials_init(VALUE self,+                                                  VALUE fallback_creds) {+  grpc_rb_xds_channel_credentials* wrapper = NULL;+  grpc_channel_credentials* grpc_fallback_creds =+      grpc_rb_get_wrapped_channel_credentials(fallback_creds);+  grpc_channel_credentials* creds =+      grpc_xds_credentials_create(grpc_fallback_creds);++  if (creds == NULL) {+    rb_raise(rb_eRuntimeError, ""could not create a credentials, not sure why"");+    return Qnil;+  }++  TypedData_Get_Struct(self, grpc_rb_xds_channel_credentials,+                       &grpc_rb_xds_channel_credentials_data_type, wrapper);+  wrapper->wrapped = creds;++  /* Add the input objects as hidden fields to preserve them. */+  rb_ivar_set(self, id_fallback_creds, fallback_creds);++  return self;+}+","nit can we add a TODO comment to de-duplicate this code with the similar method in `rb_channel_credentials.c`, after putting `ChannelCredentials` and `XdsChannelCredentials` under a common parent class?",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25330,574774133,2021-02-11T19:36:26Z,src/ruby/ext/grpc/rb_xds_server_credentials.c,"@@ -0,0 +1,162 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include ""rb_xds_server_credentials.h""++#include <grpc/grpc.h>+#include <grpc/grpc_security.h>+#include <grpc/support/log.h>+#include <ruby/ruby.h>++#include ""rb_grpc.h""+#include ""rb_grpc_imports.generated.h""+#include ""rb_server_credentials.h""++/* grpc_rb_cXdsServerCredentials is the ruby class that proxies+   grpc_server_credentials. */+static VALUE grpc_rb_cXdsServerCredentials = Qnil;++/* grpc_rb_xds_server_credentials wraps a grpc_server_credentials.  It provides+   a peer ruby object, 'mark' to hold references to objects involved in+   constructing the server credentials. */+typedef struct grpc_rb_xds_server_credentials {+  /* Holder of ruby objects involved in constructing the server credentials */+  VALUE mark;+  /* The actual server credentials */+  grpc_server_credentials* wrapped;+} grpc_rb_xds_server_credentials;++/* Destroys the server credentials instances. */+static void grpc_rb_xds_server_credentials_free(void* p) {+  grpc_rb_xds_server_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  };+  wrapper = (grpc_rb_xds_server_credentials*)p;++  /* Delete the wrapped object if the mark object is Qnil, which indicates that+     no other object is the actual owner. */+  if (wrapper->wrapped != NULL && wrapper->mark == Qnil) {+    grpc_server_credentials_release(wrapper->wrapped);+    wrapper->wrapped = NULL;+  }++  xfree(p);+}++/* Protects the mark object from GC */+static void grpc_rb_xds_server_credentials_mark(void* p) {+  grpc_rb_xds_server_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  }+  wrapper = (grpc_rb_xds_server_credentials*)p;++  /* If it's not already cleaned up, mark the mark object */+  if (wrapper->mark != Qnil) {+    rb_gc_mark(wrapper->mark);+  }+}++static const rb_data_type_t grpc_rb_xds_server_credentials_data_type = {+    ""grpc_xds_server_credentials"",+    {grpc_rb_xds_server_credentials_mark, grpc_rb_xds_server_credentials_free,+     GRPC_RB_MEMSIZE_UNAVAILABLE, NULL},+    NULL,+    NULL,+#ifdef RUBY_TYPED_FREE_IMMEDIATELY+    RUBY_TYPED_FREE_IMMEDIATELY+#endif+};++/* Allocates ServerCredential instances.++   Provides safe initial defaults for the instance fields. */+static VALUE grpc_rb_xds_server_credentials_alloc(VALUE cls) {","similar to other comment about `grpc_ruby_shutdown`, we should call `grpc_ruby_init` at the top of this function",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25330,574774604,2021-02-11T19:37:12Z,src/ruby/ext/grpc/rb_xds_server_credentials.c,"@@ -0,0 +1,162 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include ""rb_xds_server_credentials.h""++#include <grpc/grpc.h>+#include <grpc/grpc_security.h>+#include <grpc/support/log.h>+#include <ruby/ruby.h>++#include ""rb_grpc.h""+#include ""rb_grpc_imports.generated.h""+#include ""rb_server_credentials.h""++/* grpc_rb_cXdsServerCredentials is the ruby class that proxies+   grpc_server_credentials. */+static VALUE grpc_rb_cXdsServerCredentials = Qnil;++/* grpc_rb_xds_server_credentials wraps a grpc_server_credentials.  It provides+   a peer ruby object, 'mark' to hold references to objects involved in+   constructing the server credentials. */+typedef struct grpc_rb_xds_server_credentials {+  /* Holder of ruby objects involved in constructing the server credentials */+  VALUE mark;+  /* The actual server credentials */+  grpc_server_credentials* wrapped;+} grpc_rb_xds_server_credentials;++/* Destroys the server credentials instances. */+static void grpc_rb_xds_server_credentials_free(void* p) {+  grpc_rb_xds_server_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  };+  wrapper = (grpc_rb_xds_server_credentials*)p;++  /* Delete the wrapped object if the mark object is Qnil, which indicates that+     no other object is the actual owner. */+  if (wrapper->wrapped != NULL && wrapper->mark == Qnil) {+    grpc_server_credentials_release(wrapper->wrapped);+    wrapper->wrapped = NULL;+  }++  xfree(p);+}++/* Protects the mark object from GC */+static void grpc_rb_xds_server_credentials_mark(void* p) {+  grpc_rb_xds_server_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  }+  wrapper = (grpc_rb_xds_server_credentials*)p;++  /* If it's not already cleaned up, mark the mark object */+  if (wrapper->mark != Qnil) {+    rb_gc_mark(wrapper->mark);+  }+}++static const rb_data_type_t grpc_rb_xds_server_credentials_data_type = {+    ""grpc_xds_server_credentials"",+    {grpc_rb_xds_server_credentials_mark, grpc_rb_xds_server_credentials_free,+     GRPC_RB_MEMSIZE_UNAVAILABLE, NULL},+    NULL,+    NULL,+#ifdef RUBY_TYPED_FREE_IMMEDIATELY+    RUBY_TYPED_FREE_IMMEDIATELY+#endif+};++/* Allocates ServerCredential instances.++   Provides safe initial defaults for the instance fields. */+static VALUE grpc_rb_xds_server_credentials_alloc(VALUE cls) {+  grpc_rb_xds_server_credentials* wrapper =+      ALLOC(grpc_rb_xds_server_credentials);+  wrapper->wrapped = NULL;+  wrapper->mark = Qnil;+  return TypedData_Wrap_Struct(cls, &grpc_rb_xds_server_credentials_data_type,+                               wrapper);+}++/* The attribute used on the mark object to preserve the fallback_creds. */+static ID id_fallback_creds;++/*+  call-seq:+    creds = ServerCredentials.new(fallback_creds)++    fallback_creds: (ServerCredentials) fallback credentials to create+                    XDS credentials.++    Initializes ServerCredential instances. */+static VALUE grpc_rb_xds_server_credentials_init(VALUE self,+                                                 VALUE fallback_creds) {+  grpc_rb_xds_server_credentials* wrapper = NULL;+  grpc_server_credentials* creds = NULL;+",similar comments about type-checking and nil checking on `fallback_creds` as for the `rb_xds_channel_credentials.c` file,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25330,574775031,2021-02-11T19:37:34Z,src/ruby/ext/grpc/rb_xds_server_credentials.c,"@@ -0,0 +1,162 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include ""rb_xds_server_credentials.h""++#include <grpc/grpc.h>+#include <grpc/grpc_security.h>+#include <grpc/support/log.h>+#include <ruby/ruby.h>++#include ""rb_grpc.h""+#include ""rb_grpc_imports.generated.h""+#include ""rb_server_credentials.h""++/* grpc_rb_cXdsServerCredentials is the ruby class that proxies+   grpc_server_credentials. */+static VALUE grpc_rb_cXdsServerCredentials = Qnil;++/* grpc_rb_xds_server_credentials wraps a grpc_server_credentials.  It provides+   a peer ruby object, 'mark' to hold references to objects involved in+   constructing the server credentials. */+typedef struct grpc_rb_xds_server_credentials {+  /* Holder of ruby objects involved in constructing the server credentials */+  VALUE mark;+  /* The actual server credentials */+  grpc_server_credentials* wrapped;+} grpc_rb_xds_server_credentials;++/* Destroys the server credentials instances. */+static void grpc_rb_xds_server_credentials_free(void* p) {+  grpc_rb_xds_server_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  };+  wrapper = (grpc_rb_xds_server_credentials*)p;++  /* Delete the wrapped object if the mark object is Qnil, which indicates that+     no other object is the actual owner. */+  if (wrapper->wrapped != NULL && wrapper->mark == Qnil) {+    grpc_server_credentials_release(wrapper->wrapped);+    wrapper->wrapped = NULL;+  }++  xfree(p);+}++/* Protects the mark object from GC */+static void grpc_rb_xds_server_credentials_mark(void* p) {+  grpc_rb_xds_server_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  }+  wrapper = (grpc_rb_xds_server_credentials*)p;++  /* If it's not already cleaned up, mark the mark object */+  if (wrapper->mark != Qnil) {+    rb_gc_mark(wrapper->mark);+  }+}++static const rb_data_type_t grpc_rb_xds_server_credentials_data_type = {+    ""grpc_xds_server_credentials"",+    {grpc_rb_xds_server_credentials_mark, grpc_rb_xds_server_credentials_free,+     GRPC_RB_MEMSIZE_UNAVAILABLE, NULL},+    NULL,+    NULL,+#ifdef RUBY_TYPED_FREE_IMMEDIATELY+    RUBY_TYPED_FREE_IMMEDIATELY+#endif+};++/* Allocates ServerCredential instances.++   Provides safe initial defaults for the instance fields. */+static VALUE grpc_rb_xds_server_credentials_alloc(VALUE cls) {+  grpc_rb_xds_server_credentials* wrapper =+      ALLOC(grpc_rb_xds_server_credentials);+  wrapper->wrapped = NULL;+  wrapper->mark = Qnil;+  return TypedData_Wrap_Struct(cls, &grpc_rb_xds_server_credentials_data_type,+                               wrapper);+}++/* The attribute used on the mark object to preserve the fallback_creds. */+static ID id_fallback_creds;++/*+  call-seq:+    creds = ServerCredentials.new(fallback_creds)++    fallback_creds: (ServerCredentials) fallback credentials to create+                    XDS credentials.++    Initializes ServerCredential instances. */+static VALUE grpc_rb_xds_server_credentials_init(VALUE self,+                                                 VALUE fallback_creds) {+  grpc_rb_xds_server_credentials* wrapper = NULL;+  grpc_server_credentials* creds = NULL;++  grpc_server_credentials* grpc_fallback_creds =+      grpc_rb_get_wrapped_server_credentials(fallback_creds);+  creds = grpc_xds_server_credentials_create(grpc_fallback_creds);++  if (creds == NULL) {+    rb_raise(rb_eRuntimeError, ""could not create a credentials, not sure why"");",similar comment about specifying the error message as for the `rb_xds_channel_credentials.c` file,
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/25378,574908896,2021-02-11T23:55:11Z,src/ruby/pb/test/xds_client.rb,"@@ -159,23 +180,37 @@ def get_client_stats(req, _call)       rpcs_by_method: rpcs_by_method,       rpcs_by_peer: watcher['rpcs_by_peer'],       num_failures: watcher['no_remote_peer'] + watcher['rpcs_needed']-    );+    )   end    def get_client_accumulated_stats(req, _call)     $accumulated_stats_mu.synchronize do+      all_stats_per_method = {}+      $accumulated_method_stats.each do |rpc, stats_per_method|+        one_stats_per_method = LoadBalancerAccumulatedStatsResponse::MethodStats.new(+          rpcs_started: stats_per_method.rpcs_started,+          result: stats_per_method.result+        )+        all_stats_per_method[rpc] = one_stats_per_method+      end       LoadBalancerAccumulatedStatsResponse.new(         num_rpcs_started_by_method: $num_rpcs_started_by_method,         num_rpcs_succeeded_by_method: $num_rpcs_succeeded_by_method,-        num_rpcs_failed_by_method: $num_rpcs_failed_by_method+        num_rpcs_failed_by_method: $num_rpcs_failed_by_method,+        stats_per_method: all_stats_per_method,       )     end   end end +def add_stats_per_method(rpc_stats_key, status_code)",No longer needed after I removed this function.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/23483,575136213,2021-02-12T10:47:23Z,setup.py,"@@ -97,6 +103,9 @@     'License :: OSI Approved :: Apache Software License', ] +BUILD_WITH_BORING_SSL_ASM = os.environ.get('GRPC_BUILD_WITH_BORING_SSL_ASM',","np, but yeah a patch would be nice. Btw the way all the  options are read from env variables is also kind of broken, but they at least can be used because they have default value `False`.Ideally we'd replace the reading of boolean values from env variables everywhere (if you assign 'XYZ=False' or 'XYZ=false' that's still evaluated as `True` :-( )https://github.com/grpc/grpc/blob/711d1d7fe2da56896957e00fea12c84b6d1fa1e1/setup.py#L109-L169@lidizheng  since you reviewed this.",X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/25430,575350887,2021-02-12T16:35:14Z,src/core/lib/iomgr/ev_epollex_linux.cc,"@@ -163,8 +163,8 @@ static void pollable_unref(pollable* p, const grpc_core::DebugLocation& dbg_loc, struct grpc_fd {   grpc_fd(int fd, const char* name, bool track_err)       : fd(fd), track_err(track_err) {-    gpr_mu_init(&orphan_mu);-    gpr_mu_init(&pollable_mu);+    orphan_mu = new grpc_core::Mutex();",This class is a bit special in a way destroying the mutex in `destroy` not in `dtor` so I decided to use `Mutex*` here to keep the semantic.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25427,575354696,2021-02-12T16:40:51Z,src/core/lib/slice/slice_intern.cc,"@@ -44,7 +44,7 @@ using grpc_core::InternedSliceRefcount;  typedef struct slice_shard {-  gpr_mu mu;+  grpc_core::Mutex* mu;","I assume that the reason that this is dynamically allocated instead of being done inline is the style guide prohibition against global variables that are not trivially destructible?  If so, how about an alternative: change this to a direct data member, and instead change `g_shards` to be dynamically allocated, so that this entire struct can be treated as a C++ object.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25430,575363890,2021-02-12T16:55:03Z,src/core/lib/iomgr/ev_epollex_linux.cc,"@@ -163,8 +163,8 @@ static void pollable_unref(pollable* p, const grpc_core::DebugLocation& dbg_loc, struct grpc_fd {   grpc_fd(int fd, const char* name, bool track_err)       : fd(fd), track_err(track_err) {-    gpr_mu_init(&orphan_mu);-    gpr_mu_init(&pollable_mu);+    orphan_mu = new grpc_core::Mutex();","Oh, I see -- it looks like the `grpc_fd` object go onto a freelist to get reused later, and they don't actually get destroyed until global shutdown time.Is there going to be any performance problem caused by the mutex being allocated separately?  In particular, I'm wondering if that will hurt cache performance.An alternative here would be to just not destroy the mutex in `destroy()`.  Conceptually, if we're keeping around the `grpc_fd` object for possible reuse, it seems like we should do the same thing for the mutex -- it doesn't make much sense to cache one without the other.  If we change the code to create and destroy the `grpc_fd` objects with new and delete instead of `gpr_malloc()` and `gpr_free()`, then we can declare the mutexes as direct data members and have them share the lifetime of the `grpc_fd` object.  This seems like a cleaner approach overall.Also, I notice that we're doing a `memset()` on these mutex fields in `invalidate()` (lines 217-218).  Regardless of what we choose to do here, that probably needs to be updated.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25395,575370700,2021-02-12T17:05:47Z,src/core/ext/filters/client_channel/resolver/xds/xds_resolver.cc,"@@ -29,6 +29,7 @@ #include ""src/core/lib/channel/channel_args.h"" #include ""src/core/lib/iomgr/closure.h"" #include ""src/core/lib/iomgr/exec_ctx.h""+#include ""src/core/lib/security/authorization/evaluate_args.h""","I don't think the xds resolver should directly depend on the authz library.  Instead, I suggest moving `GetMetadataValue()` into src/core/lib/transport/metadata_batch.{h,cc}, where it can be part of the `grpc_metadata_batch` API.  Then we can use that function both here and in `EvaluateArgs`.A few details to get right:- For consistency with the rest of the `grpc_metadata_batch` API, we should call the function something like `grpc_metadata_batch_get_value()`, and we should make `initial_metadata` the first parameter.- It looks like we don't have any existing unit tests for the `grpc_metadata_batch` API, but let's add one for this method.  It can be added to test/core/transport/metadata_test.cc.- Please make sure to document the method's behavior in metadata_batch.h.",
16571023,ananda1066,https://api.github.com/repos/grpc/grpc/pulls/25169,577034281,2021-02-16T18:12:53Z,src/cpp/common/completion_queue_cc.cc,"@@ -96,4 +192,15 @@ bool CompletionQueue::CompletionQueueTLSCache::Flush(void** tag, bool* ok) {   return false; } +CompletionQueue* CompletionQueue::CallbackAlternativeCQ() {+  gpr_once_init(&g_once_init_callback_alternative,+                [] { g_callback_alternative_mu.Init(); });+  return g_callback_alternative_cq.Ref();+}++void CompletionQueue::ReleaseCallbackAlternativeCQ(CompletionQueue* cq) {+  GPR_DEBUG_ASSERT(cq == g_callback_alternative_cq.cq);+  g_callback_alternative_cq.Unref();",Bool returned by Unref is unused? Should this be an ASSERT as well?,
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/25169,577091611,2021-02-16T19:46:46Z,test/cpp/end2end/client_callback_end2end_test.cc,"@@ -350,13 +334,11 @@ class ClientCallbackEnd2endTest };  TEST_P(ClientCallbackEnd2endTest, SimpleRpc) {-  MAYBE_SKIP_TEST;","Ultimately, it is resolved in the table associated with the specific event engine: https://github.com/grpc/grpc/blob/master/src/core/lib/iomgr/ev_posix.h#L49There is no OSS event engine that runs in the background at the current time, which is why we didn't previously support callback API in OSS. This PR allows callback API to be used without a background iomgr.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/25472,577299467,2021-02-17T03:26:30Z,tools/release/backport_pr.sh,"@@ -0,0 +1,103 @@+#!/bin/bash+#Copyright 2021 The gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -euo pipefail++ensure_command () {+  if command -v ""$1"" 1>/dev/null 2>&1; then+    return 0+  else+    echo ""$1 is not installed. Please install it to proceed."" 1>&2+    exit 1+  fi+}++ensure_command ""curl""+ensure_command ""egrep""+ensure_command ""hub""+ensure_command ""jq""++if [ -z ""$GITHUB_TOKEN"" ]; then+  echo ""A GitHub token is required to run this script. See "" \+         ""https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token"" \+         "" for more information"" >/dev/stderr+  exit 1+fi++if [ ""$#"" != ""4"" ]; then+  echo ""USAGE: $0 PR_NUMBER GITHUB_USER BACKPORT_BRANCHES REVIEWERS"" >/dev/stderr+  echo ""   PR_NUMBER: The number for the PR to be backported."" >/dev/stderr+  echo ""   GITHUB_USER: Your GitHub username."" >/dev/stderr+  echo ""   BACKPORT_BRANCHES: A space-separated list of branches to which to backport."" >/dev/stderr+  echo ""   REVIEWERS: A comma-separated list of users add as reviewers and assignees."" >/dev/stderr+  echo """" >/dev/stderr+  echo ""Example: $0 25456 gnossen \""v1.30.x v1.31.x v1.32.x v1.33.x v1.34.x v1.35.x v1.36.x\"" \""menghanl,gnossen\""""+  exit 1+fi++echo ""This script will create a collection of backport PRs. Make sure the PR to"" \+       "" backport has already been merged. You will probably "" \+       "" have to touch your gnubby a frustrating number of times. C'est la vie.""+printf ""Press any key to continue.""+read -r RESPONSE </dev/tty+printf ""\n""++PR_NUMBER=""$1""+GITHUB_USER=""$2""++BACKPORT_BRANCHES=""$3""++REVIEWERS=""$4""++PR_DATA=$(curl -s -u ""$GITHUB_USER:$GITHUB_TOKEN"" \+          -H ""Accept: application/vnd.github.v3+json"" \+          ""https://api.github.com/repos/grpc/grpc/pulls/$PR_NUMBER"")++MERGE_COMMIT=$(echo ""$PR_DATA"" | jq -r '.merge_commit_sha')","I tested this `curl | jq` snippet against GitHub's squash & merge option for PRs, which does not create merge commits. The PR metadata's `merge_commit_sha` points to the squashed commit hash, so this logic should still work fine.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25330,578799320,2021-02-18T22:43:23Z,src/ruby/ext/grpc/rb_xds_channel_credentials.c,"@@ -0,0 +1,214 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/grpc.h>+#include <grpc/grpc_security.h>+#include <grpc/support/alloc.h>+#include <grpc/support/log.h>+#include <ruby/ruby.h>+#include <string.h>++#include ""rb_call_credentials.h""+#include ""rb_channel_credentials.h""+#include ""rb_grpc.h""+#include ""rb_grpc_imports.generated.h""+#include ""rb_xds_channel_credentials.h""++/* grpc_rb_cXdsChannelCredentials is the ruby class that proxies+   grpc_channel_credentials. */+static VALUE grpc_rb_cXdsChannelCredentials = Qnil;++/* grpc_rb_xds_channel_credentials wraps a grpc_channel_credentials.  It+ * provides a mark object that is used to hold references to any objects used to+ * create the credentials. */+typedef struct grpc_rb_xds_channel_credentials {+  /* Holder of ruby objects involved in constructing the credentials */+  VALUE mark;++  /* The actual credentials */+  grpc_channel_credentials* wrapped;+} grpc_rb_xds_channel_credentials;++static void grpc_rb_xds_channel_credentials_free_internal(void* p) {+  grpc_rb_xds_channel_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  };+  wrapper = (grpc_rb_xds_channel_credentials*)p;+  grpc_channel_credentials_release(wrapper->wrapped);+  wrapper->wrapped = NULL;++  xfree(p);+}++/* Destroys the credentials instances. */+static void grpc_rb_xds_channel_credentials_free(void* p) {+  grpc_rb_xds_channel_credentials_free_internal(p);+  grpc_ruby_shutdown();+}++/* Protects the mark object from GC */+static void grpc_rb_xds_channel_credentials_mark(void* p) {+  grpc_rb_xds_channel_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  }+  wrapper = (grpc_rb_xds_channel_credentials*)p;++  if (wrapper->mark != Qnil) {+    rb_gc_mark(wrapper->mark);+  }+}++static rb_data_type_t grpc_rb_xds_channel_credentials_data_type = {+    ""grpc_xds_channel_credentials"",+    {grpc_rb_xds_channel_credentials_mark, grpc_rb_xds_channel_credentials_free,+     GRPC_RB_MEMSIZE_UNAVAILABLE, NULL},+    NULL,+    NULL,+#ifdef RUBY_TYPED_FREE_IMMEDIATELY+    RUBY_TYPED_FREE_IMMEDIATELY+#endif+};++/* Allocates ChannelCredential instances.+   Provides safe initial defaults for the instance fields. */+static VALUE grpc_rb_xds_channel_credentials_alloc(VALUE cls) {+  grpc_ruby_init();+  grpc_rb_xds_channel_credentials* wrapper =+      ALLOC(grpc_rb_xds_channel_credentials);+  wrapper->wrapped = NULL;+  wrapper->mark = Qnil;+  return TypedData_Wrap_Struct(cls, &grpc_rb_xds_channel_credentials_data_type,+                               wrapper);+}++/* Creates a wrapping object for a given channel credentials. This should only+ * be called with grpc_channel_credentials objects that are not already+ * associated with any Ruby object. */+VALUE grpc_rb_xds_wrap_channel_credentials(grpc_channel_credentials* c,+                                           VALUE mark) {+  VALUE rb_wrapper;+  grpc_rb_xds_channel_credentials* wrapper;+  if (c == NULL) {+    return Qnil;+  }+  rb_wrapper =+      grpc_rb_xds_channel_credentials_alloc(grpc_rb_cXdsChannelCredentials);+  TypedData_Get_Struct(rb_wrapper, grpc_rb_xds_channel_credentials,+                       &grpc_rb_xds_channel_credentials_data_type, wrapper);+  wrapper->wrapped = c;+  wrapper->mark = mark;+  return rb_wrapper;+}++/* The attribute used on the mark object to hold the fallback creds. */+static ID id_fallback_creds;++/*+  call-seq:+    fallback_creds: (ChannelCredentials) fallback credentials to create+    XDS credentials+    Initializes Credential instances. */+static VALUE grpc_rb_xds_channel_credentials_init(VALUE self,+                                                  VALUE fallback_creds) {+  grpc_rb_xds_channel_credentials* wrapper = NULL;+  grpc_channel_credentials* grpc_fallback_creds =+      grpc_rb_get_wrapped_channel_credentials(fallback_creds);+  grpc_channel_credentials* creds =+      grpc_xds_credentials_create(grpc_fallback_creds);+  if (creds == NULL) {+    rb_raise(rb_eRuntimeError,+             ""the call to grpc_xds_credentials_create() failed, could not ""+             ""create a credentials, not sure why"");","nit: can we replace ""not sure why"" with ""see https://github.com/grpc/grpc/blob/master/TROUBLESHOOTING.md for debugging tips""",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25330,578800436,2021-02-18T22:45:43Z,src/ruby/ext/grpc/rb_xds_server_credentials.c,"@@ -0,0 +1,163 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include ""rb_xds_server_credentials.h""++#include <grpc/grpc.h>+#include <grpc/grpc_security.h>+#include <grpc/support/log.h>+#include <ruby/ruby.h>++#include ""rb_grpc.h""+#include ""rb_grpc_imports.generated.h""+#include ""rb_server_credentials.h""++/* grpc_rb_cXdsServerCredentials is the ruby class that proxies+   grpc_server_credentials. */+static VALUE grpc_rb_cXdsServerCredentials = Qnil;++/* grpc_rb_xds_server_credentials wraps a grpc_server_credentials.  It provides+   a peer ruby object, 'mark' to hold references to objects involved in+   constructing the server credentials. */+typedef struct grpc_rb_xds_server_credentials {+  /* Holder of ruby objects involved in constructing the server credentials */+  VALUE mark;+  /* The actual server credentials */+  grpc_server_credentials* wrapped;+} grpc_rb_xds_server_credentials;++/* Destroys the server credentials instances. */+static void grpc_rb_xds_server_credentials_free(void* p) {+  grpc_rb_xds_server_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  };+  wrapper = (grpc_rb_xds_server_credentials*)p;++  /* Delete the wrapped object if the mark object is Qnil, which indicates that+     no other object is the actual owner. */+  if (wrapper->wrapped != NULL && wrapper->mark == Qnil) {+    grpc_server_credentials_release(wrapper->wrapped);+    wrapper->wrapped = NULL;+  }++  xfree(p);+}++/* Protects the mark object from GC */+static void grpc_rb_xds_server_credentials_mark(void* p) {+  grpc_rb_xds_server_credentials* wrapper = NULL;+  if (p == NULL) {+    return;+  }+  wrapper = (grpc_rb_xds_server_credentials*)p;++  /* If it's not already cleaned up, mark the mark object */+  if (wrapper->mark != Qnil) {+    rb_gc_mark(wrapper->mark);+  }+}++static const rb_data_type_t grpc_rb_xds_server_credentials_data_type = {+    ""grpc_xds_server_credentials"",+    {grpc_rb_xds_server_credentials_mark, grpc_rb_xds_server_credentials_free,+     GRPC_RB_MEMSIZE_UNAVAILABLE, NULL},+    NULL,+    NULL,+#ifdef RUBY_TYPED_FREE_IMMEDIATELY+    RUBY_TYPED_FREE_IMMEDIATELY+#endif+};++/* Allocates ServerCredential instances.++   Provides safe initial defaults for the instance fields. */+static VALUE grpc_rb_xds_server_credentials_alloc(VALUE cls) {+  grpc_rb_xds_server_credentials* wrapper =+      ALLOC(grpc_rb_xds_server_credentials);+  wrapper->wrapped = NULL;+  wrapper->mark = Qnil;+  return TypedData_Wrap_Struct(cls, &grpc_rb_xds_server_credentials_data_type,+                               wrapper);+}++/* The attribute used on the mark object to preserve the fallback_creds. */+static ID id_fallback_creds;++/*+  call-seq:+    creds = ServerCredentials.new(fallback_creds)++    fallback_creds: (ServerCredentials) fallback credentials to create+                    XDS credentials.++    Initializes ServerCredential instances. */+static VALUE grpc_rb_xds_server_credentials_init(VALUE self,+                                                 VALUE fallback_creds) {+  grpc_rb_xds_server_credentials* wrapper = NULL;+  grpc_server_credentials* creds = NULL;++  grpc_server_credentials* grpc_fallback_creds =+      grpc_rb_get_wrapped_server_credentials(fallback_creds);+  creds = grpc_xds_server_credentials_create(grpc_fallback_creds);++  if (creds == NULL) {+    rb_raise(rb_eRuntimeError, ""could not create a credentials, not sure why"");","nit: can we replace ""not sure why"" with ""see https://github.com/grpc/grpc/blob/master/TROUBLESHOOTING.md for debugging tips""",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/25502,578878123,2021-02-19T02:17:04Z,tools/release/backport_pr.sh,"@@ -24,59 +24,100 @@ ensure_command () {   fi } +display_usage () {+  echo ""USAGE: $0 PR_NUMBER GITHUB_USER BACKPORT_BRANCHES REVIEWERS [-c PER_BACKPORT_COMMAND]"" >/dev/stderr+  echo ""   PR_NUMBER: The number for the PR to be backported."" >/dev/stderr+  echo ""   GITHUB_USER: Your GitHub username."" >/dev/stderr+  echo ""   BACKPORT_BRANCHES: A space-separated list of branches to which to backport."" >/dev/stderr","I'm not a language lawyer, so while I _think_ this is grammatically correct, it's a bit hard to parse. This is the list of branches that will receive the backported PR, yes?",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/25502,578878256,2021-02-19T02:17:32Z,tools/release/backport_pr.sh,"@@ -24,59 +24,100 @@ ensure_command () {   fi } +display_usage () {+  echo ""USAGE: $0 PR_NUMBER GITHUB_USER BACKPORT_BRANCHES REVIEWERS [-c PER_BACKPORT_COMMAND]"" >/dev/stderr+  echo ""   PR_NUMBER: The number for the PR to be backported."" >/dev/stderr+  echo ""   GITHUB_USER: Your GitHub username."" >/dev/stderr+  echo ""   BACKPORT_BRANCHES: A space-separated list of branches to which to backport."" >/dev/stderr+  echo ""   REVIEWERS: A comma-separated list of users add as reviewers and assignees."" >/dev/stderr","Missed a word. ""... A comma-separated list of users *to* add as ...""",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/25502,578890017,2021-02-19T02:53:52Z,tools/release/backport_pr.sh,"@@ -24,59 +24,100 @@ ensure_command () {   fi } +display_usage () {+  echo ""USAGE: $0 PR_NUMBER GITHUB_USER BACKPORT_BRANCHES REVIEWERS [-c PER_BACKPORT_COMMAND]"" >/dev/stderr+  echo ""   PR_NUMBER: The number for the PR to be backported."" >/dev/stderr+  echo ""   GITHUB_USER: Your GitHub username."" >/dev/stderr+  echo ""   BACKPORT_BRANCHES: A space-separated list of branches to which to backport."" >/dev/stderr+  echo ""   REVIEWERS: A comma-separated list of users add as reviewers and assignees."" >/dev/stderr+  echo ""   PER_BACKPORT_COMMAND : An optional command to run after cherrypicking the PR to the target branch."" >/dev/stderr+  echo ""     If you use this option, ensure your working directory is clean, as `git add -A` will be used to "" >/dev/stderr+  echo ""     incorporate any generated files. Try running `git clean -xdff` beforehand."" >/dev/stderr+  echo """" >/dev/stderr+  echo ""Example: $0 25456 gnossen \""v1.30.x v1.31.x v1.32.x v1.33.x v1.34.x v1.35.x v1.36.x\"" \""menghanl,gnossen\"""" >/dev/stderr+  echo ""Example: $0 25493 gnossen \""\$(seq 30 33 | xargs -n1 printf \""v1.%s.x \"")\"" \""menghanl\"" -c ./tools/dockerfile/push_testing_images.sh"" >/dev/stderr+  exit 1+}+ ensure_command ""curl"" ensure_command ""egrep"" ensure_command ""hub"" ensure_command ""jq"" +if [ ""$#"" -lt ""4"" ]; then+  display_usage+fi++PR_NUMBER=""$1""+GITHUB_USER=""$2""+BACKPORT_BRANCHES=""$3""+REVIEWERS=""$4""+shift 4++PER_BACKPORT_COMMAND=""""+while getopts ""c:"" OPT; do+  case ""$OPT"" in+    c )+      PER_BACKPORT_COMMAND=""$OPTARG""+      ;;+    \? )+      echo ""Invalid option: $OPTARG"" >/dev/stderr+      display_usage+      ;;+    : )+      echo ""Invalid option: $OPTARG requires an argument."" >/dev/stderr+      display_usage+      ;;+  esac+done++if [[ ! -z ""$(git status --porcelain)"" && ! -z ""$PER_BACKPORT_COMMAND"" ]]; then+  echo ""Your working directory is not clean. Try running `git clean -xdff`. Warning: This is irreversible."" > /dev/stderr+  exit 1+fi+ if [ -z ""$GITHUB_TOKEN"" ]; then   echo ""A GitHub token is required to run this script. See "" \          ""https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token"" \          "" for more information"" >/dev/stderr   exit 1 fi -if [ ""$#"" != ""4"" ]; then-  echo ""USAGE: $0 PR_NUMBER GITHUB_USER BACKPORT_BRANCHES REVIEWERS"" >/dev/stderr-  echo ""   PR_NUMBER: The number for the PR to be backported."" >/dev/stderr-  echo ""   GITHUB_USER: Your GitHub username."" >/dev/stderr-  echo ""   BACKPORT_BRANCHES: A space-separated list of branches to which to backport."" >/dev/stderr-  echo ""   REVIEWERS: A comma-separated list of users add as reviewers and assignees."" >/dev/stderr-  echo """" >/dev/stderr-  echo ""Example: $0 25456 gnossen \""v1.30.x v1.31.x v1.32.x v1.33.x v1.34.x v1.35.x v1.36.x\"" \""menghanl,gnossen\""""-  exit 1-fi--echo ""This script will create a collection of backport PRs. Make sure the PR to"" \-       "" backport has already been merged. You will probably "" \-       "" have to touch your gnubby a frustrating number of times. C'est la vie.""+echo ""This script will create a collection of backport PRs. You will probably "" \+       ""have to touch your gnubby a frustrating number of times. C'est la vie."" printf ""Press any key to continue."" read -r RESPONSE </dev/tty printf ""\n"" -PR_NUMBER=""$1""-GITHUB_USER=""$2""--BACKPORT_BRANCHES=""$3""--REVIEWERS=""$4""  PR_DATA=$(curl -s -u ""$GITHUB_USER:$GITHUB_TOKEN"" \           -H ""Accept: application/vnd.github.v3+json"" \           ""https://api.github.com/repos/grpc/grpc/pulls/$PR_NUMBER"") -MERGE_COMMIT=$(echo ""$PR_DATA"" | jq -r '.merge_commit_sha')+STATE=$(echo ""$PR_DATA"" | jq -r '.state')+if [ ""$STATE"" == ""merged"" ]; then","When is the state attribute set to ""merged""? It looks like GitHub lets you filter on state being ""open"", ""closed"", or ""all"" in the PR list API. https://docs.github.com/en/rest/reference/pulls#list-pull-requests",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25502,579448982,2021-02-19T20:13:21Z,tools/release/backport_pr.sh,"@@ -24,59 +24,100 @@ ensure_command () {   fi } +display_usage () {",Much better stylistically. Switched.,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25514,580089473,2021-02-22T09:16:14Z,BUILD,"@@ -496,6 +498,65 @@ grpc_cc_library(     ], ) +package_group(",I don't think we want to add any C# related rules to the public bazel BUILD.- I don't think this would actually build (you're referencing things only work & exist internally).- we don't want to add dependency on dotnet rules.- we don't really need bazel C# build externally and we don't want to maintain it.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25514,580096528,2021-02-22T09:26:53Z,src/compiler/csharp_generator.cc,"@@ -41,6 +38,9 @@ using grpc_generator::METHODTYPE_CLIENT_STREAMING; using grpc_generator::METHODTYPE_NO_STREAMING; using grpc_generator::METHODTYPE_SERVER_STREAMING; using grpc_generator::StringReplace;+using proto2::compiler::csharp::GetClassName;","you'd need to put in place some kind of switching between the internal and external namespaces (based on which version of protobuf_config.h gets used). So far you've tried to use the external-only namespace names internally (which would break the internal build) and now you're trying to use the internal-only namespaces externally and that won't work either, for the same reason.If you look at  https://github.com/grpc/grpc/blob/master/src/compiler/config_protobuf.h and https://github.com/grpc/grpc/blob/86f93801e5d452cff40d6958d21a8eb211da0654/include/grpcpp/impl/codegen/config_protobuf.h you will see that it defines a bunch of macros that allow you to change  namespaces/types of objects being used in the generator plugin code. You need to use such technique to allow calling e.g `google::protobuf::compiler::csharp::GetClassName` externally and `proto2::compiler::csharp::GetClassName` (or whatever) internally.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25514,580104470,2021-02-22T09:38:21Z,src/compiler/csharp_generator.cc,"@@ -633,7 +627,7 @@ void GenerateClientStub(Printer* out, const ServiceDescriptor* service) {             ""methodfield"", GetMethodFieldName(method));         break;       default:-        GOOGLE_LOG(FATAL) << ""Can't get here."";+        break;",I've noticed that currently csharp_generator is the only language that's using the GetMethodType() function -  other protoc plugins just use   method->client_streaming() and method->server_streaming() directly - perhaps this is the opportunity for simplification and unification. As a side effect we'd also get rid of the switch clause's `default:` branch(same above)See https://github.com/grpc/grpc/blob/86f93801e5d452cff40d6958d21a8eb211da0654/src/compiler/generator_helpers.h#L152,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25512,580255596,2021-02-22T13:39:14Z,tools/buildgen/plugins/expand_filegroups.py,"@@ -47,11 +47,10 @@ def uniquify(lst): def mako_plugin(dictionary):     """"""The exported plugin code for expand_filegroups. -  The list of libs in the build.yaml file can contain ""filegroups"" tags.-  These refer to the filegroups in the root object. We will expand and-  merge filegroups on the src, headers and public_headers properties.--  """"""+    The list of libs in the build.yaml file can contain ""filegroups"" tags.","btw, I think since the migration to extract_metadata_from_bazel.py, we might no longer have any filegroups - it might be worth considering getting rid of this ""plugin"".",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25512,580484434,2021-02-22T18:21:43Z,tools/run_tests/python_utils/jobset.py,"@@ -277,7 +277,10 @@ def start(self):                 os.makedirs(logfile_dir)             self._logfile = open(self._spec.logfilename, 'w+')         else:-            self._logfile = tempfile.TemporaryFile()+            # macOS: a series of quick os.unlink invocation might cause OS","It occurs to me on my MacBook. I tried to twitch file handle limit, or remove temporary files. But it's unclear why the `os.unlink` failed with a system error.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25528,580682482,2021-02-23T00:08:30Z,src/core/ext/filters/client_channel/resolver/xds/xds_resolver.cc,"@@ -741,6 +741,10 @@ void XdsResolver::OnListenerUpdate(XdsApi::LdsUpdate listener) {   if (route_config_name_.empty()) {     GPR_ASSERT(current_listener_.rds_update.has_value());     OnRouteConfigUpdate(std::move(*current_listener_.rds_update));+  } else {+    // HCM may contain newer filter config. We need to propagate the update as+    // config selector to the channel+    GenerateResult();","Consider the case where we get an LDS update that changes both the filter list and the RDS resource name.  In that case, we do not want to generate a result here, because we want to swap in both changes at the same time -- i.e., we don't want to swap in the new filter list until we can also apply the new RouteConfiguration.To fix this, I suggest calling `current_virtual_host_.Clear()` on line 735 above.  That way, `GenerateResult()` will be a no-op in the case where we've switched to a different RDS resource name.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25374,582038849,2021-02-24T15:08:53Z,src/csharp/Grpc.Tools/ProtoCompile.cs,"@@ -428,6 +434,15 @@ protected override string GenerateResponseFileCommands()             }             cmd.AddSwitchMaybe(""dependency_out"", DependencyOut);             cmd.AddSwitchMaybe(""error_format"", ""msvs"");++            if (AdditionalProtocOptions != null)+            {+                foreach (var additionalProtocOption in AdditionalProtocOptions)+                {+                    cmd.AddArg(""--"" + additionalProtocOption);","I think this automatic addition of ""--"" prefix is actually not very helpful as it can:- can cause confusion in terms of whether you should pass the args with or without `--` while only offering very little extra convenience- it limits the generality (I can imagine in some use cases you'd want to add some extra arguments that are not necessarily cmdline flags to your protoc invocation and Grpc.Tools adding the extra `--` to everything would spoil that).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25374,582052506,2021-02-24T15:24:52Z,src/csharp/Grpc.Tools/ProtoCompile.cs,"@@ -278,6 +278,12 @@ public class ProtoCompile : ToolTask         /// </summary>         public string[] OutputOptions { get; set; } +        /// <summary>+        /// Additional options that will be passed directly to protoc.+        /// For example, ""experimental_allow_proto3_optional""+        /// </summary>+        public string[] AdditionalProtocOptions { get; set; }","Also technically, ""Options"" is not the best term here, since in protoc terminology the ""options"" is what gets passed to `--csharp_opt=` (OutputOptions) and `--grpc_opt=` (GrpcOutputOptions), not arbitrary command line flags.",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/25541,582153701,2021-02-24T17:25:19Z,src/core/lib/surface/server.cc,"@@ -370,15 +371,25 @@ class Server::AllocatingRequestMatcherBatch    void MatchOrQueue(size_t /*start_request_queue_index*/,                     CallData* calld) override {-    BatchCallAllocation call_info = allocator_();-    GPR_ASSERT(server()->ValidateServerRequest(-                   cq(), static_cast<void*>(call_info.tag), nullptr, nullptr) ==-               GRPC_CALL_OK);-    RequestedCall* rc = new RequestedCall(-        static_cast<void*>(call_info.tag), call_info.cq, call_info.call,-        call_info.initial_metadata, call_info.details);-    calld->SetState(CallData::CallState::ACTIVATED);-    calld->Publish(cq_idx(), rc);+    server()->shutdown_blocking_refs_.fetch_add(1, std::memory_order_acq_rel);+    if (!server()->shutdown_flag_.load(std::memory_order_acquire)) {","Yes, let's do this. I'll spend the time to resolve this as it makes the code too complex otherwise.",
4779759,dapengzhang0,https://api.github.com/repos/grpc/grpc/pulls/25558,583061654,2021-02-25T18:19:52Z,src/core/ext/xds/xds_http_filters.cc,"@@ -0,0 +1,107 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/xds/xds_http_filters.h""++#include ""envoy/extensions/filters/http/router/v3/router.upb.h""+#include ""envoy/extensions/filters/http/router/v3/router.upbdefs.h""++namespace grpc_core {++const char* kXdsHttpRouterFilterConfigName =+    ""envoy.extensions.filters.http.router.v3.Router"";++namespace {++class XdsHttpRouterFilter : public XdsHttpFilterImpl {+ public:+  void PopulateSymtab(upb_symtab* symtab) const override {+    envoy_extensions_filters_http_router_v3_Router_getmsgdef(symtab);+  }++  absl::StatusOr<FilterConfig> GenerateFilterConfig(+      upb_strview serialized_filter_config, upb_arena* arena) const override {+    if (envoy_extensions_filters_http_router_v3_Router_parse(+            serialized_filter_config.data, serialized_filter_config.size,+            arena) == nullptr) {+      return absl::InvalidArgumentError(""could not parse router filter config"");+    }+    return FilterConfig{kXdsHttpRouterFilterConfigName, Json()};+  }++  absl::StatusOr<FilterConfig> GenerateFilterConfigOverride(+      upb_strview /*serialized_filter_config*/,+      upb_arena* /*arena*/) const override {+    return absl::InvalidArgumentError(+        ""router filter does not support config override"");",Should this be documented in the spec https://github.com/grpc/proposal/blob/master/A39-xds-http-filters.md#the-router-filter?,
4779759,dapengzhang0,https://api.github.com/repos/grpc/grpc/pulls/25558,583090382,2021-02-25T19:02:23Z,src/core/ext/xds/xds_http_filters.cc,"@@ -0,0 +1,107 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/xds/xds_http_filters.h""++#include ""envoy/extensions/filters/http/router/v3/router.upb.h""+#include ""envoy/extensions/filters/http/router/v3/router.upbdefs.h""++namespace grpc_core {++const char* kXdsHttpRouterFilterConfigName =+    ""envoy.extensions.filters.http.router.v3.Router"";++namespace {++class XdsHttpRouterFilter : public XdsHttpFilterImpl {+ public:+  void PopulateSymtab(upb_symtab* symtab) const override {+    envoy_extensions_filters_http_router_v3_Router_getmsgdef(symtab);+  }++  absl::StatusOr<FilterConfig> GenerateFilterConfig(+      upb_strview serialized_filter_config, upb_arena* arena) const override {+    if (envoy_extensions_filters_http_router_v3_Router_parse(+            serialized_filter_config.data, serialized_filter_config.size,+            arena) == nullptr) {+      return absl::InvalidArgumentError(""could not parse router filter config"");+    }+    return FilterConfig{kXdsHttpRouterFilterConfigName, Json()};+  }++  absl::StatusOr<FilterConfig> GenerateFilterConfigOverride(+      upb_strview /*serialized_filter_config*/,+      upb_arena* /*arena*/) const override {+    return absl::InvalidArgumentError(+        ""router filter does not support config override"");",">The default assumption should be that a filter does not support any override configMight be better to call out the NACK behavior because the current spec is only saying```Note that, for a given request, the filter's configuration will come from the top-level filter list in the HttpConnectionManager config and the most specific override. In other words, if a given filterinstance has an override in the ClusterWeight proto, that will be used; otherwise, if it has an override in the Route proto, that will be used; otherwise, if it has an override in the VirtualHost proto, that will be used; otherwise, no override will be used.```This could be misleading because it doesn't say any filter may not support any override config, and should NACK during LDS/RDS update validation time if a filter does not support override config but an override config is present. ",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25563,583137650,2021-02-25T20:17:29Z,tools/buildgen/generate_projects.sh,"@@ -18,6 +18,12 @@ set -e  export TEST=${TEST:-false} +YAML_OK=$(python3 -c ""import yaml; print(yaml.__version__.split('.') >= ['5', '4', '1'])"")++if [[ ${YAML_OK} != ""True"" ]]; then+  python3 -m pip install --upgrade --ignore-installed PyYAML==5.4.1 --user",Any reason not to just unconditionally install this version?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25563,583142467,2021-02-25T20:20:33Z,tools/buildgen/generate_projects.sh,"@@ -18,6 +18,12 @@ set -e  export TEST=${TEST:-false} +YAML_OK=$(python3 -c ""import yaml; print(yaml.__version__.split('.') >= ['5', '4', '1'])"")++if [[ ${YAML_OK} != ""True"" ]]; then+  python3 -m pip install --upgrade --ignore-installed PyYAML==5.4.1 --user","The original version upgrades PyYAML unconditionally, but people don't like the extra log when they re-generate the project manually.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25551,583152152,2021-02-25T20:31:30Z,src/core/ext/filters/client_channel/resolver/google_c2p/google_c2p_resolver.cc,"@@ -308,11 +310,16 @@ void GoogleCloud2ProdResolver::StartXdsResolver() {         {""TRAFFICDIRECTOR_DIRECTPATH_C2P_IPV6_CAPABLE"", true},     };   }+  std::string server_uri = ""directpath-trafficdirector.googleapis.com"";+  if (g_traffic_director_uri_override != nullptr &&",done (tweaked the conditional though to check for zero-length string),
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25430,583272587,2021-02-25T23:21:32Z,src/core/lib/iomgr/ev_epollex_linux.cc,"@@ -163,8 +163,8 @@ static void pollable_unref(pollable* p, const grpc_core::DebugLocation& dbg_loc, struct grpc_fd {   grpc_fd(int fd, const char* name, bool track_err)       : fd(fd), track_err(track_err) {-    gpr_mu_init(&orphan_mu);-    gpr_mu_init(&pollable_mu);+    orphan_mu = new grpc_core::Mutex();","Drive by comment - I spent some time looking at this code in https://github.com/grpc/grpc/issues/23536. As I understand the the freelist approach was just done for correctness rather than performance (so that we can safely get epoll events on destroyed fd's), the comment in https://github.com/grpc/grpc/blob/4a90992d033b341c3f55dbeb695820e893084288/src/core/lib/iomgr/ev_epollex_linux.cc#L331 explains more",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25550,583278195,2021-02-25T23:31:58Z,test/cpp/interop/client_helper.cc,"@@ -84,13 +84,11 @@ std::shared_ptr<Channel> CreateChannelForTestCase(     std::vector<         std::unique_ptr<experimental::ClientInterceptorFactoryInterface>>         interceptor_creators) {-  GPR_ASSERT(absl::GetFlag(FLAGS_server_port));-  const int host_port_buf_size = 1024;-  char host_port[host_port_buf_size];-  snprintf(host_port, host_port_buf_size, ""%s:%d"",-           absl::GetFlag(FLAGS_server_host).c_str(),-           absl::GetFlag(FLAGS_server_port));-+  std::string host_port = absl::GetFlag(FLAGS_server_host);","What do you think about server_uri? (changed to that)Preferred that over ""host"" since host may sound like it includes only the hostname component of the uri",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/25526,583878691,2021-02-26T19:47:30Z,CMakeLists.txt,"@@ -226,21 +226,18 @@ endif() set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} ${_gRPC_C_CXX_FLAGS}"") set(CMAKE_CXX_FLAGS ""${CMAKE_CXX_FLAGS} ${_gRPC_C_CXX_FLAGS}"") +if(_gRPC_PLATFORM_MAC)+  # some C++11 constructs not supported before OS X 10.10+  set(CMAKE_OSX_DEPLOYMENT_TARGET 10.10)",This has been set to all other OSX targets so I don't think it would be a problem. https://github.com/grpc/grpc/blob/35569cd544f02967ba59ef2886e7b5ae6880405b/gRPC-C%2B%2B.podspec#L38I tested building gRPC on OSX Big Sur & Intel and it worked.,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/25395,584472444,2021-03-01T06:30:03Z,src/core/ext/filters/client_channel/resolver/xds/xds_resolver.cc,"@@ -29,6 +29,7 @@ #include ""src/core/lib/channel/channel_args.h"" #include ""src/core/lib/iomgr/closure.h"" #include ""src/core/lib/iomgr/exec_ctx.h""+#include ""src/core/lib/security/authorization/evaluate_args.h""",Sorry for the delay.I added the method to grpc_metadata_batch as per your recommendation.,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25580,584815580,2021-03-01T15:39:24Z,test/distrib/cpp/run_distrib_test_cmake_arm_cross.sh,"@@ -20,6 +20,9 @@ cd ""$(dirname ""$0"")/../../.."" # Install openssl (to use instead of boringssl) apt-get update && apt-get install -y libssl-dev +# Install arm cross-compiler+apt-get update && apt-get install -y g++-6-arm-linux-gnueabihf","Ideally we should only be installing as part of the Dockerfile (pre-installing the compiler in the base docker image is easy).The `apt-get install -y libssl-dev`  above is an exception since it's part of the distribtest (we ""install"" it as an alternative to using boringssl).",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25324,584959035,2021-03-01T18:40:17Z,src/core/lib/debug/stats.cc,"@@ -150,7 +150,7 @@ std::string grpc_stats_data_as_json(const grpc_stats_data* data) {   parts.push_back(""{"");   for (size_t i = 0; i < GRPC_STATS_COUNTER_COUNT; i++) {     parts.push_back(absl::StrFormat(-        ""\""%s\"": %"" PRIdPTR, grpc_stats_counter_name[i], data->counters[i]));+        ""\""%s\"": %"" PRIdPTR "","", grpc_stats_counter_name[i], data->counters[i]));","Instead of adding in the commas in each individual element here, I suggest letting `absl::StrJoin()` do this for us.  How about something like this:```std::vector<std::string> parts;for (size_t i = 0; i < GRPC_STATS_COUNTER_COUNT; i++) {  parts.push_back(absl::StrFormat(      ""\""%s\"": %"" PRIdPTR, grpc_stats_counter_name[i], data->counters[i]));}for (size_t i = 0; i < GRPC_STATS_HISTOGRAM_COUNT; i++) {  std::vector<std::string> bucket_fields;  // ...add entries to bucket_fields...  parts.push_back(absl::StrFormat(      ""\""%s\"": [%s]"", grpc_stats_histogram_name[i],      absl::StrJoin(bucket_fields, "", "")));}return absl::StrCat(""{"", absl::StrJoin(parts, "", ""), ""}"");```",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25511,585122854,2021-03-01T23:19:57Z,WORKSPACE,"@@ -6,6 +6,16 @@ grpc_deps()  grpc_test_only_deps() +# Initialize Google APIs with only C++ and Python targets+load(""@com_google_googleapis//:repository_rules.bzl"", ""switched_rules_by_language"")++switched_rules_by_language(","Yes. Actually, I really like the pattern. I think it's something that we might want to try introducing to our own repo at some point.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25511,585125674,2021-03-01T23:26:34Z,WORKSPACE,"@@ -6,6 +6,16 @@ grpc_deps()  grpc_test_only_deps() +# Initialize Google APIs with only C++ and Python targets+load(""@com_google_googleapis//:repository_rules.bzl"", ""switched_rules_by_language"")++switched_rules_by_language(","How is this dependency used? Is it a test-only dep? If so, this is okay in its current state. If not, then this needs to go in the `grpc_extra_deps` macro. Otherwise, we'll break repos pulling us in through Bazel.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25511,585129619,2021-03-01T23:31:23Z,bazel/grpc_deps.bzl,"@@ -361,6 +361,42 @@ def grpc_deps():             ],         ) +    if ""com_google_googleapis"" not in native.existing_rules():+        http_archive(+            name = ""com_google_googleapis"",+            sha256 = ""a45019af4d3290f02eaeb1ce10990166978c807cb33a9692141a076ba46d1405"",+            strip_prefix = ""googleapis-82944da21578a53b74e547774cf62ed31a05b841"",+            urls = [+                ""https://github.com/googleapis/googleapis/archive/82944da21578a53b74e547774cf62ed31a05b841.tar.gz"",+            ],+        )++    if ""bazel_gazelle"" not in native.existing_rules():+        http_archive(+            name = ""bazel_gazelle"",+            sha256 = ""d987004a72697334a095bbaa18d615804a28280201a50ed6c234c40ccc41e493"",+            strip_prefix = ""bazel-gazelle-0.19.1"",+            urls = [+                ""https://github.com/bazelbuild/bazel-gazelle/archive/v0.19.1.tar.gz"",",My bad. Mirror added.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25511,585132428,2021-03-01T23:37:59Z,WORKSPACE,"@@ -6,6 +6,16 @@ grpc_deps()  grpc_test_only_deps() +# Initialize Google APIs with only C++ and Python targets+load(""@com_google_googleapis//:repository_rules.bzl"", ""switched_rules_by_language"")++switched_rules_by_language(","Nice idea. Added to `grpc_extra_deps`, so people should be able to get it easily.This dependency is used by one public C++ library target `grpcpp_csds` and tests. The Python targets will hopefully be needed by the interop test driver, and in near future, I might use it to port CSDS to Python.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25330,585143288,2021-03-02T00:04:30Z,src/ruby/end2end/grpc_class_init_client.rb,"@@ -80,6 +80,33 @@ def get_test_proc(grpc_class)     return proc do       GRPC::Core::ChannelCredentials.new     end+  when 'xds_channel_credentials'+    return proc do+      GRPC::Core::XdsChannelCredentials.new(GRPC::Core::ChannelCredentials.new)+    end+  when 'server_credentials'+    return proc do+      test_root = File.join(File.dirname(__FILE__), '..', 'spec', 'testdata')+      GRPC.logger.info(""test root: #{test_root}"")","nit: can we please remove this log line here, or just have it use `STDERR.puts` ?Reason for this is this test is meant to exercise the case where we load `GRPC::Core::ServerCredentials`  as the first thing we do in the process, before using any other grpc library APIs",X
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/25397,585168279,2021-03-02T01:02:11Z,src/php/lib/Grpc/ServerCallWriter.php,"@@ -0,0 +1,101 @@+<?php+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++namespace Grpc;++/**+ * This is an experimental and incomplete implementation of gRPC server+ * for PHP. APIs are _definitely_ going to be changed.+ *+ * DO NOT USE in production.+ */++class ServerCallWriter+{+    public function __construct($call)+    {+        $this->call_ = $call;+    }++    public function start(+        array $initialMetadata,+        \Google\Protobuf\Internal\Message $data = null,","As discussed, can we try to not type-hint something with the explicit class of `Google\Protobuf\Internal\Message`? And just assume `$data` is a protobuf object for now? Type-hinting something with `Internal\` in it _feels_ wrong.",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/25397,585169208,2021-03-02T01:04:21Z,src/php/tests/unit_tests/ServerCallTest.php,"@@ -0,0 +1,268 @@+<?php+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++require_once(dirname(__FILE__) . '/../../lib/Grpc/ServerCallReader.php');+require_once(dirname(__FILE__) . '/../../lib/Grpc/ServerCallWriter.php');+require_once(dirname(__FILE__) . '/../../lib/Grpc/Status.php');++// load protobuf from third_party+set_include_path(get_include_path() . PATH_SEPARATOR . dirname(__FILE__) . '/../../../../third_party/protobuf/php/src/');++spl_autoload_register(function ($className) {+$classPath = str_replace('\\', DIRECTORY_SEPARATOR, $className);+if (strpos($classPath, 'Google/Protobuf') === 0 || strpos($classPath, 'GPBMetadata/Google/Protobuf') === 0) {+require_once($classPath . '.php');","I think it's OK to leave these in the `_Test.php` code, but I'd like to avoid references to protobuf in the library code itself.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25580,585449177,2021-03-02T10:33:20Z,tools/dockerfile/distribtest/cpp_stretch_aarch64_cross_x64/Dockerfile,"@@ -0,0 +1,29 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++FROM debian:stretch","Ok, let's just keep stretch. Since the C++ distribtest are quite slow, I'd like to avoid duplication. Having tests that run for too long (without really providing a clear value) is very annoying.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25600,586747245,2021-03-03T20:15:35Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -197,21 +197,41 @@ cdef class _ServicerContext:     def set_trailing_metadata(self, object metadata):","Please also update the interface class to include the new methods: https://github.com/grpc/grpc/blob/master/src/python/grpcio/grpc/aio/_base_server.py#L134. Since they will be new APIs, please add ""This is an EXPERIMENTAL API."" in docstrings. We don't need to add ""@abc.abstractmethod"" to the new methods, since we don't want to break existing implementations of the `ServicerContext` interface.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25600,586747977,2021-03-03T20:16:44Z,src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi,"@@ -197,21 +197,41 @@ cdef class _ServicerContext:     def set_trailing_metadata(self, object metadata):","Please consider add a test case to access these values in interceptor (as your use case) in https://github.com/grpc/grpc/blob/master/src/python/grpcio_tests/tests_aio/unit/server_interceptor_test.py, to guard this change.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25602,587269310,2021-03-04T08:43:49Z,tools/run_tests/artifacts/build_artifact_python.sh,"@@ -22,8 +22,17 @@ export PYTHON=${PYTHON:-python} export PIP=${PIP:-pip} export AUDITWHEEL=${AUDITWHEEL:-auditwheel} -# Install Cython to avoid source wheel build failure.-""${PIP}"" install --upgrade cython+if [ ""$GRPC_SKIP_PIP_CYTHON_UPGRADE"" == """" ]","Mostly for consistency. The `""$variable"" == """"` construct is used very commonly in our other .sh scripts and since handling of conditions and their semantics in bash is a non-trivial topic, I just wanted to use what's used in our other scripts to reduce the intellectual load. ",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25590,587335474,2021-03-04T10:13:12Z,templates/tools/dockerfile/python_debian11.include,"@@ -0,0 +1,53 @@+FROM debian:bullseye+  +# Install Git and basic packages.+RUN apt-get update && apt-get install -y ${'\\'}+  autoconf ${'\\'}+  autotools-dev ${'\\'}+  build-essential ${'\\'}+  bzip2 ${'\\'}+  ccache ${'\\'}+  curl ${'\\'}+  dnsutils ${'\\'}+  gcc ${'\\'}+  gcc-multilib ${'\\'}+  git ${'\\'}+  golang ${'\\'}+  gyp ${'\\'}+  lcov ${'\\'}+  libc6 ${'\\'}+  libc6-dbg ${'\\'}+  libc6-dev ${'\\'}+  libgtest-dev ${'\\'}+  libtool ${'\\'}+  make ${'\\'}+  perl ${'\\'}+  strace ${'\\'}+  telnet ${'\\'}+  unzip ${'\\'}+  wget ${'\\'}+  zip && apt-get clean++#================+# Build profiling+RUN apt-get update && apt-get install -y time && apt-get clean++# Install Python 3.7 from source (and installed as a default python3)+# (Bullseye comes with Python 3.9 which isn't supported by pytype yet)+RUN apt update && apt install -y build-essential zlib1g-dev libncurses5-dev libgdbm-dev ${'\\'}","why exactly do we need python  in the sanity docker image  (and why do we need a version that's supported by pytype?)while building from source is fine, it make the docker image rebuild time much longer and it also feels a bit non-standard, so I'd like to avoid it whenever we can. is there an ETA for pytype going to start supporting python3.9? Can you at least add a todo to get rid of python3.7 once the blocker is gone? ",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25590,587338000,2021-03-04T10:16:44Z,templates/tools/dockerfile/test/sanity/Dockerfile.template,"@@ -14,16 +14,12 @@   # See the License for the specific language governing permissions and   # limitations under the License.   -  <%include file=""../../python_debian10.include""/>+  <%include file=""../../python_debian11.include""/>","it looks like after this, python_debian10.include becomes unused - please check if that's the case and remove it if so.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587357647,2021-03-04T10:43:48Z,test/distrib/cpp/run_distrib_test_cmake.sh,"@@ -60,8 +60,9 @@ popd  # Just before installing gRPC, wipe out contents of all the submodules to simulate # a standalone build from an archive+# TODO(lidiz) re-enable it once we include the xDS protos in our release archive.",What is the plan for doing that? To me it's not clear whether it's feasible to include the xDS protos in our release archive (since that one is automatically generated by github) and seeing that you're disabling a check that's very important for the distribtests to be able to provide a useful signal makes me nervous.I don't think the right approach is to break stuff in a substantial way (which is what's currently being done in this PR and that's why you're commenting out the line - to make the distribtests appear to be passing while in reality the installation process is actually being broken) and than hope in the future we are going to fix that somehow (experience shows that's rarely the case ;-) ). Same for the other distribtest script changes.,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587366609,2021-03-04T10:56:58Z,build_autogenerated.yaml,"@@ -357,14 +349,14 @@ libs:   - src/core/lib/profiling/basic_timers.cc   - src/core/lib/profiling/stap_timers.cc   deps:-  - absl/types:optional-  - absl/time:time-  - absl/synchronization:synchronization-  - absl/strings:strings-  - absl/strings:str_format-  - absl/status:status-  - absl/memory:memory   - absl/base:base+  - absl/memory:memory","in the original version of extract_metadata_from_bazel_xml.py the dependencies are being sorted in a specific way (in topological ordering to ensure the makefile build still works and alphabetically to ensure stability of the generated build_autogenerated.yaml file over multiple runs, and then the resulting list is reversed and so it ends up reverse alphabetically sorted) - it looks like you're changing the rules for sorting, which in itself is ok, but it obscures the real changes being made in this PR and it's making a careful review more difficult.It would be much better if you actually preserved the the original way the deps get sorted (so I can see what is the real impact of changes) and after that we could discuss about the ordering rules (which would be a trivial review if it was reviewed separately or at least in a separate commit).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587410857,2021-03-04T11:59:02Z,Makefile,"@@ -1609,18 +1609,18 @@ endif   ifeq ($(SYSTEM),MINGW32)-$(LIBDIR)/$(CONFIG)/grpc$(SHARED_VERSION_CORE).$(SHARED_EXT_CORE): $(LIBGRPC_OBJS)  $(ZLIB_DEP) $(CARES_DEP) $(ADDRESS_SORTING_DEP) $(RE2_DEP) $(UPB_DEP) $(GRPC_ABSEIL_DEP) $(LIBDIR)/$(CONFIG)/libgpr.a $(LIBDIR)/$(CONFIG)/libaddress_sorting.a $(LIBDIR)/$(CONFIG)/libupb.a $(OPENSSL_DEP)+$(LIBDIR)/$(CONFIG)/grpc$(SHARED_VERSION_CORE).$(SHARED_EXT_CORE): $(LIBGRPC_OBJS)  $(ZLIB_DEP) $(CARES_DEP) $(ADDRESS_SORTING_DEP) $(RE2_DEP) $(UPB_DEP) $(GRPC_ABSEIL_DEP) $(LIBDIR)/$(CONFIG)/libgpr.a $(LIBDIR)/$(CONFIG)/libupb.a $(LIBDIR)/$(CONFIG)/libaddress_sorting.a $(OPENSSL_DEP)",looks like most of the changes (if not all) in this file (and in the other generated files as well) are just due to changing the ordering rules in the dependencies list and that seems unneccesary (see the other comment).,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587418707,2021-03-04T12:11:53Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -1064,7 +1116,23 @@ def _detect_and_print_issues(build_yaml_like): all_extra_metadata.update(     _generate_build_extra_metadata_for_tests(tests, bazel_rules)) -# Step 4: Generate the final metadata for all the targets.+# Step 4: Compute the build metdata that will be used in the final build.yaml.","nit: typo ""metdata"" (multiple times)",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587437858,2021-03-04T12:41:56Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -1064,7 +1116,23 @@ def _detect_and_print_issues(build_yaml_like): all_extra_metadata.update(     _generate_build_extra_metadata_for_tests(tests, bazel_rules)) -# Step 4: Generate the final metadata for all the targets.+# Step 4: Compute the build metdata that will be used in the final build.yaml.+# The final build metdata includes transitive dependencies, and sources/headers+# expanded without intermediate dependencies.+# Example:+# '//:grpc' : { ...,+#               '_TRANSITIVE_DEPS': ['//:gpr_base', ...],+#               '_COLLAPSED_DEPS': ['gpr', ...],+#               '_COLLAPSED_SRCS': [...],+#               '_COLLAPSED_PUBLIC_HEADERS': [...],+#               '_COLLAPSED_HEADERS': [...]+#             }+_populate_transitive_metadata(bazel_rules, all_extra_metadata.keys())++# Step 4a: Update the exist test metadata with the updated build metdata.","perhaps rephrase as sth like ""for some of the extra metadata for test targets, we need to have knowledge of the transitive metadata that wasn't available earlier"". The current comment isn't very informative.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587441078,2021-03-04T12:46:34Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -387,15 +457,10 @@ def _generate_build_metadata(build_extra_metadata, bazel_rules):                     for dep in lib_dict_to_update['deps']                 ]) -    # make sure deps are listed in reverse topological order (e.g. ""grpc gpr"" and not ""gpr grpc"")","why remove this? When linking, e.g. in the Makefile, we need the libraries being linked to come in reverse topological order, otherwise the linking process can fail. In build_autogenerated.yaml, the assumption is that the dependencies are listed in reverse topological order (as they show up in the same order in the Makefile).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587446067,2021-03-04T12:53:46Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.",I think here you can reuse some info from the original comment from https://github.com/grpc/grpc/blob/1dce57f35f0162ed37cf8f683d01b805cf8ea209/tools/buildgen/extract_metadata_from_bazel_xml.py#L347The original comment has more details than what you're mentioning here.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587447760,2021-03-04T12:56:19Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it","nit: add info about expected ordering of that list (should be reverse topological ordering, elements with the same rank should be sorted e.g. alphabetically)",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587450882,2021-03-04T13:01:15Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the new collapsed_deps, the new algorithm improved cases like:","This is a valuable improvement!nit:  in the comments you should not be referring to the state before and after this PR as ""old"" and ""new"", instead you can describe the old state as  ""in the past, the build metadata was generated with a full transitive list of dependencies"" and describe the new result.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587451720,2021-03-04T13:02:46Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the new collapsed_deps, the new algorithm improved cases like:++    The previous result:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The new result:+        end2end_tests -> [grpc_test_util]","qq just to verify: if one of the collapsed intermediate dependencies of end2end_tests depended on  e.g. ""gpr"" directly, ""gpr"" would still appear in the list, is that correct?perhaps you can add a note about that (it's quite an important cornercase)",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587463967,2021-03-04T13:21:49Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the new collapsed_deps, the new algorithm improved cases like:++    The previous result:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The new result:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = []+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # The follow if is used to suppress expanding absl libs; expanding+            # absl deps are technically correct but creates too many noise+            if external_dep_name_maybe is None or not external_dep_name_maybe.startswith(+                    'absl'):+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:+                    # This item is not processed before, compute now+                    _compute_transitive_metadata(dep, bazel_rules,+                                                 bazel_label_to_dep_name)+                _deduplicate_append(+                    transitive_deps,+                    bazel_rules[dep].get('_TRANSITIVE_DEPS', []))+                _deduplicate_append(collapsed_deps,+                                    bazel_rules[dep].get('_COLLAPSED_DEPS', []))+                exclude_deps.update(bazel_rules[dep].get('_EXCLUDE_DEPS', []))++        # This dep is a public target, add it as a dependency+        if dep in bazel_label_to_dep_name:+            _deduplicate_append(transitive_deps, [bazel_label_to_dep_name[dep]])+            _deduplicate_append(collapsed_deps, [bazel_label_to_dep_name[dep]])+            exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])+            exclude_deps.add(dep)+            continue -    expansion_blocklist = set()-    to_expand = set(bazel_deps)-    while to_expand:+        # This dep is an external target, add it as a dependency+        if external_dep_name_maybe is not None:+            _deduplicate_append(transitive_deps, [external_dep_name_maybe])+            _deduplicate_append(collapsed_deps, [external_dep_name_maybe])+            continue -        # start with the last dependency to be built-        build_order = _sort_by_build_order(list(to_expand), bazel_rules,-                                           'transitive_deps')+    # Direct dependencies are part of transitive dependencies+    _deduplicate_append(transitive_deps, direct_deps)++    # Removes all duplicated intermediate labels.+    # This is the step that further shorten the deps list.+    collapsed_deps = list(+        filter(lambda x: x not in exclude_deps, collapsed_deps))++    # Compute the final source files and headers, without all intermediate dependencies.+    for dep in transitive_deps:+        if dep not in exclude_deps:","readability: it is not clear what's the semantics of ""exclude_deps"" in this context.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587479613,2021-03-04T13:44:15Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the new collapsed_deps, the new algorithm improved cases like:++    The previous result:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The new result:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = []+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()","the ""exclude_deps"" semantics in unclear and that makes the following algorithm hard to read.Should you just call it e.g ""intermediate_deps"" since  ""exclude_deps"" is context-dependent and sometimes you want to exclude those deps, but sometimes you want to accumulate them (and that's making the algorithm confusing).Also it would be good to include a proper definition of what those are:  I think it's a subset of transitive deps such that every ""intermediate dep"" can be reached by following the dependency graph of the current target in a path that doesn't visit a public dependency?It's actually quite subtle since you can have some interesting situations:Situation 1:`publicA  -> publicB -> nonpublicC` (in which case nonpublicC is NOT an ""intermediate dependency"" of publicA, since it's only included through publicB and that break our chain of collapsing)Situation 2:`publicA  -> nonpublic publicB -> nonpublicC` `publicA -> nonpublicX -> nonpublic C`In this case nonpublicC is an ""intermediate dep"" of publicA since  it's reachable through deps that are all going to be collapsed.This actually matters from the build perspective since you might need the nonpublicC to be passed to the linker when building publicA",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587489026,2021-03-04T13:56:34Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the new collapsed_deps, the new algorithm improved cases like:++    The previous result:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The new result:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = []+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # The follow if is used to suppress expanding absl libs; expanding+            # absl deps are technically correct but creates too many noise+            if external_dep_name_maybe is None or not external_dep_name_maybe.startswith(+                    'absl'):+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:","I like the idea of recursively traversing the dependency graph and marking nodes as ""done"" but I think this recursive traversal should happen in a separate for loop at the beginning of the function and only after that we should perform other processing. Like this it feels we're doing too many things at once which is not good for readability.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,587490377,2021-03-04T13:58:15Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the new collapsed_deps, the new algorithm improved cases like:++    The previous result:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The new result:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = []+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # The follow if is used to suppress expanding absl libs; expanding+            # absl deps are technically correct but creates too many noise+            if external_dep_name_maybe is None or not external_dep_name_maybe.startswith(+                    'absl'):+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:+                    # This item is not processed before, compute now+                    _compute_transitive_metadata(dep, bazel_rules,+                                                 bazel_label_to_dep_name)+                _deduplicate_append(+                    transitive_deps,+                    bazel_rules[dep].get('_TRANSITIVE_DEPS', []))+                _deduplicate_append(collapsed_deps,+                                    bazel_rules[dep].get('_COLLAPSED_DEPS', []))+                exclude_deps.update(bazel_rules[dep].get('_EXCLUDE_DEPS', []))++        # This dep is a public target, add it as a dependency+        if dep in bazel_label_to_dep_name:+            _deduplicate_append(transitive_deps, [bazel_label_to_dep_name[dep]])+            _deduplicate_append(collapsed_deps, [bazel_label_to_dep_name[dep]])+            exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])",I think Scenario 2 described in my previous comment is broken because you blindly remove all transitive deps from the list of tentative intermediate deps?,
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/25610,587628378,2021-03-04T16:38:51Z,include/grpcpp/test/client_context_test_peer.h,"@@ -0,0 +1,60 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPCPP_TEST_CLIENT_CONTEXT_TEST_PEER_H+#define GRPCPP_TEST_CLIENT_CONTEXT_TEST_PEER_H++#include <grpcpp/client_context.h>++#include <map>++namespace grpc {+namespace testing {++/// A test-only class to access private members and methods of ClientContext.+class ClientContextTestPeer {+ public:+  explicit ClientContextTestPeer(ClientContext* const ctx) : ctx_(ctx) {}++  /// Inject metadata to the ClientContext for the test. The test spouse+  /// ##must be alive when \ a ClientContext::recv_initial_metadata is called.",Also the method is called GetServerInitialMetadata,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/25590,587691770,2021-03-04T17:55:45Z,templates/tools/dockerfile/python_debian11.include,"@@ -0,0 +1,53 @@+FROM debian:bullseye+  +# Install Git and basic packages.+RUN apt-get update && apt-get install -y ${'\\'}+  autoconf ${'\\'}+  autotools-dev ${'\\'}+  build-essential ${'\\'}+  bzip2 ${'\\'}+  ccache ${'\\'}+  curl ${'\\'}+  dnsutils ${'\\'}+  gcc ${'\\'}+  gcc-multilib ${'\\'}+  git ${'\\'}+  golang ${'\\'}+  gyp ${'\\'}+  lcov ${'\\'}+  libc6 ${'\\'}+  libc6-dbg ${'\\'}+  libc6-dev ${'\\'}+  libgtest-dev ${'\\'}+  libtool ${'\\'}+  make ${'\\'}+  perl ${'\\'}+  strace ${'\\'}+  telnet ${'\\'}+  unzip ${'\\'}+  wget ${'\\'}+  zip && apt-get clean++#================+# Build profiling+RUN apt-get update && apt-get install -y time && apt-get clean++# Install Python 3.7 from source (and installed as a default python3)+# (Bullseye comes with Python 3.9 which isn't supported by pytype yet)+RUN apt update && apt install -y build-essential zlib1g-dev libncurses5-dev libgdbm-dev ${'\\'}",Sanity docker needs Python because a couple of python linters will run (e.g. pytype) and pytype doesn't support Python 3.9 yet and there is not plan yet (https://github.com/google/pytype/issues/749) so I don't expect that they support anytime soon.I don't like to build something from source either since it increases the build time and not easy to maintain but python isn't that bad for both. It just needs 1-2 minutes more and build script isn't that cryptic.,X
10470658,donnadionne,https://api.github.com/repos/grpc/grpc/pulls/25601,587693495,2021-03-04T17:58:04Z,third_party/xxhash/BUILD,"@@ -0,0 +1,11 @@+cc_library(","I tried putting it in third_party/xxhash.BUILD (which works if we were adding the dependency via http_archieve as we can specify a build file location); but since I am just doing it directly in grpc_build_system.bzl i don't have a way to specify the BUILD file location it assumes it is in the directory and named as ""BUILD""",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25617,588113241,2021-03-05T08:25:54Z,templates/CMakeLists.txt.template,"@@ -145,9 +145,9 @@   set(gRPC_CORE_VERSION     ""${settings.core_version}"")   set(gRPC_CORE_SOVERSION   ""${settings.core_version.major}"")   set(gRPC_CPP_VERSION      ""${settings.cpp_version}"")-  set(gRPC_CPP_SOVERSION    ""${settings.cpp_version.major}"")+  set(gRPC_CPP_SOVERSION    ""${settings.core_version.major}"")",the SO version of grpc++ is actually supposed to be 1.x.  The grpc library (=C core) is versioned differently (and is currently at version 15).  We'd have to have a really strong reason to change that since the change you're proposing is intuitively incorrect.,
22016,weltling,https://api.github.com/repos/grpc/grpc/pulls/25617,588213664,2021-03-05T11:07:33Z,templates/CMakeLists.txt.template,"@@ -145,9 +145,9 @@   set(gRPC_CORE_VERSION     ""${settings.core_version}"")   set(gRPC_CORE_SOVERSION   ""${settings.core_version.major}"")   set(gRPC_CPP_VERSION      ""${settings.cpp_version}"")-  set(gRPC_CPP_SOVERSION    ""${settings.cpp_version.major}"")+  set(gRPC_CPP_SOVERSION    ""${settings.core_version.major}"")","And same way, checking the artifacts:```bash$ objdump -p libgrpc++.so | grep SONAME  SONAME               libgrpc++.so.1.37$ readelf -a libgrpc++_reflection.so | grep NEED  [ 6] .gnu.version_r    VERNEED          0000000000058200  00058200 0x0000000000000001 (NEEDED)             Shared library: [libgrpc++.so.1.37] 0x0000000000000001 (NEEDED)             Shared library: [libprotobuf.so.3.15.2.0] 0x0000000000000001 (NEEDED)             Shared library: [libstdc++.so.6] 0x0000000000000001 (NEEDED)             Shared library: [libgcc_s.so.1] 0x0000000000000001 (NEEDED)             Shared library: [libc.so.6] 0x000000006ffffffe (VERNEED)            0x58200 0x000000006fffffff (VERNEEDNUM)         3```Thanks",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/25619,588561606,2021-03-05T18:29:26Z,tools/profiling/microbenchmarks/bm_diff/requirements.txt,"@@ -0,0 +1,4 @@+tabulate>=0.8+scipy>=1.0.0+requests>=2.19.0+jwt>=0.5.0","Correction: I liked that idea, and changed the test setup script to utilize this requirements file. There's some definite redundancy in our python setup scripts though, I had to change the cryptography constraints in 3 different files for example. It's not trivial to deduplicate them, so I think that should be a cleanup for another time as well.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25619,588579847,2021-03-05T18:49:31Z,tools/internal_ci/helper_scripts/prepare_build_macos_rc,"@@ -67,7 +67,8 @@ then   time git clone --depth 1 https://github.com/CocoaPods/Specs.git ~/.cocoapods/repos/master    # Needed for ios-binary-size-  time pip install --user pyyaml pyjwt==1.7.1 pyOpenSSL cryptography requests+  time pip install --user pyyaml pyjwt>=2.0.1 pyOpenSSL \+    'cryptography>=3.3.1,<4.0.0' requests","optional: if the package requirement is complex, maybe we want to put them into a requirements.txt? Just like what you did for Linux.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/25619,588617879,2021-03-05T19:30:09Z,tools/internal_ci/helper_scripts/prepare_build_macos_rc,"@@ -67,7 +67,8 @@ then   time git clone --depth 1 https://github.com/CocoaPods/Specs.git ~/.cocoapods/repos/master    # Needed for ios-binary-size-  time pip install --user pyyaml pyjwt==1.7.1 pyOpenSSL cryptography requests+  time pip install --user pyyaml pyjwt>=2.0.1 pyOpenSSL \+    'cryptography>=3.3.1,<4.0.0' requests","Done, mostly. I deduplicated one overlapping set of requirements across two build options in this script. `pip install -r ...` is still in the script twice, once for each Objective-C and Python, but I think that's ok for now.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25629,588641037,2021-03-05T19:54:27Z,src/core/lib/channel/channelz.cc,"@@ -434,7 +436,11 @@ void PopulateSocketAddressJson(Json::Object* json, const char* name,     if (!port.empty()) {       port_num = atoi(port.data());     }-    char* b64_host = grpc_base64_encode(host.data(), host.size(), false, false);+    grpc_resolved_address resolved_host;+    grpc_string_to_sockaddr(&resolved_host, host.c_str(), port_num);+    std::string packed_host = grpc_sockaddr_get_packed_host(&resolved_host);+    char* b64_host = grpc_base64_encode(packed_host.data(), packed_host.size(),","Based on a cursory reading of `grpc_base64_encode` and some experimentation with packed data including the `\0` character, this actually does work as is. Note that the function takes a size instead of just relying on null termination.Regardless, switched over to `absl::Base64Escape`.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25642,588722808,2021-03-05T21:36:51Z,templates/tools/dockerfile/gcp_api_libraries.include,"@@ -1,2 +1,2 @@ # Google Cloud platform API libraries-RUN pip install --upgrade google-api-python-client oauth2client+RUN pip install --upgrade google-auth==1.24.0 google-api-python-client==1.12.8 oauth2client==4.1.0","If we're pinning dependencies, we want to do it for the entire transitive dependency closure. Typically, I would do this as follows:```pip install ${FIRST_ORDER_DEPENDENCIES}pip freeze > requirements.lock```Then, in the Dockerfile, include `pip install -r requirements.lock`",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25642,588759004,2021-03-05T22:23:16Z,templates/tools/dockerfile/gcp_api_libraries.include,"@@ -1,2 +1,2 @@ # Google Cloud platform API libraries-RUN pip install --upgrade google-api-python-client oauth2client+RUN pip install --upgrade google-auth==1.24.0 google-api-python-client==1.12.8 oauth2client==4.1.0","The requirements.lock is pretty long. It would be tricky to manage across Dockerfiles. I think one of the points of having requirements file is allowing source control with one ground truth config. I will try to create a common requirements.lock for our CI jobs, and update our `push_testing_images.sh` to be able to fetch the requirements file while building Docker images.(BTW, this is a backport to v1.36.x to enable interop images build)",X
26072277,dfawley,https://api.github.com/repos/grpc/grpc/pulls/25641,588764742,2021-03-05T22:36:22Z,tools/run_tests/run_xds_tests.py,"@@ -1574,6 +1575,7 @@ def test_timeout(gcp, original_backend_service, instance_group):                                     testcase_name, rpc, status, qty, want)                         success = False                 if success:+                    logger.info('success')","This seems to be the convention for now: 5 other test just print a simple 'success' when they are done.  Technically it's not necessary (all the other tests don't print anything at all when they pass), and IMO, something like this belongs in the main test loop instead.",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/25621,588781019,2021-03-05T23:25:40Z,test/core/tsi/alts/fake_handshaker/fake_handshaker_server.cc,"@@ -236,6 +236,7 @@ class FakeHandshakerService : public HandshakerService::Service {     result.mutable_local_identity()->set_service_account(""local_identity"");     string key(1024, '\0');     result.set_key_data(key);+    result.set_max_frame_size(16384);","The change in this PR will be used only in the test, so it will not have any effect on envoy itself. By setting the frame size in the result, we want to verify [this code path](https://github.com/grpc/grpc/blob/master/src/core/tsi/alts/handshaker/alts_tsi_handshaker.cc#L172-L179) is indeed executed.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,588787213,2021-03-05T23:48:04Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.","```    All other intermediate dependencies will be merged, which means their    source file, headers, etc. will be collected into one build target. This    step of processing will greatly reduce the complexity of the generated    build specifications for other build systems, like CMake, Make, setuptools.```I updated the comment to mention other build systems instead. I think the same set of info is conveyed here:1. We will squash intermediate deps;2. This helps other build systems.About the original TODO note, I don't have a strong opinion on whether we should use intermediate deps or not, so I copied the TODO note to `_populate_transitive_metadata`.",
26072277,dfawley,https://api.github.com/repos/grpc/grpc/pulls/25641,588791105,2021-03-06T00:03:04Z,tools/run_tests/run_xds_tests.py,"@@ -69,10 +69,11 @@     'header_matching',     'circuit_breaking',     'timeout',+    'fault_injection', ]",We have to change the scripts in the various repos to start including it.  E.g.https://github.com/grpc/grpc/blob/master/tools/internal_ci/linux/grpc_xds_bazel_test_in_docker.shhttps://github.com/grpc/grpc-go/blob/master/test/kokoro/xds.shhttps://github.com/grpc/grpc-java/blob/master/buildscripts/kokoro/xds.sh(etc),X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,588793007,2021-03-06T00:10:08Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it","A description about the order is added (it will be alphabetical). The collapsed deps (or optimized deps) won't have two dependencies that has build order issue. For example, if `dep A -> dep B`, then the collapsed deps will be `[A]` instead of `[B, A]`.Good catch on the ordering, I have sorted the list.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,588793571,2021-03-06T00:12:36Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the new collapsed_deps, the new algorithm improved cases like:","Reword the comment about ""new"" and ""previous"". Emm... I think the `the build metadata was generated with a full transitive list of dependencies` description might not be accurate here, since it wasn't a full transitive list of dependencies.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,588795165,2021-03-06T00:19:03Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -356,7 +426,7 @@ def _generate_build_metadata(build_extra_metadata, bazel_rules):         # while in bazel builds it is customary to define larger number of smaller         # ""sublibraries"". The need for elision (and expansion)         # of intermediate libraries can be re-evaluated in the future.-        _expand_intermediate_deps(lib_dict, lib_names, bazel_rules)+        # _expand_intermediate_deps(lib_dict, lib_names, bazel_rules)",The TODO note is moved to `_populate_transitive_metadata`. The description is merged with docstring in `_compute_transitive_metadata`.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,588796259,2021-03-06T00:23:55Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -1064,7 +1116,23 @@ def _detect_and_print_issues(build_yaml_like): all_extra_metadata.update(     _generate_build_extra_metadata_for_tests(tests, bazel_rules)) -# Step 4: Generate the final metadata for all the targets.+# Step 4: Compute the build metdata that will be used in the final build.yaml.+# The final build metdata includes transitive dependencies, and sources/headers+# expanded without intermediate dependencies.+# Example:+# '//:grpc' : { ...,+#               '_TRANSITIVE_DEPS': ['//:gpr_base', ...],+#               '_COLLAPSED_DEPS': ['gpr', ...],+#               '_COLLAPSED_SRCS': [...],+#               '_COLLAPSED_PUBLIC_HEADERS': [...],+#               '_COLLAPSED_HEADERS': [...]+#             }+_populate_transitive_metadata(bazel_rules, all_extra_metadata.keys())++# Step 4a: Update the exist test metadata with the updated build metdata.",Good catch! Updated to:```# Certain build metadata of certain test targets depend on the transitive# metadata that wasn't available earlier.```,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,588798204,2021-03-06T00:32:20Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -136,22 +160,50 @@ def _extract_nonpublic_headers(bazel_rule):     return list(sorted(result))  -def _extract_sources(bazel_rule):+def _extract_sources(bazel_rule: BuildMetadata) -> List[str]:     """"""Gets list of source files from a bazel rule""""""     result = []-    for dep in bazel_rule['srcs']:-        if dep.startswith('//') and (dep.endswith('.cc') or dep.endswith('.c')-                                     or dep.endswith('.proto')):-            result.append(_extract_source_file_path(dep))+    for src in bazel_rule['srcs']:+        if src.endswith('.cc') or src.endswith('.c') or src.endswith('.proto'):+            if src.startswith('//'):+                # This source file is local to gRPC+                result.append(_extract_source_file_path(src))+            elif RE_MATCH_SUPPORTED_EXTERNAL_LIBS.match(src):+                # This source file is external, and we need to translate the+                # @REPO_NAME to a valid path prefix. At this stage, we need+                # to check repo name, since the label/path mapping is not+                # available in BUILD files.+                external_library_name = RE_MATCH_SUPPORTED_EXTERNAL_LIBS.match(+                    src).group(1)+                result.append(+                    src.replace(+                        f""@{external_library_name}//"",+                        EXTERNAL_LIB_LABEL_PATH_MAP[external_library_name]).+                    replace(':', ""/""))+            else:+                # We don't know how to map these labels to a concrete source+                # file path, so let's skip them.+                pass     return list(sorted(result))  -def _extract_deps(bazel_rule):+def _extract_deps(bazel_rule: BuildMetadata,+                  bazel_rules: BuildDict) -> List[str]:     """"""Gets list of deps from from a bazel rule""""""-    return list(sorted(bazel_rule['deps']))---def _create_target_from_bazel_rule(target_name, bazel_rules):+    deps = set(bazel_rule['deps'])+    for src in bazel_rule['srcs']:+        if not src.endswith('.cc') and not src.endswith(+                '.c') and not src.endswith('.proto'):+            if src in bazel_rules:+                # This label doesn't point to a source file, but another Bazel","Here is the source of evil [`pgv_proto_library.bzl`](https://github.com/envoyproxy/protoc-gen-validate/blob/b79666f392e895cccd77bf22a366dcac4aebd3e2/bazel/pgv_proto_library.bzl#L49). And they are using Bazel native `cc_library` for this, so I think this must be allowed for Bazel BUILD.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,588802668,2021-03-06T00:53:51Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the new collapsed_deps, the new algorithm improved cases like:++    The previous result:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The new result:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = []+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # The follow if is used to suppress expanding absl libs; expanding+            # absl deps are technically correct but creates too many noise+            if external_dep_name_maybe is None or not external_dep_name_maybe.startswith(+                    'absl'):+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:+                    # This item is not processed before, compute now+                    _compute_transitive_metadata(dep, bazel_rules,+                                                 bazel_label_to_dep_name)+                _deduplicate_append(+                    transitive_deps,+                    bazel_rules[dep].get('_TRANSITIVE_DEPS', []))+                _deduplicate_append(collapsed_deps,+                                    bazel_rules[dep].get('_COLLAPSED_DEPS', []))+                exclude_deps.update(bazel_rules[dep].get('_EXCLUDE_DEPS', []))++        # This dep is a public target, add it as a dependency+        if dep in bazel_label_to_dep_name:+            _deduplicate_append(transitive_deps, [bazel_label_to_dep_name[dep]])+            _deduplicate_append(collapsed_deps, [bazel_label_to_dep_name[dep]])+            exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])+            exclude_deps.add(dep)+            continue -    expansion_blocklist = set()-    to_expand = set(bazel_deps)-    while to_expand:+        # This dep is an external target, add it as a dependency+        if external_dep_name_maybe is not None:+            _deduplicate_append(transitive_deps, [external_dep_name_maybe])+            _deduplicate_append(collapsed_deps, [external_dep_name_maybe])+            continue -        # start with the last dependency to be built-        build_order = _sort_by_build_order(list(to_expand), bazel_rules,-                                           'transitive_deps')+    # Direct dependencies are part of transitive dependencies+    _deduplicate_append(transitive_deps, direct_deps)++    # Removes all duplicated intermediate labels.+    # This is the step that further shorten the deps list.+    collapsed_deps = list(+        filter(lambda x: x not in exclude_deps, collapsed_deps))++    # Compute the final source files and headers, without all intermediate dependencies.+    for dep in transitive_deps:+        if dep not in exclude_deps:","I added a short example run of this loop, please let me know if it makes sense or not:```    # Imaging a build target X has transitive deps [A, B, C, D, E]. C and E are    # public build targets. And D is a transitive dependency of C.    #    # Translate the condition into dependency graph:    #   X -> [A, B, C, D, E]    #   C -> [D]    #   Public targets [X, C, E]    #    # The collapsed dependencies of X: [C, E].    # The excluded dependencies of X: [C, D, E].    #    # Target X should include source files and headers of [X, A, B] as final    # build.```",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,588808205,2021-03-06T01:21:04Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the new collapsed_deps, the new algorithm improved cases like:++    The previous result:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The new result:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = []+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()","`exclude_deps` is not intermediate dependencies, but the dependency labels that are already included by collapsed_deps. I added a example below, but I think it also applies here:```    # Imaging a build target X has transitive deps [A, B, C, D, E]. C and E are    # public build targets. And D is a transitive dependency of C.    #    # Translate the condition into dependency graph:    #   X -> [A, B, C, D, E]    #   C -> [D]    #   Public targets [X, C, E]    #    # The collapsed dependencies of X: [C, E].    # The excluded dependencies of X: [C, D, E].    #    # Target X should include source files and headers of [X, A, B] as final    # build.```Emm... if this algorithm is broken, our tests should broke too, right? After all, build systems are not smart enough of fetching sources/headers that is not included in the BUILD spec.I totally understand the worry here, and I fell into that trap once when I'm implementing this algorithm. The recursion is useful to produce the **final dependencies** list and the **excluded dependencies** list, but won't work for source files and headers. Here's why:```bash# Let's say we have following deps graph:X -> [A, B, C]Y -> [C]Z -> [X, Y]Public targets: [X, Y, Z]# When our recursion reaches X, it will include the source file from C.# When we reach Y, it will also include the source file from C.# Or:X.srcs = merge(X.srcs, A.srcs, B.srcs, C.srcs)Y.srcs = merge(Y.srcs, C.srcs)# Is Z.srcs = merge(X.srcs, Y.srcs)? No.# The correct formula is finding Z's intermediate dependencies:Z.intermediate_deps = Z.transitive_deps - merge(X.exclude_deps, Y.exclude_deps, Z.exlucde_deps)# Then we can get Z's source file:Z.srcs = merge(Z.intermediate_deps...)```So, given number of targets `N`, the time complexity of the new algorithm is not `O(n)` but `O(n^2)`. Previous algorithm  is also `O(n^2)`, because for every public targets it needs to traverse most of its dependencies. Technically, the new algorithm has smaller overhead, because intermediate deps will only be visited once.---About sanity tests, today, some source files are included multiple times (see [head](https://paste.googleplex.com/6169744689332224), and [this PR](https://paste.googleplex.com/5851856241688576)), this PR is no worse than what we have on HEAD.---I might have corner cases that not well thought-out. Please name them, and we can discuss in details.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/25628,589634776,2021-03-08T18:03:45Z,include/grpcpp/impl/codegen/security/auth_context.h,"@@ -42,7 +42,7 @@ class AuthPropertyIterator   AuthPropertyIterator operator++(int);   bool operator==(const AuthPropertyIterator& rhs) const;   bool operator!=(const AuthPropertyIterator& rhs) const;-  const AuthProperty operator*();+  AuthProperty operator*();","This is an interesting case. It implements pointer dereference semantics, but returns _by value_. Should this have maybe returned a const reference instead?",
28025951,HannahShiSFB,https://api.github.com/repos/grpc/grpc/pulls/25640,590121789,2021-03-09T09:23:29Z,examples/php/greeter_and_routeguide_multi_server.php,"@@ -0,0 +1,47 @@+<?php+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++require dirname(__FILE__) . '/../../src/php/lib/Grpc/MethodDescriptor.php';",It's temporary. These lines will be deleted after php server feature release.,
28025951,HannahShiSFB,https://api.github.com/repos/grpc/grpc/pulls/25640,590121898,2021-03-09T09:23:36Z,examples/php/greeter_server.php,"@@ -0,0 +1,44 @@+<?php+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++require dirname(__FILE__) . '/../../src/php/lib/Grpc/MethodDescriptor.php';",It's temporary. These lines will be deleted after php server feature release.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25619,590188395,2021-03-09T10:27:21Z,tools/internal_ci/helper_scripts/requirements.macos.txt,"@@ -0,0 +1,7 @@+cryptography==3.4.6+PyJWT==2.0.1+pyOpenSSL==20.0.1+PyYAML==5.4.1+requests==2.25.1+scipy==1.5.4","Now the list looks more reasonable, thanks!",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,590255213,2021-03-09T11:27:58Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.","Oh, sorry for not being clearer - my question was was about the purpose of ""_EXCLUDE_DEPS"" -  The name ""_exclude_deps"" is not very specific (what gets excluded from where seems important) and the commentary ""the intermediate dependency label"" also doesn't explain much (frankly I don't know what it's supposed to mean).Is it correct to say that the _exclude_deps   are all transitive deps of all our public deps? (at least it seems to be that way from reading the code).",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,590294383,2021-03-09T12:03:49Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -247,119 +244,197 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    build specifications for other build systems, like CMake, Make, setuptools.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list in alphabetical order;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the collapsed_deps, the algorithm improved cases like:++    The result in the past:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The result of the algorithm:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = []+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # Suppress expanding absl libs; expanding absl deps are technically+            # correct but creates too much noise+            if external_dep_name_maybe is None or not external_dep_name_maybe.startswith(+                    'absl'):+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:+                    # This item is not processed before, compute now+                    _compute_transitive_metadata(dep, bazel_rules,+                                                 bazel_label_to_dep_name)+                _deduplicate_append(+                    transitive_deps,+                    bazel_rules[dep].get('_TRANSITIVE_DEPS', []))+                _deduplicate_append(collapsed_deps,+                                    bazel_rules[dep].get('_COLLAPSED_DEPS', []))+                exclude_deps.update(bazel_rules[dep].get('_EXCLUDE_DEPS', []))++        # This dep is a public target, add it as a dependency+        if dep in bazel_label_to_dep_name:+            _deduplicate_append(transitive_deps, [bazel_label_to_dep_name[dep]])+            _deduplicate_append(collapsed_deps, [bazel_label_to_dep_name[dep]])+            exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])",I think if you added a comment here `# add all the transitive deps of our every public dep to exclude list since we want to avoid building sources that are already built our dependencies` it would make the intent clearer?,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,590297759,2021-03-09T12:08:19Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -247,119 +244,197 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    build specifications for other build systems, like CMake, Make, setuptools.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list in alphabetical order;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the collapsed_deps, the algorithm improved cases like:++    The result in the past:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The result of the algorithm:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = []+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # Suppress expanding absl libs; expanding absl deps are technically+            # correct but creates too much noise+            if external_dep_name_maybe is None or not external_dep_name_maybe.startswith(+                    'absl'):+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:+                    # This item is not processed before, compute now+                    _compute_transitive_metadata(dep, bazel_rules,+                                                 bazel_label_to_dep_name)+                _deduplicate_append(+                    transitive_deps,+                    bazel_rules[dep].get('_TRANSITIVE_DEPS', []))+                _deduplicate_append(collapsed_deps,+                                    bazel_rules[dep].get('_COLLAPSED_DEPS', []))+                exclude_deps.update(bazel_rules[dep].get('_EXCLUDE_DEPS', []))++        # This dep is a public target, add it as a dependency+        if dep in bazel_label_to_dep_name:+            _deduplicate_append(transitive_deps, [bazel_label_to_dep_name[dep]])+            _deduplicate_append(collapsed_deps, [bazel_label_to_dep_name[dep]])+            exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])+            exclude_deps.add(dep)","above you say that ""exclude_deps"" is ""the intemediate dependency labels"", but on this line you're adding something that you know is a direct public dep (= not an intermediate dep). Is that intended? It seems wrong.also, a few lines lower you're removing  exclude_deps from collapsed_deps (in `collapsed_deps = list(filter(lambda x: x not in exclude_deps, collapsed_deps))`, and it seems to me that that would remove all the direct public deps from _collapsed_deps  - but IIUC those are deps you actually want to surface in build.yaml eventually.  I'm confused.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,590319231,2021-03-09T12:28:06Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -247,119 +244,197 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    build specifications for other build systems, like CMake, Make, setuptools.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list in alphabetical order;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the collapsed_deps, the algorithm improved cases like:++    The result in the past:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The result of the algorithm:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = []+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # Suppress expanding absl libs; expanding absl deps are technically+            # correct but creates too much noise+            if external_dep_name_maybe is None or not external_dep_name_maybe.startswith(+                    'absl'):+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:+                    # This item is not processed before, compute now+                    _compute_transitive_metadata(dep, bazel_rules,+                                                 bazel_label_to_dep_name)+                _deduplicate_append(+                    transitive_deps,+                    bazel_rules[dep].get('_TRANSITIVE_DEPS', []))+                _deduplicate_append(collapsed_deps,+                                    bazel_rules[dep].get('_COLLAPSED_DEPS', []))+                exclude_deps.update(bazel_rules[dep].get('_EXCLUDE_DEPS', []))++        # This dep is a public target, add it as a dependency+        if dep in bazel_label_to_dep_name:+            _deduplicate_append(transitive_deps, [bazel_label_to_dep_name[dep]])+            _deduplicate_append(collapsed_deps, [bazel_label_to_dep_name[dep]])+            exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])+            exclude_deps.add(dep)+            continue -    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']+        # This dep is an external target, add it as a dependency+        if external_dep_name_maybe is not None:+            _deduplicate_append(transitive_deps, [external_dep_name_maybe])+            _deduplicate_append(collapsed_deps, [external_dep_name_maybe])+            continue -    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+    # Direct dependencies are part of transitive dependencies+    _deduplicate_append(transitive_deps, direct_deps)++    # Removes all duplicated intermediate labels.+    # This is the step that further shorten the deps list.+    collapsed_deps = list(+        filter(lambda x: x not in exclude_deps, collapsed_deps))++    # Compute the final source files and headers for this build target whose+    # name is `rule_name` (input argument of this function).+    #+    # Imaging a build target X has transitive deps [A, B, C, D, E]. C and E are+    # public build targets. And D is a transitive dependency of C.+    #+    # Translate the condition into dependency graph:+    #   X -> [A, B, C, D, E]+    #   C -> [D]+    #   Public targets [X, C, E]+    #+    # The collapsed dependencies of X: [C, E].+    # The excluded dependencies of X: [C, D, E].+    #+    # Target X should include source files and headers of [X, A, B] as final+    # build.+    for dep in transitive_deps:+        if dep not in exclude_deps:+            if dep in bazel_rules:+                collapsed_srcs.update(_extract_sources(bazel_rules[dep]))+                collapsed_public_headers.update(+                    _extract_public_headers(bazel_rules[dep]))+                collapsed_headers.update(+                    _extract_nonpublic_headers(bazel_rules[dep]))++    # This item is a ""visited"" flag+    bazel_rule['_PROCESSING_DONE'] = True+    # Following items are described in the docstinrg.+    bazel_rule['_TRANSITIVE_DEPS'] = transitive_deps+    bazel_rule['_COLLAPSED_DEPS'] = list(sorted(collapsed_deps))","if you're sorting the collapsed_deps anyway, you can save yourself the trouble  with storing it in a list and calling `_deduplicate_append`.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,590326327,2021-03-09T12:34:04Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -247,119 +244,197 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    build specifications for other build systems, like CMake, Make, setuptools.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list in alphabetical order;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the collapsed_deps, the algorithm improved cases like:++    The result in the past:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The result of the algorithm:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = []+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # Suppress expanding absl libs; expanding absl deps are technically+            # correct but creates too much noise+            if external_dep_name_maybe is None or not external_dep_name_maybe.startswith(+                    'absl'):+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:+                    # This item is not processed before, compute now+                    _compute_transitive_metadata(dep, bazel_rules,+                                                 bazel_label_to_dep_name)+                _deduplicate_append(+                    transitive_deps,+                    bazel_rules[dep].get('_TRANSITIVE_DEPS', []))+                _deduplicate_append(collapsed_deps,+                                    bazel_rules[dep].get('_COLLAPSED_DEPS', []))+                exclude_deps.update(bazel_rules[dep].get('_EXCLUDE_DEPS', []))++        # This dep is a public target, add it as a dependency+        if dep in bazel_label_to_dep_name:+            _deduplicate_append(transitive_deps, [bazel_label_to_dep_name[dep]])+            _deduplicate_append(collapsed_deps, [bazel_label_to_dep_name[dep]])+            exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])+            exclude_deps.add(dep)+            continue -    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']+        # This dep is an external target, add it as a dependency+        if external_dep_name_maybe is not None:+            _deduplicate_append(transitive_deps, [external_dep_name_maybe])+            _deduplicate_append(collapsed_deps, [external_dep_name_maybe])+            continue -    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+    # Direct dependencies are part of transitive dependencies+    _deduplicate_append(transitive_deps, direct_deps)++    # Removes all duplicated intermediate labels.+    # This is the step that further shorten the deps list.+    collapsed_deps = list(+        filter(lambda x: x not in exclude_deps, collapsed_deps))++    # Compute the final source files and headers for this build target whose+    # name is `rule_name` (input argument of this function).+    #+    # Imaging a build target X has transitive deps [A, B, C, D, E]. C and E are+    # public build targets. And D is a transitive dependency of C.+    #+    # Translate the condition into dependency graph:+    #   X -> [A, B, C, D, E]+    #   C -> [D]+    #   Public targets [X, C, E]+    #+    # The collapsed dependencies of X: [C, E].+    # The excluded dependencies of X: [C, D, E].+    #+    # Target X should include source files and headers of [X, A, B] as final+    # build.+    for dep in transitive_deps:+        if dep not in exclude_deps:+            if dep in bazel_rules:+                collapsed_srcs.update(_extract_sources(bazel_rules[dep]))+                collapsed_public_headers.update(+                    _extract_public_headers(bazel_rules[dep]))+                collapsed_headers.update(+                    _extract_nonpublic_headers(bazel_rules[dep]))++    # This item is a ""visited"" flag+    bazel_rule['_PROCESSING_DONE'] = True+    # Following items are described in the docstinrg.+    bazel_rule['_TRANSITIVE_DEPS'] = transitive_deps+    bazel_rule['_COLLAPSED_DEPS'] = list(sorted(collapsed_deps))","Btw, since _COLLAPSED_DEPS get directly exposed as ""deps"" in build.yaml, you should know that the targets really need to come in  reverse topological order sorting.In our Makefile, we simply pass the depdencies to the link in the order they are list in build.yaml and the order of passing libraries to the linker does matter. I think in your case this doesn't break anything since our Makefile currently only has a few targets and you happen to pass the deps in the right order (basically by coincidence), but that doesn't mean that the overall approach is correct (and not ensuring the ordering is correct could mean that the build suddenly breaks in the future because of e.g. a refactoring in BUILD where the generated deps don't come in the right order anyway).",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,590339433,2021-03-09T12:49:01Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -34,14 +34,36 @@ import yaml import xml.etree.ElementTree as ET import os+import collections import sys-import build_cleaner--_ROOT = os.path.abspath(os.path.join(os.path.dirname(sys.argv[0]), '../..'))-os.chdir(_ROOT)+import re+from typing import List, Any, Dict, Optional, Iterable +import build_cleaner -def _bazel_query_xml_tree(query):+PROJECT_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..',+                            '..')+os.chdir(PROJECT_ROOT)++BuildMetadata = Dict[str, Any]+BuildDict = Dict[str, BuildMetadata]+BuildYaml = Dict[str, Any]++# Map from the external repository's labels to their path prefixes.+EXTERNAL_LIB_LABEL_PATH_MAP = dict(+    envoy_api='third_party/envoy-api/',","I'm wondering what's the minimum set of changes you need to do to extract_metadata_from_bazel_xml.py to actually include the build.yaml generation logic for the external protos. I'm not convinced all the `extract_metadata_from_bazel_xml.py` changes you're proposing are actually required for that (I'm not against those changes per se, they actually might be useful, but I think they should be considered separately unless you absolutely need them to get the external proto generation stuff done).",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25655,590666262,2021-03-09T19:41:45Z,src/core/ext/transport/chttp2/server/chttp2_server.cc,"@@ -86,6 +86,20 @@ class Chttp2ServerListener : public Server::ListenerInterface {    void Orphan() override; +  // The interface of RefCounted<> has been manually implemented here to take a ref on tcp_server_ instead. Note that, the handshaker needs tcp_server_ to exist for the lifetime of the handshake since it's needed by acceptor. Sharing refs between the listener and tcp_server_ is just an optimization to avoid taking additional refs on the listener, since TcpServerShutdownComplete already holds a ref to the listener.",Please wrap long line at 80 characters.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25655,590666666,2021-03-09T19:42:23Z,src/core/ext/transport/chttp2/server/chttp2_server.cc,"@@ -86,6 +86,20 @@ class Chttp2ServerListener : public Server::ListenerInterface {    void Orphan() override; +  // The interface of RefCounted<> has been manually implemented here to take a ref on tcp_server_ instead. Note that, the handshaker needs tcp_server_ to exist for the lifetime of the handshake since it's needed by acceptor. Sharing refs between the listener and tcp_server_ is just an optimization to avoid taking additional refs on the listener, since TcpServerShutdownComplete already holds a ref to the listener.+  void IncrementRefCount() {","This method can be private, and you can make `RefCountedPtr<>` a friend of this class.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25676,591902791,2021-03-10T21:59:11Z,tools/run_tests/run_xds_tests.py,"@@ -2595,7 +2595,12 @@ def __init__(self, compute, alpha_compute, project, project_num):         client_env['GRPC_XDS_BOOTSTRAP'] = bootstrap_path         client_env['GRPC_XDS_EXPERIMENTAL_CIRCUIT_BREAKING'] = 'true'         client_env['GRPC_XDS_EXPERIMENTAL_ENABLE_TIMEOUT'] = 'true'-        client_env['GRPC_XDS_EXPERIMENTAL_FAULT_INJECTION'] = 'true'+        # Temporarily turn off fault injection, because HTTPFault filter isn't+        # handled correctly yet in CPP and Go. And setting would break all the+        # other tests. Uncomment the following line when the support is+        # complete.+        #+        # client_env['GRPC_XDS_EXPERIMENTAL_FAULT_INJECTION'] = 'true'",That sounds really bad.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25687,592786349,2021-03-11T23:05:47Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -7523,6 +7518,28 @@ TEST_P(XdsEnabledServerTest, ListenerAddressMismatch) {   EXPECT_TRUE(rpc_failed); } +TEST_P(XdsEnabledServerTest, UseOriginalDstNotSupported) {+  Listener listener;+  listener.set_name(+      absl::StrCat(""grpc/server?xds.resource.listening_address="",+                   ipv6_only_ ? ""[::1]:"" : ""127.0.0.1:"", backends_[0]->port()));+  balancers_[0]->ads_service()->SetLdsResource(listener);+  listener.mutable_address()->mutable_socket_address()->set_address(+      ipv6_only_ ? ""::1"" : ""127.0.0.1"");+  listener.mutable_address()->mutable_socket_address()->set_port_value(+      backends_[0]->port());+  listener.add_filter_chains()->add_filters()->mutable_typed_config()->PackFrom(+      HttpConnectionManager());+  listener.mutable_use_original_dst()->set_value(true);+  balancers_[0]->ads_service()->SetLdsResource(listener);+  CheckRpcSendFailure(1, RpcOptions().set_wait_for_ready(true));","Yes.  `wait_for_ready` only comes into play when the channel transitions into `TRANSIENT_FAILURE`, and that won't happen until the connection attempt has failed.  Using `wait_for_ready` prevents the call from failing when the channel goes into `TRANSIENT_FAILURE`, which means that the reason the call is ultimately failing is that it hits its deadline.  If you check the failure state of this RPC, I believe you'll see that it's `DEADLINE_EXCEEDED` instead of `UNAVAILABLE`, which is what we should be looking for in this test.Hmm... That having been said, even if we don't use `wait_for_ready`, how do we know that the server has actually gotten the config by the time this RPC fails?  The RPC has a 1s timeout.  If the client fails to connect to the server before the server actually receives the config, then the RPC sent by the client will fail before we know for sure that the server would not start listening -- this part of the test would pass even though the server is not behaving correctly.Of course, if that happens, then the following check (for the LDS resource being NACKed) will fail.  But do we know that that check is not flaky?  It seems like even if the server is behaving correctly, the test could fail some percentage of the time due to the server not yet having had time to NACK the resource by the time we check the status.Maybe this test would be more reliable if we used `backends_[0]->notifier()->WaitOnServingStatusChange()` to check explicitly what the backend's status is?  Once we get the status, we know that the server has seen the config, and then we can check that the client's RPC fails and that the xDS server sees the NACK.Actually, it seems like this same comment applies to basically all of the xDS-enabled gRPC server tests.  Maybe we should use the status notifier for all of these tests, now that we have it?",X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/25538,592879034,2021-03-12T03:05:48Z,src/compiler/BUILD,"@@ -69,6 +69,7 @@ grpc_cc_library(         ""schema_interface.h"",     ],     external_deps = [+        ""absl/strings"",",Can you exclude this from the dependency and implement replace_all without absl? This is because code_generator's build is a bit tighter so it cannot depend on abseil sometimes. We might be able to address this but it'd be quicker to change it not to rely on it.,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/25538,592879849,2021-03-12T03:08:25Z,src/compiler/generator_helpers.h,"@@ -245,9 +247,9 @@ inline std::string GenerateCommentsWithPrefix(     if (elem.empty()) {       oss << prefix << ""\n"";     } else if (elem[0] == ' ') {-      oss << prefix << elem << ""\n"";+      oss << prefix << absl::StrReplaceAll(elem, {{""$"", ""$$""}}) << ""\n"";",Can you define '$' somewhere and reference it rather than using it directly? and make all generators use the same constant; https://github.com/grpc/grpc/blob/d2c2d66a03fcc2a8244361ea3479c60ac81055da/src/compiler/csharp_generator.cc#L752,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,592911252,2021-03-12T05:00:39Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it","I did some experiment around the `make` artifacts. Since `Makefile` are now Core-only, I just tested if Core works. Here is a simple code that prints channelz stats, which underlying invokes methods from `gpr`/`abseil`. If the linking has failed or has missing component, it should fail:```c#include <stdio.h>#include <grpc/grpc.h>#include <grpc/grpc.h>int main(int argc, char** argv) {  grpc_init();  printf(""%s\n"", grpc_channelz_get_top_channels(0));  grpc_shutdown();  return 0;}``````$ gcc simple.c -lgrpc && ./a.out{""end"":true}```The output seems fine, and the binary isn't crashing. TBH, I think I'm weak on how `ld` functions, but if you have a test that can proof this build is broken, I can try it out. So far, I haven't seen evidence that that this new linking method won't work.I guess your worry might came true if we strip symbols aggressively for each of the `.a`. CC @gnossen can you help us?",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,592912182,2021-03-12T05:04:32Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.","I think the ""transitive deps of all our public deps"" description is very accurate. Updated the comment:```* _EXCLUDE_DEPS: transitive dependencies of public deps that this build      target depends on, and they will be excluded in the final build metadata.```",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,592914963,2021-03-12T05:14:59Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -245,98 +242,171 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name--    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']--    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    CMake build files.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers.+    * _EXCLUDE_DEPS: the intermediate dependency labels.++    For the new collapsed_deps, the new algorithm improved cases like:++    The previous result:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The new result:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = []+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # The follow if is used to suppress expanding absl libs; expanding+            # absl deps are technically correct but creates too many noise+            if external_dep_name_maybe is None or not external_dep_name_maybe.startswith(+                    'absl'):+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:+                    # This item is not processed before, compute now+                    _compute_transitive_metadata(dep, bazel_rules,+                                                 bazel_label_to_dep_name)+                _deduplicate_append(+                    transitive_deps,+                    bazel_rules[dep].get('_TRANSITIVE_DEPS', []))+                _deduplicate_append(collapsed_deps,+                                    bazel_rules[dep].get('_COLLAPSED_DEPS', []))+                exclude_deps.update(bazel_rules[dep].get('_EXCLUDE_DEPS', []))++        # This dep is a public target, add it as a dependency+        if dep in bazel_label_to_dep_name:+            _deduplicate_append(transitive_deps, [bazel_label_to_dep_name[dep]])+            _deduplicate_append(collapsed_deps, [bazel_label_to_dep_name[dep]])+            exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])+            exclude_deps.add(dep)+            continue -    expansion_blocklist = set()-    to_expand = set(bazel_deps)-    while to_expand:+        # This dep is an external target, add it as a dependency+        if external_dep_name_maybe is not None:+            _deduplicate_append(transitive_deps, [external_dep_name_maybe])+            _deduplicate_append(collapsed_deps, [external_dep_name_maybe])+            continue -        # start with the last dependency to be built-        build_order = _sort_by_build_order(list(to_expand), bazel_rules,-                                           'transitive_deps')+    # Direct dependencies are part of transitive dependencies+    _deduplicate_append(transitive_deps, direct_deps)++    # Removes all duplicated intermediate labels.+    # This is the step that further shorten the deps list.+    collapsed_deps = list(+        filter(lambda x: x not in exclude_deps, collapsed_deps))++    # Compute the final source files and headers, without all intermediate dependencies.+    for dep in transitive_deps:+        if dep not in exclude_deps:","Updated as suggested:```    # Imaging a public target PX has transitive deps [IA, IB, PY, IC, PZ]. PX,    # PY and PZ are public build targets. And IA, IB, IC are intermediate    # targets. In addition, PY depends on IC.    #    # Translate the condition into dependency graph:    #   PX -> [IA, IB, PY, IC, PZ]    #   PY -> [IC]    #   Public targets: [PX, PY, PZ]    #    # The collapsed dependencies of PX: [PY, PZ].    # The excluded dependencies of X: [PY, IC, PZ].    # (IC is excluded as a dependency of PX. It is already included in PY, hence    # it would be redundant to include it again.)    #    # Target PX should include source files and headers of [PX, IA, IB] as final    # build metadata.```",X
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/25640,592915183,2021-03-12T05:15:36Z,examples/php/greeter_server.php,"@@ -0,0 +1,44 @@+<?php+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++require dirname(__FILE__) . '/../../src/php/lib/Grpc/MethodDescriptor.php';+require dirname(__FILE__) . '/../../src/php/lib/Grpc/Status.php';+require dirname(__FILE__) . '/../../src/php/lib/Grpc/ServerCallReader.php';+require dirname(__FILE__) . '/../../src/php/lib/Grpc/ServerCallWriter.php';+require dirname(__FILE__) . '/../../src/php/lib/Grpc/ServerContext.php';+require dirname(__FILE__) . '/../../src/php/lib/Grpc/RpcServer.php';+require dirname(__FILE__) . '/vendor/autoload.php';++class Greeter extends Helloworld\GreeterServiceStub+{+    public function SayHello(+        \Helloworld\HelloRequest $request,+        \Grpc\ServerContext $serverContext+    ): array {+        $name = $request->getName();+        $response = new \Helloworld\HelloReply();+        $response->setMessage(""Hello "" . $name);+        return [$response];","Why does this `[$response]` need to be an array? Shouldn't it just be `return $response;`? Same for all the unary methods in the rest of the examples.I see. So user is expecting to return `list($response, $initialMetadata, $status)` here? Then let's figure out what's a better way to do this.As a user I expect to write `return $response;`. If I need to return initial metadata and status, perhaps we should do it via the `$serverContext`. Let's see how other languages do this.Yea, check out how Ruby implemented the interop test [here](https://github.com/grpc/grpc/blob/master/src/ruby/pb/test/server.rb#L179-L185). The return value of the RPC method itself is just the response proto. Everything else, like the status code, message, metadata are all set to the `_call` object. For us here in PHP, we can do it via this `$serverContext` object.",X
22016,weltling,https://api.github.com/repos/grpc/grpc/pulls/25617,593239766,2021-03-12T15:04:37Z,templates/CMakeLists.txt.template,"@@ -145,9 +145,9 @@   set(gRPC_CORE_VERSION     ""${settings.core_version}"")   set(gRPC_CORE_SOVERSION   ""${settings.core_version.major}"")   set(gRPC_CPP_VERSION      ""${settings.cpp_version}"")-  set(gRPC_CPP_SOVERSION    ""${settings.cpp_version.major}"")+  set(gRPC_CPP_SOVERSION    ""${settings.core_version.major}"")","Rebased to latest master. @jtattermusch please let me know if there's a better approach to address this issue. Perhaps putting the whole version would be even better, fe see what boost does. A proper handling of SONAME would be really useful for downstreams and similar.Thanks",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,593249851,2021-03-12T15:18:15Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -247,119 +244,205 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    build specifications for other build systems, like CMake, Make, setuptools.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list in alphabetical order;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers;+    * _EXCLUDE_DEPS: transitive dependencies of public deps that this build+      target depends on, and they will be excluded in the final build metadata.++    For the collapsed_deps, the algorithm improved cases like:++    The result in the past:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The result of the algorithm:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.","nit:  ""We care about the order of dependencies list, so it will be a list. "" is no longer true.besides that, does `transitive_deps = []` really need to be a list?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,593258204,2021-03-12T15:29:12Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -247,119 +244,205 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    build specifications for other build systems, like CMake, Make, setuptools.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list in alphabetical order;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers;+    * _EXCLUDE_DEPS: transitive dependencies of public deps that this build+      target depends on, and they will be excluded in the final build metadata.++    For the collapsed_deps, the algorithm improved cases like:++    The result in the past:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The result of the algorithm:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = set()+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # Suppress expanding absl libs; expanding absl deps are technically+            # correct but creates too much noise+            if external_dep_name_maybe is None or not external_dep_name_maybe.startswith(+                    'absl'):+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:+                    # This item is not processed before, compute now+                    _compute_transitive_metadata(dep, bazel_rules,+                                                 bazel_label_to_dep_name)+                _deduplicate_append(+                    transitive_deps,+                    bazel_rules[dep].get('_TRANSITIVE_DEPS', []))+                collapsed_deps.update(+                    collapsed_deps, bazel_rules[dep].get('_COLLAPSED_DEPS', []))+                exclude_deps.update(bazel_rules[dep].get('_EXCLUDE_DEPS', []))++        # This dep is a public target, add it as a dependency+        if dep in bazel_label_to_dep_name:+            _deduplicate_append(transitive_deps, [bazel_label_to_dep_name[dep]])+            collapsed_deps.update(collapsed_deps,+                                  [bazel_label_to_dep_name[dep]])+            # Add all the transitive deps of our every public dep to exclude+            # list since we want to avoid building sources that are already+            # built our dependencies+            exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])+            exclude_deps.add(dep)+            continue -    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']+        # This dep is an external target, add it as a dependency+        if external_dep_name_maybe is not None:+            _deduplicate_append(transitive_deps, [external_dep_name_maybe])+            collapsed_deps.update(collapsed_deps, [external_dep_name_maybe])+            continue -    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+    # Direct dependencies are part of transitive dependencies+    _deduplicate_append(transitive_deps, direct_deps)++    # Remove intermediate targets that our public dependencies already depend","not sure I follow:  you're saying ""remove intermediate targets"", but from line 336  (`exclude_deps.add(dep)`) it seems that exclude_deps also contain public deps. So the comment seems inaccurate.Q: if collapsed_deps contain public deps (which I think they need to), how come skipping the exclude deps here doesn't result in skipping our direct public dependency that really should have been included?Example:  PA -> [PB, IX, IY]on line 336, you add PB to exclude_deps and  then on this line, you will filter out PB from collapsed_deps.  So PA's deps will end up being empty, which is wrong collapsed deps should contain the info that PA -> PB.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,593258908,2021-03-12T15:30:03Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -247,119 +244,205 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    build specifications for other build systems, like CMake, Make, setuptools.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list in alphabetical order;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers;+    * _EXCLUDE_DEPS: transitive dependencies of public deps that this build+      target depends on, and they will be excluded in the final build metadata.++    For the collapsed_deps, the algorithm improved cases like:++    The result in the past:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The result of the algorithm:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = set()+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # Suppress expanding absl libs; expanding absl deps are technically+            # correct but creates too much noise+            if external_dep_name_maybe is None or not external_dep_name_maybe.startswith(+                    'absl'):+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:+                    # This item is not processed before, compute now+                    _compute_transitive_metadata(dep, bazel_rules,+                                                 bazel_label_to_dep_name)+                _deduplicate_append(+                    transitive_deps,+                    bazel_rules[dep].get('_TRANSITIVE_DEPS', []))+                collapsed_deps.update(+                    collapsed_deps, bazel_rules[dep].get('_COLLAPSED_DEPS', []))+                exclude_deps.update(bazel_rules[dep].get('_EXCLUDE_DEPS', []))++        # This dep is a public target, add it as a dependency+        if dep in bazel_label_to_dep_name:+            _deduplicate_append(transitive_deps, [bazel_label_to_dep_name[dep]])+            collapsed_deps.update(collapsed_deps,+                                  [bazel_label_to_dep_name[dep]])+            # Add all the transitive deps of our every public dep to exclude+            # list since we want to avoid building sources that are already+            # built our dependencies+            exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])+            exclude_deps.add(dep)+            continue -    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']+        # This dep is an external target, add it as a dependency+        if external_dep_name_maybe is not None:+            _deduplicate_append(transitive_deps, [external_dep_name_maybe])+            collapsed_deps.update(collapsed_deps, [external_dep_name_maybe])+            continue -    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+    # Direct dependencies are part of transitive dependencies+    _deduplicate_append(transitive_deps, direct_deps)++    # Remove intermediate targets that our public dependencies already depend+    # on. This is the step that further shorten the deps list.+    collapsed_deps = list(","one more issue:when initializing, collapsed_deps was a set() . Now you're assigning a list.",
28025951,HannahShiSFB,https://api.github.com/repos/grpc/grpc/pulls/25640,593472434,2021-03-12T21:54:41Z,examples/php/greeter_server.php,"@@ -0,0 +1,44 @@+<?php+/*+ *+ * Copyright 2020 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++require dirname(__FILE__) . '/../../src/php/lib/Grpc/MethodDescriptor.php';+require dirname(__FILE__) . '/../../src/php/lib/Grpc/Status.php';+require dirname(__FILE__) . '/../../src/php/lib/Grpc/ServerCallReader.php';+require dirname(__FILE__) . '/../../src/php/lib/Grpc/ServerCallWriter.php';+require dirname(__FILE__) . '/../../src/php/lib/Grpc/ServerContext.php';+require dirname(__FILE__) . '/../../src/php/lib/Grpc/RpcServer.php';+require dirname(__FILE__) . '/vendor/autoload.php';++class Greeter extends Helloworld\GreeterServiceStub+{+    public function SayHello(+        \Helloworld\HelloRequest $request,+        \Grpc\ServerContext $serverContext+    ): array {+        $name = $request->getName();+        $response = new \Helloworld\HelloReply();+        $response->setMessage(""Hello "" . $name);+        return [$response];","conform to the existing greeter client.[list($response, $status) = $client->SayHello($request)->wait();](https://github.com/grpc/grpc/blob/b516dbfa21c5a2065f8a99a5890316d7da75d238/examples/php/greeter_client.php#L34)",
113840,mattnworb,https://api.github.com/repos/grpc/grpc/pulls/25708,594516096,2021-03-15T16:58:53Z,include/grpc/impl/codegen/compression_types.h,"@@ -58,9 +58,9 @@ typedef enum {   GRPC_COMPRESS_NONE = 0,   GRPC_COMPRESS_DEFLATE,   GRPC_COMPRESS_GZIP,+  GRPC_COMPRESS_SNAPPY,","a question I had here: is the int value of an enum option like `GRPC_COMPRESS_STREAM_GZIP` considered a part of the public API?  I wasn't sure if it is a bad practice to insert a new enum value in the middle of possible values here like this, which causes the int value of `GRPC_COMPRESS_STREAM_GZIP` to move up by one. For instance https://github.com/grpc/grpc/blob/master/doc/compression_cookbook.md#disable-compression-algorithms mentions using these int values to compute a bitset to set channel arguments like GRPC_COMPRESSION_CHANNEL_ENABLED_ALGORITHMS_BITSETAdding `GRPC_COMPRESS_SNAPPY` here allowed the implementation of `grpc_compression_bitset_to_message_bitset` etc to remain unchanged, but I'm curious if the positions in the enum need to stay stable once added.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/25714,594531214,2021-03-15T17:17:23Z,src/core/ext/transport/chttp2/transport/chttp2_transport.h,"@@ -49,4 +49,13 @@ void grpc_chttp2_transport_start_reading(     grpc_transport* transport, grpc_slice_buffer* read_buffer,     grpc_closure* notify_on_receive_settings); +namespace grpc_core {+// Takes a bool input parameter which is true when a transport is being+// constructed and false when a trasport is being destructed.+typedef void (*TestOnlyGlobalHttp2TransportInitCallback)(bool);","If you didn't want to go with the separate shutdown method, you could make this a bit more self-documenting by adding a parameter name, something like `is_constructing`",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/25687,594797501,2021-03-16T01:24:34Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -7523,6 +7518,28 @@ TEST_P(XdsEnabledServerTest, ListenerAddressMismatch) {   EXPECT_TRUE(rpc_failed); } +TEST_P(XdsEnabledServerTest, UseOriginalDstNotSupported) {+  Listener listener;+  listener.set_name(+      absl::StrCat(""grpc/server?xds.resource.listening_address="",+                   ipv6_only_ ? ""[::1]:"" : ""127.0.0.1:"", backends_[0]->port()));+  balancers_[0]->ads_service()->SetLdsResource(listener);+  listener.mutable_address()->mutable_socket_address()->set_address(+      ipv6_only_ ? ""::1"" : ""127.0.0.1"");+  listener.mutable_address()->mutable_socket_address()->set_port_value(+      backends_[0]->port());+  listener.add_filter_chains()->add_filters()->mutable_typed_config()->PackFrom(+      HttpConnectionManager());+  listener.mutable_use_original_dst()->set_value(true);+  balancers_[0]->ads_service()->SetLdsResource(listener);+  CheckRpcSendFailure(1, RpcOptions().set_wait_for_ready(true));",">Hmm... That having been said, even if we don't use wait_for_ready, how do we know that the server has actually gotten the config by the time this RPC fails? The RPC has a 1s timeout. If the client fails to connect to the server before the server actually receives the config, then the RPC sent by the client will fail before we know for sure that the server would not start listening -- this part of the test would pass even though the server is not behaving correctly.I'm guessing that it wasn't flaky because the 1s timeout was enough for the clients to connect.>Maybe this test would be more reliable if we used backends_[0]->notifier()->WaitOnServingStatusChange() to check explicitly what the backend's status is? Once we get the status, we know that the server has seen the config, and then we can check that the client's RPC fails and that the xDS server sees the NACK.We can't use the status notifier in cases where we expect a NACK, since the config fetcher does not see the error update for NACKs. It only sees the error update when there is a timeout/resource deletion/error in its own processing of the update.I've updated the tests to loop over the response state so that the tests are more reliable.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25617,595079043,2021-03-16T11:24:25Z,CONCEPTS.md,"@@ -37,6 +37,11 @@ thread. The gRPC programming surface in most languages comes in both synchronous and asynchronous flavors. +### ABI compatibility","this definitely doesn't belong here.I think you can add a note to BUILDING.md (if you find no good place for it, feel free to append a section at the end of it).Also, I think think it would be better to put this more in context. 1.) The SONAME only affects our cmake builds and 2.) the remark you wrote only describes the relationship between ABI compatibility and the SONAME.But somehow as-is, this section sounds as if you're making general claims about ABI compatibility of gRPC and gRPC++ and that's not the intent. Please rephrase to make it clear what's the scope of the information this provides (e.g. we could call it ""A note on SONAME and its ABI compatibility implications in the cmake build"".",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25704,595817792,2021-03-17T08:49:10Z,tools/run_tests/artifacts/artifact_targets.py,"@@ -118,24 +114,27 @@ def pre_build_jobspecs(self):     def build_jobspec(self):         environ = {}         if self.platform == 'linux_extra':-            # Raspberry Pi build-            environ['PYTHON'] = '/usr/local/bin/python{}'.format(+            # Crosscompilation build for armv7 (e.g. Raspberry Pi)+            environ['PYTHON'] = '/opt/python/{}/bin/python3'.format(                 self.py_version)-            environ['PIP'] = '/usr/local/bin/pip{}'.format(self.py_version)-            # https://github.com/resin-io-projects/armv7hf-debian-qemu/issues/9-            # A QEMU bug causes submodule update to freeze, so we copy directly-            environ['RELATIVE_COPY_PATH'] = '.'-            # Parallel builds are counterproductive in emulated environment-            environ['GRPC_PYTHON_BUILD_EXT_COMPILER_JOBS'] = '1'-            extra_args = ' --entrypoint=/usr/bin/qemu-arm-static '+            environ['PIP'] = '/opt/python/{}/bin/pip3'.format(self.py_version)+            environ['GRPC_SKIP_PIP_CYTHON_UPGRADE'] = 'TRUE'+            environ['GRPC_SKIP_TWINE_CHECK'] = 'TRUE'+            # when crosscompiling, we need to force statically linking libstdc+++            # otherwise libstdc++ symbols would be too new and the resulting+            # wheel wouldn't pass the auditwheel check.+            # This is needed because C core won't build with GCC 4.8 that's+            # included in the default dockcross toolchain and we needed+            # to opt into using a slighly newer version of GCC.+            environ['GRPC_PYTHON_BUILD_WITH_STATIC_LIBSTDCXX'] = 'TRUE'+             return create_docker_jobspec(                 self.name,-                'tools/dockerfile/grpc_artifact_linux_{}'.format(self.arch),+                'tools/dockerfile/grpc_artifact_python_linux_{}'.format(+                    self.arch),                 'tools/run_tests/artifacts/build_artifact_python.sh',                 environ=environ,-                timeout_seconds=60 * 60 * 7,-                docker_base_image='quay.io/grpc/raspbian_{}'.format(self.arch),-                extra_docker_args=extra_args)+                timeout_seconds=60 * 60)","1hr should really be enough for building  a single python wheel (there's actually running more jobs in parallel, but still). In the past we've been blindly increasing timeout whenever we saw a timeout (instead of investigating and trying to reduce the duration) and it gave us nothing but degradation tower slower and slower CI tests. It didn't actually lead to decreased flakiness since sometimes the build / run has simply just got stuck for interesting reasons and having very high timeouts just prevented us from seeing the actual problem. So no, without crystal clear evidence (and a prior genuine attempt to decrease the duration somehow) I'm not really will to increase any timeouts. ;-)",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,596155880,2021-03-17T15:54:38Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -34,14 +34,36 @@ import yaml import xml.etree.ElementTree as ET import os+import collections import sys-import build_cleaner--_ROOT = os.path.abspath(os.path.join(os.path.dirname(sys.argv[0]), '../..'))-os.chdir(_ROOT)+import re+from typing import List, Any, Dict, Optional, Iterable +import build_cleaner -def _bazel_query_xml_tree(query):+PROJECT_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..',+                            '..')+os.chdir(PROJECT_ROOT)++BuildMetadata = Dict[str, Any]+BuildDict = Dict[str, BuildMetadata]+BuildYaml = Dict[str, Any]++# Map from the external repository's labels to their path prefixes.+EXTERNAL_LIB_LABEL_PATH_MAP = dict(+    envoy_api='third_party/envoy-api/',+    com_google_googleapis='third_party/googleapis/',+    com_github_cncf_udpa='third_party/udpa/',+    com_envoyproxy_protoc_gen_validate='third_party/protoc-gen-validate/',+    opencensus_proto='third_party/opencensus-proto/src/')+# The external repositories that we are currently able to support the metadata+# extraction. We aim to support arbitrary external libraries in the long term.","""We aim to support arbitrary external libraries in the long term"" - i'm not convinced that is actually what we want.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,597179940,2021-03-18T19:28:19Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -247,119 +244,205 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name+def _deduplicate_append(target: List[str], pending: List[str]) -> None:+    """"""Append the list without duplicated items in place.+    +    Expected to be used to create a order-stable dependency list.+    """"""+    seen = set(target)+    for dep in pending:+        if dep not in seen:+            target.append(dep)+++def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    build specifications for other build systems, like CMake, Make, setuptools.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list in alphabetical order;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers;+    * _EXCLUDE_DEPS: transitive dependencies of public deps that this build+      target depends on, and they will be excluded in the final build metadata.++    For the collapsed_deps, the algorithm improved cases like:++    The result in the past:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The result of the algorithm:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = []+    # We care about the order of dependencies list, so it will be a list.+    collapsed_deps = set()+    # We don't care about the order of srcs and headers, they will be sorted.+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))+    exclude_deps = set()++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # Suppress expanding absl libs; expanding absl deps are technically+            # correct but creates too much noise+            if external_dep_name_maybe is None or not external_dep_name_maybe.startswith(+                    'absl'):+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:+                    # This item is not processed before, compute now+                    _compute_transitive_metadata(dep, bazel_rules,+                                                 bazel_label_to_dep_name)+                _deduplicate_append(+                    transitive_deps,+                    bazel_rules[dep].get('_TRANSITIVE_DEPS', []))+                collapsed_deps.update(+                    collapsed_deps, bazel_rules[dep].get('_COLLAPSED_DEPS', []))+                exclude_deps.update(bazel_rules[dep].get('_EXCLUDE_DEPS', []))++        # This dep is a public target, add it as a dependency+        if dep in bazel_label_to_dep_name:+            _deduplicate_append(transitive_deps, [bazel_label_to_dep_name[dep]])+            collapsed_deps.update(collapsed_deps,+                                  [bazel_label_to_dep_name[dep]])+            # Add all the transitive deps of our every public dep to exclude+            # list since we want to avoid building sources that are already+            # built our dependencies+            exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])+            exclude_deps.add(dep)+            continue -    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']+        # This dep is an external target, add it as a dependency+        if external_dep_name_maybe is not None:+            _deduplicate_append(transitive_deps, [external_dep_name_maybe])+            collapsed_deps.update(collapsed_deps, [external_dep_name_maybe])+            continue -    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+    # Direct dependencies are part of transitive dependencies+    _deduplicate_append(transitive_deps, direct_deps)++    # Remove intermediate targets that our public dependencies already depend","As I mentioned in https://github.com/grpc/grpc/pull/25758, the direct public dependency will be included in `exclude_dep`. It saves cycles to compute this list with recursion, but if you insist we can compute it for every node like https://github.com/grpc/grpc/pull/25758, which does improve the readability.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,597190375,2021-03-18T19:40:01Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -34,14 +34,36 @@ import yaml import xml.etree.ElementTree as ET import os+import collections import sys-import build_cleaner--_ROOT = os.path.abspath(os.path.join(os.path.dirname(sys.argv[0]), '../..'))-os.chdir(_ROOT)+import re+from typing import List, Any, Dict, Optional, Iterable +import build_cleaner -def _bazel_query_xml_tree(query):+PROJECT_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..',+                            '..')+os.chdir(PROJECT_ROOT)++BuildMetadata = Dict[str, Any]+BuildDict = Dict[str, BuildMetadata]+BuildYaml = Dict[str, Any]++# Map from the external repository's labels to their path prefixes.+EXTERNAL_LIB_LABEL_PATH_MAP = dict(+    envoy_api='third_party/envoy-api/',","The source file in external repo looks like:```@envoy-api//envoy/foo/bar.proto```But this is not actionable for CMake, so we need to translate it into the actual file location:```third_party/envoy-api/envoy/foo/bar.proto```I would blame Bazel for this confusing label naming mechanism.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25758,597516726,2021-03-19T09:15:41Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -247,119 +198,193 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name+def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    build specifications for other build systems, like CMake, Make, setuptools.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list in alphabetical order;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers;+    * _EXCLUDE_DEPS: intermediate targets to exclude when performing collapsing+      of sources and dependencies. ++    For the collapsed_deps, the algorithm improved cases like:++    The result in the past:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The result of the algorithm:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = set()+    collapsed_deps = set()+    exclude_deps = set()+    collapsed_srcs = set(_extract_sources(bazel_rule))+    collapsed_public_headers = set(_extract_public_headers(bazel_rule))+    collapsed_headers = set(_extract_nonpublic_headers(bazel_rule))++    for dep in direct_deps:+        external_dep_name_maybe = _external_dep_name_from_bazel_dependency(dep)++        if dep in bazel_rules:+            # Descend recursively, but no need to do that for external deps+            if external_dep_name_maybe is None:+                if ""_PROCESSING_DONE"" not in bazel_rules[dep]:+                    # This item is not processed before, compute now+                    _compute_transitive_metadata(dep, bazel_rules,+                                                 bazel_label_to_dep_name)++                transitive_deps.update(bazel_rules[dep].get(+                    '_TRANSITIVE_DEPS', []))+                collapsed_deps.update(+                    collapsed_deps, bazel_rules[dep].get('_COLLAPSED_DEPS', []))+                exclude_deps.update(bazel_rules[dep].get('_EXCLUDE_DEPS', []))++        # This dep is a public target, add it as a dependency+        if dep in bazel_label_to_dep_name:+            transitive_deps.update([bazel_label_to_dep_name[dep]])+            collapsed_deps.update(collapsed_deps,+                                  [bazel_label_to_dep_name[dep]])+            # Add all the transitive deps of our every public dep to exclude+            # list since we want to avoid building sources that are already+            # built by our dependencies+            exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])+            continue -    target_name = target_dict['name']-    bazel_deps = target_dict['_DEPS_BAZEL']+        # This dep is an external target, add it as a dependency+        if external_dep_name_maybe is not None:+            transitive_deps.update([external_dep_name_maybe])+            collapsed_deps.update(collapsed_deps, [external_dep_name_maybe])+            continue -    # initial values-    public_headers = set(target_dict['_PUBLIC_HEADERS_BAZEL'])-    headers = set(target_dict['_HEADERS_BAZEL'])-    src = set(target_dict['_SRC_BAZEL'])-    deps = set()+    # Direct dependencies are part of transitive dependencies+    transitive_deps.update(direct_deps)++    # Calculate transitive public deps (needed for collapsing sources)+    transitive_public_deps = set(+        filter(lambda x: x in bazel_label_to_dep_name, transitive_deps))","So the previous code ""happened to work"" but it didn't seem to me that was intentional.This is addressing the concern I raised in https://github.com/grpc/grpc/pull/25272/files#r590297759 and I didn't feel it was addressed properly by you.  The change I made is best seen when you look at commit https://github.com/grpc/grpc/pull/25758/commits/fbcce34d9c5905d21f1b93e0df933d50634c3ff3The issue was that in the original version of the code, you were adding ""transitive deps of our public dep"" to exclude_deps. (that's correct) But there was an extra [`exclude_deps.add(dep)`](https://github.com/grpc/grpc/pull/25758/commits/fbcce34d9c5905d21f1b93e0df933d50634c3ff3#diff-bc692953802a07e8317fec2620a3c28d6439f6a5baeb8412a8171becf6cfea25L323) line that was also adding the public dependency itself to exclude_deps - but inconsistently with collapsed_deps and transitive_deps, it's adding  `dep` (= the bazel lable) instead of the public depdency name (`bazel_label_to_dep_name[dep]`).Later you  are excluding  the exclude_deps from collapsed_deps - but in this case you only want to remove the intermediate deps, not the public deps (because they are actual dependencies that need to be visible in the result; public deps should only be excluded when collapsing sources and headers). But as you can see,  you are filtering `filter(lambda x: x not in exclude_deps, collapsed_deps)` which from the code looks like your intent is to  removing the public deps as well (which doesn't make sense). It happens to work because the collapsed_deps contain converted names for public deps (using `bazel_label_to_dep_name`), while exclude_deps contain the bazel labels for the public deps.  So the logic is basically wrong, but it happens to work due to exclude_deps containing  bazel label instead of dependency name by mistake.This wasn't easy to spot, but as I was looking at the algorithm, it didn't make sense (it should not work as I pointed out previously in my comments). But after experimenting a little bit I found out the bazel_label_to_dep_name[dep] vs dep difference.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25758,597521792,2021-03-19T09:23:32Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -247,119 +198,193 @@ def _external_dep_name_from_bazel_dependency(bazel_dep):         return None  -def _expand_intermediate_deps(target_dict, public_dep_names, bazel_rules):-    # Some of the libraries defined by bazel won't be exposed in build.yaml-    # We call these ""intermediate"" dependencies. This method expands-    # the intermediate deps for given target (populates library's-    # headers, sources and dicts as if the intermediate dependency never existed)--    # use this dictionary to translate from bazel labels to dep names-    bazel_label_to_dep_name = {}-    for dep_name in public_dep_names:-        bazel_label_to_dep_name[_get_bazel_label(dep_name)] = dep_name+def _compute_transitive_metadata(+        rule_name: str, bazel_rules: Any,+        bazel_label_to_dep_name: Dict[str, str]) -> None:+    """"""Computes the final build metadata for Bazel target with rule_name.++    The dependencies that will appear on the deps list are:++    * Public build targets including binaries and tests;+    * External targets, like absl, re2.++    All other intermediate dependencies will be merged, which means their+    source file, headers, etc. will be collected into one build target. This+    step of processing will greatly reduce the complexity of the generated+    build specifications for other build systems, like CMake, Make, setuptools.++    The final build metadata are:+    * _TRANSITIVE_DEPS: all the transitive dependencies including intermediate+                        targets;+    * _COLLAPSED_DEPS:  dependencies that fits our requirement above, and it+                        will remove duplicated items and produce the shortest+                        possible dependency list in alphabetical order;+    * _COLLAPSED_SRCS:  the merged source files;+    * _COLLAPSED_PUBLIC_HEADERS: the merged public headers;+    * _COLLAPSED_HEADERS: the merged non-public headers;+    * _EXCLUDE_DEPS: intermediate targets to exclude when performing collapsing+      of sources and dependencies. ++    For the collapsed_deps, the algorithm improved cases like:++    The result in the past:+        end2end_tests -> [grpc_test_util, grpc, gpr, address_sorting, upb]+        grpc_test_util -> [grpc, gpr, address_sorting, upb, ...]+        grpc -> [gpr, address_sorting, upb, ...]+    +    The result of the algorithm:+        end2end_tests -> [grpc_test_util]+        grpc_test_util -> [grpc]+        grpc -> [gpr, address_sorting, upb, ...]+    """"""+    bazel_rule = bazel_rules[rule_name]+    direct_deps = _extract_deps(bazel_rule, bazel_rules)+    transitive_deps = set()","I don't think so.1. when I made this change I regenerated the yaml files and there was no difference in the generated files2. the transitive deps aren't actually exposed anywhere in the resulting yaml file, it's just an internal field (fields like collapsed_deps get sorted, which would cancel any effect of transitive deps being ordered).3. it doesn't matter since the algorithm is agressively eliminating transitive public dependencies in the output. If something depends on both grpc and gpr, only `grpc` will end up in the final list of dependencies.  So (as you pointed out in previous dicussion), the build order doesn't matter since in all cases where it would matter (e.g. gpr coming before grpc, which would be incorrect), the final collapsed_deps will  never contain both libraries that have a dependency relationship between them.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,597545849,2021-03-19T10:00:29Z,src/xds-proto/gen_build_yaml.py,"@@ -0,0 +1,116 @@+#!/usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Generate build metadata for external protos (mostly xDS-related).++The build metadata will be used in CMakeLists to ensure xDS proto dependencies+are present before compilation.+""""""++import os+import subprocess+import xml.etree.ElementTree as ET+from dataclasses import asdict, dataclass+from typing import List++import yaml+++@dataclass()+class ExternalProtoLibrary:+    # The relative path of this proto library should be. Preferably, it should+    # match the submodule path.+    destination: str+    # The prefix to remove in order to insure the proto import is correct. For+    # more info, see description of https://github.com/grpc/grpc/pull/25272.+    proto_prefix: str+    # Following 3 fields should be filled by build metadata from Bazel.+    url: str = ''+    hash: str = ''+    strip_prefix: str = ''+++EXTERNAL_PROTO_LIBRARIES = {+    'envoy_api':+        ExternalProtoLibrary(destination='third_party/envoy-api',+                             proto_prefix='third_party/envoy-api/'),+    'com_google_googleapis':+        ExternalProtoLibrary(destination='third_party/googleapis',+                             proto_prefix='third_party/googleapis/'),+    'com_github_cncf_udpa':+        ExternalProtoLibrary(destination='third_party/udpa',+                             proto_prefix='third_party/udpa/'),+    'com_envoyproxy_protoc_gen_validate':+        ExternalProtoLibrary(destination='third_party/protoc-gen-validate',+                             proto_prefix='third_party/protoc-gen-validate/'),+    'opencensus_proto':+        ExternalProtoLibrary(destination='third_party/opencensus-proto/src',+                             proto_prefix='third_party/opencensus-proto/src/'),+}+++@dataclass()+class _HttpArchive:+    name: str = ''+    url: str = ''+    hash: str = ''+    strip_prefix: str = ''+++def _fetch_raw_http_archives() -> ET.Element:+    """"""Get xml output of bazel query invocation, parsed as XML tree""""""+    output = subprocess.check_output([+        'tools/bazel', 'query', '--output', 'xml',+        'kind(http_archive, //external:*)'+    ])+    return ET.fromstring(output)+++def _parse_http_archives(xml_tree: ET.Element) -> List[ExternalProtoLibrary]:+    result = []+    for xml_http_archive in xml_tree:+        if xml_http_archive.tag != 'rule' or xml_http_archive.attrib[+                'class'] != 'http_archive':+            continue+        http_archive = _HttpArchive()+        for xml_node in xml_http_archive:+            if xml_node.attrib['name'] == 'name':+                http_archive.name = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'urls':+                http_archive.url = xml_node[0].attrib['value']+            if xml_node.attrib['name'] == 'url':+                http_archive.url = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'sha256':+                http_archive.hash = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'strip_prefix':+                http_archive.strip_prefix = xml_node.attrib['value']+        if http_archive.name not in EXTERNAL_PROTO_LIBRARIES:+            continue+        lib = EXTERNAL_PROTO_LIBRARIES[http_archive.name]+        lib.url = http_archive.url+        lib.hash = http_archive.hash+        lib.strip_prefix = http_archive.strip_prefix+        result.append(lib)+    return result+++def main():+    xml_tree = _fetch_raw_http_archives()+    libraries = _parse_http_archives(xml_tree)+    libraries.sort(key=lambda x: x.destination)+    print(yaml.dump(dict(external_libraries=list(map(asdict, libraries)))))","over time, we've been trying to reduce the amount of different build entities (lib, target, filegroup, ..) in build.yaml and to make the custom build.yaml structure simpler  (by e.g. removing unnecessary fields, objects etc.). By adding a concept of `external_library`, this PR is doing the exact opposite and that worries me.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,597663069,2021-03-19T13:08:18Z,src/xds-proto/gen_build_yaml.py,"@@ -0,0 +1,116 @@+#!/usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Generate build metadata for external protos (mostly xDS-related).++The build metadata will be used in CMakeLists to ensure xDS proto dependencies+are present before compilation.+""""""++import os+import subprocess+import xml.etree.ElementTree as ET+from dataclasses import asdict, dataclass+from typing import List++import yaml+++@dataclass()+class ExternalProtoLibrary:+    # The relative path of this proto library should be. Preferably, it should+    # match the submodule path.+    destination: str+    # The prefix to remove in order to insure the proto import is correct. For+    # more info, see description of https://github.com/grpc/grpc/pull/25272.+    proto_prefix: str+    # Following 3 fields should be filled by build metadata from Bazel.+    url: str = ''+    hash: str = ''+    strip_prefix: str = ''+++EXTERNAL_PROTO_LIBRARIES = {+    'envoy_api':","you have custom metadata for these libraries already in extract_metadata_from_bazel_xml.py (in the EXTERNAL_LIB_LABEL_PATH_MAP global), so I suspect needing to list these libraries in multiple places (each time with some extra info) is going to be difficult to maintain.Since extract_metadata_from_bazel_xml.py   already has logic  for running various `bazel query` commands and then it postprocesses the data and generated build_autogenerated.yaml, I don't understand why you want to have a separate ""gen_build_yaml.py"" here (which is going to be added to the final yaml by generate_build_additions.sh  anyway). If you moved the logic from here extract_metadata_from_bazel_xml.py, the generated metadata would at least show up in build_autogenerated.yaml (which makes it more obvious to digest how this works). ",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,597676685,2021-03-19T13:27:54Z,cmake/xds_proto.cmake,"@@ -0,0 +1,39 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set(_xDS_Proto_TEMPORARY_DIR ${CMAKE_BINARY_DIR}/xds_proto)+file(MAKE_DIRECTORY ${_xDS_Proto_TEMPORARY_DIR})++# This is bascially Bazel's http_archive.+# Note that strip_prefix strips the directory path prefix of the extracted+# archive contect, and it may strip multiple directory.+function(fetch_check_extract destination url hash strip_prefix)+  # Fetch and validate+  set(_xDS_Proto_TEMPORARY_FILE ${_xDS_Proto_TEMPORARY_DIR}/${strip_prefix}.tar.gz)",is naming the downloaded file as`${strip_prefix}.tar.gz` a trick how to make stripping easier later?,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,597687222,2021-03-19T13:42:39Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -44,6 +44,29 @@ BuildDict = Dict[str, BuildMetadata] BuildYaml = Dict[str, Any] +import build_cleaner++PROJECT_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..',+                            '..')+os.chdir(PROJECT_ROOT)","why do you need the PROJECT_ROOT and os.chdir(PROJECT_ROOT)? It seems unused. If so, please remove",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,597703344,2021-03-19T14:03:25Z,templates/CMakeLists.txt.template,"@@ -345,49 +370,53 @@   #   Add custom commands to process ``.proto`` files to C++ using protoc and   #   GRPC plugin::   #-  #     protobuf_generate_grpc_cpp [<ARGN>...]+  #     protobuf_generate_grpc_cpp <FILE_LOCATION> <IMPORT_PATH>   #-  #   ``ARGN``-  #     ``.proto`` files+  #   ``FILE_LOCATION``+  #     The relative path of the ``.proto`` file to the project root+  #   ``IMPORT_PATH``+  #     The proto import path that itself expected to be placed in   #-  function(protobuf_generate_grpc_cpp)-    if(NOT ARGN)+  function(protobuf_generate_grpc_cpp FILE_LOCATION IMPORT_PATH)+    if(NOT FILE_LOCATION)       message(SEND_ERROR ""Error: PROTOBUF_GENERATE_GRPC_CPP() called without any proto files"")       return()     endif()      set(_protobuf_include_path -I . -I <%text>${_gRPC_PROTOBUF_WELLKNOWN_INCLUDE_DIR}</%text>)-    foreach(FIL <%text>${ARGN}</%text>)-      get_filename_component(ABS_FIL <%text>${FIL}</%text> ABSOLUTE)-      get_filename_component(FIL_WE <%text>${FIL}</%text> NAME_WE)-      file(RELATIVE_PATH REL_FIL <%text>${CMAKE_CURRENT_SOURCE_DIR}</%text> <%text>${ABS_FIL}</%text>)-      get_filename_component(REL_DIR <%text>${REL_FIL}</%text> DIRECTORY)-      set(RELFIL_WE ""<%text>${REL_DIR}/${FIL_WE}</%text>"")--      #if cross-compiling, find host plugin-      if(CMAKE_CROSSCOMPILING)-        find_program(_gRPC_CPP_PLUGIN grpc_cpp_plugin)-      else()-        set(_gRPC_CPP_PLUGIN $<TARGET_FILE:grpc_cpp_plugin>)-      endif()--      add_custom_command(-        OUTPUT <%text>""${_gRPC_PROTO_GENS_DIR}/${RELFIL_WE}.grpc.pb.cc""</%text>-               <%text>""${_gRPC_PROTO_GENS_DIR}/${RELFIL_WE}.grpc.pb.h""</%text>-               <%text>""${_gRPC_PROTO_GENS_DIR}/${RELFIL_WE}_mock.grpc.pb.h""</%text>-               <%text>""${_gRPC_PROTO_GENS_DIR}/${RELFIL_WE}.pb.cc""</%text>-               <%text>""${_gRPC_PROTO_GENS_DIR}/${RELFIL_WE}.pb.h""</%text>-        COMMAND <%text>${_gRPC_PROTOBUF_PROTOC_EXECUTABLE}</%text>-        ARGS --grpc_out=<%text>generate_mock_code=true:${_gRPC_PROTO_GENS_DIR}</%text>-             --cpp_out=<%text>${_gRPC_PROTO_GENS_DIR}</%text>-             --plugin=protoc-gen-grpc=<%text>${_gRPC_CPP_PLUGIN}</%text>-             <%text>${_protobuf_include_path}</%text>-             <%text>${REL_FIL}</%text>-        DEPENDS <%text>${ABS_FIL}</%text> <%text>${_gRPC_PROTOBUF_PROTOC}</%text> grpc_cpp_plugin-        WORKING_DIRECTORY <%text>${CMAKE_CURRENT_SOURCE_DIR}</%text>-        COMMENT ""Running gRPC C++ protocol buffer compiler on <%text>${FIL}</%text>""-        VERBATIM)-    endforeach()+    # compute the relative path+    get_filename_component(ABS_FIL <%text>${_gRPC_PROTO_SRCS_DIR}/${IMPORT_PATH}</%text> ABSOLUTE)+    get_filename_component(FIL_WE <%text>${_gRPC_PROTO_SRCS_DIR}/${IMPORT_PATH}</%text> NAME_WE)+    file(RELATIVE_PATH REL_FIL <%text>${_gRPC_PROTO_SRCS_DIR}</%text> <%text>${ABS_FIL}</%text>)+    get_filename_component(REL_DIR <%text>${REL_FIL}</%text> DIRECTORY)+    set(RELFIL_WE ""<%text>${REL_DIR}/${FIL_WE}</%text>"")+    # copy the proto file to a centralized location","why do you need to copy the files to the a centralized location?If this is because the .proto files need to see the .proto files they import, it seems that there would be a problem with dependencies of .proto files. Looks like you're copying proto files to a centralized location one by one, so by the time you copy A.proto (which depends on B.proto), B.proto might not be there yet?If that's true, the order in which .proto files are listed in CMakeLists.txt is important and that seems pretty fragile (expecially because there's no easy what to influence the order in which they are listed).",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,597712403,2021-03-19T14:15:26Z,src/xds-proto/gen_build_yaml.py,"@@ -0,0 +1,116 @@+#!/usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Generate build metadata for external protos (mostly xDS-related).++The build metadata will be used in CMakeLists to ensure xDS proto dependencies+are present before compilation.+""""""++import os+import subprocess+import xml.etree.ElementTree as ET+from dataclasses import asdict, dataclass+from typing import List++import yaml+++@dataclass()+class ExternalProtoLibrary:+    # The relative path of this proto library should be. Preferably, it should+    # match the submodule path.+    destination: str+    # The prefix to remove in order to insure the proto import is correct. For+    # more info, see description of https://github.com/grpc/grpc/pull/25272.+    proto_prefix: str+    # Following 3 fields should be filled by build metadata from Bazel.+    url: str = ''+    hash: str = ''+    strip_prefix: str = ''+++EXTERNAL_PROTO_LIBRARIES = {+    'envoy_api':+        ExternalProtoLibrary(destination='third_party/envoy-api',+                             proto_prefix='third_party/envoy-api/'),+    'com_google_googleapis':+        ExternalProtoLibrary(destination='third_party/googleapis',+                             proto_prefix='third_party/googleapis/'),+    'com_github_cncf_udpa':+        ExternalProtoLibrary(destination='third_party/udpa',+                             proto_prefix='third_party/udpa/'),+    'com_envoyproxy_protoc_gen_validate':+        ExternalProtoLibrary(destination='third_party/protoc-gen-validate',+                             proto_prefix='third_party/protoc-gen-validate/'),+    'opencensus_proto':+        ExternalProtoLibrary(destination='third_party/opencensus-proto/src',+                             proto_prefix='third_party/opencensus-proto/src/'),+}+++@dataclass()+class _HttpArchive:+    name: str = ''+    url: str = ''+    hash: str = ''+    strip_prefix: str = ''+++def _fetch_raw_http_archives() -> ET.Element:+    """"""Get xml output of bazel query invocation, parsed as XML tree""""""+    output = subprocess.check_output([+        'tools/bazel', 'query', '--output', 'xml',+        'kind(http_archive, //external:*)'+    ])+    return ET.fromstring(output)+++def _parse_http_archives(xml_tree: ET.Element) -> List[ExternalProtoLibrary]:+    result = []+    for xml_http_archive in xml_tree:+        if xml_http_archive.tag != 'rule' or xml_http_archive.attrib[+                'class'] != 'http_archive':+            continue+        http_archive = _HttpArchive()+        for xml_node in xml_http_archive:+            if xml_node.attrib['name'] == 'name':+                http_archive.name = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'urls':+                http_archive.url = xml_node[0].attrib['value']+            if xml_node.attrib['name'] == 'url':+                http_archive.url = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'sha256':+                http_archive.hash = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'strip_prefix':+                http_archive.strip_prefix = xml_node.attrib['value']+        if http_archive.name not in EXTERNAL_PROTO_LIBRARIES:+            continue+        lib = EXTERNAL_PROTO_LIBRARIES[http_archive.name]+        lib.url = http_archive.url+        lib.hash = http_archive.hash+        lib.strip_prefix = http_archive.strip_prefix+        result.append(lib)+    return result+++def main():+    xml_tree = _fetch_raw_http_archives()+    libraries = _parse_http_archives(xml_tree)+    libraries.sort(key=lambda x: x.destination)+    print(yaml.dump(dict(external_libraries=list(map(asdict, libraries)))))","also, you're ambitiously calling this the new entity `external_library` but it's unclear what is the future plan with it.Is it intended to be only used for importing .proto files (in which case it should be e.g. `external_proto_library`) or are you somehow planning to use it for other libraries in the future as well? If so, since there's no detail no plan on that, it's unclear to me whether that would actually work or not and if it would be useful (more research would be needed).",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,597850095,2021-03-19T17:18:21Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -44,6 +44,29 @@ BuildDict = Dict[str, BuildMetadata] BuildYaml = Dict[str, Any] +import build_cleaner++PROJECT_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..',+                            '..')+os.chdir(PROJECT_ROOT)","Removed. With this line, I hoped this script can be invoked anywhere. But that seems like a useless feature.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,597862736,2021-03-19T17:37:29Z,test/distrib/cpp/run_distrib_test_cmake.bat,"@@ -66,7 +66,7 @@ popd  @rem Just before installing gRPC, wipe out contents of all the submodules to simulate @rem a standalone build from an archive-git submodule deinit --all --force","`deinit` leaves an empty folder, like `third_party/udpa`. But the submodule polling mechanism only works if the submodule's folder isn't exist. Alternatively, we can validate the integrity of each submodule but they will be much more complex, since the source archive contains no Git information.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,597881631,2021-03-19T18:07:14Z,tools/run_tests/sanity/check_bazel_workspace.py,"@@ -74,6 +75,7 @@     'opencensus_proto',     'com_envoyproxy_protoc_gen_validate',     'com_google_googleapis',+    'com_github_cncf_udpa',","Removed. TIL this is an exemption list, I thought it is adding extra checks.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25762,598007111,2021-03-19T22:34:56Z,bazel/grpc_build_system.bzl,"@@ -128,6 +128,11 @@ def grpc_cc_library(         linkstatic = linkstatic,     ) +def grpc_cc_library_xds(","Please add a comment here saying that once we can depend on the xDS protos internally, this should go away, and all build rules using it should be changed back to use `grpc_cc_library`.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/25836,603608226,2021-03-29T20:57:05Z,tools/run_tests/run_xds_tests.py,"@@ -2728,6 +2756,11 @@ def __init__(self, compute, alpha_compute, project, project_num):                         client_process.returncode)                 result.state = 'PASSED'                 result.returncode = 0+            except PotentialInfrastructureError as infra_error:+                logger.exception('Test case %s skipped', test_case)+                skipped_tests.append(test_case)+                result.state = 'SKIPPED'+                result.message = str(e)","I was trying to figure out from the rendered functions how 'SKIPPED' results are rendered. Unless I'm missing something, the error message is hardcoded:https://github.com/grpc/grpc/blob/86f93801e5d452cff40d6958d21a8eb211da0654/tools/run_tests/python_utils/report_utils.py#L119-L120Might make sense to make a fake run and confirm if that hardcoded message `'Skipped'` needs to be replaced with `result.message` when it's not empty.",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/25794,603826357,2021-03-30T06:50:59Z,Rakefile,"@@ -147,11 +144,12 @@ task 'gem:native' do     # Truncate grpc_c.*.ruby files because they're for Windows only.     File.truncate('grpc_c.32.ruby', 0)     File.truncate('grpc_c.64.ruby', 0)-    ['x86_64-linux', 'x86-linux'].each do |plat|-      run_rake_compiler plat, <<-EOT+    ['x86_64-linux', 'x86-linux', 'x86_64-darwin', 'arm64-darwin'].each do |plat|","I believe that we we run `rake gem:native` on macos hosts (for the mac package build), this will also build the `'x86_64-darwin` packages on macos - so I want to say we're building this packages twice: once on linux via docker and again on macos.Since this set of cross-compilation rake tasks is only needed for cross-compilation from linux, can we run this conditionally, only under linux?I think that will take care of the macos package build timeout that happened in the last package build link posted.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,604439149,2021-03-30T21:19:38Z,CMakeLists.txt,"@@ -256,6 +256,53 @@ include(cmake/ssl.cmake) include(cmake/upb.cmake) include(cmake/xxhash.cmake) include(cmake/zlib.cmake)+include(cmake/xds_proto.cmake)++# Setup external proto libraray at third_party/envoy-api+if (NOT EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/third_party/envoy-api)+  fetch_check_extract(+    ${CMAKE_CURRENT_SOURCE_DIR}/third_party/envoy-api+    https://storage.googleapis.com/grpc-bazel-mirror/github.com/envoyproxy/data-plane-api/archive/18b54850c9b7ba29a4ab67cbd7ed7eab7b0bbdb2.tar.gz","Duplicated the `fetch_check_extract` for every provided URL. CMake will try each download URL using the same order as Bazel. If one download passed, the if condition will ensure to stop duplicated downloads.Chinese programmers will find a way to get cross the great firewall. This failure doesn't stop the build from proceeding, and people can find other ways to download the submodule (e.g., using Git). ",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,604441475,2021-03-30T21:23:57Z,cmake/xds_proto.cmake,"@@ -0,0 +1,39 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set(_xDS_Proto_TEMPORARY_DIR ${CMAKE_BINARY_DIR}/xds_proto)",Updated to `set(_download_archive_TEMPORARY_DIR ${CMAKE_BINARY_DIR}/http_archives)`. This is a temporary location to store intermediate files.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,604441924,2021-03-30T21:24:53Z,cmake/xds_proto.cmake,"@@ -0,0 +1,39 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set(_xDS_Proto_TEMPORARY_DIR ${CMAKE_BINARY_DIR}/xds_proto)","This is a temporary directory, the content in it will be removed. I'm not sure what is the best location to store intermediate content. Feel free to suggest :)",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,604445127,2021-03-30T21:30:39Z,src/xds-proto/gen_build_yaml.py,"@@ -0,0 +1,116 @@+#!/usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Generate build metadata for external protos (mostly xDS-related).++The build metadata will be used in CMakeLists to ensure xDS proto dependencies+are present before compilation.+""""""++import os+import subprocess+import xml.etree.ElementTree as ET+from dataclasses import asdict, dataclass+from typing import List++import yaml+++@dataclass()+class ExternalProtoLibrary:+    # The relative path of this proto library should be. Preferably, it should+    # match the submodule path.+    destination: str+    # The prefix to remove in order to insure the proto import is correct. For+    # more info, see description of https://github.com/grpc/grpc/pull/25272.+    proto_prefix: str+    # Following 3 fields should be filled by build metadata from Bazel.+    url: str = ''+    hash: str = ''+    strip_prefix: str = ''+++EXTERNAL_PROTO_LIBRARIES = {+    'envoy_api':","Good idea. Entire logic moved into `extract_metadata_from_bazel_xml`. The new ""external_libraries"" build metdata is added to `build_autogenerated.yaml`.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,604445805,2021-03-30T21:32:04Z,src/xds-proto/gen_build_yaml.py,"@@ -0,0 +1,116 @@+#!/usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Generate build metadata for external protos (mostly xDS-related).++The build metadata will be used in CMakeLists to ensure xDS proto dependencies+are present before compilation.+""""""++import os+import subprocess+import xml.etree.ElementTree as ET+from dataclasses import asdict, dataclass+from typing import List++import yaml+++@dataclass()+class ExternalProtoLibrary:+    # The relative path of this proto library should be. Preferably, it should+    # match the submodule path.+    destination: str+    # The prefix to remove in order to insure the proto import is correct. For+    # more info, see description of https://github.com/grpc/grpc/pull/25272.+    proto_prefix: str+    # Following 3 fields should be filled by build metadata from Bazel.+    url: str = ''+    hash: str = ''+    strip_prefix: str = ''+++EXTERNAL_PROTO_LIBRARIES = {+    'envoy_api':+        ExternalProtoLibrary(destination='third_party/envoy-api',+                             proto_prefix='third_party/envoy-api/'),+    'com_google_googleapis':+        ExternalProtoLibrary(destination='third_party/googleapis',+                             proto_prefix='third_party/googleapis/'),+    'com_github_cncf_udpa':+        ExternalProtoLibrary(destination='third_party/udpa',+                             proto_prefix='third_party/udpa/'),+    'com_envoyproxy_protoc_gen_validate':+        ExternalProtoLibrary(destination='third_party/protoc-gen-validate',+                             proto_prefix='third_party/protoc-gen-validate/'),+    'opencensus_proto':+        ExternalProtoLibrary(destination='third_party/opencensus-proto/src',+                             proto_prefix='third_party/opencensus-proto/src/'),+}+++@dataclass()+class _HttpArchive:+    name: str = ''+    url: str = ''+    hash: str = ''+    strip_prefix: str = ''+++def _fetch_raw_http_archives() -> ET.Element:",This is removed in `extract_metadata_from_bazel_xml`. Using `_bazel_query_xml_tree` instead.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,604449157,2021-03-30T21:38:32Z,src/xds-proto/gen_build_yaml.py,"@@ -0,0 +1,116 @@+#!/usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Generate build metadata for external protos (mostly xDS-related).++The build metadata will be used in CMakeLists to ensure xDS proto dependencies+are present before compilation.+""""""++import os+import subprocess+import xml.etree.ElementTree as ET+from dataclasses import asdict, dataclass+from typing import List++import yaml+++@dataclass()+class ExternalProtoLibrary:+    # The relative path of this proto library should be. Preferably, it should+    # match the submodule path.+    destination: str+    # The prefix to remove in order to insure the proto import is correct. For+    # more info, see description of https://github.com/grpc/grpc/pull/25272.+    proto_prefix: str+    # Following 3 fields should be filled by build metadata from Bazel.+    url: str = ''+    hash: str = ''+    strip_prefix: str = ''+++EXTERNAL_PROTO_LIBRARIES = {+    'envoy_api':+        ExternalProtoLibrary(destination='third_party/envoy-api',+                             proto_prefix='third_party/envoy-api/'),+    'com_google_googleapis':+        ExternalProtoLibrary(destination='third_party/googleapis',+                             proto_prefix='third_party/googleapis/'),+    'com_github_cncf_udpa':+        ExternalProtoLibrary(destination='third_party/udpa',+                             proto_prefix='third_party/udpa/'),+    'com_envoyproxy_protoc_gen_validate':+        ExternalProtoLibrary(destination='third_party/protoc-gen-validate',+                             proto_prefix='third_party/protoc-gen-validate/'),+    'opencensus_proto':+        ExternalProtoLibrary(destination='third_party/opencensus-proto/src',+                             proto_prefix='third_party/opencensus-proto/src/'),+}+++@dataclass()+class _HttpArchive:+    name: str = ''+    url: str = ''+    hash: str = ''+    strip_prefix: str = ''+++def _fetch_raw_http_archives() -> ET.Element:+    """"""Get xml output of bazel query invocation, parsed as XML tree""""""+    output = subprocess.check_output([+        'tools/bazel', 'query', '--output', 'xml',+        'kind(http_archive, //external:*)'+    ])+    return ET.fromstring(output)+++def _parse_http_archives(xml_tree: ET.Element) -> List[ExternalProtoLibrary]:+    result = []+    for xml_http_archive in xml_tree:+        if xml_http_archive.tag != 'rule' or xml_http_archive.attrib[+                'class'] != 'http_archive':+            continue+        http_archive = _HttpArchive()+        for xml_node in xml_http_archive:+            if xml_node.attrib['name'] == 'name':+                http_archive.name = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'urls':+                http_archive.url = xml_node[0].attrib['value']+            if xml_node.attrib['name'] == 'url':+                http_archive.url = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'sha256':+                http_archive.hash = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'strip_prefix':+                http_archive.strip_prefix = xml_node.attrib['value']+        if http_archive.name not in EXTERNAL_PROTO_LIBRARIES:+            continue+        lib = EXTERNAL_PROTO_LIBRARIES[http_archive.name]+        lib.url = http_archive.url+        lib.hash = http_archive.hash+        lib.strip_prefix = http_archive.strip_prefix+        result.append(lib)+    return result+++def main():+    xml_tree = _fetch_raw_http_archives()+    libraries = _parse_http_archives(xml_tree)+    libraries.sort(key=lambda x: x.destination)+    print(yaml.dump(dict(external_libraries=list(map(asdict, libraries)))))",I'm keeping it as `external_library` since we might move upb/xxhash into this list. WDYT?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,604451378,2021-03-30T21:43:01Z,templates/CMakeLists.txt.template,"@@ -322,6 +331,19 @@   include(cmake/upb.cmake)   include(cmake/xxhash.cmake)   include(cmake/zlib.cmake)+  include(cmake/xds_proto.cmake)++  % for external_library in external_libraries:+  # Setup external proto libraray at ${external_library.destination}+  if (NOT EXISTS <%text>${CMAKE_CURRENT_SOURCE_DIR}</%text>/${external_library.destination})","Adding several comments, describing how the if condition works and what the download function is doing.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,604452134,2021-03-30T21:44:32Z,templates/CMakeLists.txt.template,"@@ -345,49 +370,53 @@   #   Add custom commands to process ``.proto`` files to C++ using protoc and   #   GRPC plugin::   #-  #     protobuf_generate_grpc_cpp [<ARGN>...]+  #     protobuf_generate_grpc_cpp <FILE_LOCATION> <IMPORT_PATH>   #-  #   ``ARGN``-  #     ``.proto`` files+  #   ``FILE_LOCATION``+  #     The relative path of the ``.proto`` file to the project root+  #   ``IMPORT_PATH``","It's the ProtoBuf import path, where the source file itself thinks it should belong to. Feel free to suggest alternatives.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,604460690,2021-03-30T22:01:37Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -272,6 +322,7 @@ def _compute_transitive_metadata(             # list since we want to avoid building sources that are already             # built by our dependencies             exclude_deps.update(bazel_rules[dep]['_TRANSITIVE_DEPS'])+            exclude_deps.add(dep)",Reverted to your version.I don't think it's worth either of our time to proof it's right or wrong. It's a trade off between CPU cycles and cleanness. I think both are correct as long as they generate correct result within acceptable time.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25871,606442418,2021-04-02T21:58:42Z,tools/run_tests/run_xds_tests.py,"@@ -48,6 +49,13 @@ logger.addHandler(console_handler) logger.setLevel(logging.WARNING) +# Suppress excessive logs for gRPC Python+original_grpc_trace = os.environ.pop('GRPC_TRACE')+original_grpc_verbosity = os.environ.pop('GRPC_VERBOSITY')+# Suppress not-essential logs for GCP clients+logging.getLogger('google_auth_httplib2').setLevel(logging.WARNING)+logging.getLogger('googleapiclient.discovery').setLevel(logging.WARNING)","This eliminates the ""Make Requests"" and ""URL being requested"" stuff. If the underlying HTTP failed, it will raise an exception that we won't miss.What GCP client log do you think we should preserve here?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25881,607358417,2021-04-05T21:58:13Z,tools/run_tests/run_xds_tests.py,"@@ -190,7 +190,7 @@ def parse_port_range(port_arg):                   default='global/networks/default',                   help='GCP network to use') argp.add_argument('--service_port_range',-                  default='8080:8110',+                  default='8080:8280',","optional: we can expand the port range to a much larger numbers. E.g., 1k? 50k?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25898,609622736,2021-04-08T12:04:36Z,tools/internal_ci/linux/grpc_e2e_performance_v2.sh,"@@ -19,5 +19,28 @@ cd $(dirname $0)/../../..  source tools/internal_ci/helper_scripts/prepare_build_linux_rc +# This is to insure we can push and pull images from gcr.io, we do not+# necessarily need it to run load tests, but would need it when we employ+# pre-built images in the optimization.+gcloud auth configure-docker++# Connect to benchmarks-prod cluster+gcloud config set project grpc-testing+gcloud container clusters get-credentials benchmarks-prod \+    --zone us-central1-b --project grpc-testing++# This step subject to change, just easier to get one file to run a single test+wget https://raw.githubusercontent.com/grpc/test-infra/master/config/samples/go_example_loadtest.yaml++# The original version of the client is a bit old, update to a newer version.+kubectl version --client+curl -LO ""https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl""",nit: I'd just hardcode the last stable version available now. You're probably not very dependent on kubectl version so the extra complexity is unnecessary.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25511,609933170,2021-04-08T17:27:39Z,WORKSPACE,"@@ -6,6 +6,16 @@ grpc_deps()  grpc_test_only_deps() +# Initialize Google APIs with only C++ and Python targets+load(""@com_google_googleapis//:repository_rules.bzl"", ""switched_rules_by_language"")++switched_rules_by_language(","@Kernald I wasn't aware that `switched_rules_by_language` did this. Yes, this sounds very problematic. You could easily get into a diamond dependency situation where you depend on repo A and repo B, both using different `switched_rules_by_language` calls and with one always failing depending on the order in which you pull them in.@lidizheng Creating [an issue](https://github.com/grpc/grpc/issues/25931) to track this.",X
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/25942,610754777,2021-04-09T16:19:41Z,include/grpc/grpc_security_constants.h,"@@ -29,6 +29,15 @@ extern ""C"" { #define GRPC_X509_CN_PROPERTY_NAME ""x509_common_name"" #define GRPC_X509_SAN_PROPERTY_NAME ""x509_subject_alternative_name"" #define GRPC_X509_PEM_CERT_PROPERTY_NAME ""x509_pem_cert""+// Please note that internally, we just faithfully pass whatever value we got by+// calling SSL_get_peer_certificate() in OpenSSL/BoringSSL. This will mean in","Actually, the code calls `SSL_get_peer_cert_chain()`. ",
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/25942,610756352,2021-04-09T16:22:19Z,include/grpc/grpc_security_constants.h,"@@ -29,6 +29,15 @@ extern ""C"" { #define GRPC_X509_CN_PROPERTY_NAME ""x509_common_name"" #define GRPC_X509_SAN_PROPERTY_NAME ""x509_subject_alternative_name"" #define GRPC_X509_PEM_CERT_PROPERTY_NAME ""x509_pem_cert""+// Please note that internally, we just faithfully pass whatever value we got by+// calling SSL_get_peer_certificate() in OpenSSL/BoringSSL. This will mean in+// OpenSSL, the following conditions might apply:+// 1. on the server side, the full chain would be the whole chain without the+// leaf certificate","I would rephraseOne the client side, this property returns the full certificate chain. On the server side, this property will return the certificate chain without the leaf certificate. Application can use GRPC_X509_PEM_CERT_PROPERTY_NAME to get the peer leaf certificate.",X
19913700,jiangtaoli2016,https://api.github.com/repos/grpc/grpc/pulls/25942,610757320,2021-04-09T16:24:01Z,include/grpc/grpc_security_constants.h,"@@ -29,6 +29,15 @@ extern ""C"" { #define GRPC_X509_CN_PROPERTY_NAME ""x509_common_name"" #define GRPC_X509_SAN_PROPERTY_NAME ""x509_subject_alternative_name"" #define GRPC_X509_PEM_CERT_PROPERTY_NAME ""x509_pem_cert""+// Please note that internally, we just faithfully pass whatever value we got by+// calling SSL_get_peer_certificate() in OpenSSL/BoringSSL. This will mean in+// OpenSSL, the following conditions might apply:+// 1. on the server side, the full chain would be the whole chain without the+// leaf certificate+// 2. when session resumption happens, the full chain would be empty+// BoringSSL wouldn't have this issue. For more, please refer to the official+// OpenSSL manual:+// https://www.openssl.org/docs/man1.0.2/man3/SSL_get_peer_certificate.html.",nit: use this link https://www.openssl.org/docs/man1.1.0/man3/SSL_get_peer_cert_chain.html,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/25942,610807544,2021-04-09T17:49:19Z,src/core/tsi/ssl_transport_security.h,"@@ -40,6 +40,15 @@ extern ""C"" {  #define TSI_X509_PEM_CERT_PROPERTY ""x509_pem_cert"" +// Please note that internally, we just faithfully pass whatever value we got by","OK, I removed it.I just thought adding a comment here would be a ""reminder"" for all other languages, otherwise if the field is exposed by other languages, we will need to remember to add similar comments.  ",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/25943,610826605,2021-04-09T18:23:27Z,tools/internal_ci/linux/grpc_xds_php_test_in_docker.sh,"@@ -93,4 +93,4 @@ GRPC_VERBOSITY=debug GRPC_TRACE=xds_client,xds_resolver,xds_cluster_manager_lb,c   --gcp_suffix=$(date '+%s') \   --verbose \   ${XDS_V3_OPT-} \-  --client_cmd='./src/php/bin/run_xds_client.sh --server=xds:///{server_uri} --stats_port={stats_port} --qps={qps} {fail_on_failed_rpc} {rpcs_to_send} {metadata_to_send}'+  --client_cmd='php -d extension=grpc.so -d extension=pthreads.so src/php/tests/interop/xds_client.php --server=xds:///{server_uri} --stats_port={stats_port} --qps={qps} {fail_on_failed_rpc} {rpcs_to_send} {metadata_to_send}'",To reviewers: this is me trying to tag along to debug some mysterious SIGTERM causing some flakes in other PHP xds interop tests. Can safely ignore this for now. This has nothing to do with the `fault_injection` test we are trying to add in this PR.,
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/25943,610887610,2021-04-09T20:26:57Z,tools/internal_ci/linux/grpc_xds_php_test_in_docker.sh,"@@ -93,4 +93,4 @@ GRPC_VERBOSITY=debug GRPC_TRACE=xds_client,xds_resolver,xds_cluster_manager_lb,c   --gcp_suffix=$(date '+%s') \   --verbose \   ${XDS_V3_OPT-} \-  --client_cmd='./src/php/bin/run_xds_client.sh --server=xds:///{server_uri} --stats_port={stats_port} --qps={qps} {fail_on_failed_rpc} {rpcs_to_send} {metadata_to_send}'+  --client_cmd='php -d extension=grpc.so -d extension=pthreads.so src/php/tests/interop/xds_client.php --server=xds:///{server_uri} --stats_port={stats_port} --qps={qps} {fail_on_failed_rpc} {rpcs_to_send} {metadata_to_send}'","A flake looks like this: https://fusion.corp.googleusercontent.com/file/autoview?fileUri=googlefile%3A%2Fbigstore%2Fgrpc-testing-kokoro-prod%2Ftest_result_public%2Fprod%2Fgrpc%2Fcore%2Fmaster%2Flinux%2Fgrpc_xds_v3_php%2F308%2F20210409-062404%2Fgithub%2Fgrpc%2Freports%2Fping_pong%2Fsponge_log.log&filename=test.log&invocationId=d28ddda5-34ae-4a98-8b1a-e65a5ee9036d&actionName=invocations%2Fd28ddda5-34ae-4a98-8b1a-e65a5ee9036d%2Ftargets%2Fgithub%252Fgrpc%252Freports%252Fping_pong%2FconfiguredTargets%2Fdefault%2Factions%2Ftest_attempt0&contentType=ansiThe SIGTERM handler you see at the end, is the one I expect to receive after each test case is finished, so that I can kill my PHP client. But in this flake, I am receiving that SIGTERM much earlier than I expected. ",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25272,611538005,2021-04-12T11:12:54Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -628,6 +699,52 @@ def _generate_build_extra_metadata_for_tests(     return test_metadata  +def _parse_http_archives(xml_tree: ET.Element) -> List[ExternalProtoLibrary]:+    """"""Parse Bazel http_archive rule into ExternalProtoLibrary objects.""""""+    result = []+    for xml_http_archive in xml_tree:+        if xml_http_archive.tag != 'rule' or xml_http_archive.attrib[+                'class'] != 'http_archive':+            continue+        # A distilled Python representation of Bazel http_archive+        http_archive = dict()+        for xml_node in xml_http_archive:+            if xml_node.attrib['name'] == 'name':+                http_archive[""name""] = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'urls':+                http_archive[""urls""] = []+                for url_node in xml_node:+                    http_archive[""urls""].append(url_node.attrib['value'])+            if xml_node.attrib['name'] == 'url':+                http_archive[""urls""] = [xml_node.attrib['value']]+            if xml_node.attrib['name'] == 'sha256':+                http_archive[""hash""] = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'strip_prefix':+                http_archive[""strip_prefix""] = xml_node.attrib['value']+        if http_archive[""name""] not in EXTERNAL_PROTO_LIBRARIES:+            # If this http archive is not one of the external proto libraries,+            # we don't want to include it as a CMake target+            continue+        for url in http_archive[""urls""]:","strictly speaking, duplicating the external_library entries when there's multiple URLs is not correct, but I can understand that it makes the logic in the CMakeLists.txt simpler to implement (but it also makes it a bit harder to understand).Since the sha256 and strip_prefix for the mirrored urls always needs to be the same, I think it would be better for the `external_library` entity in the build_autogenerated.yaml to allow URL to be a list of URLs (this is not a problem at all for the yaml file). That way, the ""metadata"" in build_autogenerated.yaml would be more correct (because you'd keep the clear mapping between http_achives and ""external_library"" entries). The duplication download_archive rules can then be trivially done in CMakeLists.txt.template (and at that point it would be also easier to understand the logic behind it - you could even add an explanatory comment for cases where ""urls"" contains multiple entries.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25272,611847059,2021-04-12T18:08:02Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -628,6 +699,52 @@ def _generate_build_extra_metadata_for_tests(     return test_metadata  +def _parse_http_archives(xml_tree: ET.Element) -> List[ExternalProtoLibrary]:+    """"""Parse Bazel http_archive rule into ExternalProtoLibrary objects.""""""+    result = []+    for xml_http_archive in xml_tree:+        if xml_http_archive.tag != 'rule' or xml_http_archive.attrib[+                'class'] != 'http_archive':+            continue+        # A distilled Python representation of Bazel http_archive+        http_archive = dict()+        for xml_node in xml_http_archive:+            if xml_node.attrib['name'] == 'name':+                http_archive[""name""] = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'urls':+                http_archive[""urls""] = []+                for url_node in xml_node:+                    http_archive[""urls""].append(url_node.attrib['value'])+            if xml_node.attrib['name'] == 'url':+                http_archive[""urls""] = [xml_node.attrib['value']]+            if xml_node.attrib['name'] == 'sha256':+                http_archive[""hash""] = xml_node.attrib['value']+            if xml_node.attrib['name'] == 'strip_prefix':+                http_archive[""strip_prefix""] = xml_node.attrib['value']+        if http_archive[""name""] not in EXTERNAL_PROTO_LIBRARIES:+            # If this http archive is not one of the external proto libraries,+            # we don't want to include it as a CMake target+            continue+        for url in http_archive[""urls""]:","Done. Updated the `external_library` to contain multiple URLs, and added comments if there are multiple download URLs for one library.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25955,611906105,2021-04-12T19:44:09Z,src/core/lib/security/authorization/cel_authorization_engine.cc,"@@ -132,21 +132,20 @@ std::unique_ptr<mock_cel::Activation> CelAuthorizationEngine::CreateActivation(       activation->InsertValue(kHeaders,                               mock_cel::CelValue::CreateMap(headers_.get()));     } else if (elem == kSourceAddress) {-      absl::string_view source_address(args.GetPeerAddress());+      std::string source_address(args.GetPeerAddress());       if (!source_address.empty()) {         activation->InsertValue(-            kSourceAddress,-            mock_cel::CelValue::CreateStringView(source_address));+            kSourceAddress, mock_cel::CelValue::CreateString(&source_address));","I don't think this change is correct.  It looks to me like the real `CelValue` implementation is storing only a `string_view` pointing to whatever string you pass in here, regardless of whether that string is passed in as a `string_view` or a `string`.  But this code is now constructing a local `string` variable and using it to construct the `CelValue`.  The local `string` variable will go out of scope as soon as this block ends, which will leave the `CelValue` pointing to a string that no longer exists.",X
10470658,donnadionne,https://api.github.com/repos/grpc/grpc/pulls/25961,612137274,2021-04-13T05:21:13Z,src/core/ext/filters/client_channel/retry_filter.cc,"@@ -525,6 +528,58 @@ class RetryFilter::CallData {   grpc_metadata_batch send_trailing_metadata_; }; +//+// RetryFilter::CallData::CallStackDestructionBarrier+//++// A class to track the existence of LoadBalancedCall call stacks that+// we've created.  We wait until all such call stacks have been+// destroyed before we return the on_call_stack_destruction closure up+// to the surface.+//+// The parent RetryFilter::CallData object holds a ref to this object.+// When it is destroyed, it will store the on_call_stack_destruction+// closure from the surface in this object and then release its ref.+// We also take a ref to this object for each LB call we create, and+// those refs are not released until the LB call stack is destroyed.+// When this object is destroyed, it will invoke the+// on_call_stack_destruction closure from the surface.+class RetryFilter::CallData::CallStackDestructionBarrier+    : public RefCounted<CallStackDestructionBarrier, PolymorphicRefCount,+                        kUnrefCallDtor> {+ public:+  CallStackDestructionBarrier() {}++  ~CallStackDestructionBarrier() {+    // TODO(yashkt) : This can potentially be a Closure::Run+    ExecCtx::Run(DEBUG_LOCATION, on_call_stack_destruction_, GRPC_ERROR_NONE);+  }++  // Set the closure from the surface.  This closure will be invoked+  // when this object is destroyed.+  void set_on_call_stack_destruction(grpc_closure* on_call_stack_destruction) {+    on_call_stack_destruction_ = on_call_stack_destruction;+  }++  // Invoked to get an on_call_stack_destruction closure for a new LB call.+  grpc_closure* MakeLbCallDestructionClosure(CallData* calld) {+    Ref().release();  // Ref held by callback.+    grpc_closure* on_lb_call_destruction_complete =+        calld->arena_->New<grpc_closure>();+    GRPC_CLOSURE_INIT(on_lb_call_destruction_complete,+                      OnLbCallDestructionComplete, this, nullptr);+    return on_lb_call_destruction_complete;+  }++ private:+  static void OnLbCallDestructionComplete(void* arg, grpc_error* error) {","error is unused leading to src/core/ext/filters/client_channel/retry_filter.cc:575:66: error: unused parameter 'error' [-Werror,-Wunused-parameter]  static void OnLbCallDestructionComplete(void* arg, grpc_error* error) {",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25976,613824416,2021-04-15T07:31:59Z,tools/run_tests/performance/loadtest_config.py,"@@ -211,12 +248,18 @@ def main() -> None:         help='Select a category of tests to run.')     argp.add_argument(         '--client_language',+        action='append',         choices=language_choices,-        help='Select only scenarios with a specified client language.')+        default=[],+        help='Add additional scenarios with this specified client language.',","I think the right semantics is ""include cross-language scenarios with the specified client language. Without this argument being specified, only the standard single-language scenarios run will run and crosslanguage scenarios that override the client language will be skipped."" and rename the arg to e.g. `--allow_client_language`.Same for `--server_language`. ",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25976,613849413,2021-04-15T08:09:41Z,tools/run_tests/performance/loadtest_template.py,"@@ -0,0 +1,190 @@+#!/usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This script generates a load test configuration template from a collection of+# load test configurations.+#+# The following example generates a basic template from the example configs+# in https://github.com/grpc/test-infra/tree/master/config/samples:+#+# $ ./tools/run_tests/performance/loadtest_template.py \+#     -i ../test-infra/config/samples/*.yaml \+#     -o ./tools/run_tests/performance/templates/basic_template.yaml \+#     --name basic_template++import argparse+import sys++from typing import Any, Dict, Iterable, Mapping++import yaml++import loadtest_config++DEFAULT_KEYS = {+    'client_pool': '${client_pool}',+    'server_pool': '${server_pool}',+    'big_query_table': '${big_query_table}',+    'timeoutSeconds': '',+    'ttlSeconds': '',+}+++def validate_keys(keys: Mapping[str, str]) -> None:+    """"""Validates that substitution keys are a subset of default keys.""""""+    extra_keys = set(keys).difference(DEFAULT_KEYS)+    if extra_keys:+        raise ValueError('Unrecognized replacement keys: %s', ' '.join(keys))+++def loadtest_set_keys(+    template: Mapping[str, Any],+    keys: Mapping[str, str],+) -> None:+    """"""Sets substitution keys in the template.++     These keys are set so they can be replaced later by the config gemerator.+     """"""+    if keys.get('client_pool'):+        client_pool = keys['client_pool']+        clients = template['spec']['clients']+        for client in clients:+            client['pool'] = client_pool++    if keys.get('server_pool'):+        server_pool = keys['server_pool']+        servers = template['spec']['servers']+        for server in servers:+            server['pool'] = server_pool++    if keys.get('big_query_table'):+        template['spec']['big_query_table'] = keys['big_query_table']++    for key in ('timeoutSeconds', 'ttlSeconds'):+        if keys.get(key):+            template[key] = keys[key]+++def loadtest_template(input_file_names: Iterable[str], keys: Mapping[str, str],+                      metadata: Mapping[str, Any]) -> Dict[str, Any]:+    """"""Generates the load test template.""""""+    clients = list()+    servers = list()+    spec = dict()+    client_languages = set()+    server_languages = set()+    template = {+        'apiVersion': 'e2etest.grpc.io/v1',+        'kind': 'LoadTest',+        'metadata': metadata,+    }+    for input_file_name in input_file_names:+        with open(input_file_name) as f:+            input_config = yaml.safe_load(f.read())++            if input_config.get('apiVersion') != template['apiVersion']:+                raise ValueError('Unexpected api version in file {}: {}'.format(+                    input_file_name, input_config.get('apiVersion')))+            if input_config.get('kind') != template['kind']:+                raise ValueError('Unexpected kind in file {}: {}'.format(+                    input_file_name, input_config.get('kind')))++            for client in input_config['spec']['clients']:+                if client['language'] in client_languages:+                    continue+                clients.append(client)+                client_languages.add(client['language'])++            for server in input_config['spec']['servers']:+                if server['language'] in server_languages:+                    continue+                servers.append(server)+                server_languages.add(server['language'])++            input_spec = input_config['spec']+            del input_spec['clients']+            del input_spec['servers']+            del input_spec['scenariosJSON']+            spec.update(input_config['spec'])++    clients.sort(key=lambda x: x['language'])+    servers.sort(key=lambda x: x['language'])++    spec.update({+        'clients': clients,+        'servers': servers,+    })++    template['spec'] = spec++    loadtest_set_keys(template, keys)++    return template+++def main() -> None:+    argp = argparse.ArgumentParser(+        description='Creates a load test config generator template.')+    argp.add_argument('-i',+                      '--inputs',+                      action='extend',+                      nargs='*',+                      type=str,+                      required=True,+                      help='Input files.')+    argp.add_argument('-o',+                      '--output',+                      type=str,+                      help='Output file. Outputs to stdout if not set.')+    argp.add_argument('-k',","the concept of keys and their semantics is a bit odd and it's not explained in the help string, so the values to pass here feels a bit like magic.  Also the name ""keys"" isn't really descriptive.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25976,613849741,2021-04-15T08:10:11Z,tools/run_tests/performance/loadtest_template.py,"@@ -0,0 +1,190 @@+#!/usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This script generates a load test configuration template from a collection of+# load test configurations.+#+# The following example generates a basic template from the example configs+# in https://github.com/grpc/test-infra/tree/master/config/samples:+#+# $ ./tools/run_tests/performance/loadtest_template.py \+#     -i ../test-infra/config/samples/*.yaml \+#     -o ./tools/run_tests/performance/templates/basic_template.yaml \+#     --name basic_template++import argparse+import sys++from typing import Any, Dict, Iterable, Mapping++import yaml++import loadtest_config++DEFAULT_KEYS = {+    'client_pool': '${client_pool}',+    'server_pool': '${server_pool}',+    'big_query_table': '${big_query_table}',+    'timeoutSeconds': '',+    'ttlSeconds': '',+}+++def validate_keys(keys: Mapping[str, str]) -> None:+    """"""Validates that substitution keys are a subset of default keys.""""""+    extra_keys = set(keys).difference(DEFAULT_KEYS)+    if extra_keys:+        raise ValueError('Unrecognized replacement keys: %s', ' '.join(keys))+++def loadtest_set_keys(+    template: Mapping[str, Any],+    keys: Mapping[str, str],+) -> None:+    """"""Sets substitution keys in the template.++     These keys are set so they can be replaced later by the config gemerator.+     """"""+    if keys.get('client_pool'):+        client_pool = keys['client_pool']+        clients = template['spec']['clients']+        for client in clients:+            client['pool'] = client_pool++    if keys.get('server_pool'):+        server_pool = keys['server_pool']+        servers = template['spec']['servers']+        for server in servers:+            server['pool'] = server_pool++    if keys.get('big_query_table'):+        template['spec']['big_query_table'] = keys['big_query_table']++    for key in ('timeoutSeconds', 'ttlSeconds'):+        if keys.get(key):+            template[key] = keys[key]+++def loadtest_template(input_file_names: Iterable[str], keys: Mapping[str, str],+                      metadata: Mapping[str, Any]) -> Dict[str, Any]:+    """"""Generates the load test template.""""""+    clients = list()+    servers = list()+    spec = dict()+    client_languages = set()+    server_languages = set()+    template = {+        'apiVersion': 'e2etest.grpc.io/v1',+        'kind': 'LoadTest',+        'metadata': metadata,+    }+    for input_file_name in input_file_names:+        with open(input_file_name) as f:+            input_config = yaml.safe_load(f.read())++            if input_config.get('apiVersion') != template['apiVersion']:+                raise ValueError('Unexpected api version in file {}: {}'.format(+                    input_file_name, input_config.get('apiVersion')))+            if input_config.get('kind') != template['kind']:+                raise ValueError('Unexpected kind in file {}: {}'.format(+                    input_file_name, input_config.get('kind')))++            for client in input_config['spec']['clients']:+                if client['language'] in client_languages:+                    continue+                clients.append(client)+                client_languages.add(client['language'])++            for server in input_config['spec']['servers']:+                if server['language'] in server_languages:+                    continue+                servers.append(server)+                server_languages.add(server['language'])++            input_spec = input_config['spec']+            del input_spec['clients']+            del input_spec['servers']+            del input_spec['scenariosJSON']+            spec.update(input_config['spec'])++    clients.sort(key=lambda x: x['language'])+    servers.sort(key=lambda x: x['language'])++    spec.update({+        'clients': clients,+        'servers': servers,+    })++    template['spec'] = spec++    loadtest_set_keys(template, keys)++    return template+++def main() -> None:+    argp = argparse.ArgumentParser(+        description='Creates a load test config generator template.')+    argp.add_argument('-i',+                      '--inputs',+                      action='extend',+                      nargs='*',+                      type=str,+                      required=True,+                      help='Input files.')+    argp.add_argument('-o',+                      '--output',+                      type=str,+                      help='Output file. Outputs to stdout if not set.')+    argp.add_argument('-k',+                      '--keys',+                      action='extend',+                      nargs='*',+                      default=[],+                      type=str,+                      help='Value of keys to insert, in the form key=value.')+    argp.add_argument('-n',+                      '--name',+                      default='',+                      type=str,+                      help='Name to insert.')",help string should mention that this will become metadata.name in the final template.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25976,613851393,2021-04-15T08:12:42Z,tools/run_tests/performance/loadtest_template.py,"@@ -0,0 +1,190 @@+#!/usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This script generates a load test configuration template from a collection of+# load test configurations.+#+# The following example generates a basic template from the example configs+# in https://github.com/grpc/test-infra/tree/master/config/samples:+#+# $ ./tools/run_tests/performance/loadtest_template.py \+#     -i ../test-infra/config/samples/*.yaml \+#     -o ./tools/run_tests/performance/templates/basic_template.yaml \+#     --name basic_template++import argparse+import sys++from typing import Any, Dict, Iterable, Mapping++import yaml++import loadtest_config++DEFAULT_KEYS = {+    'client_pool': '${client_pool}',+    'server_pool': '${server_pool}',+    'big_query_table': '${big_query_table}',+    'timeoutSeconds': '',+    'ttlSeconds': '',+}+++def validate_keys(keys: Mapping[str, str]) -> None:+    """"""Validates that substitution keys are a subset of default keys.""""""+    extra_keys = set(keys).difference(DEFAULT_KEYS)+    if extra_keys:+        raise ValueError('Unrecognized replacement keys: %s', ' '.join(keys))+++def loadtest_set_keys(+    template: Mapping[str, Any],+    keys: Mapping[str, str],+) -> None:+    """"""Sets substitution keys in the template.++     These keys are set so they can be replaced later by the config gemerator.+     """"""+    if keys.get('client_pool'):+        client_pool = keys['client_pool']+        clients = template['spec']['clients']+        for client in clients:+            client['pool'] = client_pool++    if keys.get('server_pool'):+        server_pool = keys['server_pool']+        servers = template['spec']['servers']+        for server in servers:+            server['pool'] = server_pool++    if keys.get('big_query_table'):+        template['spec']['big_query_table'] = keys['big_query_table']++    for key in ('timeoutSeconds', 'ttlSeconds'):+        if keys.get(key):+            template[key] = keys[key]+++def loadtest_template(input_file_names: Iterable[str], keys: Mapping[str, str],+                      metadata: Mapping[str, Any]) -> Dict[str, Any]:+    """"""Generates the load test template.""""""+    clients = list()+    servers = list()+    spec = dict()+    client_languages = set()+    server_languages = set()+    template = {+        'apiVersion': 'e2etest.grpc.io/v1',+        'kind': 'LoadTest',+        'metadata': metadata,+    }+    for input_file_name in input_file_names:+        with open(input_file_name) as f:+            input_config = yaml.safe_load(f.read())++            if input_config.get('apiVersion') != template['apiVersion']:+                raise ValueError('Unexpected api version in file {}: {}'.format(+                    input_file_name, input_config.get('apiVersion')))+            if input_config.get('kind') != template['kind']:+                raise ValueError('Unexpected kind in file {}: {}'.format(+                    input_file_name, input_config.get('kind')))++            for client in input_config['spec']['clients']:+                if client['language'] in client_languages:+                    continue+                clients.append(client)+                client_languages.add(client['language'])++            for server in input_config['spec']['servers']:+                if server['language'] in server_languages:+                    continue+                servers.append(server)+                server_languages.add(server['language'])++            input_spec = input_config['spec']+            del input_spec['clients']+            del input_spec['servers']+            del input_spec['scenariosJSON']+            spec.update(input_config['spec'])++    clients.sort(key=lambda x: x['language'])+    servers.sort(key=lambda x: x['language'])++    spec.update({+        'clients': clients,+        'servers': servers,+    })++    template['spec'] = spec++    loadtest_set_keys(template, keys)++    return template+++def main() -> None:+    argp = argparse.ArgumentParser(+        description='Creates a load test config generator template.')+    argp.add_argument('-i',+                      '--inputs',+                      action='extend',+                      nargs='*',+                      type=str,+                      required=True,+                      help='Input files.')+    argp.add_argument('-o',+                      '--output',+                      type=str,+                      help='Output file. Outputs to stdout if not set.')+    argp.add_argument('-k',+                      '--keys',+                      action='extend',+                      nargs='*',+                      default=[],+                      type=str,+                      help='Value of keys to insert, in the form key=value.')+    argp.add_argument('-n',+                      '--name',+                      default='',+                      type=str,+                      help='Name to insert.')+    argp.add_argument('-a',+                      '--annotations',+                      action='extend',+                      nargs='+',+                      default=[],+                      type=str,+                      help='Annotations to insert, in the form key=value.')",help string should mention that these will become metadata.annotations in the final template.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25975,614872479,2021-04-16T14:15:40Z,tools/distrib/python/xds_protos/setup.py,"@@ -0,0 +1,145 @@+#! /usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""A PyPI package for xDS protos generated Python code.""""""++import sys+import os+import setuptools+import pkg_resources++from grpc_tools import protoc++# We might not want to compile all the protos+EXCLUDE_PROTO_PACKAGES_LIST = [+    # Requires extra dependency to Prometheus protos+    'envoy/service/metrics/v2',+    'envoy/service/metrics/v3',+    'envoy/service/metrics/v4alpha',+]++# Compute the pathes+WORK_DIR = os.path.dirname(os.path.abspath(__file__))+GRPC_ROOT = os.path.abspath(os.path.join(WORK_DIR, '..', '..', '..', '..'))+XDS_PROTO_ROOT = os.path.join(GRPC_ROOT, 'third_party', 'envoy-api')+UDPA_PROTO_ROOT = os.path.join(GRPC_ROOT, 'third_party', 'udpa')+GOOGLEAPIS_ROOT = os.path.join(GRPC_ROOT, 'third_party', 'googleapis')+VALIDATE_ROOT = os.path.join(GRPC_ROOT, 'third_party', 'protoc-gen-validate')+OPENCENSUS_PROTO_ROOT = os.path.join(GRPC_ROOT, 'third_party',+                                     'opencensus-proto', 'src')+WELL_KNOWN_PROTOS_INCLUDE = pkg_resources.resource_filename(+    'grpc_tools', '_proto')+OUTPUT_PATH = WORK_DIR++# Prepare the test file generation+TEST_FILE_NAME = 'generated_file_import_test.py'+TEST_IMPORTS = []+++def add_test_import(proto_package_path: str,+                    file_name: str,+                    service: bool = False):+    TEST_IMPORTS.append(""from %s import %s\n"" % (proto_package_path.replace(+        '/', '.'), file_name.replace('.proto', '_pb2')))+    if service:+        TEST_IMPORTS.append(""from %s import %s\n"" % (proto_package_path.replace(+            '/', '.'), file_name.replace('.proto', '_pb2_grpc')))+++# Prepare Protoc command+COMPILE_PROTO_ONLY = [+    'grpc_tools.protoc',+    '--proto_path={}'.format(XDS_PROTO_ROOT),+    '--proto_path={}'.format(UDPA_PROTO_ROOT),+    '--proto_path={}'.format(GOOGLEAPIS_ROOT),+    '--proto_path={}'.format(VALIDATE_ROOT),+    '--proto_path={}'.format(WELL_KNOWN_PROTOS_INCLUDE),+    '--proto_path={}'.format(OPENCENSUS_PROTO_ROOT),+    '--python_out={}'.format(OUTPUT_PATH),+]+COMPILE_BOTH = COMPILE_PROTO_ONLY + ['--grpc_python_out={}'.format(OUTPUT_PATH)]+++# Compile xDS protos+def has_grpc_service(proto_package_path: str) -> bool:+    return proto_package_path.startswith('envoy/service')+++def compile_protos(proto_root: str, sub_dir: str = '.') -> None:+    for root, _, files in os.walk(os.path.join(proto_root, sub_dir)):+        proto_package_path = os.path.relpath(root, proto_root)+        if proto_package_path in EXCLUDE_PROTO_PACKAGES_LIST:+            print(f'Skipping package {proto_package_path}')+            continue+        for file_name in files:+            if file_name.endswith('.proto'):+                # Compile proto+                if has_grpc_service(proto_package_path):+                    return_code = protoc.main(COMPILE_BOTH ++                                              [os.path.join(root, file_name)])+                    add_test_import(proto_package_path, file_name, service=True)+                else:+                    return_code = protoc.main(COMPILE_PROTO_ONLY ++                                              [os.path.join(root, file_name)])+                    add_test_import(proto_package_path,+                                    file_name,+                                    service=False)+                if return_code != 0:+                    raise Exception('error: {} failed'.format(COMPILE_BOTH))+++compile_protos(XDS_PROTO_ROOT)+compile_protos(UDPA_PROTO_ROOT)+# We don't want to compile the entire GCP surface API, just the essential ones+compile_protos(GOOGLEAPIS_ROOT, os.path.join('google', 'api'))+compile_protos(GOOGLEAPIS_ROOT, os.path.join('google', 'rpc'))+compile_protos(GOOGLEAPIS_ROOT, os.path.join('google', 'longrunning'))+compile_protos(GOOGLEAPIS_ROOT, os.path.join('google', 'logging'))+compile_protos(GOOGLEAPIS_ROOT, os.path.join('google', 'type'))+compile_protos(VALIDATE_ROOT, 'validate')+compile_protos(OPENCENSUS_PROTO_ROOT)+++# Generate __init__.py files for+def create_init_file(path: str) -> None:+    f = open(os.path.join(path, ""__init__.py""), 'w')+    f.close()+++create_init_file(WORK_DIR)+for root, _, _ in os.walk(os.path.join(WORK_DIR, 'envoy')):+    create_init_file(root)++# Generate test file+with open(os.path.join(WORK_DIR, TEST_FILE_NAME), 'w') as f:+    f.writelines(TEST_IMPORTS)++# Use setuptools to build Python package+CLASSIFIERS = [+    'Development Status :: 3 - Alpha',+    'Programming Language :: Python',+    'Programming Language :: Python :: 2',+    'Programming Language :: Python :: 3',+    'License :: OSI Approved :: Apache Software License',+]+setuptools.setup(+    name='xds-protos',+    version='0.0.1',+    packages=setuptools.find_packages(where=""."", exclude=[TEST_FILE_NAME]),+    description='Generated Python code from envoyproxy/data-plane-api',+    author='The gRPC Authors',+    author_email='grpc-io@googlegroups.com',+    url='https://grpc.io',+    license='Apache License 2.0',","No dependency on `protobuf`,`grpcio-tools`, or `grpcio`?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25951,615024273,2021-04-16T17:47:48Z,src/python/grpcio/grpc/__init__.py,"@@ -298,6 +298,36 @@ class Status(six.with_metaclass(abc.ABCMeta)): class RpcError(Exception):     """"""Raised by the gRPC library to indicate non-OK-status RPC termination."""""" +    def trailing_metadata(self):","If you're adding these three methods, why not also `initial_metadata`?",X
3767331,KapJI,https://api.github.com/repos/grpc/grpc/pulls/25951,615067445,2021-04-16T19:07:12Z,src/python/grpcio/grpc/__init__.py,"@@ -298,6 +298,36 @@ class Status(six.with_metaclass(abc.ABCMeta)): class RpcError(Exception):","I think that's actually even better, there is no need to duplicate this.",X
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/25976,615502681,2021-04-19T02:08:42Z,tools/run_tests/performance/loadtest_template.py,"@@ -0,0 +1,199 @@+#!/usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This script generates a load test configuration template from a collection of+# load test configurations.+#+# The following example generates a basic template from the example configs+# in https://github.com/grpc/test-infra/tree/master/config/samples:+#+# $ ./tools/run_tests/performance/loadtest_template.py \+#     -i ../test-infra/config/samples/*.yaml \+#     -o ./tools/run_tests/performance/templates/basic_template.yaml \+#     --name basic_template++import argparse+import sys++from typing import Any, Dict, Iterable, Mapping, Type++import yaml++import loadtest_config++TEMPLATE_FILE_HEADER_COMMENT = """"""+# Template generated from load test configurations by loadtest_template.py.+#+# Configuration templates contain client and server configurations for multiple+# languages, and may contain template substitution keys. These templates are+# used to generate load test configurations by selecting clients and servers for+# the required languages. Load test configuration generation is performed by+# loadtest_config.py. See documentation below:+# https://github.com/grpc/grpc/blob/master/tools/run_tests/performance/README.md+""""""+++def loadtest_template(+        input_file_names: Iterable[str],+        metadata: Mapping[str, Any],  # Hello+        inject_client_pool: bool,+        inject_server_pool: bool,+        inject_big_query_table: bool,+        inject_timeout_seconds: bool,+        inject_ttl_seconds: bool) -> Dict[str, Any]:","I would expect this to be added to the input files to loadtest_template.py. If the inputs already have substitution keys, these will be passed unchanged to the output. The ""--inject_""... options are only for a subset of options that are likely to be useful for all kinds of tests – running on different pools, or running as part of a larger set of tests that may require a larger timeout. Image locations are at the same time too specific to one case (for instance, prebuilt images) and unlikely to change when you run the test multiple times or under different conditions.",X
67486458,wanlin31,https://api.github.com/repos/grpc/grpc/pulls/25976,616052245,2021-04-19T17:45:51Z,tools/run_tests/performance/loadtest_template.py,"@@ -0,0 +1,199 @@+#!/usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This script generates a load test configuration template from a collection of+# load test configurations.+#+# The following example generates a basic template from the example configs+# in https://github.com/grpc/test-infra/tree/master/config/samples:+#+# $ ./tools/run_tests/performance/loadtest_template.py \+#     -i ../test-infra/config/samples/*.yaml \+#     -o ./tools/run_tests/performance/templates/basic_template.yaml \+#     --name basic_template++import argparse+import sys++from typing import Any, Dict, Iterable, Mapping, Type++import yaml++import loadtest_config++TEMPLATE_FILE_HEADER_COMMENT = """"""+# Template generated from load test configurations by loadtest_template.py.+#+# Configuration templates contain client and server configurations for multiple+# languages, and may contain template substitution keys. These templates are+# used to generate load test configurations by selecting clients and servers for+# the required languages. Load test configuration generation is performed by+# loadtest_config.py. See documentation below:+# https://github.com/grpc/grpc/blob/master/tools/run_tests/performance/README.md+""""""+++def loadtest_template(+        input_file_names: Iterable[str],+        metadata: Mapping[str, Any],  # Hello+        inject_client_pool: bool,+        inject_server_pool: bool,+        inject_big_query_table: bool,+        inject_timeout_seconds: bool,+        inject_ttl_seconds: bool) -> Dict[str, Any]:","I agree. So the template for each pre-built images would have format like (I took a server example, the client would have a different field key)```run:    image: ${pre_built_server_image_registry}    arg:[""som argument""]```The field `image: ${pre_built_image_registry}` will stay the same when loadtest_template.py generate the big have-everything template file. We can then pass the key when calling loadtest_config.py using```./tools/run_tests/performance/loadtest_config.py -s ""pre_built_server_image_registry""=""some_image_repository ""```Thank you for explaining this to me. :) ",X
26072277,dfawley,https://api.github.com/repos/grpc/grpc/pulls/26007,616180110,2021-04-19T21:10:54Z,tools/run_tests/run_xds_tests.py,"@@ -1782,6 +1815,90 @@ def _delay(pct):         set_validate_for_proxyless(gcp, True)  +def test_csds(gcp, original_backend_service, instance_group, server_uri):+    logger.info('Running csds')++    logger.info('waiting for original backends to become healthy')+    wait_for_healthy_backends(gcp, original_backend_service, instance_group)++    logger.info('start sending traffic')+    configure_client(+        rpc_types=[messages_pb2.ClientConfigureRequest.RpcType.UNARY_CALL])++    # Test case timeout: 5 minutes+    deadline = time.time() + 300+    cnt = 0+    while time.time() <= deadline:+        client_config = get_client_xds_config_dump()+        logger.info('received xDS config %s', json.dumps(client_config,+                                                         indent=2))+        if client_config is not None:+            # Got the xDS config dump, now validate it+            ok = True+            try:+                if client_config['node']['locality']['zone'] != args.zone:",Are there consts you can use (e.g. from the generated proto code) instead of these strings?  Or: why convert to JSON instead of returning the data as a proto message?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26007,616211800,2021-04-19T22:13:06Z,tools/run_tests/run_xds_tests.py,"@@ -1782,6 +1815,90 @@ def _delay(pct):         set_validate_for_proxyless(gcp, True)  +def test_csds(gcp, original_backend_service, instance_group, server_uri):+    logger.info('Running csds')++    logger.info('waiting for original backends to become healthy')+    wait_for_healthy_backends(gcp, original_backend_service, instance_group)++    logger.info('start sending traffic')+    configure_client(+        rpc_types=[messages_pb2.ClientConfigureRequest.RpcType.UNARY_CALL])++    # Test case timeout: 5 minutes+    deadline = time.time() + 300+    cnt = 0+    while time.time() <= deadline:+        client_config = get_client_xds_config_dump()+        logger.info('received xDS config %s', json.dumps(client_config,+                                                         indent=2))+        if client_config is not None:+            # Got the xDS config dump, now validate it+            ok = True+            try:+                if client_config['node']['locality']['zone'] != args.zone:+                    logger.info('Invalid zone %s != %s',+                                client_config['node']['locality']['zone'],+                                args.zone)+                    ok = False+                seen = set()+                for xds_config in client_config['xds_config']:+                    if 'listener_config' in xds_config:+                        listener_name = xds_config['listener_config'][+                            'dynamic_listeners'][0]['active_state']['listener'][+                                'name']+                        if listener_name != server_uri:+                            logger.info('Invalid Listener name %s != %s',+                                        listener_name, server_uri)+                            ok = False+                        else:+                            seen.add('lds')+                    elif 'route_config' in xds_config:+                        num_vh = len(+                            xds_config['route_config']['dynamic_route_configs']+                            [0]['route_config']['virtual_hosts'])+                        if num_vh <= 0:+                            logger.info('Invalid number of VirtualHosts %s',+                                        num_vh)+                            ok = False+                        else:+                            seen.add('rds')+                    elif 'cluster_config' in xds_config:+                        cluster_type = xds_config['cluster_config'][+                            'dynamic_active_clusters'][0]['cluster']['type']+                        if cluster_type != 'EDS':+                            logger.info('Invalid cluster type %s != EDS',+                                        cluster_type)+                            ok = False+                        else:+                            seen.add('cds')+                    elif 'endpoint_config' in xds_config:+                        sub_zone = xds_config[""endpoint_config""][+                            ""dynamic_endpoint_configs""][0][""endpoint_config""][+                                ""endpoints""][0][""locality""][""sub_zone""]+                        if args.zone not in sub_zone:+                            logger.info('Invalid endpoint sub_zone %s',+                                        sub_zone)+                            ok = False+                        else:+                            seen.add('eds')+                if len(seen) != 4:+                    logger.info('Incomplete xDS config dump, seen=%s', seen)+                    ok = False+            except:+                logger.exception('Error in xDS config dump:')+                ok = False+            finally:+                if ok:+                    break+        logger.info('csds attempt %d failed', cnt)+        # Give the client some time to fetch xDS resources+        time.sleep(2)+        cnt += 1++    logger.info('success')","Oops, good catch. Raising an exception if timed out, tested manually that it will fail if times up.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/25999,616823319,2021-04-20T15:55:54Z,tools/distrib/python/xds_protos/build.py,"@@ -0,0 +1,135 @@+#! /usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Builds the content of xds-protos package""""""++import os+import pkg_resources+from grpc_tools import protoc++# We might not want to compile all the protos+EXCLUDE_PROTO_PACKAGES_LIST = [+    # Requires extra dependency to Prometheus protos+    'envoy/service/metrics/v2',+    'envoy/service/metrics/v3',+    'envoy/service/metrics/v4alpha',+]++# Compute the pathes+WORK_DIR = os.path.dirname(os.path.abspath(__file__))+GRPC_ROOT = os.path.abspath(os.path.join(WORK_DIR, '..', '..', '..', '..'))+XDS_PROTO_ROOT = os.path.join(GRPC_ROOT, 'third_party', 'envoy-api')+UDPA_PROTO_ROOT = os.path.join(GRPC_ROOT, 'third_party', 'udpa')+GOOGLEAPIS_ROOT = os.path.join(GRPC_ROOT, 'third_party', 'googleapis')+VALIDATE_ROOT = os.path.join(GRPC_ROOT, 'third_party', 'protoc-gen-validate')+OPENCENSUS_PROTO_ROOT = os.path.join(GRPC_ROOT, 'third_party',+                                     'opencensus-proto', 'src')+WELL_KNOWN_PROTOS_INCLUDE = pkg_resources.resource_filename(+    'grpc_tools', '_proto')+OUTPUT_PATH = WORK_DIR++# Prepare the test file generation+TEST_FILE_NAME = 'generated_file_import_test.py'+TEST_IMPORTS = []+++def add_test_import(proto_package_path: str,+                    file_name: str,+                    service: bool = False):+    TEST_IMPORTS.append(""from %s import %s\n"" % (proto_package_path.replace(+        '/', '.'), file_name.replace('.proto', '_pb2')))+    if service:+        TEST_IMPORTS.append(""from %s import %s\n"" % (proto_package_path.replace(+            '/', '.'), file_name.replace('.proto', '_pb2_grpc')))+++# Prepare Protoc command+COMPILE_PROTO_ONLY = [+    'grpc_tools.protoc',+    '--proto_path={}'.format(XDS_PROTO_ROOT),+    '--proto_path={}'.format(UDPA_PROTO_ROOT),+    '--proto_path={}'.format(GOOGLEAPIS_ROOT),+    '--proto_path={}'.format(VALIDATE_ROOT),+    '--proto_path={}'.format(WELL_KNOWN_PROTOS_INCLUDE),+    '--proto_path={}'.format(OPENCENSUS_PROTO_ROOT),+    '--python_out={}'.format(OUTPUT_PATH),+]+COMPILE_BOTH = COMPILE_PROTO_ONLY + ['--grpc_python_out={}'.format(OUTPUT_PATH)]+++def has_grpc_service(proto_package_path: str) -> bool:+    return proto_package_path.startswith('envoy/service')","Is this assumption guaranteed going forward? What's the behavior when you use `--grpc_python_out` on a file with no services? If it emits an empty file or doesn't emit any file at all, you might just want to unconditionally supply this flag.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/25999,616832145,2021-04-20T16:06:48Z,tools/distrib/python/xds_protos/build.py,"@@ -0,0 +1,135 @@+#! /usr/bin/env python3+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Builds the content of xds-protos package""""""++import os+import pkg_resources+from grpc_tools import protoc++# We might not want to compile all the protos+EXCLUDE_PROTO_PACKAGES_LIST = [+    # Requires extra dependency to Prometheus protos+    'envoy/service/metrics/v2',+    'envoy/service/metrics/v3',+    'envoy/service/metrics/v4alpha',+]++# Compute the pathes+WORK_DIR = os.path.dirname(os.path.abspath(__file__))+GRPC_ROOT = os.path.abspath(os.path.join(WORK_DIR, '..', '..', '..', '..'))+XDS_PROTO_ROOT = os.path.join(GRPC_ROOT, 'third_party', 'envoy-api')+UDPA_PROTO_ROOT = os.path.join(GRPC_ROOT, 'third_party', 'udpa')+GOOGLEAPIS_ROOT = os.path.join(GRPC_ROOT, 'third_party', 'googleapis')+VALIDATE_ROOT = os.path.join(GRPC_ROOT, 'third_party', 'protoc-gen-validate')+OPENCENSUS_PROTO_ROOT = os.path.join(GRPC_ROOT, 'third_party',+                                     'opencensus-proto', 'src')+WELL_KNOWN_PROTOS_INCLUDE = pkg_resources.resource_filename(+    'grpc_tools', '_proto')+OUTPUT_PATH = WORK_DIR++# Prepare the test file generation+TEST_FILE_NAME = 'generated_file_import_test.py'+TEST_IMPORTS = []+++def add_test_import(proto_package_path: str,+                    file_name: str,+                    service: bool = False):+    TEST_IMPORTS.append(""from %s import %s\n"" % (proto_package_path.replace(+        '/', '.'), file_name.replace('.proto', '_pb2')))+    if service:+        TEST_IMPORTS.append(""from %s import %s\n"" % (proto_package_path.replace(+            '/', '.'), file_name.replace('.proto', '_pb2_grpc')))+++# Prepare Protoc command+COMPILE_PROTO_ONLY = [+    'grpc_tools.protoc',+    '--proto_path={}'.format(XDS_PROTO_ROOT),+    '--proto_path={}'.format(UDPA_PROTO_ROOT),+    '--proto_path={}'.format(GOOGLEAPIS_ROOT),+    '--proto_path={}'.format(VALIDATE_ROOT),+    '--proto_path={}'.format(WELL_KNOWN_PROTOS_INCLUDE),+    '--proto_path={}'.format(OPENCENSUS_PROTO_ROOT),+    '--python_out={}'.format(OUTPUT_PATH),+]+COMPILE_BOTH = COMPILE_PROTO_ONLY + ['--grpc_python_out={}'.format(OUTPUT_PATH)]+++def has_grpc_service(proto_package_path: str) -> bool:+    return proto_package_path.startswith('envoy/service')","Our ProtoBuf plugin will create a file if the `--grpc_python_out` flag is given. The file will have a line to import the generated `_pb.py` of that proto. When trying to import generated proto modules, the extra `_pb_grpc` will appear in the code hinter (I'm using VSCode). I added the logic to only generate service when we are certain there is a gRPC service.So far, I think Envoy protos are doing a good job to keep services under `envoy/service`. If there are any exceptions, I can update the code. Or, do you think we should use regex to read the proto file?",X
62662355,mkruskal-google,https://api.github.com/repos/grpc/grpc/pulls/25996,616973848,2021-04-20T19:26:04Z,include/grpcpp/generic/generic_stub.h,"@@ -123,30 +128,34 @@ class TemplatedGenericStub final {     /// Setup and start a unary call to a named method \a method using     /// \a context and specifying the \a request and \a response buffers.     void UnaryCall(ClientContext* context, const std::string& method,-                   const RequestType* request, ResponseType* response,+                   const char* suffix_for_stats, const RequestType* request,","I totally agree that in isolation, this is a questionable design choice (and I second guessed it at least once during implementation).  IIUC GenericStub is analogous to AnonymousStub, which isn't fixed until call-time.  If the method is allowed to freely change (i.e. as a parameter here), the suffix should also.  Internally, this feature is needed by Extensible Stubs.  I *could* move this to an overloaded signature as a migration step though, if you'd feel more comfortable with that",X
67486458,wanlin31,https://api.github.com/repos/grpc/grpc/pulls/25976,617030448,2021-04-20T21:03:59Z,tools/run_tests/performance/README.md,"@@ -209,29 +212,166 @@ https://github.com/grpc/test-infra/tree/master/config/samples  The script [loadtest_config.py](./loadtest_config.py) generates LoadTest configurations for tests running a set of scenarios. The configurations are-written in multipart YAML format, either to a file or to stdout.+written in multipart YAML format, either to a file or to stdout. Each+configuration contains a single embedded scenario. -The LoadTest configurations are generated from a template. The example-configurations above can be used as templates.+The LoadTest configurations are generated from a template. Any configuration can+be used as a template, as long as it supports the languages that need to be","I am not sure I understand this sentence correctly. I think you mean: any load test configuration yaml file can be used as a template, as long as it follows rules set in loadtest_config.py, such as having substitution keys as ${some_filed}. Is that right?",
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/25640,617066872,2021-04-20T22:12:31Z,src/compiler/php_generator_helpers.h,"@@ -28,8 +28,11 @@ namespace grpc_php_generator {  inline std::string GetPHPServiceClassname(     const grpc::protobuf::ServiceDescriptor* service,-    const std::string& class_suffix) {-  return service->name() + (class_suffix == """" ? ""Client"" : class_suffix);+    const std::string& class_suffix, bool is_server) {+  return service->name() ++         (class_suffix == """" ? (is_server ? ""Service"" : ""Client"")","The dilemma here is that, if the service is already named `TestService` for example, then we will be generating the class `TestServiceServiceStub` in `TestServiceServiceStub.php`.Python would have called it `TestServiceServicer`. C# calls it `TestServiceBase`. Ruby and C++ just call it `TestService`.I am leaning towards `TestServiceStub` for server and `TestServiceClient` for client.Now the problem is that in the Math / Greeter example, the service name in the proto is just `Math` and `Greeter`. So the generated classes would be `MathStub` and `MathClient`, `GreeterStub` and `GreeterClient`. I guess that's OK.So the end result would look like```diff --git a/src/compiler/php_generator_helpers.h b/src/compiler/php_generator_helpers.h                      index 46d592af65..89c153a6e6 100644                                                                           --- a/src/compiler/php_generator_helpers.h                                                                    +++ b/src/compiler/php_generator_helpers.h                                                                    @@ -30,7 +30,7 @@ inline std::string GetPHPServiceClassname(                                                       const grpc::protobuf::ServiceDescriptor* service,                                                             const std::string& class_suffix, bool is_server) {                                                          return service->name() +                                                                                   -         (class_suffix == """" ? (is_server ? ""Service"" : ""Client"")                                            +         (class_suffix == """" ? (is_server ? """" : ""Client"")                                                                                 : class_suffix) +                                                                         (is_server ? ""Stub"" : """");                                                                           }           ```Please re-generate the helloworld greeter and route_guide generated files.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26041,617744137,2021-04-21T17:26:34Z,tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh,"@@ -23,6 +23,7 @@ readonly TEST_DRIVER_REPO_URL=""https://github.com/grpc/grpc.git"" readonly TEST_DRIVER_BRANCH=""${TEST_DRIVER_BRANCH:-master}"" readonly TEST_DRIVER_PATH=""tools/run_tests/xds_k8s_test_driver"" readonly TEST_DRIVER_PROTOS_PATH=""src/proto/grpc/testing""+readonly TEST_DRIVER_REPO_DIR_USE_EXISTING=1","1. Does it work without setting `TEST_DRIVER_REPO_DIR`?2. It's better to define `TEST_DRIVER_REPO_DIR_USE_EXISTING` in repo-specific script next to, https://github.com/grpc/grpc/blob/feff79abc76a7aa821ffa305dfd5ef06e2156d8a/tools/internal_ci/linux/grpc_xds_k8s.sh#L28",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26002,618332316,2021-04-22T11:54:41Z,tools/gcp/github_stats_tracking/fetch_data.py,"@@ -24,7 +24,7 @@ def get_stats_from_github():     # Please set the access token properly before deploying.     assert ACCESS_TOKEN","I think this script is no longer used. Can you verify that and if so, delete it?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26002,618332731,2021-04-22T11:55:18Z,tools/github/pr_latency.py,"@@ -34,7 +34,7 @@ import json import logging import pprint-import urllib2+import urllib.request, urllib.error, urllib.parse","I think this script is no longer used. Can you verify that and if so, delete it?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26002,618333603,2021-04-22T11:56:36Z,tools/line_count/summarize-history.py,"@@ -30,7 +30,7 @@ def daterange(start, end):  for dt in daterange(start_date, end_date):","this might still be used by https://fusion.corp.google.com/projectanalysis/summary/KOKORO/prod:grpc%2Fcore%2Fmaster%2Flinux%2Fgrpc_line_count. if not, please delete it (along with other line_count stuff).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26002,618336372,2021-04-22T12:00:51Z,tools/run_tests/python_utils/upload_rbe_results.py,"@@ -18,7 +18,7 @@ import os import json import sys-import urllib2+import urllib.request, urllib.error, urllib.parse","let's double check the upload_rbe_results.py script actually still works -  if not, we'd silently looks the bigquery data for go/grpc-flaky and we really don't want that.",
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/26055,618528197,2021-04-22T15:54:44Z,tools/internal_ci/linux/grpc_e2e_performance_v2.sh,"@@ -37,17 +37,18 @@ tools/run_tests/performance/loadtest_config.py -l go \     -s big_query_table=e2e_benchmarks.experimental_results \     -s timeout_seconds=900 --prefix=""kokoro-test"" -u ""$(date +%Y%m%d%H%M%S)"" \     -r go_generic_sync_streaming_ping_pong_secure -o ./loadtest.yaml+    +# Dump the contents of the loadtest.yaml (since loadtest_config.py doesn't+# list the scenarios that will be run).+cat ./loadtest.yaml",This is going to be a very long file when more scenarios are included. I suggest instead something like this:grep '    scenarios: ' ./loadtest.yaml,
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/26055,618723662,2021-04-22T20:39:43Z,tools/internal_ci/linux/grpc_e2e_performance_v2.sh,"@@ -37,17 +37,18 @@ tools/run_tests/performance/loadtest_config.py -l go \     -s big_query_table=e2e_benchmarks.experimental_results \     -s timeout_seconds=900 --prefix=""kokoro-test"" -u ""$(date +%Y%m%d%H%M%S)"" \     -r go_generic_sync_streaming_ping_pong_secure -o ./loadtest.yaml+    +# Dump the contents of the loadtest.yaml (since loadtest_config.py doesn't+# list the scenarios that will be run).+cat ./loadtest.yaml","Actually with a '^' at the start of the pattern too, grep '^    scenarios: ' ./loadtest.yaml.",
394885,soheilhy,https://api.github.com/repos/grpc/grpc/pulls/25981,619251429,2021-04-23T14:10:12Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -1914,6 +1931,37 @@ void ClientChannel::CallData::Destroy(   } } +void ClientChannel::CallData::PreCancel(grpc_call_element* elem,+                                        grpc_error_handle error) {+  auto* calld = static_cast<CallData*>(elem->call_data);+  auto* chand = static_cast<ClientChannel*>(elem->channel_data);+  // Check if we're queued pending a resolver result.+  {+    MutexLock lock(&chand->resolution_mu_);+    if (calld->queued_pending_resolver_result_) {+      if (GRPC_TRACE_FLAG_ENABLED(grpc_client_channel_routing_trace)) {+        gpr_log(GPR_INFO,+                ""chand=%p calld=%p: cancelling resolver queued pick: %s"", chand,+                calld, grpc_error_string(error));+      }+      // Remove pick from list of queued picks.+      calld->MaybeRemoveCallFromResolverQueuedCallsLocked(elem);+      // Fail pending batches on the call.+      calld->PendingBatchesFail(elem, error,+                                YieldCallCombinerIfPendingBatchesFound);+      return;+    }+  }+  // Not pending resolver result, so check if we have a dynamic call.+  MutexLock lock(&calld->dynamic_call_creation_mu_);+  if (calld->dynamic_call_ != nullptr) {+    calld->dynamic_call_->PreCancel(error);","I'm not sure how long `PreCancel` would take in general, but does this call have to happen under lock? (e.g., could we just get the pointer and store a flag that the call is being run?)",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25981,619381305,2021-04-23T17:19:18Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -2542,7 +2554,6 @@ ClientChannel::LoadBalancedCall::LoadBalancedCall(       call_start_time_(args.start_time),",I don't understand this comment.  The `args` that's being moved on line 2425 is a local variable in the `ClientChannel::CallData::CreateDynamicCall()` method.  The `args` that's being used here is a parameter being passed into the `ClientChannel::LoadBalancedCall::LoadBalancedCall()` ctor.  I think these are completely different objects.  Am I missing something here?,X
14166415,sanjaypujare,https://api.github.com/repos/grpc/grpc/pulls/26065,619912771,2021-04-26T01:00:05Z,tools/run_tests/run_xds_tests.py,"@@ -950,6 +969,289 @@ def prepare_services_for_urlmap_tests(gcp, original_backend_service,     return original_backend_instances, alternate_backend_instances  +def test_metadata_filter(gcp, original_backend_service, instance_group,+                         alternate_backend_service, same_zone_instance_group):+    logger.info(""Running test_metadata_filter"")+    wait_for_healthy_backends(gcp, original_backend_service, instance_group)+    original_backend_instances = get_instance_names(gcp, instance_group)+    alternate_backend_instances = get_instance_names(gcp,+                                                     same_zone_instance_group)+    patch_backend_service(gcp, alternate_backend_service,+                          [same_zone_instance_group])+    wait_for_healthy_backends(gcp, alternate_backend_service,+                              same_zone_instance_group)+    try:+        with open(bootstrap_path) as f:+            md = json.load(f)['node']['metadata']+            match_labels = []+            for k,v in md.items():+                match_labels.append({'name': k,+                                     'value': v})++        not_match_labels = [{'name': 'fake',+                             'value': 'fail'}]+        test_route_rules = [+        # test MATCH_ALL+        [+          {+            'priority': 0,+            'matchRules': [+                {+                    'prefixMatch': '/',+                    'metadataFilters': [+                        {+                            'filterMatchCriteria': 'MATCH_ALL',+                            'filterLabels': not_match_labels+                        }+                    ]+                }+            ],+            'service': original_backend_service.url+          },+          {+            'priority': 1,+            'matchRules': [+                {+                    'prefixMatch': '/',+                    'metadataFilters': [+                        {+                            'filterMatchCriteria': 'MATCH_ALL',+                            'filterLabels': match_labels+                        }+                    ]+                }+            ],+            'service': alternate_backend_service.url+          },+        ],+        # test mixing MATCH_ALL and MATCH_ANY+        # test MATCH_ALL: super set labels won't match+        [+          {+            'priority': 0,+            'matchRules': [+                {+                    'prefixMatch': '/',+                    'metadataFilters': [+                        {+                            'filterMatchCriteria': 'MATCH_ALL',+                            'filterLabels': not_match_labels + match_labels+                        }+                    ]+                }+            ],+            'service': original_backend_service.url+          },+          {+            'priority': 1,+            'matchRules': [+                {+                    'prefixMatch': '/',+                    'metadataFilters': [+                        {+                            'filterMatchCriteria': 'MATCH_ANY',+                            'filterLabels': not_match_labels + match_labels+                        }+                    ]+                }+            ],+            'service': alternate_backend_service.url+          },+        ],+        # test MATCH_ANY+        [+          {+            'priority': 0,+            'matchRules': [+                {+                    'prefixMatch': '/',+                    'metadataFilters': [+                        {+                            'filterMatchCriteria': 'MATCH_ANY',+                            'filterLabels': not_match_labels+                        }+                    ]+                }+            ],+            'service': original_backend_service.url+          },+          {+            'priority': 1,+            'matchRules': [+                {+                    'prefixMatch': '/',+                    'metadataFilters': [+                        {+                            'filterMatchCriteria': 'MATCH_ANY',+                            'filterLabels': not_match_labels + match_labels+                        }+                    ]+                }+            ],+            'service': alternate_backend_service.url+          },+        ]+        ]++        for route_rules in test_route_rules:+            wait_until_all_rpcs_go_to_given_backends(original_backend_instances,+                                                     _WAIT_FOR_STATS_SEC)+            patch_url_map_backend_service(gcp, original_backend_service,+                                          route_rules=route_rules)+            wait_until_no_rpcs_go_to_given_backends(original_backend_instances,+                                                    _WAIT_FOR_STATS_SEC)+            wait_until_all_rpcs_go_to_given_backends(alternate_backend_instances,+                                                     _WAIT_FOR_STATS_SEC)+            patch_url_map_backend_service(gcp, original_backend_service)+    finally:+        patch_backend_service(gcp, alternate_backend_service, [])+++def test_api_listener(gcp, backend_service, instance_group,","How about naming this func to reflect the intent ""LDS switches to the second API listener when the first one is deleted"". Something like ""test_duplicate_api_listener"" or even better ""test_duplicate_routing_config"" (since user creates the  map+tp+fr pipeline and not the api listener)",X
4111145,YifeiZhuang,https://api.github.com/repos/grpc/grpc/pulls/26065,620554832,2021-04-26T18:40:28Z,tools/run_tests/run_xds_tests.py,"@@ -950,6 +969,289 @@ def prepare_services_for_urlmap_tests(gcp, original_backend_service,     return original_backend_instances, alternate_backend_instances  +def test_metadata_filter(gcp, original_backend_service, instance_group,+                         alternate_backend_service, same_zone_instance_group):+    logger.info(""Running test_metadata_filter"")+    wait_for_healthy_backends(gcp, original_backend_service, instance_group)+    original_backend_instances = get_instance_names(gcp, instance_group)+    alternate_backend_instances = get_instance_names(gcp,+                                                     same_zone_instance_group)+    patch_backend_service(gcp, alternate_backend_service,+                          [same_zone_instance_group])+    wait_for_healthy_backends(gcp, alternate_backend_service,+                              same_zone_instance_group)+    try:+        with open(bootstrap_path) as f:+            md = json.load(f)['node']['metadata']+            match_labels = []+            for k,v in md.items():+                match_labels.append({'name': k,+                                     'value': v})++        not_match_labels = [{'name': 'fake',+                             'value': 'fail'}]+        test_route_rules = [+        # test MATCH_ALL+        [+          {+            'priority': 0,+            'matchRules': [+                {+                    'prefixMatch': '/',+                    'metadataFilters': [+                        {+                            'filterMatchCriteria': 'MATCH_ALL',+                            'filterLabels': not_match_labels+                        }+                    ]+                }+            ],+            'service': original_backend_service.url+          },+          {+            'priority': 1,+            'matchRules': [+                {+                    'prefixMatch': '/',+                    'metadataFilters': [+                        {+                            'filterMatchCriteria': 'MATCH_ALL',+                            'filterLabels': match_labels+                        }+                    ]+                }+            ],+            'service': alternate_backend_service.url+          },+        ],+        # test mixing MATCH_ALL and MATCH_ANY+        # test MATCH_ALL: super set labels won't match+        [+          {+            'priority': 0,+            'matchRules': [+                {+                    'prefixMatch': '/',+                    'metadataFilters': [+                        {+                            'filterMatchCriteria': 'MATCH_ALL',+                            'filterLabels': not_match_labels + match_labels+                        }+                    ]+                }+            ],+            'service': original_backend_service.url+          },+          {+            'priority': 1,+            'matchRules': [+                {+                    'prefixMatch': '/',+                    'metadataFilters': [+                        {+                            'filterMatchCriteria': 'MATCH_ANY',+                            'filterLabels': not_match_labels + match_labels+                        }+                    ]+                }+            ],+            'service': alternate_backend_service.url+          },+        ],+        # test MATCH_ANY+        [+          {+            'priority': 0,+            'matchRules': [+                {+                    'prefixMatch': '/',+                    'metadataFilters': [+                        {+                            'filterMatchCriteria': 'MATCH_ANY',+                            'filterLabels': not_match_labels+                        }+                    ]+                }+            ],+            'service': original_backend_service.url+          },+          {+            'priority': 1,+            'matchRules': [+                {+                    'prefixMatch': '/',+                    'metadataFilters': [+                        {+                            'filterMatchCriteria': 'MATCH_ANY',+                            'filterLabels': not_match_labels + match_labels+                        }+                    ]+                }+            ],+            'service': alternate_backend_service.url+          },+        ]+        ]++        for route_rules in test_route_rules:+            wait_until_all_rpcs_go_to_given_backends(original_backend_instances,+                                                     _WAIT_FOR_STATS_SEC)+            patch_url_map_backend_service(gcp, original_backend_service,+                                          route_rules=route_rules)+            wait_until_no_rpcs_go_to_given_backends(original_backend_instances,+                                                    _WAIT_FOR_STATS_SEC)+            wait_until_all_rpcs_go_to_given_backends(alternate_backend_instances,+                                                     _WAIT_FOR_STATS_SEC)+            patch_url_map_backend_service(gcp, original_backend_service)+    finally:+        patch_backend_service(gcp, alternate_backend_service, [])+++def test_api_listener(gcp, backend_service, instance_group,","This test case is sort of mixed of corner cases of api listener. Not only duplicate api listener, but delete api listener case (L1151).",
4111145,YifeiZhuang,https://api.github.com/repos/grpc/grpc/pulls/26065,620716676,2021-04-26T23:20:46Z,tools/run_tests/run_xds_tests.py,"@@ -950,6 +969,289 @@ def prepare_services_for_urlmap_tests(gcp, original_backend_service,     return original_backend_instances, alternate_backend_instances  +def test_metadata_filter(gcp, original_backend_service, instance_group,+                         alternate_backend_service, same_zone_instance_group):+    logger.info(""Running test_metadata_filter"")+    wait_for_healthy_backends(gcp, original_backend_service, instance_group)+    original_backend_instances = get_instance_names(gcp, instance_group)+    alternate_backend_instances = get_instance_names(gcp,+                                                     same_zone_instance_group)+    patch_backend_service(gcp, alternate_backend_service,+                          [same_zone_instance_group])+    wait_for_healthy_backends(gcp, alternate_backend_service,+                              same_zone_instance_group)+    try:+        with open(bootstrap_path) as f:+            md = json.load(f)['node']['metadata']+            match_labels = []+            for k,v in md.items():+                match_labels.append({'name': k,+                                     'value': v})++        not_match_labels = [{'name': 'fake',+                             'value': 'fail'}]+        test_route_rules = [","Indeed, I've verified that multiple route rules each matches the metadata filter are allowed, and the first match route rule is selected for all the traffic. ",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26064,621374362,2021-04-27T16:15:51Z,src/core/lib/iomgr/timer.cc,"@@ -29,6 +30,7 @@ void grpc_set_timer_impl(grpc_timer_vtable* vtable) {  void grpc_timer_init(grpc_timer* timer, grpc_millis deadline,                      grpc_closure* closure) {+  grpc_core::ExecCtx::Get()->InvalidateNow();","This doesn't seem like it will really help, because by the time we get here, the calling code has already used `ExecCtx::Now()` to compute the value of `deadline` that it passed into us.This may help avoid the *next* iteration of this loop, but it doesn't really solve the problem of the deadline being computed wrong for the initial iteration.",X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/26091,621442090,2021-04-27T17:33:12Z,src/cpp/client/channel_cc.cc,"@@ -243,25 +244,31 @@ class ShutdownCallback : public grpc_experimental_completion_queue_functor { ::grpc::CompletionQueue* Channel::CallbackCQ() {   // TODO(vjpai): Consider using a single global CQ for the default CQ   // if there is no explicit per-channel CQ registered+  CompletionQueue* callback_cq = callback_cq_.load(std::memory_order_acquire);+  if (callback_cq != nullptr) {+    return callback_cq;+  }   grpc::internal::MutexLock l(&mu_);-  if (callback_cq_ == nullptr) {+  callback_cq = callback_cq_.load(std::memory_order_relaxed);+  if (callback_cq == nullptr) {     if (grpc_iomgr_run_in_background()) {       // gRPC-core provides the backing needed for the preferred CQ type        auto* shutdown_callback = new ShutdownCallback;-      callback_cq_ =+      callback_cq =           new ::grpc::CompletionQueue(grpc_completion_queue_attributes{               GRPC_CQ_CURRENT_VERSION, GRPC_CQ_CALLBACK,               GRPC_CQ_DEFAULT_POLLING, shutdown_callback});        // Transfer ownership of the new cq to its own shutdown callback-      shutdown_callback->TakeCQ(callback_cq_);+      shutdown_callback->TakeCQ(callback_cq);     } else {       // Otherwise we need to use the alternative CQ variant-      callback_cq_ = CompletionQueue::CallbackAlternativeCQ();+      callback_cq = CompletionQueue::CallbackAlternativeCQ();     }+    callback_cq_.store(callback_cq, std::memory_order_release);",We use this pattern so frequently maybe we should have a wrapper for it.,
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/26065,621668896,2021-04-27T22:48:30Z,tools/run_tests/run_xds_tests.py,"@@ -2866,6 +3217,26 @@ def __init__(self, compute, alpha_compute, project, project_num):                     test_timeout(gcp, backend_service, instance_group)                 elif test_case == 'fault_injection':                     test_fault_injection(gcp, backend_service, instance_group)+                elif test_case == 'api_listener':+                    if CLIENT_HOSTS:+                        logger.info('skipping api_listener test case because '+                                    'client processes on existing client hosts '+                                    'are out of test scope.')+                        continue+                    client_process = test_api_listener(gcp, backend_service, instance_group,","What happens to the previously started `client_process`? Oh, I see it is terminated inside this method, but that's fairly hard to track. Is there any way to restructure things to start the client process with whatever params these tests need in the beginning, so we avoid starting up a (useless?) client_process here just to then terminate it within these methods?",X
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/26065,621670090,2021-04-27T22:51:15Z,tools/run_tests/run_xds_tests.py,"@@ -84,6 +84,9 @@     'traffic_splitting',     'path_matching',     'header_matching',+    'api_listener',",Is there a specification for the new tests? Would be helpful to add to https://github.com/grpc/grpc/blob/master/doc/xds-test-descriptions.md,
4111145,YifeiZhuang,https://api.github.com/repos/grpc/grpc/pulls/26065,621717562,2021-04-28T00:25:26Z,tools/run_tests/run_xds_tests.py,"@@ -2866,6 +3217,26 @@ def __init__(self, compute, alpha_compute, project, project_num):                     test_timeout(gcp, backend_service, instance_group)                 elif test_case == 'fault_injection':                     test_fault_injection(gcp, backend_service, instance_group)+                elif test_case == 'api_listener':+                    if CLIENT_HOSTS:+                        logger.info('skipping api_listener test case because '+                                    'client processes on existing client hosts '+                                    'are out of test scope.')+                        continue+                    client_process = test_api_listener(gcp, backend_service, instance_group,","> I see it is terminated inside this method, but that's fairly hard to track.I actually have similar concern.In the `test_api_listener` test case, it was not because i need a different parameter than the current setting, but to avoid failures in `finally` stage to put back the configurations for the next test.  During test I had to use a new `urlmap+tp+fr` (non `0.0.0.0` and proxyValidation=false) and delete the old `urpmap+tp+fr`. At the end of the test, I have to recover to the a `0.0.0.0` and proxylessValidation=true. I was worried that during that time window, the `0.0.0.0` and original port combination has been taken by someone else to cause the tests to be flaky, which is hard to detect. That is why I decided to repalce with a new client_process that will definitely be successful selecting a new port.But I think in `test_api_listener` this is fairly less worrisome than in the `forwarding_rule_port_match` test case.In the `forwarding_rule_port_match` test, we actually make use the original params to test the port not matching case (in [1.a](https://docs.google.com/document/d/1CM3QdAnV0cCd1NiKRj9gSz8Pt38JHPMYme8SJB1fEEw/edit?resourcekey=0-s8rSM-pJenGSrSvSC8EH-A#)), then we terminate the `client_process` and create another one with params we need, then we recover to the original one.Alternatively for `forwarding_rule_port_match` test case, we can split  ([1.a](https://docs.google.com/document/d/1CM3QdAnV0cCd1NiKRj9gSz8Pt38JHPMYme8SJB1fEEw/edit?resourcekey=0-s8rSM-pJenGSrSvSC8EH-A#) and  [1.bcd](https://docs.google.com/document/d/1CM3QdAnV0cCd1NiKRj9gSz8Pt38JHPMYme8SJB1fEEw/edit?resourcekey=0-s8rSM-pJenGSrSvSC8EH-A#)) and for the latter we can do as you suggested.",X
4111145,YifeiZhuang,https://api.github.com/repos/grpc/grpc/pulls/26065,621718137,2021-04-28T00:27:10Z,tools/run_tests/run_xds_tests.py,"@@ -478,6 +482,21 @@ def wait_until_all_rpcs_go_to_given_backends(backends,                                    allow_failures=False)  +def wait_until_no_rpcs_go_to_given_backends(backends, timeout_sec):",no don't see how to make that happen yet because the logic in the middle is completely opposite,
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/26065,621753997,2021-04-28T02:04:05Z,tools/run_tests/run_xds_tests.py,"@@ -2866,6 +3217,26 @@ def __init__(self, compute, alpha_compute, project, project_num):                     test_timeout(gcp, backend_service, instance_group)                 elif test_case == 'fault_injection':                     test_fault_injection(gcp, backend_service, instance_group)+                elif test_case == 'api_listener':+                    if CLIENT_HOSTS:+                        logger.info('skipping api_listener test case because '+                                    'client processes on existing client hosts '+                                    'are out of test scope.')+                        continue+                    client_process = test_api_listener(gcp, backend_service, instance_group,","As I understand it, the client process is only used by the single test case method (e.g., `test_api_listener`). Each test case gets its own client process. So I don't think there is a need to restore or return the ""original"" client_process when these test methods finish. I think this means you can simplify `test_api_listener` to not accept (or return) a `client_process` parameter, and `forwarding_rule_port_match` doesn't need to return a new `client_process`.",X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/26099,622503351,2021-04-28T20:11:48Z,src/core/lib/gprpp/status_helper.cc,"@@ -200,6 +214,29 @@ absl::optional<std::string> StatusGetStr(const absl::Status& status,   return {}; } +void StatusSetTime(absl::Status* status, StatusTimeProperty key,+                   absl::Time time) {+  status->SetPayload(GetStatusTimePropertyUrl(key),+                     absl::Cord(absl::string_view(+                         reinterpret_cast<const char*>(&time), sizeof(time))));",I don't think that they have different performance since absl::Time is 16 bytes big and grpc_millis is 8 bytes big on 64bits and both end up calling the same function to get the time. Moreover grpc_millis isn't in gpr.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25981,623255298,2021-04-29T17:32:26Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -340,6 +352,11 @@ class DynamicTerminationFilter::CallData {     calld->lb_call_->StartTransportStreamOpBatch(batch);   } +  static void PreCancel(grpc_call_element* elem, grpc_error_handle error) {+    auto* calld = static_cast<CallData*>(elem->call_data);+    calld->lb_call_->PreCancel(error);","Yes.  `SetPollent()` will always have been called before this is called, because the dynamic filter stack does that on call creation:https://github.com/grpc/grpc/blob/266d0f7c052bdb686e93525c587e2b9513b35fa7/src/core/ext/filters/client_channel/dynamic_filters.cc#L59",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25981,623260839,2021-04-29T17:40:38Z,src/core/lib/security/transport/client_auth_filter.cc,"@@ -110,8 +108,51 @@ struct call_data {   grpc_auth_metadata_context auth_md_context =       grpc_auth_metadata_context();  // Zero-initialize the C struct.   grpc_closure async_result_closure;-  grpc_closure check_call_host_cancel_closure;-  grpc_closure get_request_metadata_cancel_closure;++  // Mutex guarding async cancellation.+  //+  // There are two async calls done in this filter: check_call_host()+  // and get_request_metadata(), both invoked while holding the call+  // combiner.  The client_auth_pre_cancel_call() function, which is+  // *not* run in the call combiner, needs to cancel either of these+  // operations if they are in flight, so that we can quickly yield the+  // call combiner and thus allow the cancel_stream op to come down in a+  // timely manner.  In both cases, if the async operation has already+  // completed, then a cancellation of that operation is a no-op.+  //+  // If client_auth_pre_cancel_call() runs before check_call_host() is+  // started, it will set pre_cancelled.  When the code in the call+  // combiner sees that pre_cancelled is set, it will yield the call+  // combiner instead of invoking check_call_host().+  //+  // If client_auth_pre_cancel_call() runs after check_call_host() is+  // started but before get_request_metadata() is started, it will+  // attempt to cancel check_call_host() (which may be a no-op) and+  // set pre_cancelled.  When the code in the call combiner sees that+  // pre_cancelled is set, it will yield the call combiner instead of+  // invoking get_request_metadata().+  //+  // If client_auth_pre_cancel_call() runs after get_request_metadata()+  // is started, it will attempt to cancel get_request_metadata() (which+  // may be a no-op).+  //+  // This mutex should not cause contention *except* when a cancellation+  // is occurring, because all accesses to it other than in+  // client_auth_pre_cancel_call() are done while holding the call combiner.+  grpc_core::Mutex pre_cancel_mu;+  bool pre_cancelled ABSL_GUARDED_BY(pre_cancel_mu) = false;+  bool check_call_host_started ABSL_GUARDED_BY(pre_cancel_mu) = false;+  bool get_request_metadata_started ABSL_GUARDED_BY(pre_cancel_mu) = false;++  // The error seen from a cancel_stream op.+  grpc_error_handle cancel_error = GRPC_ERROR_NONE;++  // The batch containing send_initial_metadata is stored here if we+  // receive a pre-cancellation but have not yet seen the cancel_stream+  // op.  This ensures that the batch is failed with the right error,+  // which may affect the call's status if the batch also contains+  // recv_trailing_metadata.+  grpc_transport_stream_op_batch* send_initial_metadata_batch = nullptr;","Because the error passed to `pre_cancel_call()` might not be the same as the error sent down with the `cancel_stream` op, and it's the latter value that we need to use for the `recv_trailing_metadata` batch.  (I don't remember at this point which test(s) it was that surfaced this problem, but there were test failures that required this fix.)",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/25981,623355553,2021-04-29T19:53:29Z,src/core/lib/security/transport/client_auth_filter.cc,"@@ -110,8 +108,51 @@ struct call_data {   grpc_auth_metadata_context auth_md_context =       grpc_auth_metadata_context();  // Zero-initialize the C struct.   grpc_closure async_result_closure;-  grpc_closure check_call_host_cancel_closure;-  grpc_closure get_request_metadata_cancel_closure;++  // Mutex guarding async cancellation.+  //+  // There are two async calls done in this filter: check_call_host()+  // and get_request_metadata(), both invoked while holding the call+  // combiner.  The client_auth_pre_cancel_call() function, which is+  // *not* run in the call combiner, needs to cancel either of these+  // operations if they are in flight, so that we can quickly yield the+  // call combiner and thus allow the cancel_stream op to come down in a+  // timely manner.  In both cases, if the async operation has already+  // completed, then a cancellation of that operation is a no-op.+  //+  // If client_auth_pre_cancel_call() runs before check_call_host() is+  // started, it will set pre_cancelled.  When the code in the call+  // combiner sees that pre_cancelled is set, it will yield the call+  // combiner instead of invoking check_call_host().+  //+  // If client_auth_pre_cancel_call() runs after check_call_host() is+  // started but before get_request_metadata() is started, it will+  // attempt to cancel check_call_host() (which may be a no-op) and+  // set pre_cancelled.  When the code in the call combiner sees that+  // pre_cancelled is set, it will yield the call combiner instead of+  // invoking get_request_metadata().+  //+  // If client_auth_pre_cancel_call() runs after get_request_metadata()+  // is started, it will attempt to cancel get_request_metadata() (which+  // may be a no-op).+  //+  // This mutex should not cause contention *except* when a cancellation+  // is occurring, because all accesses to it other than in+  // client_auth_pre_cancel_call() are done while holding the call combiner.+  grpc_core::Mutex pre_cancel_mu;+  bool pre_cancelled ABSL_GUARDED_BY(pre_cancel_mu) = false;+  bool check_call_host_started ABSL_GUARDED_BY(pre_cancel_mu) = false;+  bool get_request_metadata_started ABSL_GUARDED_BY(pre_cancel_mu) = false;++  // The error seen from a cancel_stream op.+  grpc_error_handle cancel_error = GRPC_ERROR_NONE;++  // The batch containing send_initial_metadata is stored here if we+  // receive a pre-cancellation but have not yet seen the cancel_stream+  // op.  This ensures that the batch is failed with the right error,+  // which may affect the call's status if the batch also contains+  // recv_trailing_metadata.+  grpc_transport_stream_op_batch* send_initial_metadata_batch = nullptr;","You're right, we do.  Sorry, I must have been thinking of an earlier variant of this code when I wrote that.I looked back at which test failure prompted this change to jog my memory, and it was this one:https://source.cloud.google.com/results/invocations/ed092efc-40e8-496d-ae9f-013e2021a8cb/targets/%2F%2Ftest%2Fcpp%2Fend2end:end2end_test@poller%3Depoll1/tests;group=ProxyEnd2end%2FProxyEnd2endTest;test=RpcDeadlineExpires%2F0;row=1The test was seeing UNKNOWN instead of DEADLINE_EXCEEDED.  I think the problem was that the batch containing `recv_trailing_metadata` had not yet hit the filter at the point at which `pre_cancel_call()` was invoked (because `pre_cancel_call()` is not invoked from within the call combiner, so the two are not synchronized from the perspective of the filter stack).  So that batch wound up getting failed with a different error, and that's the error that dictates the status code of the call.IIRC, the actual problem that caused that test failure was in the client_channel filter, which I fixed in 91c43c8, but then I realized that the client_auth filter would have the same problem, so I fixed it (and some other edge cases I noticed) in 7b50bdc.  It might be that none of our tests would actually trigger this case in the client_auth filter, but it still seems safer to handle it properly.",X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/25981,623376826,2021-04-29T20:29:34Z,src/core/lib/security/transport/client_auth_filter.cc,"@@ -110,8 +108,51 @@ struct call_data {   grpc_auth_metadata_context auth_md_context =       grpc_auth_metadata_context();  // Zero-initialize the C struct.   grpc_closure async_result_closure;-  grpc_closure check_call_host_cancel_closure;-  grpc_closure get_request_metadata_cancel_closure;++  // Mutex guarding async cancellation.+  //+  // There are two async calls done in this filter: check_call_host()+  // and get_request_metadata(), both invoked while holding the call+  // combiner.  The client_auth_pre_cancel_call() function, which is+  // *not* run in the call combiner, needs to cancel either of these+  // operations if they are in flight, so that we can quickly yield the+  // call combiner and thus allow the cancel_stream op to come down in a+  // timely manner.  In both cases, if the async operation has already+  // completed, then a cancellation of that operation is a no-op.+  //+  // If client_auth_pre_cancel_call() runs before check_call_host() is+  // started, it will set pre_cancelled.  When the code in the call+  // combiner sees that pre_cancelled is set, it will yield the call+  // combiner instead of invoking check_call_host().+  //+  // If client_auth_pre_cancel_call() runs after check_call_host() is+  // started but before get_request_metadata() is started, it will+  // attempt to cancel check_call_host() (which may be a no-op) and+  // set pre_cancelled.  When the code in the call combiner sees that+  // pre_cancelled is set, it will yield the call combiner instead of+  // invoking get_request_metadata().+  //+  // If client_auth_pre_cancel_call() runs after get_request_metadata()+  // is started, it will attempt to cancel get_request_metadata() (which+  // may be a no-op).+  //+  // This mutex should not cause contention *except* when a cancellation+  // is occurring, because all accesses to it other than in+  // client_auth_pre_cancel_call() are done while holding the call combiner.+  grpc_core::Mutex pre_cancel_mu;+  bool pre_cancelled ABSL_GUARDED_BY(pre_cancel_mu) = false;+  bool check_call_host_started ABSL_GUARDED_BY(pre_cancel_mu) = false;+  bool get_request_metadata_started ABSL_GUARDED_BY(pre_cancel_mu) = false;++  // The error seen from a cancel_stream op.+  grpc_error_handle cancel_error = GRPC_ERROR_NONE;++  // The batch containing send_initial_metadata is stored here if we+  // receive a pre-cancellation but have not yet seen the cancel_stream+  // op.  This ensures that the batch is failed with the right error,+  // which may affect the call's status if the batch also contains+  // recv_trailing_metadata.+  grpc_transport_stream_op_batch* send_initial_metadata_batch = nullptr;",I don't think saving the previous batch is the right approach here. We should instead check who is responsible for invoking `recv_trailing_metadata` with the correct status/error and make sure that that error is being propagated.,
4111145,YifeiZhuang,https://api.github.com/repos/grpc/grpc/pulls/26065,623520336,2021-04-30T01:06:37Z,tools/run_tests/run_xds_tests.py,"@@ -2866,6 +3217,26 @@ def __init__(self, compute, alpha_compute, project, project_num):                     test_timeout(gcp, backend_service, instance_group)                 elif test_case == 'fault_injection':                     test_fault_injection(gcp, backend_service, instance_group)+                elif test_case == 'api_listener':+                    if CLIENT_HOSTS:+                        logger.info('skipping api_listener test case because '+                                    'client processes on existing client hosts '+                                    'are out of test scope.')+                        continue+                    client_process = test_api_listener(gcp, backend_service, instance_group,","Yes it looks I can avoid touching `client_process` completely in `api_listener_test`.But still have to deal with `client_process` in `forwarding_rule_port_match` because within `forwarding_rule_port_match` I tested against original `client_process` and then terminate original `client_process` and spin up a new one for test default port purposes. So either we split the test case into multiple, which is the latest change and it seems cleaner to me.Alternatively we can combine them and I still have return a client_process as I do not want to bypass the existing exception handling between each test cases. And it would be error prone and hard to debug. There are tradeoffs. I might be biased so let me know what you think. ",X
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/26065,623634458,2021-04-30T06:11:16Z,tools/run_tests/run_xds_tests.py,"@@ -2866,6 +3217,26 @@ def __init__(self, compute, alpha_compute, project, project_num):                     test_timeout(gcp, backend_service, instance_group)                 elif test_case == 'fault_injection':                     test_fault_injection(gcp, backend_service, instance_group)+                elif test_case == 'api_listener':+                    if CLIENT_HOSTS:+                        logger.info('skipping api_listener test case because '+                                    'client processes on existing client hosts '+                                    'are out of test scope.')+                        continue+                    client_process = test_api_listener(gcp, backend_service, instance_group,","Sorry, I missed this last comment. I think just setting the `client_process` in the main loop to None after invoking `forwarding_rule_port_match` would be sufficient to work around the error handling between test. Or create a list of test methods (containing `[forwarding_rule_port_match]`) where `client_process` is not created at all in the main loop. I agree either gets a bit messy...but any approach that avoids creating an entirely new client process that only exists to avoid a non-zero error code seems good enough to me.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26124,623727405,2021-04-30T09:02:31Z,tools/dockerfile/interoptest/grpc_interop_go1.16/Dockerfile,"@@ -0,0 +1,37 @@+# Copyright 2015 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++FROM golang:1.16++# Using login shell removes Go from path, so we add it.+RUN ln -s /usr/local/go/bin/go /usr/local/bin++#====================+# Python dependencies++# Install dependencies++RUN apt-get update && apt-get install -y \+    python-all-dev \+    python3-all-dev \+    python-setuptools++# Install Python packages from PyPI+RUN curl https://bootstrap.pypa.io/pip/2.7/get-pip.py | python2.7+RUN pip install --upgrade pip==19.3.1+RUN pip install virtualenv==16.7.9+RUN pip install futures==2.2.0 enum34==1.0.4 protobuf==3.5.2.post1 six==1.15.0 twisted==17.5.0",This file is generated from a template. So these numbers come from the `python_deps.include` filehttps://github.com/grpc/grpc/blob/fb6669b17150e56f20f95f71b31f7055960affd3/templates/tools/dockerfile/interoptest/grpc_interop_go/Dockerfile.template#L20,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/26114,624041183,2021-04-30T17:23:04Z,src/python/grpcio_csds/grpc_csds/__init__.py,"@@ -0,0 +1,49 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Channelz debug service implementation in gRPC Python.""""""++from grpc._cython import cygrpc++from google.protobuf import json_format+try:+    from envoy.service.status.v3 import csds_pb2, csds_pb2_grpc+except ImportError:+    from src.proto.grpc.testing.xds.v3 import csds_pb2, csds_pb2_grpc","I'm surprised that the only suffixes these import paths have in common is `v3`. If there were more there, I would recommend that you use a combination of `imports` and [`strip_prefixes`](https://github.com/grpc/grpc/blob/68aed165a746450c907a35a07aa342e49d1032e3/bazel/python_rules.bzl#L221).This isn't ideal, but it's probably okay. The only problem I can foresee is issues not being caught because of behavior differences between the Bazel version and the setuptools version.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26114,624051497,2021-04-30T17:39:08Z,tools/distrib/python/xds_protos/setup.py,"@@ -41,7 +38,7 @@ SETUP_REQUIRES = INSTALL_REQUIRES + ['grpcio-tools'] setuptools.setup(     name='xds-protos',-    version='0.0.5',+    version='0.0.8',","To test Bazel installation, I tried 0.0.6 to remove grpcio as a dependency. It works, but feels a bit hacky.0.0.7 added grpcio back into requirements. 0.0.8 added the `bdist_wheel` for Python 2.",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/26146,625214637,2021-05-03T16:40:57Z,src/cpp/server/server_cc.cc,"@@ -67,6 +67,15 @@ namespace { // max-threads set) to the server builder. #define DEFAULT_MAX_SYNC_SERVER_THREADS INT_MAX +// Give a useful status error message if the resource is exhausted specifically+// because the server threadpool is full.+const char* kServerThreadpoolExhausted = ""Server Threadpool Exhausted"";++// Although we might like to give a useful status error message on unimplemented+// RPCs, it's not always possible since that also would need to be added across+// languages and isn't actually required by the spec.","Every language has its own API and implementation. They might each run into this condition in different ways. This is in contrast to the unimplemented method, which is language independent in the sense that it only depends on the service specification and the set of methods actually implemented. For what it's worth, I am okay with this PR being rejected. It was a user feature request to help with debugging, not a fundamental bug fix.",X
16571023,ananda1066,https://api.github.com/repos/grpc/grpc/pulls/26146,625220632,2021-05-03T16:50:55Z,src/cpp/server/server_cc.cc,"@@ -67,6 +67,15 @@ namespace { // max-threads set) to the server builder. #define DEFAULT_MAX_SYNC_SERVER_THREADS INT_MAX +// Give a useful status error message if the resource is exhausted specifically+// because the server threadpool is full.+const char* kServerThreadpoolExhausted = ""Server Threadpool Exhausted"";++// Although we might like to give a useful status error message on unimplemented+// RPCs, it's not always possible since that also would need to be added across+// languages and isn't actually required by the spec.","I see the value of the PR since I can understand that being pretty hard to debug - I'm more wondering if there are other similar error messages we could add at this point, maybe not to this PR but generally? For instance, a user asked to specify more clearly whether proto parsing failure occurred - is there a way to detect this internally? ",X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/26146,625307438,2021-05-03T19:15:34Z,src/cpp/server/server_cc.cc,"@@ -67,6 +67,15 @@ namespace { // max-threads set) to the server builder. #define DEFAULT_MAX_SYNC_SERVER_THREADS INT_MAX +// Give a useful status error message if the resource is exhausted specifically+// because the server threadpool is full.+const char* kServerThreadpoolExhausted = ""Server Threadpool Exhausted"";++// Although we might like to give a useful status error message on unimplemented+// RPCs, it's not always possible since that also would need to be added across+// languages and isn't actually required by the spec.","Great idea for a sequence of follow-ons. I've created a new Github project called debuggability. We can also use create some tracking issues for that project based on the topics that @sheenaqotj left for us (as well as any others that come up)WRT proto parsing: we detect that in our calls of `Deserialize` but probably don't give useful feedback. It is also controversial that we currently return that as status `INTERNAL` but some users (including some PTBs) would prefer that to be `INVALID_ARGUMENT`. That has a long history but I think we have approval for starting a migration of that to `INVALID_ARGUMENT`. That said, that would be a multi-quarter multi-language project.Side note: this PR passes TGP so I don't think it hurts anybody that we know of.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26080,625755068,2021-05-04T12:53:55Z,src/csharp/Grpc.Tools/build/_protobuf/Google.Protobuf.Tools.targets,"@@ -75,6 +75,10 @@       <!-- Next try OS and CPU resolved by ProtoToolsPlatform. -->       <Protobuf_ToolsOs Condition="" '$(Protobuf_ToolsOs)' == '' "">$(_Protobuf_ToolsOs)</Protobuf_ToolsOs>       <Protobuf_ToolsCpu Condition="" '$(Protobuf_ToolsCpu)' == '' "">$(_Protobuf_ToolsCpu)</Protobuf_ToolsCpu>+ +      <!-- Use x64 on macosx arm64 until a native protoc is shipped -->+      <Protobuf_ToolsCpu Condition="" '$(Protobuf_ToolsCpu)' == 'arm64' and '$(Protobuf_ToolsOs)' == 'macosx' "">x64</Protobuf_ToolsCpu>","If you want to override the cpu architecture, you should do it in the C# code(e.g. in the https://github.com/grpc/grpc/blob/master/src/csharp/Grpc.Tools/ProtoToolsPlatform.cs), not here.The msbuild files already way too complex, they weren't designed for hacks like this and they are also much harder to test than the C# code.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26165,625943321,2021-05-04T16:49:34Z,doc/grpc_xds_features.md,"@@ -44,3 +44,7 @@ Features | gRFCs  | [C++, Python,<br> Ruby, PHP](https://github.com/grpc/grpc/re Request matching based on:<ul><li>[Path](https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route_components.proto#route-routematch) (prefix, full path and safe regex)</li><ul><li>[case_sensitive](https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route_components.proto#route-routematch) must be true else config is NACKed</li></ul><li>[Headers](https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route_components.proto#route-headermatcher)</li></ul>Request routing to multiple clusters based on [weights](https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route_components.proto#route-weightedcluster) | [A28](https://github.com/grpc/proposal/blob/master/A28-xds-traffic-splitting-and-routing.md) | v1.31.0 | v1.31.0 | v1.31.0 | v1.3.0 | Case insensitive prefix/full path matching:<ul><li>[case_sensitive](https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route_components.proto#route-routematch) can be true or false</li></ul> | | v1.34.0 | v1.34.0 | v1.34.0 | v1.3.0 | Support for [xDS v3 APIs](https://www.envoyproxy.io/docs/envoy/latest/api-v3/api) | [A30](https://github.com/grpc/proposal/blob/master/A30-xds-v3.md) | v1.36.0 | v1.36.0 | v1.36.0 | |+[Maximum Stream Duration](https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/route/v3/route_components.proto#config-route-v3-routeaction-maxstreamduration):<ul><li>Only max_stream_duration is supported.</li></ul> | [A31](https://github.com/grpc/proposal/blob/master/A31-xds-timeout-support-and-config-selector.md) | v1.37.1  | v1.37.0 | v1.37.0 | |+[Circuit Breaking](https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/cluster/v3/circuit_breaker.proto):<ul><li>Only max_requests is supported.</li></ul> | [A32](https://github.com/grpc/proposal/blob/master/A32-xds-circuit-breaking.md) | v1.37.1 (N/A for PHP) | v1.37.0 | v1.37.0 | |+[Fault Injection](https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/http/fault/v3/fault.proto):<br> Only the following fields are supported:<ul><li>delay</li><li>abort</li><li>max_active_faults</li><li>headers</li></ul> | [A33](https://github.com/grpc/proposal/blob/master/A33-Fault-Injection.md) | v1.37.1  | v1.37.0 | v1.37.0 | |+[Client Status Discovery Service](https://github.com/envoyproxy/envoy/blob/main/api/envoy/service/status/v3/csds.proto) | [A40](https://github.com/grpc/proposal/blob/master/A40-csds-support.md) | v1.37.1 (Only C++)  | v1.37.0 | v1.37.0 | |","Maybe we can follow the pattern of the ""Load Balancing"" section above:**Dump xDS Configuration via [CSDS](https://github.com/envoyproxy/envoy/blob/main/api/envoy/service/status/v3/csds.proto):**- Print in-effective xDS resources or reasons of update rejection.",
35056280,srini100,https://api.github.com/repos/grpc/grpc/pulls/26165,625949947,2021-05-04T16:58:30Z,doc/grpc_xds_features.md,"@@ -44,3 +44,7 @@ Features | gRFCs  | [C++, Python,<br> Ruby, PHP](https://github.com/grpc/grpc/re Request matching based on:<ul><li>[Path](https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route_components.proto#route-routematch) (prefix, full path and safe regex)</li><ul><li>[case_sensitive](https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route_components.proto#route-routematch) must be true else config is NACKed</li></ul><li>[Headers](https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route_components.proto#route-headermatcher)</li></ul>Request routing to multiple clusters based on [weights](https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route_components.proto#route-weightedcluster) | [A28](https://github.com/grpc/proposal/blob/master/A28-xds-traffic-splitting-and-routing.md) | v1.31.0 | v1.31.0 | v1.31.0 | v1.3.0 | Case insensitive prefix/full path matching:<ul><li>[case_sensitive](https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route_components.proto#route-routematch) can be true or false</li></ul> | | v1.34.0 | v1.34.0 | v1.34.0 | v1.3.0 | Support for [xDS v3 APIs](https://www.envoyproxy.io/docs/envoy/latest/api-v3/api) | [A30](https://github.com/grpc/proposal/blob/master/A30-xds-v3.md) | v1.36.0 | v1.36.0 | v1.36.0 | |+[Maximum Stream Duration](https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/route/v3/route_components.proto#config-route-v3-routeaction-maxstreamduration):<ul><li>Only max_stream_duration is supported.</li></ul> | [A31](https://github.com/grpc/proposal/blob/master/A31-xds-timeout-support-and-config-selector.md) | v1.37.1  | v1.37.0 | v1.37.0 | |+[Circuit Breaking](https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/cluster/v3/circuit_breaker.proto):<ul><li>Only max_requests is supported.</li></ul> | [A32](https://github.com/grpc/proposal/blob/master/A32-xds-circuit-breaking.md) | v1.37.1 (N/A for PHP) | v1.37.0 | v1.37.0 | |+[Fault Injection](https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/http/fault/v3/fault.proto):<br> Only the following fields are supported:<ul><li>delay</li><li>abort</li><li>max_active_faults</li><li>headers</li></ul> | [A33](https://github.com/grpc/proposal/blob/master/A33-Fault-Injection.md) | v1.37.1  | v1.37.0 | v1.37.0 | |",The gRFCs linked here have all the details. Users should be reading the gRFCs so keeping the list simple here is more manageable for futures updates too.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26148,626431854,2021-05-05T09:58:44Z,CMakeLists.txt,"@@ -10844,11 +10844,14 @@ target_link_libraries(grpc_cpp_plugin   if(gRPC_INSTALL)-  install(TARGETS grpc_cpp_plugin EXPORT gRPCTargets+  install(TARGETS grpc_cpp_plugin","I think the idea of the change itself is fair, but there are a few issues here:- the file CMakeLists.txt is actually generated from a template (so you need to modify the template https://github.com/grpc/grpc/blob/master/templates/CMakeLists.txt.template first and then regenerate).- grpc_cpp_plugin is not the only plugin -> there are plugins for other languages that should probably behave the same",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26109,626443656,2021-05-05T10:18:39Z,src/csharp/experimental/build_native_ext_for_ios.sh,"@@ -28,7 +28,7 @@ function build {     PATH_CC=""$(xcrun --sdk $SDK --find clang)""     PATH_CXX=""$(xcrun --sdk $SDK --find clang++)"" -    CPPFLAGS=""-O2 -Wframe-larger-than=16384 -arch $ARCH -isysroot $(xcrun --sdk $SDK --show-sdk-path) -mios-version-min=9.0 -DPB_NO_PACKED_STRUCTS=1""",I'm not convinced that this is actually enough to put the boringssl symbol prefixes in place.Have you tested this or was this just an experiment?https://github.com/google/boringssl/blob/master/BUILDING.md#building-with-prefixed-symbols,
1237070,julianxhokaxhiu,https://api.github.com/repos/grpc/grpc/pulls/26109,626560890,2021-05-05T13:23:43Z,src/csharp/experimental/build_native_ext_for_ios.sh,"@@ -28,7 +28,7 @@ function build {     PATH_CC=""$(xcrun --sdk $SDK --find clang)""     PATH_CXX=""$(xcrun --sdk $SDK --find clang++)"" -    CPPFLAGS=""-O2 -Wframe-larger-than=16384 -arch $ARCH -isysroot $(xcrun --sdk $SDK --show-sdk-path) -mios-version-min=9.0 -DPB_NO_PACKED_STRUCTS=1""","To explain it a little bit:- `-DBORINGSSL_PREFIX=GRPC` will kick in this logic https://github.com/google/boringssl/blob/1cf78cd290542e8575c5139907d61dbad5b6f49d/include/openssl/base.h#L79- Luckily, in the repo this file is already provided ( and maintained through your support script ) in this location: https://github.com/grpc/grpc/tree/master/src/boringssl- Although as it's a global header, all we need to do is make sure the compiler can resolve it, so we include that path in the global search paths by using `-Isrc/boringssl`All the rest will be auto-wired up by boringssl and when grpc will statically link to it, will find symbols already renamed.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/26170,626752199,2021-05-05T17:13:30Z,examples/python/xds/server.py,"@@ -52,46 +62,75 @@ def SayHello(self, request: helloworld_pb2.HelloRequest,             message=f""Hello {request.name} from {self._hostname}!"")  -def serve(port: int, hostname: str, secure: bool):-    server = grpc.server(-        futures.ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()),-        xds=secure)--    # Add the application servicer to the server.-    helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(hostname), server)+def _configure_maintenance_server(server: grpc.Server,+                                  maintenance_port: int) -> None:+    listen_address = f""{_LISTEN_HOST}:{maintenance_port}""+    server.add_insecure_port(listen_address)      # Create a health check servicer. We use the non-blocking implementation     # to avoid thread starvation.     health_servicer = health.HealthServicer(         experimental_non_blocking=True,-        experimental_thread_pool=futures.ThreadPoolExecutor(max_workers=1))-    health_pb2_grpc.add_HealthServicer_to_server(health_servicer, server)+        experimental_thread_pool=futures.ThreadPoolExecutor(+            max_workers=_THREAD_POOL_SIZE))      # Create a tuple of all of the services we want to export via reflection.     services = tuple(         service.full_name         for service in helloworld_pb2.DESCRIPTOR.services_by_name.values()) + (             reflection.SERVICE_NAME, health.SERVICE_NAME) -    # Add the reflection service to the server.+    # Mark all services as healthy.+    health_pb2_grpc.add_HealthServicer_to_server(health_servicer, server)+    for service in services:+        health_servicer.set(service, health_pb2.HealthCheckResponse.SERVING)     reflection.enable_server_reflection(services, server)-    listen_address = f""[::]:{port}""-    if not secure:+++def _configure_greeter_server(server: grpc.Server, port: int, secure_mode: bool,+                              hostname) -> None:+    # Add the application servicer to the server.+    helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(hostname), server)+    listen_address = f""{_LISTEN_HOST}:{port}""+    if not secure_mode:         server.add_insecure_port(listen_address)     else:-        print(""Running with xDS Server credentials"")+        # Use xDS credentials.+        logger.info(""Running with xDS Server credentials"")++        # Fall back to insecure credentials.         server_fallback_creds = grpc.insecure_server_credentials()         server_creds = grpc.xds_server_credentials(server_fallback_creds)         server.add_secure_port(listen_address, server_creds)-    server.start() -    # Mark all services as healthy.-    overall_server_health = """"-    for service in services + (overall_server_health,):-        health_servicer.set(service, health_pb2.HealthCheckResponse.SERVING) -    # Park the main application thread.-    server.wait_for_termination()+def serve(port: int, hostname: str, maintenance_port: int,+          secure_mode: bool) -> None:+    if port == maintenance_port:+        # If maintenance port and port are the same, start a single server.+        server = grpc.server(+            futures.ThreadPoolExecutor(max_workers=_THREAD_POOL_SIZE))+        _configure_greeter_server(server, port, secure_mode, hostname)","> What happens if the port and maintenance port is the same but secure mode is used?There's a check for this in before the call to `serve`. It's considered an illegal argument. Maybe there's some aspect of this I'm not getting?> Or do not allow the maintenance port to be specified. It can be regular service port + 1We could do this, but we'd probably want to change the other languages to align on their CLI arguments. I'm following C++'s pattern here. That's why I had to write my own bool-parsing function. Absl flags differ in their behavior with respect to bools from `argparse`.",X
14166415,sanjaypujare,https://api.github.com/repos/grpc/grpc/pulls/26170,626754637,2021-05-05T17:17:09Z,examples/python/xds/server.py,"@@ -52,46 +62,75 @@ def SayHello(self, request: helloworld_pb2.HelloRequest,             message=f""Hello {request.name} from {self._hostname}!"")  -def serve(port: int, hostname: str, secure: bool):-    server = grpc.server(-        futures.ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()),-        xds=secure)--    # Add the application servicer to the server.-    helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(hostname), server)+def _configure_maintenance_server(server: grpc.Server,+                                  maintenance_port: int) -> None:+    listen_address = f""{_LISTEN_HOST}:{maintenance_port}""+    server.add_insecure_port(listen_address)      # Create a health check servicer. We use the non-blocking implementation     # to avoid thread starvation.     health_servicer = health.HealthServicer(         experimental_non_blocking=True,-        experimental_thread_pool=futures.ThreadPoolExecutor(max_workers=1))-    health_pb2_grpc.add_HealthServicer_to_server(health_servicer, server)+        experimental_thread_pool=futures.ThreadPoolExecutor(+            max_workers=_THREAD_POOL_SIZE))      # Create a tuple of all of the services we want to export via reflection.     services = tuple(         service.full_name         for service in helloworld_pb2.DESCRIPTOR.services_by_name.values()) + (             reflection.SERVICE_NAME, health.SERVICE_NAME) -    # Add the reflection service to the server.+    # Mark all services as healthy.+    health_pb2_grpc.add_HealthServicer_to_server(health_servicer, server)+    for service in services:+        health_servicer.set(service, health_pb2.HealthCheckResponse.SERVING)     reflection.enable_server_reflection(services, server)-    listen_address = f""[::]:{port}""-    if not secure:+++def _configure_greeter_server(server: grpc.Server, port: int, secure_mode: bool,+                              hostname) -> None:+    # Add the application servicer to the server.+    helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(hostname), server)+    listen_address = f""{_LISTEN_HOST}:{port}""+    if not secure_mode:         server.add_insecure_port(listen_address)     else:-        print(""Running with xDS Server credentials"")+        # Use xDS credentials.+        logger.info(""Running with xDS Server credentials"")++        # Fall back to insecure credentials.         server_fallback_creds = grpc.insecure_server_credentials()         server_creds = grpc.xds_server_credentials(server_fallback_creds)         server.add_secure_port(listen_address, server_creds)-    server.start() -    # Mark all services as healthy.-    overall_server_health = """"-    for service in services + (overall_server_health,):-        health_servicer.set(service, health_pb2.HealthCheckResponse.SERVING) -    # Park the main application thread.-    server.wait_for_termination()+def serve(port: int, hostname: str, maintenance_port: int,+          secure_mode: bool) -> None:+    if port == maintenance_port:+        # If maintenance port and port are the same, start a single server.+        server = grpc.server(+            futures.ThreadPoolExecutor(max_workers=_THREAD_POOL_SIZE))+        _configure_greeter_server(server, port, secure_mode, hostname)","> We could do this, but we'd probably want to change the other languages to align on their CLI arguments. I'm following C++'s pattern here. That's why I had to write my own bool-parsing function. Absl flags differ in their behavior with respect to bools from `argparse`.Java follows the pattern https://github.com/grpc/grpc-java/blob/master/examples/example-xds/src/main/java/io/grpc/examples/helloworldxds/XdsHelloWorldServer.java#L65 so you have a precedent and we can then get C++ to follow that :-)",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26176,627627251,2021-05-06T17:27:57Z,tools/run_tests/task_runner.py,"@@ -73,6 +73,11 @@ def _create_build_map():                   default=False,                   action='store_const',                   const=True)+argp.add_argument('-x',+                  '--xml_report',+                  default='report_taskrunner_sponge_log.xml',+                  type=str,+                  help='Filename for the JUnit-compatible XML report')",Maybe we need to use the `xml_report` somewhere?https://github.com/grpc/grpc/blob/dd54a74fefb2065549b409862da701c7e68acd3c/tools/run_tests/task_runner.py#L117,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26180,627689359,2021-05-06T18:59:09Z,include/grpc/event_engine/event_engine.h,"@@ -208,42 +245,59 @@ class EventEngine {     /// Called with the result of a TXT record lookup     using LookupTXTCallback = std::function<void(absl::Status, std::string)>; -    virtual ~DNSResolver() = 0;+    virtual ~DNSResolver() = default; -    // TODO(hork): define status codes for the callback-    /// Asynchronously resolve an address. \a default_port may be a non-numeric-    /// named service port, and will only be used if \a address does not already-    /// contain a port component.+    /// Asynchronously resolve an address.+    ///+    /// \a default_port may be a non-numeric named service port, and will only+    /// be used if \a address does not already contain a port component.+    ///+    /// \a on_resolve will be called exactly once. Implementations are expected","Suggest wording this similarly to `Endpoint::Read()` above:""""""The \a on_resolve callback will be called when the lookup is complete.  Implementations should pass the appropriate statuses to the callback. For example, callbacks might expect to receive DEADLINE_EXCEEDED when the deadline is exceeded or CANCELLED if the lookup was cancelled.""""""",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26191,627814628,2021-05-06T22:53:05Z,src/core/ext/xds/xds_api.cc,"@@ -830,14 +830,23 @@ bool IsEds(absl::string_view type_url) {  }  // namespace +// If gRPC is built with -DGRPC_XDS_USER_AGENT_SUFFIX=""..."", that string+// will be appended to the user agent reported to the xDS server.+#ifdef GRPC_XDS_USER_AGENT_SUFFIX+#define _GRPC_XDS_USER_AGENT_SUFFIX ""##GRPC_XDS_USER_AGENT_SUFFIX##""","I don't think the `##` cpp operators are doing what you expect here.  I think that works only if the macro following the operator is an argument of the macro currently being expanded, not if it's a completely independent macro.My intention here was that the macro value you define would have the quotes already in the value -- i.e., you should be passing `-DGRPC_XDS_USER_AGENT_SUFFIX=""\""Python\""""` to the build.Also, note that this needs to add a leading space to the string, or else there won't be a space separating the macro value from the existing value.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26176,628007825,2021-05-07T07:59:19Z,tools/internal_ci/linux/grpc_distribtests_python.cfg,"@@ -0,0 +1,26 @@+# Copyright 2017 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# Config file for the internal CI (in protobuf text format)++# Location of the continuous shell script in repository.+build_file: ""grpc/tools/internal_ci/linux/grpc_distribtests_python.sh""+timeout_mins: 360","The last 2 runs I made (by ""kidnapping"" and existing PR test job) took between 2 and 2.5 hours. While that's not great, it's also not terrible and it's much faster than the full multilanguage artifacts - packages - distribtest flow.I can reduce the timeout to 240mins to still allow some buffer.On ARM7 build: we actually no longer run these, in one of my relatively recent PRs I changed the ""linux_extra"" so that they are crosscompiled (instead of building under an emulator), which is almost as fast as a regular build.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26180,628374380,2021-05-07T17:01:41Z,include/grpc/event_engine/event_engine.h,"@@ -208,42 +245,59 @@ class EventEngine {     /// Called with the result of a TXT record lookup     using LookupTXTCallback = std::function<void(absl::Status, std::string)>; -    virtual ~DNSResolver() = 0;+    virtual ~DNSResolver() = default; -    // TODO(hork): define status codes for the callback-    /// Asynchronously resolve an address. \a default_port may be a non-numeric-    /// named service port, and will only be used if \a address does not already-    /// contain a port component.+    /// Asynchronously resolve an address.+    ///+    /// \a default_port may be a non-numeric named service port, and will only+    /// be used if \a address does not already contain a port component.+    ///+    /// \a on_resolve will be called exactly once. Implementations are expected","I don't actually think that ""when the lookup is complete"" implies anything about the success or failure of the lookup, especially since the rest of the paragraph talks about the status being passed to the callback.  But if you're concerned about that, how about this wording:""""""When the lookup is complete, the \a on_resolve callback will be invoked with a status indicating the success or failure of the lookup.  Implementations should pass the appropriate statuses to the callback. For example, callbacks might expect to receive DEADLINE_EXCEEDED when the deadline is exceeded or CANCELLED if the lookup was cancelled.""""""Hmmm... Would it make sense to change the callback arguments from `absl::Status` and `std::vector<ResolvedAddress>` to a single argument of type `absl::StatusOr<std::vector<ResolvedAddress>>`?   That way, the API makes it clearer that it will either fail or get a result -- i.e., there will never be a case where both arguments are relevant at the same time.",X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/26159,630290572,2021-05-11T15:33:31Z,src/cpp/server/server_cc.cc,"@@ -417,7 +417,9 @@ class Server::SyncRequest final : public grpc::internal::CompletionQueueTag {                                  : server_->resource_exhausted_handler_.get();       deserialized_request_ = handler->Deserialize(call_, request_payload_,                                                    &request_status_, nullptr);-+      if (!request_status_.ok()) {+        gpr_log(GPR_ERROR, ""Failed to deserialize message."");",I feel like this should be a GPR_INFO rather than GPR_ERROR. GPR_ERROR is more about library failures rather than bad input coming in. Same with the below.,
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/26159,630443016,2021-05-11T18:57:13Z,src/cpp/server/server_cc.cc,"@@ -417,7 +417,9 @@ class Server::SyncRequest final : public grpc::internal::CompletionQueueTag {                                  : server_->resource_exhausted_handler_.get();       deserialized_request_ = handler->Deserialize(call_, request_payload_,                                                    &request_status_, nullptr);-+      if (!request_status_.ok()) {+        gpr_log(GPR_ERROR, ""Failed to deserialize message."");","I could see this as either `INFO` or `DEBUG`. I guess looking at comparative use, it should probably be `DEBUG` since we do use that for things like missing metadata, etc.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26225,630988808,2021-05-12T12:20:51Z,tools/internal_ci/linux/grpc_e2e_performance_v2.sh,"@@ -29,65 +29,65 @@ gcloud config set project grpc-testing gcloud container clusters get-credentials benchmarks-prod \     --zone us-central1-b --project grpc-testing -# Set up environment variables-PREBUILT_IMAGE_PREFIX=""gcr.io/grpc-testing/e2etesting/pre_built_workers/""${KOKORO_BUILD_INITIATOR}+# Set up environment variables.+PREBUILT_IMAGE_PREFIX=""gcr.io/grpc-testing/e2etesting/pre_built_workers/${KOKORO_BUILD_INITIATOR}"" UNIQUE_IDENTIFIER=$(date +%Y%m%d%H%M%S) ROOT_DIRECTORY_OF_DOCKERFILES=""../test-infra/containers/pre_built_workers/"" GRPC_CORE_GITREF=""$(git ls-remote https://github.com/grpc/grpc.git master | cut -c1-7)""","since you're running this from a clone of grpc/grpc at a specific commit (likely master, but could be a pull request as well), it makes no sense to ask for the `git ls-remote https://github.com/grpc/grpc.git master` - just use the current local commit, it's more correct and allows you to tests e.g. release branches, pull requests etc.For grpc-java and grpc-go since we don't have a local clone, it's fine to ask github  the head of master branch. (This is actually very similar to the behavior we already use for interop_tests).This is fine to address as a followup PR, but it should be addresed.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26225,630991469,2021-05-12T12:24:33Z,tools/internal_ci/linux/grpc_e2e_performance_v2.sh,"@@ -29,65 +29,65 @@ gcloud config set project grpc-testing gcloud container clusters get-credentials benchmarks-prod \     --zone us-central1-b --project grpc-testing -# Set up environment variables-PREBUILT_IMAGE_PREFIX=""gcr.io/grpc-testing/e2etesting/pre_built_workers/""${KOKORO_BUILD_INITIATOR}+# Set up environment variables.+PREBUILT_IMAGE_PREFIX=""gcr.io/grpc-testing/e2etesting/pre_built_workers/${KOKORO_BUILD_INITIATOR}"" UNIQUE_IDENTIFIER=$(date +%Y%m%d%H%M%S) ROOT_DIRECTORY_OF_DOCKERFILES=""../test-infra/containers/pre_built_workers/"" GRPC_CORE_GITREF=""$(git ls-remote https://github.com/grpc/grpc.git master | cut -c1-7)"" GRPC_GO_GITREF=""$(git ls-remote https://github.com/grpc/grpc-go.git master | cut -c1-7)"" GRPC_JAVA_GITREF=""$(git ls-remote https://github.com/grpc/grpc-java.git master | cut -c1-7)"" --# Clone test-infra repository to one upper level directory than grpc-cd ..+# Clone test-infra repository to one upper level directory than grpc.+pushd .. git clone --recursive https://github.com/grpc/test-infra.git-cd grpc+cd test-infra+go build -o bin/runner cmd/runner/main.go+popd -# If there is a error within the function, will exit directly-deleteImages() {-  echo ""an error has occurred after pushing the images, deleting images""-  go run ../test-infra/tools/delete_prebuilt_workers/delete_prebuilt_workers.go \-  -p $PREBUILT_IMAGE_PREFIX \-  -t $UNIQUE_IDENTIFIER+# Build test configurations.+buildConfigs() {+    local pool=""$1""+    shift+    tools/run_tests/performance/loadtest_config.py ""$@"" \+        -t ./tools/run_tests/performance/templates/loadtest_template_prebuilt_all_languages.yaml \+        -s client_pool=""${pool}"" -s server_pool=""${pool}"" \+        -s big_query_table=e2e_benchmarks.experimental_results \+        -s timeout_seconds=900 \+        -s prebuilt_image_prefix=$PREBUILT_IMAGE_PREFIX \+        -s prebuilt_image_tag=$UNIQUE_IDENTIFIER \+        --prefix=$KOKORO_BUILD_INITIATOR -u $UNIQUE_IDENTIFIER -u ""${pool}"" \+        -a pool=""${pool}"" --category=scalable \+        --allow_client_language=c++ --allow_server_language=c++ \+        -o ""./loadtest_with_prebuilt_workers_${pool}.yaml"" }-trap deleteImages EXIT -# Build and push prebuilt images for running tests-go run ../test-infra/tools/prepare_prebuilt_workers/prepare_prebuilt_workers.go \-  -l cxx:$GRPC_CORE_GITREF \-  -l csharp:$GRPC_CORE_GITREF \-  -l go:$GRPC_GO_GITREF \-  -l java:$GRPC_JAVA_GITREF \-  -l python:$GRPC_CORE_GITREF \-  -l ruby:$GRPC_CORE_GITREF \-  -p $PREBUILT_IMAGE_PREFIX \-  -t $UNIQUE_IDENTIFIER \-  -r $ROOT_DIRECTORY_OF_DOCKERFILES+buildConfigs workers-8core -l c++ -l csharp -l go -l java -l python -l ruby -# This is subject to change. Runs a single test and does not wait for the-# result.-tools/run_tests/performance/loadtest_config.py -l c++ -l go \-    -t ./tools/run_tests/performance/templates/loadtest_template_basic_all_languages.yaml \-    -s client_pool=workers-8core -s server_pool=workers-8core \-    -s big_query_table=e2e_benchmarks.experimental_results \-    -s timeout_seconds=900 \-    -s prebuilt_image_prefix=$PREBUILT_IMAGE_PREFIX \-    -s prebuilt_image_tag=$UNIQUE_IDENTIFIER \-    --prefix=$KOKORO_BUILD_INITIATOR -u $UNIQUE_IDENTIFIER \-    -r '(go_generic_sync_streaming_ping_pong_secure|go_protobuf_sync_unary_ping_pong_secure|cpp_protobuf_async_streaming_qps_unconstrained_secure)$' \-    -o ./loadtest_with_prebuilt_images.yaml+buildConfigs workers-32core -l c++ -l csharp -l go -l java -# Dump the contents of the loadtest_with_prebuilt_images.yaml (since-# loadtest_config.py doesn't list the scenarios that will be run).-cat ./loadtest_with_prebuilt_images.yaml+# Delete prebuilt images on exit.+deleteImages() {+    echo ""deleting images on exit""+    go run ../test-infra/tools/delete_prebuilt_workers/delete_prebuilt_workers.go \+    -p $PREBUILT_IMAGE_PREFIX \+    -t $UNIQUE_IDENTIFIER+}+trap deleteImages EXIT -# The original version of the client is old, update to the latest release-# version v1.21.0.-kubectl version --client-curl -sSL -O https://dl.k8s.io/release/v1.21.0/bin/linux/amd64/kubectl-sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl-chmod +x kubectl-sudo mv kubectl $(which kubectl)-kubectl version --client+# Build and push prebuilt images for running tests.+go run ../test-infra/tools/prepare_prebuilt_workers/prepare_prebuilt_workers.go \+    -l cxx:$GRPC_CORE_GITREF \+    -l csharp:$GRPC_CORE_GITREF \+    -l go:$GRPC_GO_GITREF \+    -l java:$GRPC_JAVA_GITREF \+    -l python:$GRPC_CORE_GITREF \+    -l ruby:$GRPC_CORE_GITREF \+    -p $PREBUILT_IMAGE_PREFIX \+    -t $UNIQUE_IDENTIFIER \+    -r $ROOT_DIRECTORY_OF_DOCKERFILES -kubectl apply -f ./loadtest_with_prebuilt_images.yaml+# Run tests.+../test-infra/bin/runner \+    -i ../grpc/loadtest_with_prebuilt_workers_workers-8core.yaml \+    -i ../grpc/loadtest_with_prebuilt_workers_workers-32core.yaml \+    -a pool -c workers-8core:8 -c workers-32core:8","nit:  the meaning of `-a` and `-c` is not obvious from the command, consider using long flag name for clarity?",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/26253,632825961,2021-05-14T21:53:36Z,tools/run_tests/helper_scripts/clean_xds_interop_resources.py,"@@ -0,0 +1,168 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import logging+import json+import sys+import re+import os+import functools+import subprocess+import datetime+from dataclasses import dataclass+from typing import List, Any++# Parses commandline arguments+parser = argparse.ArgumentParser()+parser.add_argument('--dry_run', action='store_true')+args = parser.parse_args()++# Type alias+Json = Any++# Configures this script+KEEP_PERIOD = datetime.timedelta(hours=24)+GCLOUD = os.environ.get('GCLOUD', 'gcloud')+GCLOUD_CMD_TIMEOUT_S = datetime.timedelta(seconds=120).total_seconds()+ZONE = 'us-central1-a'+SECONDARY_ZONE = 'us-west1-b'+PROJECT = 'grpc-testing'+++@functools.lru_cache()+def get_expire_timestamp():+    return (datetime.datetime.now() - KEEP_PERIOD).isoformat()+++def exec_gcloud(*cmds: List[str]) -> Json:+    cmds = [GCLOUD, '--project', PROJECT] + list(cmds)+    if 'list' in cmds:+        # Add arguments to shape the list output+        cmds.extend([+            '--format', 'json', '--filter',+            f'creationTimestamp<={get_expire_timestamp()}'+        ])+    if args.dry_run and 'delete' in cmds:+        # Skip deletion for dry-runs+        logging.debug('> Skipped: %s', "" "".join(cmds))+        return None+    # Executing the gcloud command+    logging.debug('Executing: %s', "" "".join(cmds))+    proc = subprocess.Popen(cmds,+                            stdout=subprocess.PIPE,+                            stderr=subprocess.PIPE,+                            text=True)+    returncode = proc.wait(timeout=GCLOUD_CMD_TIMEOUT_S)+    if returncode:+        logging.error('> Failed to execute cmd [%s], returned %d, stderr: %s',+                      "" "".join(cmds), returncode, proc.stderr.read())+        return None+    return json.loads(proc.stdout.read())+++def remove_relative_resources_run_xds_tests(suffix: str):+    """"""Removing GCP resources created by run_xds_tests.py.""""""+    logging.info('Removing run_xds_tests.py resources with suffix [%s]', suffix)+    exec_gcloud('compute', 'forwarding-rules', 'delete',+                f'test-forwarding-rule{suffix}', '--global')+    exec_gcloud('compute', 'target-http-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud('alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud('compute', 'url-maps', 'delete', f'test-map{suffix}')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-alternate{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-extra{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-more-extra{suffix}', '--global')+    exec_gcloud('compute', 'firewall-rules', 'delete', f'test-fw-rule{suffix}')+    exec_gcloud('compute', 'health-checks', 'delete', f'test-hc{suffix}')+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig{suffix}', '--zone', ZONE)+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-same-zone{suffix}', '--zone', ZONE)+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-secondary-zone{suffix}', '--zone', SECONDARY_ZONE)+    exec_gcloud('compute', 'instance-templates', 'delete',+                f'test-template{suffix}')+++def remove_relative_resources_psm_sec(prefix: str):+    """"""Removing GCP resources created by PSM Sec framework.""""""+    logging.info('Removing PSM Security resources with prefix [%s]', prefix)+    exec_gcloud('compute', 'forwarding-rules', 'delete',+                f'{prefix}-forwarding-rule', '--global')+    exec_gcloud('alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'{prefix}-target-proxy')+    exec_gcloud('compute', 'url-maps', 'delete', f'{prefix}-url-map')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'{prefix}-backend-service', '--global')+    exec_gcloud('compute', 'health-checks', 'delete', f'{prefix}-health-check')+    exec_gcloud('compute', 'firewall-rules', 'delete',+                f'{prefix}-allow-health-checks')+    exec_gcloud('alpha', 'network-security', 'server-tls-policies', 'delete',+                f'{prefix}-server-tls-policy')+    exec_gcloud('alpha', 'network-security', 'client-tls-policies', 'delete',+                f'{prefix}-client-tls-policy')+++def check_one_type_of_gcp_resources(list_cmd: List[str],+                                    suffix_search: str = '',","It seems these arguments are _very_, _very_ poorly named. Horribly named. They have _nothing_ to do with prefix or suffix matches (as the regex determines that, not this function) _and_ they are heavily semantically important as they clean up different resource types.",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/26253,632840077,2021-05-14T22:19:47Z,tools/run_tests/helper_scripts/clean_xds_interop_resources.py,"@@ -0,0 +1,168 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import logging+import json+import sys+import re+import os+import functools+import subprocess+import datetime+from dataclasses import dataclass+from typing import List, Any++# Parses commandline arguments+parser = argparse.ArgumentParser()+parser.add_argument('--dry_run', action='store_true')+args = parser.parse_args()++# Type alias+Json = Any++# Configures this script+KEEP_PERIOD = datetime.timedelta(hours=24)+GCLOUD = os.environ.get('GCLOUD', 'gcloud')+GCLOUD_CMD_TIMEOUT_S = datetime.timedelta(seconds=120).total_seconds()+ZONE = 'us-central1-a'+SECONDARY_ZONE = 'us-west1-b'+PROJECT = 'grpc-testing'+++@functools.lru_cache()+def get_expire_timestamp():+    return (datetime.datetime.now() - KEEP_PERIOD).isoformat()+++def exec_gcloud(*cmds: List[str]) -> Json:+    cmds = [GCLOUD, '--project', PROJECT] + list(cmds)+    if 'list' in cmds:+        # Add arguments to shape the list output+        cmds.extend([+            '--format', 'json', '--filter',+            f'creationTimestamp<={get_expire_timestamp()}'+        ])+    if args.dry_run and 'delete' in cmds:+        # Skip deletion for dry-runs+        logging.debug('> Skipped: %s', "" "".join(cmds))+        return None+    # Executing the gcloud command+    logging.debug('Executing: %s', "" "".join(cmds))+    proc = subprocess.Popen(cmds,+                            stdout=subprocess.PIPE,+                            stderr=subprocess.PIPE,+                            text=True)+    returncode = proc.wait(timeout=GCLOUD_CMD_TIMEOUT_S)+    if returncode:+        logging.error('> Failed to execute cmd [%s], returned %d, stderr: %s',+                      "" "".join(cmds), returncode, proc.stderr.read())+        return None+    return json.loads(proc.stdout.read())+++def remove_relative_resources_run_xds_tests(suffix: str):+    """"""Removing GCP resources created by run_xds_tests.py.""""""+    logging.info('Removing run_xds_tests.py resources with suffix [%s]', suffix)+    exec_gcloud('compute', 'forwarding-rules', 'delete',+                f'test-forwarding-rule{suffix}', '--global')+    exec_gcloud('compute', 'target-http-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud('alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud('compute', 'url-maps', 'delete', f'test-map{suffix}')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-alternate{suffix}', '--global')","This seems very prone to drift; it seems this should really be part of the test framework. Is there a reason we're making this a separate script and not just doing an extra cleanup at the beginning/end of the test frameworks? That's what we've done for some other similar cases in the past.I could understand this script if we plan to trash it after a quarter or so, but it didn't sound like how this was proposed.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/26253,632846923,2021-05-14T22:44:28Z,tools/run_tests/helper_scripts/clean_xds_interop_resources.py,"@@ -0,0 +1,168 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import logging+import json+import sys+import re+import os+import functools+import subprocess+import datetime+from dataclasses import dataclass+from typing import List, Any++# Parses commandline arguments+parser = argparse.ArgumentParser()+parser.add_argument('--dry_run', action='store_true')+args = parser.parse_args()++# Type alias+Json = Any++# Configures this script+KEEP_PERIOD = datetime.timedelta(hours=24)+GCLOUD = os.environ.get('GCLOUD', 'gcloud')+GCLOUD_CMD_TIMEOUT_S = datetime.timedelta(seconds=120).total_seconds()+ZONE = 'us-central1-a'+SECONDARY_ZONE = 'us-west1-b'+PROJECT = 'grpc-testing'+++@functools.lru_cache()+def get_expire_timestamp():+    return (datetime.datetime.now() - KEEP_PERIOD).isoformat()+++def exec_gcloud(*cmds: List[str]) -> Json:+    cmds = [GCLOUD, '--project', PROJECT] + list(cmds)+    if 'list' in cmds:+        # Add arguments to shape the list output+        cmds.extend([+            '--format', 'json', '--filter',+            f'creationTimestamp<={get_expire_timestamp()}'+        ])+    if args.dry_run and 'delete' in cmds:+        # Skip deletion for dry-runs+        logging.debug('> Skipped: %s', "" "".join(cmds))+        return None+    # Executing the gcloud command+    logging.debug('Executing: %s', "" "".join(cmds))+    proc = subprocess.Popen(cmds,+                            stdout=subprocess.PIPE,+                            stderr=subprocess.PIPE,+                            text=True)+    returncode = proc.wait(timeout=GCLOUD_CMD_TIMEOUT_S)","Since you used `subprocess.PIPE`, [this call could deadlock](https://docs.python.org/3/library/subprocess.html#subprocess.Popen.wait) if a long error is printed. Consider using a temporary file instead.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26253,632866207,2021-05-15T00:11:49Z,tools/run_tests/helper_scripts/clean_xds_interop_resources.py,"@@ -0,0 +1,168 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import logging+import json+import sys+import re+import os+import functools+import subprocess+import datetime+from dataclasses import dataclass+from typing import List, Any++# Parses commandline arguments+parser = argparse.ArgumentParser()+parser.add_argument('--dry_run', action='store_true')+args = parser.parse_args()++# Type alias+Json = Any++# Configures this script+KEEP_PERIOD = datetime.timedelta(hours=24)+GCLOUD = os.environ.get('GCLOUD', 'gcloud')+GCLOUD_CMD_TIMEOUT_S = datetime.timedelta(seconds=120).total_seconds()+ZONE = 'us-central1-a'+SECONDARY_ZONE = 'us-west1-b'+PROJECT = 'grpc-testing'+++@functools.lru_cache()+def get_expire_timestamp():+    return (datetime.datetime.now() - KEEP_PERIOD).isoformat()+++def exec_gcloud(*cmds: List[str]) -> Json:+    cmds = [GCLOUD, '--project', PROJECT] + list(cmds)+    if 'list' in cmds:+        # Add arguments to shape the list output+        cmds.extend([+            '--format', 'json', '--filter',+            f'creationTimestamp<={get_expire_timestamp()}'+        ])+    if args.dry_run and 'delete' in cmds:+        # Skip deletion for dry-runs+        logging.debug('> Skipped: %s', "" "".join(cmds))+        return None+    # Executing the gcloud command+    logging.debug('Executing: %s', "" "".join(cmds))+    proc = subprocess.Popen(cmds,+                            stdout=subprocess.PIPE,+                            stderr=subprocess.PIPE,+                            text=True)+    returncode = proc.wait(timeout=GCLOUD_CMD_TIMEOUT_S)+    if returncode:+        logging.error('> Failed to execute cmd [%s], returned %d, stderr: %s',+                      "" "".join(cmds), returncode, proc.stderr.read())+        return None+    return json.loads(proc.stdout.read())+++def remove_relative_resources_run_xds_tests(suffix: str):+    """"""Removing GCP resources created by run_xds_tests.py.""""""+    logging.info('Removing run_xds_tests.py resources with suffix [%s]', suffix)+    exec_gcloud('compute', 'forwarding-rules', 'delete',+                f'test-forwarding-rule{suffix}', '--global')+    exec_gcloud('compute', 'target-http-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud('alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud('compute', 'url-maps', 'delete', f'test-map{suffix}')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-alternate{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-extra{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-more-extra{suffix}', '--global')+    exec_gcloud('compute', 'firewall-rules', 'delete', f'test-fw-rule{suffix}')+    exec_gcloud('compute', 'health-checks', 'delete', f'test-hc{suffix}')+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig{suffix}', '--zone', ZONE)+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-same-zone{suffix}', '--zone', ZONE)+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-secondary-zone{suffix}', '--zone', SECONDARY_ZONE)+    exec_gcloud('compute', 'instance-templates', 'delete',+                f'test-template{suffix}')+++def remove_relative_resources_psm_sec(prefix: str):+    """"""Removing GCP resources created by PSM Sec framework.""""""+    logging.info('Removing PSM Security resources with prefix [%s]', prefix)+    exec_gcloud('compute', 'forwarding-rules', 'delete',+                f'{prefix}-forwarding-rule', '--global')+    exec_gcloud('alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'{prefix}-target-proxy')+    exec_gcloud('compute', 'url-maps', 'delete', f'{prefix}-url-map')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'{prefix}-backend-service', '--global')+    exec_gcloud('compute', 'health-checks', 'delete', f'{prefix}-health-check')+    exec_gcloud('compute', 'firewall-rules', 'delete',+                f'{prefix}-allow-health-checks')+    exec_gcloud('alpha', 'network-security', 'server-tls-policies', 'delete',+                f'{prefix}-server-tls-policy')+    exec_gcloud('alpha', 'network-security', 'client-tls-policies', 'delete',+                f'{prefix}-client-tls-policy')+++def check_one_type_of_gcp_resources(list_cmd: List[str],+                                    suffix_search: str = '',","Argument names updated to `old_framework_residual_search` and `new_framework_residual_search`. Feel free to suggest alternatives.We can split this function into 2, but that means we need to run the ""list"" command twice, or pass the resources around.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26253,632867083,2021-05-15T00:16:42Z,tools/run_tests/helper_scripts/clean_xds_interop_resources.py,"@@ -0,0 +1,168 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import logging+import json+import sys+import re+import os+import functools+import subprocess+import datetime+from dataclasses import dataclass+from typing import List, Any++# Parses commandline arguments+parser = argparse.ArgumentParser()+parser.add_argument('--dry_run', action='store_true')+args = parser.parse_args()++# Type alias+Json = Any++# Configures this script+KEEP_PERIOD = datetime.timedelta(hours=24)+GCLOUD = os.environ.get('GCLOUD', 'gcloud')+GCLOUD_CMD_TIMEOUT_S = datetime.timedelta(seconds=120).total_seconds()+ZONE = 'us-central1-a'+SECONDARY_ZONE = 'us-west1-b'+PROJECT = 'grpc-testing'+++@functools.lru_cache()+def get_expire_timestamp():+    return (datetime.datetime.now() - KEEP_PERIOD).isoformat()+++def exec_gcloud(*cmds: List[str]) -> Json:+    cmds = [GCLOUD, '--project', PROJECT] + list(cmds)+    if 'list' in cmds:+        # Add arguments to shape the list output+        cmds.extend([+            '--format', 'json', '--filter',+            f'creationTimestamp<={get_expire_timestamp()}'+        ])+    if args.dry_run and 'delete' in cmds:+        # Skip deletion for dry-runs+        logging.debug('> Skipped: %s', "" "".join(cmds))+        return None+    # Executing the gcloud command+    logging.debug('Executing: %s', "" "".join(cmds))+    proc = subprocess.Popen(cmds,+                            stdout=subprocess.PIPE,+                            stderr=subprocess.PIPE,+                            text=True)+    returncode = proc.wait(timeout=GCLOUD_CMD_TIMEOUT_S)","Updated https://docs.python.org/3/library/subprocess.html#subprocess.Popen.communicate.I somehow can't get `tempfile` work, `Popen` refuses to write to a temporary file. It could be a certain problem with macOS, but using `Popen.communicate()` is simpler.",
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/26249,633060846,2021-05-16T08:23:00Z,src/cpp/client/client_context.cc,"@@ -178,4 +180,10 @@ void ClientContext::SetGlobalCallbacks(GlobalCallbacks* client_callbacks) {   g_client_callbacks = client_callbacks; } +bool ClientContext::trailers_only() const {+  return initial_metadata_received_ &&+         recv_initial_metadata_.arr()->count == 0 &&","You're obviously right. My mental model was that for a 0 message, that init metadata and trailing metadata would be pressed together into the trailers, but that would obviously make any init metadata into trailing metadata. It's almost like we should have a DEBUG_ASSERT in place here to specify that if this is true that the array count had better be 0.",
10470658,donnadionne,https://api.github.com/repos/grpc/grpc/pulls/26244,633262276,2021-05-17T06:47:42Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -6858,17 +6858,22 @@ TEST_P(CdsTest, RingHashNoHashPolicy) {   gpr_setenv(""GRPC_XDS_EXPERIMENTAL_ENABLE_RING_HASH"", ""true"");   const double kDistribution50Percent = 0.5;   const double kErrorTolerance = 0.05;+  const uint32_t kRpcTimeoutMs = 5000;","It takes seconds to create the Picker for ring hash policy, and we do that multiple times: idle->connecting, connecting->ready, and for multiple channel changesI0517 06:37:57.427480420      20 ring_hash.cc:343]           donna starting to create the picker: the ringI0517 06:37:59.829846291      20 ring_hash.cc:376]           [RH 0x7b14000014f0 picker 0x7b0c00017e20] created picker from subchannel_list=0x7b7400012200 with 100000 ring entriesSo the very first RPC can timeout",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26244,633638515,2021-05-17T15:32:57Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -6858,17 +6858,22 @@ TEST_P(CdsTest, RingHashNoHashPolicy) {   gpr_setenv(""GRPC_XDS_EXPERIMENTAL_ENABLE_RING_HASH"", ""true"");   const double kDistribution50Percent = 0.5;   const double kErrorTolerance = 0.05;+  const uint32_t kRpcTimeoutMs = 5000;","Woah -- that's surprising.  Why is it taking so long to create the picker?As evidenced by this test, we create new pickers whenever any one individual subchannel changes state, and that can happen pretty frequently, so picker creation really needs to be fast.  If it's taking this long, then we need to find a way to optimize it.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26253,633762970,2021-05-17T18:25:50Z,tools/run_tests/helper_scripts/clean_xds_interop_resources.py,"@@ -0,0 +1,170 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import datetime+import functools+import json+import logging+import os+import re+import subprocess+import sys+from dataclasses import dataclass+from typing import Any, List, Optional++# Parses commandline arguments+parser = argparse.ArgumentParser()+parser.add_argument('--dry_run', action='store_true')+args = parser.parse_args()++# Type alias+Json = Any++# Configures this script+KEEP_PERIOD = datetime.timedelta(hours=24)+COMMAND_TIMEOUT_S = datetime.timedelta(seconds=180).total_seconds()+GCLOUD = os.environ.get('GCLOUD', 'gcloud')+ZONE = 'us-central1-a'+SECONDARY_ZONE = 'us-west1-b'+PROJECT = 'grpc-testing'++ROOT = os.path.abspath(os.path.join(os.path.dirname(sys.argv[0]), '../../..'))+RUN_XDS_TESTS = os.path.join(ROOT, 'tools', 'run_tests', 'run_xds_tests.py')+++@functools.lru_cache()+def get_expire_timestamp():+    return (datetime.datetime.now() - KEEP_PERIOD).isoformat()+++def exec_command(*cmds: List[str], cwd: Optional[str] = None) -> Optional[str]:+    logging.debug('Executing: %s', "" "".join(cmds))+    proc = subprocess.Popen(cmds,+                            stdout=subprocess.PIPE,+                            stderr=subprocess.PIPE,+                            text=True,+                            cwd=cwd)+    # Overcome the potential buffer deadlocking+    # https://docs.python.org/3/library/subprocess.html#subprocess.Popen.communicate+    try:+        outs, errs = proc.communicate(timeout=COMMAND_TIMEOUT_S)+    except subprocess.TimeoutExpired:+        proc.kill()+        outs, errs = proc.communicate()+    if proc.returncode:+        logging.error('> Failed to execute cmd [%s], returned %d, stderr: %s',+                      "" "".join(cmds), proc.returncode, errs)+        return None+    return outs+++def exec_gcloud(*cmds: List[str]) -> Json:+    cmds = [GCLOUD, '--project', PROJECT] + list(cmds)+    if 'list' in cmds:+        # Add arguments to shape the list output+        cmds.extend([+            '--format', 'json', '--filter',+            f'creationTimestamp<={get_expire_timestamp()}'+        ])+    # Executing the gcloud command+    return json.loads(exec_command(*cmds))+++def remove_relative_resources_run_xds_tests(suffix: str):+    """"""Removing GCP resources created by run_xds_tests.py.""""""+    logging.info('Removing run_xds_tests.py resources with suffix [%s]', suffix)+    cmds = [+        sys.executable, RUN_XDS_TESTS, '--clean_only',+        f'--gcp_suffix=""{suffix}""', '--verbose'+    ]+    if args.dry_run:+        logging.debug('> Skipped: %s', "" "".join(cmds))+        return+    exec_command(cmds, cwd=ROOT)+++def remove_relative_resources_psm_sec(prefix: str):+    """"""Removing GCP resources created by PSM Sec framework.""""""+    logging.info('Removing PSM Security resources with prefix [%s]', prefix)+    cmds = [+        sys.executable,+        '-m',+        'bin.run_td_setup',+        '--cmd=cleanup',+        '--flagfile=config/grpc-testing.cfg',+        f'--namespace={prefix}',+        # Following arguments doesn't matter+        '--kube_context=DUMMY_DOES_MATTER',+        '--server_image=DUMMY_DOES_MATTER',+        '--client_image=DUMMY_DOES_MATTER'+    ]+    if args.dry_run:+        logging.debug('> Skipped: %s', "" "".join(cmds))+        return+    exec_command(cmds,+                 cwd=os.path.join(ROOT, 'tools', 'run_tests',+                                  'xds_k8s_test_driver'))+++def check_one_type_of_gcp_resources(list_cmd: List[str],+                                    old_framework_residual_search: str = '',+                                    new_framework_residual_search: str = ''):+    logging.info('Checking GCP resources with %s or %s',+                 old_framework_residual_search, new_framework_residual_search)+    for resource in exec_gcloud(*list_cmd):+        if old_framework_residual_search:+            result = re.search(old_framework_residual_search, resource['name'])+            if result is not None:+                remove_relative_resources_run_xds_tests(result.group(1))+                continue++        if new_framework_residual_search:+            result = re.search(new_framework_residual_search, resource['name'])+            if result is not None:+                remove_relative_resources_psm_sec(result.group(1))+                continue+++def check_costly_gcp_resources() -> None:+    check_one_type_of_gcp_resources(+        ['compute', 'forwarding-rules', 'list'],","Updated to only look for url-maps, and removed unnecessary functions. Thanks for the suggestion!(Our backend service quota is much larger than others, 500.) Emm... the naming format won't be changed frequently in both frameworks. Otherwise, their own clean-up won't work as good.Searching for url-maps can free 10 sets of resources, which is good enough. But if we consider searching for unused backend services, we can release 5 additional backend services.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26230,633795095,2021-05-17T19:09:35Z,tools/run_tests/xds_k8s_test_driver/framework/rpc/grpc_csds.py,"@@ -0,0 +1,59 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""+This contains helpers for gRPC services defined in+https://github.com/envoyproxy/envoy/blob/main/api/envoy/service/status/v3/csds.proto+""""""++import queue+import logging+from typing import Optional, Callable++import grpc+from envoy.service.status.v3 import csds_pb2+from envoy.service.status.v3 import csds_pb2_grpc++# Envoy protos provided by PyPI package xds-protos+# Needs to import the generated Python file to load descriptors+from envoy.extensions.filters.network.http_connection_manager.v3 import http_connection_manager_pb2+from envoy.extensions.filters.common.fault.v3 import fault_pb2+from envoy.extensions.filters.http.fault.v3 import fault_pb2+from envoy.extensions.filters.http.router.v3 import router_pb2","To confirm, is this enough? In my playground java client for CSDS, I had to import a bit more:```import io.envoyproxy.envoy.config.cluster.v3.Cluster;import io.envoyproxy.envoy.config.listener.v3.Listener;import io.envoyproxy.envoy.extensions.filters.http.fault.v3.HTTPFault;import io.envoyproxy.envoy.extensions.filters.http.router.v3.Router;import io.envoyproxy.envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager;import io.envoyproxy.envoy.service.status.v3.ClientStatusDiscoveryServiceGrpc;import io.envoyproxy.envoy.service.status.v3.ClientStatusDiscoveryServiceGrpc.ClientStatusDiscoveryServiceBlockingStub;import io.envoyproxy.envoy.service.status.v3.ClientStatusDiscoveryServiceGrpc.ClientStatusDiscoveryServiceStub;import io.envoyproxy.envoy.service.status.v3.ClientStatusRequest;import io.envoyproxy.envoy.service.status.v3.ClientStatusResponse;import io.envoyproxy.envoy.type.matcher.v3.NodeMatcher;```",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26230,633829115,2021-05-17T20:04:55Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -155,6 +156,20 @@ def assertSuccessfulRpcs(self,             0,             msg=f'Expected all RPCs to succeed: {failed} of {num_rpcs} failed') +    def assertXdsConfigExists(self, test_client: XdsTestClient):+        config = test_client.csds.fetch_client_status()+        self.assertIsNotNone(config)+        seen = set()+        want = frozenset([+            'listener_config', 'cluster_config', 'route_config',+            'endpoint_config'+        ])+        for xds_config in config.xds_config:+            seen.add(xds_config.WhichOneof('per_xds_config'))+        logger.debug('Received xDS config dump: %s',+                     json_format.MessageToJson(config, indent=2))+        self.assertEqual(want, seen)","```suggestion        self.assertSameElements(want, seen)```This one has a bit nicer output. F.e `assertSameElements()` output if `endpoint_config` missing, and `unexpected_config` present:```AssertionError: Expected, but missing:  ['endpoint_config']Unexpected, but present:  ['unexpected_config']```Vs `assertEqual()`:```AssertionError: {'endpoint_config', 'listener_config', 'c[26 chars]fig'} != frozenset({'listener_config', 'unexpected[39 chars]ig'})```",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26255,633864414,2021-05-17T20:57:27Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/traffic_director.py,"@@ -290,6 +292,26 @@ def delete_forwarding_rule(self, force=False):         self.compute.delete_forwarding_rule(name)         self.forwarding_rule = None +    def create_firewall_rule(self, force=False):+        name = self._ns_name(self.FIREWALL_RULE_NAME)+        logging.info('Creating firewall rule ""%s"" in network ""%s""', name,+                     self.network)+        resource = self.compute.create_firewall_rule(+            name, self.network_url, ['35.191.0.0/16', '130.211.0.0/22'],","Let's make IPs configurable via flags, with  `35.191.0.0/16`, `130.211.0.0/22` being the default value. This is needed for staging, where Healthcheck IPs are different.See `DEFINE_list` or `DEFINE_spaceseplist` to accept lists via flags: https://abseil.io/docs/python/guides/flags#flag-types",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26255,633878567,2021-05-17T21:24:02Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/traffic_director.py,"@@ -290,6 +292,26 @@ def delete_forwarding_rule(self, force=False):         self.compute.delete_forwarding_rule(name)         self.forwarding_rule = None +    def create_firewall_rule(self, force=False):","Done. I wasn't sure the reason of using the `force` argument. So far, the naming of GCP resources is relatively fixed.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26255,633920342,2021-05-17T23:02:51Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/traffic_director.py,"@@ -290,6 +292,26 @@ def delete_forwarding_rule(self, force=False):         self.compute.delete_forwarding_rule(name)         self.forwarding_rule = None +    def create_firewall_rule(self, force=False):+        name = self._ns_name(self.FIREWALL_RULE_NAME)+        logging.info('Creating firewall rule ""%s"" in network ""%s""', name,+                     self.network)+        resource = self.compute.create_firewall_rule(+            name, self.network_url, ['35.191.0.0/16', '130.211.0.0/22'],+            ['8080-8100'])",Done. Along with the other comment.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26230,633923783,2021-05-17T23:12:56Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -155,6 +156,22 @@ def assertSuccessfulRpcs(self,             0,             msg=f'Expected all RPCs to succeed: {failed} of {num_rpcs} failed') +    def assertXdsConfigExists(self, test_client: XdsTestClient):+        config = test_client.csds.fetch_client_status(log_level=logging.INFO)+        self.assertIsNotNone(config)+        seen = set()+        want = frozenset([+            'listener_config',+            'cluster_config',+            'route_config',+            'endpoint_config'","> The presence of a trailing comma is also used as **a hint** to our Python code auto-formatter YAPF to direct it to auto-format the container of items to **one item per line** when the , after the final element is present.If the trailing-comma is present, the behavior of `yapf` is okay. But if the trailing-comma is missing, I think `yapf` should be more smart, whether to add the comma and stretch the list, or move the brackets to the ending line.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26255,633927459,2021-05-17T23:23:40Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -33,6 +33,10 @@ from framework.test_app import server_app  logger = logging.getLogger(__name__)+_ENSURE_FIREWALL = flags.DEFINE_bool(",You moved it to traffic_director.py. Take a look at `xds_flags.py`: https://github.com/grpc/grpc/blob/master/tools/run_tests/xds_k8s_test_driver/framework/xds_flags.py.It contains all xds-specific flags (as opposed to https://github.com/grpc/grpc/blob/master/tools/run_tests/xds_k8s_test_driver/framework/xds_flags_k8s.py. which contains xds flags specific to K8S.,X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26255,633929295,2021-05-17T23:27:50Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -265,6 +265,11 @@ def setUp(self):             resource_prefix=self.namespace,             network=self.network) +        # Ensures the firewall exist",This is `SecurityXdsKubernetesTestCase`. You probably meant to add it to the super class: `XdsKubernetesTestCase`.,X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26255,633932954,2021-05-17T23:39:13Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -265,6 +265,11 @@ def setUp(self):             resource_prefix=self.namespace,             network=self.network) +        # Ensures the firewall exist+        if xds_flags.ENSURE_FIREWALL.value:","May I ask you to parse this value in XdsKubernetesTestCase.setUpClass and store it to `cls.ensure_firewall`, similar to how the rest of the flags are handled?This will be useful if we'll be separating `RegularXdsKubernetesTestCase`, `XdsKubernetesTestCase`, `SecurityXdsKubernetesTestCase` to individual files.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26255,633933906,2021-05-17T23:41:58Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -265,6 +265,11 @@ def setUp(self):             resource_prefix=self.namespace,             network=self.network) +        # Ensures the firewall exist","Oooops, in the super class there's no access to `self.td` yet. I guess we'd have to add it both to `SecurityXdsKubernetesTestCase` and `RegularXdsKubernetesTestCase`, after  `self.td` is initialized.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26255,633934958,2021-05-17T23:45:07Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/traffic_director.py,"@@ -290,6 +293,33 @@ def delete_forwarding_rule(self, force=False):         self.compute.delete_forwarding_rule(name)         self.forwarding_rule = None +    def create_firewall_rule(self,+                             force=False,+                             allowed_ports: List[str] = ['8080-8100']):",[Using a mutable default value as an argument](https://docs.quantifiedcode.com/python-anti-patterns/correctness/mutable_default_value_as_argument.html) is an anti-pattern. I think it's fair to not provide the default value here.,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26134,633947927,2021-05-18T00:23:56Z,test/core/surface/BUILD,"@@ -123,6 +123,7 @@ grpc_cc_test(     deps = [         ""//:gpr"",         ""//:grpc"",+        ""//:grpc_authorization_provider"",","@markdroth On adding the APIs to grpc_security.h, and then running the generate_projects script, it updates public_headers_must_be_c89 filehttps://github.com/grpc/grpc/blob/master/test/core/surface/public_headers_must_be_c89.cWithout adding this target here, we get error like""undefined reference to grpc_authorization_policy_provider_static_data_create"" as it cannot locate the definition. Similarly for release API.Since grpc_authorization_provider has dependency on RE2, we may not want to do this.Also, when we add the Setter for GRPC_ARG_AUTHORIZATION_POLICY_PROVIDER in ServerBuilder, we would have to include grpc++_authorization_provider target (which pulls in dependency on RE2) to grpc++_base, something we were trying to avoid.https://github.com/grpc/grpc/blob/master/include/grpcpp/server_builder.hPlease let me know if I am missing anything. It looks like the current BUILD structure may not work.",X
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/26248,633958638,2021-05-18T00:58:12Z,tools/run_tests/run_xds_tests.py,"@@ -3006,13 +3053,7 @@ def __init__(self, compute, alpha_compute, project, project_num):     if args.use_existing_gcp_resources:         logger.info('Reusing existing GCP resources')         get_health_check(gcp, health_check_name)-        try:-            get_health_check_firewall_rule(gcp, firewall_name)-        except googleapiclient.errors.HttpError as http_error:-            # Firewall rule may be auto-deleted periodically depending on GCP-            # project settings.-            logger.exception('Failed to find firewall rule, recreating')-            create_health_check_firewall_rule(gcp, firewall_name)+        get_health_check_firewall_rule(gcp, firewall_name)","I'm thinking maybe we should limit the effect of this change to only `--reuse-resources` and `--testcases=""""`.Add a function to do the `try catch`, and leave the field (e.g. `gcp.health_check_firewall_rule`) unset during exception. `clean_up()` already does checks, so `None`s are handled.We still want this to fail if it's trying to run certain tests reusing the resources, right?",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26255,634752776,2021-05-18T21:14:52Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -185,6 +186,12 @@ def assertAllBackendsReceivedRpcs(self, lb_stats):  class RegularXdsKubernetesTestCase(XdsKubernetesTestCase): +    @classmethod+    def setUpClass(cls):+        super().setUpClass()+        if cls.server_maintenance_port is None:+            cls.server_maintenance_port = cls.server_port","I don't think this change strictly required anymore, but I like it. If we keep it though, it should be```suggestion            cls.server_maintenance_port = server_app.KubernetesServerRunner.DEFAULT_MAINTENANCE_PORT```Or something like `_DEFAULT_MAINTENANCE_PORT`, if storing it as an alias similar to `_DEFAULT_SECURE_MODE_MAINTENANCE_PORT` https://github.com/grpc/grpc/blob/6b4b5a7e5e15a9b01c9eea71ae95385726de062e/tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py#L55-L56",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26255,634753408,2021-05-18T21:16:05Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -195,6 +202,11 @@ def setUp(self):             resource_prefix=self.namespace,             network=self.network) +        # Ensures the firewall exist+        if self.ensure_firewall:+            self.td.create_firewall_rule(+                allowed_ports=xds_flags.FIREWALL_ALLOWED_PORTS.value)","Similar to `self.ensure_firewall`, please store `xds_flags.FIREWALL_ALLOWED_PORTS.value` in a class property.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26255,634792155,2021-05-18T22:25:42Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -185,6 +187,12 @@ def assertAllBackendsReceivedRpcs(self, lb_stats):  class RegularXdsKubernetesTestCase(XdsKubernetesTestCase): +    @classmethod+    def setUpClass(cls):+        super().setUpClass()+        if cls.server_maintenance_port is None:+            cls.server_maintenance_port = server_app.KubernetesServerRunner.DEFAULT_MAINTENANCE_PORT","Yeah... I think today only PyLint complains about line too long (>100), YAPF just formats it in best-effort. Manually folded this line.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26283,634817184,2021-05-18T23:31:10Z,tools/run_tests/run_xds_tests.py,"@@ -2238,6 +2238,35 @@ def test_csds(gcp, original_backend_service, instance_group, server_uri):                        test_csds_timeout_s)  +def maybe_write_sponge_properties():+    """"""Writing test infos to enable more advanced testgrid searches.""""""+    if 'KOKORO_ARTIFACTS_DIR' not in os.environ:+        return++    project_root = os.path.join(os.path.dirname(os.path.abspath(__file__)),+                                '../..')+    git_origin_url = subprocess.getoutput('git -C ""%s"" remote get-url origin' %+                                          project_root)+    git_commit_short = subprocess.getoutput(+        'git -C ""%s"" rev-parse --short HEAD' % project_root)++    properties = [+        # Technically, 'TESTS_FORMAT_VERSION' is not required for run_xds_tests.+        # We keep it here so one day we may merge the process of writing sponge+        # properties.+        'TESTS_FORMAT_VERSION,2',+        'TESTGRID_EXCLUDE,%s' % os.environ.get('TESTGRID_EXCLUDE', 0),+        'GIT_ORIGIN_URL,%s' % git_origin_url,",🤦  why we clone in this way...```+ mkdir -p /var/local/git+ git clone /var/local/jenkins/grpc /var/local/git/grpc```I will find another way to pass this info through.,
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26158,636380042,2021-05-20T18:48:38Z,tools/run_tests/xds_k8s_test_driver/kubernetes-manifests/client-secure.deployment.yaml,"@@ -41,6 +41,10 @@ spec:             value: ""true""           - name: GRPC_XDS_EXPERIMENTAL_V3_SUPPORT             value: ""true""+          - name: GRPC_VERBOSITY+            value: ""DEBUG""+          - name: GRPC_TRACE+            value: ""xds_client,xds_resolver,xds_cluster_manager_lb,cds_lb,xds_cluster_resolver_lb,priority_lb,xds_cluster_impl_lb,weighted_target_lb""","I think it's better to [burn these](https://docs.docker.com/engine/reference/builder/#env) in concrete language-specific docker images. I don't think these are a part of standard contract, and will apply to java/go/nodejs.Here's how I added a language-specific env var in java:https://github.com/grpc/grpc-java/blob/869b395ec02b771b4711aaae04337a63b474be71/buildscripts/xds-k8s/test-client.Dockerfile#L12",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26296,636851545,2021-05-21T11:40:47Z,tools/internal_ci/helper_scripts/delete_leftover_loadtests.sh,"@@ -0,0 +1,34 @@+#!/usr/bin/env bash+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++echo ""BEGIN Listing and deleting leftover tests.""++# Find tests that have running pods and are in Errored state, and delete them.+kubectl get pods --no-headers -o jsonpath='{range .items[*]}{.metadata.ownerReferences[0].name}{"" ""}{.status.phase}{""\n""}{end}' \","1. why do we need this at all? If all loadtest pods reliably self-destruct themselves X minutes after they start (which is something we discussed and it quite easy to achieve), how can we  ever have pods that keep running forever?2. I though each loadtest has a time-to-live set to sth like 24h, after which it gets deleted automatically by the controller. Does this mean that mechanism is not working reliably.3. from the command, I can't tell if this ""cleanup"" phase would potentially also delete pods / loadtests that weren't started by the CI at all, but were actually started manually by someone (e.g. me) when performing a manual experiment. I really wouldn't want the CI job to accidentally delete stuff from my experiment under my hands and I don't see anything in this command that would prevent that from happening.My concern is that by adding this, we are trying to substitute having a reliable benchmarking harness (where under normal circumstances no ""leftover"" jobs should exist imho because of 1 and 2.) by periodically running a cleanup script that IMHO can do more harm than good (e.g. delete people's failed experiments under their hands before they have a chance to debug what went wrong. Or it can simply mask that some components in our harness aren't working reliably).",X
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/26296,637035197,2021-05-21T15:59:53Z,tools/internal_ci/helper_scripts/delete_leftover_loadtests.sh,"@@ -0,0 +1,34 @@+#!/usr/bin/env bash+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++echo ""BEGIN Listing and deleting leftover tests.""++# Find tests that have running pods and are in Errored state, and delete them.+kubectl get pods --no-headers -o jsonpath='{range .items[*]}{.metadata.ownerReferences[0].name}{"" ""}{.status.phase}{""\n""}{end}' \","Changed the PR to list rather than delete the pods in this condition.Listing the pods is useful in the long term to monitor for issues (for instance, if the ""self-destruct"" mechanism doesn't work).Deleting the pods would have been useful as a workaround, to keep the continuous runs from failing while we put in place and test a mechanism to achieve this ""self-destruct"" (not only for the driver but for the unresponsive workers, which may well turn out to be unresponsive to the self-destruct signal as well).Since we are no longer deleting the pods, we have two options to keep the lights on in the meantime:a. Disable the offending test. I am sending out https://github.com/grpc/grpc/pull/26339 to achieve that.b. Increase the capacity of our node pools to support 20 simultaneous tests instead of 8 (we currently run only 4 tests simultaneously on the two kokoro jobs, and the other 4 were set aside for issues such as this – too few, as it turns out).The automatic deletion of tests (and pods) after 24h works reliably. The timeout of 15 minutes is also respected by the test, which marks itself as Errored after this time. The problem is in the interaction of the driver and workers, in the particular case where a worker becomes unresponsive. The driver sends messages to the workers telling them to quit. If the workers are unresponsive, they keep running. The driver appears to wait for a response forever, and not quit either, so the driver and worker pods stay in Running state forever (or until they are deleted, at the end of 24 hours).I will apply option (b) if option (a) is not preferred. We will then work on the implementation of the ""self-destruct"".In the meantime, I would like to merge this PR to enable monitoring in case other tests misbehave.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26058,637137944,2021-05-21T18:46:39Z,src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi,"@@ -12,10 +12,23 @@ # See the License for the specific language governing permissions and # limitations under the License. +cimport cpython++import threading+import time  cdef int _INTERRUPT_CHECK_PERIOD_MS = 200  +cdef grpc_event _grpc_completion_queue_next(grpc_completion_queue *cq,+                                            gpr_timespec deadline,+                                            void *reserved) except *:+    # Wrap the call to the C++ core in an 'except *' helper to propagate any+    # errors raised by the gevent poller callback+    with nogil:+      return grpc_completion_queue_next(cq, deadline, reserved)","I played around the client. The QPS is much lower than my expectation, it should be 5k-7k for unary, 1 thread, gevent. Anyway, the old/new result demonstrated a 4% QPS regression (753.2 to 723.3).Can you explain more about why we need `except *` to catch the exception, instead of using `PyErr_CheckSignals`?",
67916678,beandrad,https://api.github.com/repos/grpc/grpc/pulls/26058,637187911,2021-05-21T19:45:27Z,src/python/grpcio_tests/tests_gevent/unit/close_channel_test.py,"@@ -0,0 +1,92 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import unittest+from concurrent import futures+from src.proto.grpc.testing import empty_pb2, messages_pb2, test_pb2_grpc+import grpc+import gevent+import sys+from gevent.pool import Group+from tests_gevent.unit._test_server import start_test_server, UNARY_CALL_WITH_SLEEP_VALUE++_UNARY_CALL_METHOD_WITH_SLEEP = '/grpc.testing.TestService/UnaryCallWithSleep'+++class CloseChannelTest(unittest.TestCase):++    def setUp(self):+        self._server_target, self._server = start_test_server()+        self._channel = grpc.insecure_channel(self._server_target)+        self._channel_closed = False+        self._unhandled_greenlet_exit = False+        sys.excepthook = self._global_exception_handler++    def tearDown(self):+        self._server.stop(None)+        self._channel.close()++    def test_graceful_close(self):+        UnaryCallWithSleep = self._channel.unary_unary(+            _UNARY_CALL_METHOD_WITH_SLEEP,+            request_serializer=messages_pb2.SimpleRequest.SerializeToString,+            response_deserializer=messages_pb2.SimpleResponse.FromString,+        )+        _, response = UnaryCallWithSleep.with_call(messages_pb2.SimpleRequest())+        self._channel.close()++        self.assertEqual(grpc.StatusCode.OK, response.code())++    def test_graceful_close_in_greenlet(self):+        group = Group()+        greenlet = group.spawn(self._run_client)+        gevent.sleep(UNARY_CALL_WITH_SLEEP_VALUE * 2)+        self._channel_closed = True+        gevent.sleep(UNARY_CALL_WITH_SLEEP_VALUE)+        self._channel.close()+        group.killone(greenlet)+        self.assertFalse(self._unhandled_greenlet_exit,+                         ""Unhandled GreenletExit"")++    def test_ungraceful_close_in_greenlet(self):+        group = Group()+        greenlet = group.spawn(self._run_client)+        gevent.sleep(UNARY_CALL_WITH_SLEEP_VALUE * 2)+        group.killone(greenlet)+        gevent.sleep(UNARY_CALL_WITH_SLEEP_VALUE)+        self.assertFalse(self._unhandled_greenlet_exit,+                         ""Unhandled GreenletExit"")++    def _run_client(self):+        UnaryCallWithSleep = self._channel.unary_unary(+            _UNARY_CALL_METHOD_WITH_SLEEP,+            request_serializer=messages_pb2.SimpleRequest.SerializeToString,+            response_deserializer=messages_pb2.SimpleResponse.FromString,+        )+        while not self._channel_closed:","`_channel_closed` is just used so that requests are not sent once the channel is closed, but tbh it's not needed (or even sending the request). The purpose of this test is to check whether the `gevent` exception (`GreenletExit`) that kills the greenlet is handled in the event poller. ",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26058,637198930,2021-05-21T20:00:09Z,src/python/grpcio_tests/tests_gevent/unit/close_channel_test.py,"@@ -0,0 +1,92 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import unittest+from concurrent import futures+from src.proto.grpc.testing import empty_pb2, messages_pb2, test_pb2_grpc+import grpc+import gevent+import sys+from gevent.pool import Group+from tests_gevent.unit._test_server import start_test_server, UNARY_CALL_WITH_SLEEP_VALUE++_UNARY_CALL_METHOD_WITH_SLEEP = '/grpc.testing.TestService/UnaryCallWithSleep'+++class CloseChannelTest(unittest.TestCase):++    def setUp(self):+        self._server_target, self._server = start_test_server()+        self._channel = grpc.insecure_channel(self._server_target)+        self._channel_closed = False+        self._unhandled_greenlet_exit = False+        sys.excepthook = self._global_exception_handler++    def tearDown(self):+        self._server.stop(None)+        self._channel.close()++    def test_graceful_close(self):+        UnaryCallWithSleep = self._channel.unary_unary(+            _UNARY_CALL_METHOD_WITH_SLEEP,+            request_serializer=messages_pb2.SimpleRequest.SerializeToString,+            response_deserializer=messages_pb2.SimpleResponse.FromString,+        )+        _, response = UnaryCallWithSleep.with_call(messages_pb2.SimpleRequest())+        self._channel.close()++        self.assertEqual(grpc.StatusCode.OK, response.code())++    def test_graceful_close_in_greenlet(self):+        group = Group()+        greenlet = group.spawn(self._run_client)+        gevent.sleep(UNARY_CALL_WITH_SLEEP_VALUE * 2)+        self._channel_closed = True+        gevent.sleep(UNARY_CALL_WITH_SLEEP_VALUE)+        self._channel.close()+        group.killone(greenlet)+        self.assertFalse(self._unhandled_greenlet_exit,+                         ""Unhandled GreenletExit"")++    def test_ungraceful_close_in_greenlet(self):+        group = Group()+        greenlet = group.spawn(self._run_client)+        gevent.sleep(UNARY_CALL_WITH_SLEEP_VALUE * 2)+        group.killone(greenlet)+        gevent.sleep(UNARY_CALL_WITH_SLEEP_VALUE)+        self.assertFalse(self._unhandled_greenlet_exit,+                         ""Unhandled GreenletExit"")++    def _run_client(self):+        UnaryCallWithSleep = self._channel.unary_unary(+            _UNARY_CALL_METHOD_WITH_SLEEP,+            request_serializer=messages_pb2.SimpleRequest.SerializeToString,+            response_deserializer=messages_pb2.SimpleResponse.FromString,+        )+        while not self._channel_closed:","Sorry for the nitpicking: Sleep + bool synchronization can be confusing and flaky in the long term. If possible, can we switch to a more deterministic logic and remove the sleep statements?E.g., in `test_graceful_close` and `test_graceful_close_in_greenlet`, we expect the RPC to finish, we can use a simple unary call without sleep instead. For `test_ungraceful_close_in_greenlet`, we don't expect the RPC to finish, so the server method can sleep forever.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26058,637199873,2021-05-21T20:01:58Z,src/python/grpcio_tests/tests_gevent/unit/close_channel_test.py,"@@ -0,0 +1,92 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import unittest+from concurrent import futures+from src.proto.grpc.testing import empty_pb2, messages_pb2, test_pb2_grpc+import grpc+import gevent+import sys+from gevent.pool import Group+from tests_gevent.unit._test_server import start_test_server, UNARY_CALL_WITH_SLEEP_VALUE++_UNARY_CALL_METHOD_WITH_SLEEP = '/grpc.testing.TestService/UnaryCallWithSleep'+++class CloseChannelTest(unittest.TestCase):","This is a great test that checks the behavior around `GreenletExit`, which is a special exception. Can we try to inject a normal Python exception, and ensure it will populate to application?",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/26340,637295996,2021-05-21T23:25:53Z,src/python/grpcio/grpc/_cython/_cygrpc/credentials.pyx.pxi,"@@ -58,7 +58,10 @@ cdef int _get_metadata(void *state,         cb(user_data, NULL, 0, status, c_error_details)   args = context.service_url, context.method_name, callback,   plugin = <object>state-  plugin._stored_ctx.run(_spawn_callback_async, plugin, args)+  if plugin._stored_ctx is not None:","Can you explain how that would happen? `plugin` is an instance of [`grpc._plugin_wrapping._Plugin`](https://github.com/grpc/grpc/blob/9dabaecedabf36336c366fd62b138dd17951a357/src/python/grpcio/grpc/_plugin_wrapping.py#L67), not of [`grpc.AuthMetadataPlugin`](https://github.com/grpc/grpc/blob/9dabaecedabf36336c366fd62b138dd17951a357/src/python/grpcio/grpc/__init__.py#L613). I don't see any way for user customizations to affect the presence of this field.",X
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/26248,637376509,2021-05-22T08:52:41Z,tools/run_tests/run_xds_tests.py,"@@ -2541,62 +2541,96 @@ def create_global_forwarding_rule(gcp,   def get_health_check(gcp, health_check_name):-    result = gcp.compute.healthChecks().get(-        project=gcp.project, healthCheck=health_check_name).execute()-    gcp.health_check = GcpResource(health_check_name, result['selfLink'])+    try:+        result = gcp.compute.healthChecks().get(+            project=gcp.project, healthCheck=health_check_name).execute()+        gcp.health_check = GcpResource(health_check_name, result['selfLink'])+    except Exception as e:+        gcp.errors.append(e)+        gcp.health_check = GcpResource(health_check_name, None)   def get_health_check_firewall_rule(gcp, firewall_name):-    result = gcp.compute.firewalls().get(project=gcp.project,-                                         firewall=firewall_name).execute()-    gcp.health_check_firewall_rule = GcpResource(firewall_name,-                                                 result['selfLink'])+    try:+        result = gcp.compute.firewalls().get(project=gcp.project,+                                             firewall=firewall_name).execute()+        gcp.health_check_firewall_rule = GcpResource(firewall_name,+                                                     result['selfLink'])","I can add this back in. But, in my recent experience, the automatic deletion is now frequent enough that long-running tests (many of these test cases take >10 minutes) will almost certainly fail due to the firewall rule being deleted mid-run. So this only really helps with short test cases like ping pong, and seemed likely to lead to confusion when manually running the longer test cases.",X
67916678,beandrad,https://api.github.com/repos/grpc/grpc/pulls/26058,637444697,2021-05-22T19:57:36Z,src/python/grpcio_tests/tests_gevent/unit/close_channel_test.py,"@@ -0,0 +1,92 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import unittest+from concurrent import futures+from src.proto.grpc.testing import empty_pb2, messages_pb2, test_pb2_grpc+import grpc+import gevent+import sys+from gevent.pool import Group+from tests_gevent.unit._test_server import start_test_server, UNARY_CALL_WITH_SLEEP_VALUE++_UNARY_CALL_METHOD_WITH_SLEEP = '/grpc.testing.TestService/UnaryCallWithSleep'+++class CloseChannelTest(unittest.TestCase):",I added `test_kill_greenlet_with_generic_exception`; it kills the greenlet with `Exception` instead of using the default `GreenletExit` and checks whether the greenlet returns the corresponding exception.,
67916678,beandrad,https://api.github.com/repos/grpc/grpc/pulls/26058,637446135,2021-05-22T20:13:08Z,src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi,"@@ -12,10 +12,23 @@ # See the License for the specific language governing permissions and # limitations under the License. +cimport cpython++import threading+import time  cdef int _INTERRUPT_CHECK_PERIOD_MS = 200  +cdef grpc_event _grpc_completion_queue_next(grpc_completion_queue *cq,+                                            gpr_timespec deadline,+                                            void *reserved) except *:+    # Wrap the call to the C++ core in an 'except *' helper to propagate any+    # errors raised by the gevent poller callback+    with nogil:+      return grpc_completion_queue_next(cq, deadline, reserved)","Otherwise the loop in [`_next()`](https://github.com/grpc/grpc/blob/99ac3cb03340a5dd4407bd696d6d5176b1d7eac7/src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi#L39) never breaks. Another solution is https://github.com/grpc/grpc/pull/26058/commits/b82a655e97af08120627f950cbed49c2449b239b, but I'm not sure if it'd have any side effect. ",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26331,638069293,2021-05-24T15:51:00Z,src/core/ext/filters/client_channel/http_proxy.cc,"@@ -107,6 +107,17 @@ char* GetHttpProxyServer(const grpc_channel_args* args, char** user_cred) {   return proxy_name; } +// Adds the default port (443) if target does not contain a port.+std::string MaybeAddDefaultPort(absl::string_view target) {+  absl::string_view host;+  absl::string_view port;+  SplitHostPort(target, &host, &port);+  if (port.empty()) {+    return JoinHostPort(host, 443);","I'd ideally prefer not to hard-code the default this way, independently of the default in the DNS resolver that specifies the same thing.  But unfortunately, I don't have a better suggestion for how to handle this, so I guess we can live with it.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26331,638070054,2021-05-24T15:52:09Z,src/core/ext/filters/client_channel/http_proxy.cc,"@@ -176,7 +187,8 @@ class HttpProxyMapper : public ProxyMapperInterface {     grpc_arg args_to_add[2];     args_to_add[0] = grpc_channel_arg_string_create(         const_cast<char*>(GRPC_ARG_HTTP_CONNECT_SERVER),-        const_cast<char*>(absl::StripPrefix(uri->path(), ""/"").data()));+        const_cast<char*>(+            MaybeAddDefaultPort(absl::StripPrefix(uri->path(), ""/"")).data()));",Please use `c_str()` instead of `data()` here.  (Pre-existing bug.),
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26315,638143977,2021-05-24T17:34:45Z,src/python/grpcio_testing/grpc_testing/BUILD.bazel,"@@ -0,0 +1,25 @@+package(default_visibility = [""//visibility:public""])++py_library(+    name = ""common"",+    srcs = [""_common.py""],+)++py_library(+    name = ""time"",+    srcs = [""_time.py""],+)++py_library(+    name = ""grpcio_testing"",+    srcs = [""__init__.py""],",I know creating a library for each file is more ideal for Bazel. But here we could use a glob to reduce future maintenance need. WDYT?CC @gnossen,
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/26331,638203160,2021-05-24T18:53:53Z,src/core/ext/filters/client_channel/http_proxy.cc,"@@ -176,7 +187,8 @@ class HttpProxyMapper : public ProxyMapperInterface {     grpc_arg args_to_add[2];     args_to_add[0] = grpc_channel_arg_string_create(         const_cast<char*>(GRPC_ARG_HTTP_CONNECT_SERVER),-        const_cast<char*>(absl::StripPrefix(uri->path(), ""/"").data()));+        const_cast<char*>(+            MaybeAddDefaultPort(absl::StripPrefix(uri->path(), ""/"")).data()));",starting c++ 11 c_str() and data() do the same thing https://en.cppreference.com/w/cpp/string/basic_string/data,
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26344,638209207,2021-05-24T19:04:43Z,tools/run_tests/xds_k8s_test_driver/README.md,"@@ -125,22 +124,199 @@ python3 -m tests.baseline_test \   --server_xds_port=""$(($RANDOM%1000 + 34567))"" ``` +## Local development+This test driver allows running tests locally against remote GKE clusters, right+from your dev environment. You need:++1. Follow [installation](#installation) instructions+2. Authenticated `gcloud`+3. `kubectl` context (see [Configure GKE cluster access](#configure-gke-cluster-access))+4. Run tests with `--debug_use_port_forwarding` argument. The test driver +   will automatically start and stop port forwarding using+   `kubectl` subprocesses. (experimental)+ ### Setup test configuration  There are many arguments to be passed into the test run. You can save the-arguments to a config file for your development environment. Please take a look-at-https://github.com/grpc/grpc/blob/master/tools/run_tests/xds_k8s_test_driver/config/local-dev.cfg.example.-You can create your own config by:+arguments to a config file (""flagfile"") for your development environment.+Use [`config/local-dev.cfg.example`](https://github.com/grpc/grpc/blob/master/tools/run_tests/xds_k8s_test_driver/config/local-dev.cfg.example)+as a starting point:  ```shell cp config/local-dev.cfg.example config/local-dev.cfg ``` -### Clean-up resources+Learn more about flagfiles in [abseil documentation](https://abseil.io/docs/python/guides/flags#a-note-about---flagfile).++### Helper scripts+You can use interop xds-k8s [`bin/`](https://github.com/grpc/grpc/tree/master/tools/run_tests/xds_k8s_test_driver/bin)+scripts to configure TD, start k8s instances step-by-step, and keep them alive+for as long as you need. ++* To run helper scripts using local config:+  * `python -m bin.script_name --flagfile=config/local-dev.cfg`+  * `./run.sh bin/script_name.py` automatically appends the flagfile+* Use `--help` to see script-specific argument+* Use `--helpfull` to see all available argument++#### Overview+```shell+# Helper tool to configure Traffic Director with different security options+python -m bin.run_td_setup --help++# Helper tools to run the test server, client (with or without security)+python -m bin.run_test_server --help+python -m bin.run_test_client --help++# Helper tool to verify different security configurations via channelz+python -m bin.run_channelz --help+```++#### `./run.sh` helper+Use `./run.sh` to execute helper scripts and tests with `config/local-dev.cfg`.","Ah. I was thinking about `local-dev.cfg.example` more as a minimal setup needed to hit the ground running. The idea is that you copy the file, change a couple of values, and it's ready to go. I want this to contain essentials + flags specific to local dev + flags likely to change.I don't think it'll make sense to list all available flags in this file:1. `--helpfull` already does it2. Lower user entry barrier for the first setup3. It's easy to get out of sync",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26331,638261057,2021-05-24T20:47:38Z,src/core/ext/filters/client_channel/http_proxy.cc,"@@ -107,6 +107,17 @@ char* GetHttpProxyServer(const grpc_channel_args* args, char** user_cred) {   return proxy_name; } +// Adds the default port (443) if target does not contain a port.+std::string MaybeAddDefaultPort(absl::string_view target) {+  absl::string_view host;+  absl::string_view port;+  SplitHostPort(target, &host, &port);+  if (port.empty()) {+    return JoinHostPort(host, 443);",That sounds fine.  It's already defined independently in both of the DNS resolver implementations:https://github.com/grpc/grpc/blob/c97da0fc25b9bd8d1eead7b76d7a8a1b60625956/src/core/ext/filters/client_channel/resolver/dns/native/dns_resolver.cc#L51https://github.com/grpc/grpc/blob/c97da0fc25b9bd8d1eead7b76d7a8a1b60625956/src/core/ext/filters/client_channel/resolver/dns/c_ares/dns_resolver_ares.cc#L63,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26296,638556467,2021-05-25T08:10:38Z,tools/internal_ci/helper_scripts/delete_leftover_loadtests.sh,"@@ -0,0 +1,34 @@+#!/usr/bin/env bash+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++echo ""BEGIN Listing and deleting leftover tests.""++# Find tests that have running pods and are in Errored state, and delete them.+kubectl get pods --no-headers -o jsonpath='{range .items[*]}{.metadata.ownerReferences[0].name}{"" ""}{.status.phase}{""\n""}{end}' \","I don't see why the ""self-destruct"" mechanism would be unreliable if done right.e.g. when starting the pod, before you start any driver or worker process, you can start a background shell script that waits for given timeout and after that emits a `kill -9` (SIGKILL) to hard kill the driver / worker process. I have hard time imagining circumstances where kill -9 wouldn't work (it certainly would work for killing qps_worker or driver regardless of how ""badly"" they got stuck).The mechanism needs to be in place for all the pods we run (not just the driver, but also the workers - the workers are in a sense more likely to get stuck since they are component that gets stressed the most when the benchmark runs).",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26356,639099149,2021-05-25T18:48:22Z,src/core/ext/filters/client_channel/resolver/xds/xds_resolver.cc,"@@ -649,10 +652,13 @@ ConfigSelector::CallConfig XdsResolver::XdsConfigSelector::GetCallConfig(         case XdsApi::Route::HashPolicy::HEADER:           new_hash = HeaderHashHelper(hash_policy, args.initial_metadata);           break;-        case XdsApi::Route::HashPolicy::CHANNEL_ID:-          new_hash =-              static_cast<uint64_t>(reinterpret_cast<uintptr_t>(resolver));+        case XdsApi::Route::HashPolicy::CHANNEL_ID: {+          std::string address_str = absl::StrFormat(","But this change just maps that to another fixed number, so it will still be just as deterministic; it will just always be a number in the middle of the ring instead of always being a number at the start of the ring.  I don't see how that helps.What actually breaks if you just use the address directly?  Do tests fail?If what we need here is more randomization, then maybe an alternative would be to just use a random number the same way that we're doing when there is no hash policy (i.e., calling `rand()` twice, once for high-order bits and once for low-order bits) and storing that value in the resolver so that we use the same value for all RPCs with this hash policy.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26367,640842410,2021-05-27T17:51:04Z,tools/run_tests/xds_k8s_test_driver/framework/rpc/grpc_testing.py,"@@ -16,11 +16,11 @@ https://github.com/grpc/grpc/blob/master/src/proto/grpc/testing/test.proto """""" import logging-from typing import Optional+from typing import Optional, Iterable, Tuple  import grpc -import framework.rpc+from framework.rpc.grpc import GrpcClientHelper","AFAIK this is against google style guide:https://google.github.io/styleguide/pyguide.html#22-imports> Use import statements for packages and modules only, not for individual classes or functions. Imports from the typing module, typing_extensions module, and the six.moves module are exempt from this rule.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26367,640848496,2021-05-27T17:59:43Z,tools/run_tests/xds_k8s_test_driver/framework/test_app/server_app.py,"@@ -266,7 +268,8 @@ def run(self,                 pod, remote_port=maintenance_port)             rpc_host = self.k8s_namespace.PORT_FORWARD_LOCAL_ADDRESS -        return XdsTestServer(ip=pod_ip,+        return XdsTestServer(name=pod.metadata.name,","`XdsTestServer` is supposed to represent [xds test server](https://github.com/grpc/grpc/blob/master/doc/xds-test-descriptions.md#server) available via remote connection. It shouldn't be coupled to a specific platform running it. `pod.metadata.name` is specific to k8s.However, I'm not opposed to adding something like this. Maybe hostname?",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26367,640858073,2021-05-27T18:08:36Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_testcase.py,"@@ -0,0 +1,590 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""A test framework built for urlMap related xDS test cases.""""""++import abc+import unittest+import json+import time+import inspect+import threading+import functools+from typing import Any, Union, Tuple, Mapping, Iterable, Optional++from framework import xds_flags+from framework import xds_k8s_flags+from framework.infrastructure import gcp+from framework.infrastructure import k8s+from framework.infrastructure import traffic_director+from framework.test_app import client_app+from framework.test_app import server_app++from absl import flags+from absl import logging+from absl.testing import absltest+from google.protobuf import json_format+import googleapiclient+from kubernetes.utils.create_from_yaml import FailToCreateError++logging.set_verbosity(logging.DEBUG)+flags.adopt_module_key_flags(xds_flags)+flags.adopt_module_key_flags(xds_k8s_flags)++_STRATEGY = flags.DEFINE_enum('strategy',+                              default='create',+                              enum_values=['create', 'keep', 'reuse'],+                              help='Strategy of GCP resources management')+# TODO(lidiz) find a better way to filter test cases",see `-k` flag https://docs.python.org/3/library/unittest.html#cmdoption-unittest-k,
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26367,640864866,2021-05-27T18:19:24Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_testcase.py,"@@ -0,0 +1,590 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""A test framework built for urlMap related xDS test cases.""""""++import abc+import unittest+import json+import time+import inspect+import threading+import functools+from typing import Any, Union, Tuple, Mapping, Iterable, Optional++from framework import xds_flags+from framework import xds_k8s_flags+from framework.infrastructure import gcp+from framework.infrastructure import k8s+from framework.infrastructure import traffic_director+from framework.test_app import client_app+from framework.test_app import server_app++from absl import flags+from absl import logging+from absl.testing import absltest+from google.protobuf import json_format+import googleapiclient+from kubernetes.utils.create_from_yaml import FailToCreateError++logging.set_verbosity(logging.DEBUG)+flags.adopt_module_key_flags(xds_flags)+flags.adopt_module_key_flags(xds_k8s_flags)++_STRATEGY = flags.DEFINE_enum('strategy',+                              default='create',+                              enum_values=['create', 'keep', 'reuse'],+                              help='Strategy of GCP resources management')+# TODO(lidiz) find a better way to filter test cases+_TEST_FILTER = flags.DEFINE_string(+    'test_filter',+    default='',+    help='Only run test case which match the given substring')++_URL_MAP_PROPAGATE_TIMEOUT_SEC = 600+_URL_MAP_PROPAGATE_CHECK_INTERVAL_SEC = 2+_K8S_NAMESPACE_DELETE_DEADLINE_SEC = 600+_K8S_NAMESPACE_DELETE_CHECK_INTERVAL_SEC = 10++_RPC_DISTRIBUTION_READY_TIMEOUT_SEC = 600++_url_map_change_aggregator = None+_gcp_resource_manager = None++# Type aliases+JsonType = Any+HostRule = JsonType+PathMatcher = JsonType++RpcTypeUnaryCall = 'UNARY_CALL'+RpcTypeEmptyCall = 'EMPTY_CALL'+++def _split_camel(s: str, delimiter: str = '-') -> str:+    """"""Turn camel case name to snake-case-like name.""""""+    return ''.join([delimiter + c.lower() if c.isupper() else c for c in s+                   ]).lstrip(delimiter)+++def _ensure_k8s_namespace_removed(namespace: k8s.KubernetesNamespace):+    try:+        # Start deletion+        namespace.delete(0)+    except Exception as e:+        # Maybe the namespace doesn't exist at all, that's okay+        logging.info('Namespace deletion failed with %s: %s', type(e), e)+    # The namespace enters Terminating state, which lasts minutes.+    deadline = time.time() + _K8S_NAMESPACE_DELETE_DEADLINE_SEC+    while time.time() < deadline:+        if namespace.get() is None:+            break+        logging.info('K8s namespace ""%s"" still exists',+                     xds_flags.NAMESPACE.value)+        time.sleep(_K8S_NAMESPACE_DELETE_CHECK_INTERVAL_SEC)+    # Deletion failed with timeout+    if time.time() >= deadline:+        raise RuntimeError(+            'failed to delete K8s namespace ""%s"" within %s seconds',+            xds_flags.NAMESPACE.value, _K8S_NAMESPACE_DELETE_DEADLINE_SEC)+    # Deletion succeed!+    logging.info('K8s namespace ""%s"" deleted', xds_flags.NAMESPACE.value)+++class GcpResourceManager:","I assume this will move to its own file, similar to TrafficDirectorManager.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26367,640866808,2021-05-27T18:22:17Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/compute.py,"@@ -181,6 +181,9 @@ def create_url_map(                 }],             }) +    def create_url_map_with_content(self, url_map: Any) -> GcpResource:",Head's up: I'm working on a big update for all classes in infrastructure.gcp. A lot of renames and logic changes.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26367,640879144,2021-05-27T18:41:48Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_testcase.py,"@@ -0,0 +1,590 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""A test framework built for urlMap related xDS test cases.""""""++import abc+import unittest+import json+import time+import inspect+import threading+import functools+from typing import Any, Union, Tuple, Mapping, Iterable, Optional++from framework import xds_flags+from framework import xds_k8s_flags+from framework.infrastructure import gcp+from framework.infrastructure import k8s+from framework.infrastructure import traffic_director+from framework.test_app import client_app+from framework.test_app import server_app++from absl import flags+from absl import logging+from absl.testing import absltest+from google.protobuf import json_format+import googleapiclient+from kubernetes.utils.create_from_yaml import FailToCreateError++logging.set_verbosity(logging.DEBUG)+flags.adopt_module_key_flags(xds_flags)+flags.adopt_module_key_flags(xds_k8s_flags)++_STRATEGY = flags.DEFINE_enum('strategy',+                              default='create',+                              enum_values=['create', 'keep', 'reuse'],+                              help='Strategy of GCP resources management')+# TODO(lidiz) find a better way to filter test cases","A large part of testing behavior is controlled by flags, but `unittest` command itself doesn't allow command line arguments. And `absltest` is built for Bazel, many functionalities, like test loading, test filtering, are controlled by Bazel flags. If we instrument these tests with Bazel, we will get them for free.Technically, we can hack the arguments, like hard code `args` or build a portable from environment var to absl flags. That doesn't feel ideal.Please let me know if you think we should support the `unittest` command.```usage: python3 -m unittest [-h] [-v] [-q] [--locals] [-f] [-c] [-b] [-k TESTNAMEPATTERNS] [tests ...]positional arguments:  tests                a list of any number of test modules, classes and test methods.optional arguments:  -h, --help           show this help message and exit  -v, --verbose        Verbose output  -q, --quiet          Quiet output  --locals             Show local variables in tracebacks  -f, --failfast       Stop on first fail or error  -c, --catch          Catch Ctrl-C and display results so far  -b, --buffer         Buffer stdout and stderr during tests  -k TESTNAMEPATTERNS  Only run tests which match the given substringExamples:  python3 -m unittest test_module               - run tests from test_module  python3 -m unittest module.TestClass          - run tests from module.TestClass  python3 -m unittest module.Class.test_method  - run specified test method  python3 -m unittest path/to/test_file.py      - run tests from test_file.pyusage: python3 -m unittest discover [-h] [-v] [-q] [--locals] [-f] [-c] [-b] [-k TESTNAMEPATTERNS] [-s START] [-p PATTERN] [-t TOP]optional arguments:  -h, --help            show this help message and exit  -v, --verbose         Verbose output  -q, --quiet           Quiet output  --locals              Show local variables in tracebacks  -f, --failfast        Stop on first fail or error  -c, --catch           Catch Ctrl-C and display results so far  -b, --buffer          Buffer stdout and stderr during tests  -k TESTNAMEPATTERNS   Only run tests which match the given substring  -s START, --start-directory START                        Directory to start discovery ('.' default)  -p PATTERN, --pattern PATTERN                        Pattern to match tests ('test*.py' default)  -t TOP, --top-level-directory TOP                        Top level directory of project (defaults to start directory)```",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26405,643522808,2021-06-01T22:25:31Z,src/python/grpcio_tests/tests/unit/_contextvars_propagation_test.py,"@@ -112,6 +114,46 @@ def test_propagation_to_auth_plugin(self):                 response = stub(_REQUEST, wait_for_ready=True)                 self.assertEqual(_REQUEST, response) +    def test_concurrent_propagation(self):+        _THREAD_COUNT = 32+        _RPC_COUNT = 32++        set_up_expected_context()+        with _server() as port:+            target = ""localhost:{}"".format(port)+            local_credentials = grpc.local_channel_credentials()+            test_call_credentials = TestCallCredentials()+            call_credentials = grpc.metadata_call_credentials(+                test_call_credentials, ""test call credentials"")+            composite_credentials = grpc.composite_channel_credentials(+                local_credentials, call_credentials)+            start_event = threading.Event()++            def _run_on_thread(exception_queue):+                try:+                    for _ in range(_THREAD_COUNT):+                        with grpc.secure_channel(+                                target, composite_credentials) as channel:+                            start_event.wait()","We may want a stronger synchronization here. The worker threads might be created by the testing thread, but the channel may not be created yet, so it might not be truly concurrent here.E.g., we could have some counter, or list of events.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/26398,643561780,2021-06-02T00:02:20Z,tools/run_tests/performance/loadtest_template.py,"@@ -81,42 +101,56 @@ def loadtest_template(                     input_file_name, input_config.get('kind')))              for client in input_config['spec']['clients']:-                if client['language'] in client_languages:-                    continue+                del client['name']                 if inject_client_pool:                     client['pool'] = '${client_pool}'-                clients.append(client)-                client_languages.add(client['language'])+                if client['language'] not in clientmap:+                    clientmap[client['language']] = []+                insert_worker(client, clientmap[client['language']])              for server in input_config['spec']['servers']:-                if server['language'] in server_languages:-                    continue+                del server['name']                 if inject_server_pool:                     server['pool'] = '${server_pool}'-                servers.append(server)-                server_languages.add(server['language'])+                if server['language'] not in servermap:+                    servermap[server['language']] = []+                insert_worker(server, servermap[server['language']])              input_spec = input_config['spec']             del input_spec['clients']             del input_spec['servers']             del input_spec['scenariosJSON']             spec.update(input_config['spec']) -    clients.sort(key=lambda x: x['language'])-    servers.sort(key=lambda x: x['language'])+    uniquify_workers(clientmap)+    uniquify_workers(servermap)      spec.update({-        'clients': clients,-        'servers': servers,+        'clients':+            sum((clientmap[language] for language in sorted(clientmap)),","That's a clever way of flattening a list of lists! FWIW, I don't think it's terribly efficient, but this is not performance-sensitive code.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/26398,643565704,2021-06-02T00:14:34Z,tools/run_tests/performance/loadtest_examples.sh,"@@ -0,0 +1,115 @@+#!/bin/bash+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# This script generates a set of load test examples from templates.++LOADTEST_CONFIG=tools/run_tests/performance/loadtest_config.py++if (( $# < 1 )); then+    echo ""Usage: ${0} <output directory>""+    exit 1+fi++if [[ ! -x ""${LOADTEST_CONFIG}"" ]]; then+    echo ""${LOADTEST_CONFIG} not found.""+    exit 1+fi++outputbasedir=""${1}""++mkdir -p ""${outputbasedir}/templates""++example_file() {+    local scenario=""${1}""+    local suffix=""${2}""+    if [[ ""${scenario#cpp_}"" != ""${scenario}"" ]]; then+        echo ""cxx${suffix}""+        return+    fi+    if [[ ""${scenario#python_asyncio_}"" != ""${scenario}"" ]]; then+        echo ""python_asyncio${suffix}""+        return+    fi+    echo ""${scenario%%_*}${suffix}""+}++example_language() {+    local filename=""${1}""+    if [[ ""${filename#cxx_}"" != ""${filename}"" ]]; then+        echo ""c++""+        return+    fi+    if [[ ""${filename#python_asyncio_}"" != ""${filename}"" ]]; then+        echo ""python_asyncio""+        return+    fi+    echo ""${filename%%_*}""+}++scenarios=(+    ""cpp_generic_async_streaming_ping_pong_secure""+    ""csharp_protobuf_async_unary_ping_pong""+    ""go_generic_sync_streaming_ping_pong_secure""+    ""java_generic_async_streaming_ping_pong_secure""+    ""node_to_node_generic_async_streaming_ping_pong_secure""+    ""php7_protobuf_php_extension_to_cpp_protobuf_sync_unary_ping_pong""+    ""python_generic_sync_streaming_ping_pong""+    ""python_asyncio_generic_async_streaming_ping_pong""+    ""ruby_protobuf_sync_streaming_ping_pong""+)++basic_example() {+    local -r scenario=""${1}""+    local -r outputdir=""${2}""+    local -r outputfile=""$(example_file ""${scenario}"" _example_loadtest.yaml)""+    local -r language=""$(example_language ""${outputfile}"")""+    ${LOADTEST_CONFIG} \+        -l ""${language}"" \+        -t ./tools/run_tests/performance/templates/loadtest_template_basic_all_languages.yaml \+        -s client_pool= -s server_pool= -s big_query_table= \","This looks incomplete, missing the `value` part of `-s key=value`.",
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/26398,643574813,2021-06-02T00:44:01Z,tools/run_tests/performance/loadtest_template.py,"@@ -81,42 +101,56 @@ def loadtest_template(                     input_file_name, input_config.get('kind')))              for client in input_config['spec']['clients']:-                if client['language'] in client_languages:-                    continue+                del client['name']                 if inject_client_pool:                     client['pool'] = '${client_pool}'-                clients.append(client)-                client_languages.add(client['language'])+                if client['language'] not in clientmap:+                    clientmap[client['language']] = []+                insert_worker(client, clientmap[client['language']])              for server in input_config['spec']['servers']:-                if server['language'] in server_languages:-                    continue+                del server['name']                 if inject_server_pool:                     server['pool'] = '${server_pool}'-                servers.append(server)-                server_languages.add(server['language'])+                if server['language'] not in servermap:+                    servermap[server['language']] = []+                insert_worker(server, servermap[server['language']])              input_spec = input_config['spec']             del input_spec['clients']             del input_spec['servers']             del input_spec['scenariosJSON']             spec.update(input_config['spec']) -    clients.sort(key=lambda x: x['language'])-    servers.sort(key=lambda x: x['language'])+    uniquify_workers(clientmap)+    uniquify_workers(servermap)      spec.update({-        'clients': clients,-        'servers': servers,+        'clients':+            sum((clientmap[language] for language in sorted(clientmap)),","Thanks! It is more efficient than it looks, because it is just extending the list assigned to ""start"". I don't think you can do much better than that.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26407,643964687,2021-06-02T13:31:12Z,src/csharp/Grpc.Core.Api/CallOptions.cs,"@@ -47,14 +47,34 @@ public struct CallOptions         /// <param name=""credentials"">Credentials to use for this call.</param>         public CallOptions(Metadata headers = null, DateTime? deadline = null, CancellationToken cancellationToken = default(CancellationToken),                            WriteOptions writeOptions = null, ContextPropagationToken propagationToken = null, CallCredentials credentials = null)+            : this(waitForReady: false, headers, deadline, cancellationToken, writeOptions, propagationToken, credentials)+        {+        }++        /// <summary>+        /// Creates a new instance of <c>CallOptions</c> struct.+        /// </summary>+        /// <param name=""headers"">Headers to be sent with the call.</param>+        /// <param name=""deadline"">Deadline for the call to finish. null means no deadline.</param>+        /// <param name=""cancellationToken"">Can be used to request cancellation of the call.</param>+        /// <param name=""writeOptions"">Write options that will be used for this call.</param>+        /// <param name=""propagationToken"">Context propagation token obtained from <see cref=""ServerCallContext""/>.</param>+        /// <param name=""credentials"">Credentials to use for this call.</param>+        /// <param name=""waitForReady"">+        /// If <c>true</c> and channel is in <c>ChannelState.TransientFailure</c>, the call will attempt waiting for the channel to recover+        /// instead of failing immediately (which is the default ""FailFast"" semantics).+        /// Note: experimental API that can change or be removed without any prior notice.+        /// </param>+        public CallOptions(bool waitForReady, Metadata headers = null, DateTime? deadline = null, CancellationToken cancellationToken = default(CancellationToken),","it seems odd that the `waitForReady`  argument is the only one that's not optional. Also, the ordering of the arguments seems strange (why is waitForReady first?). I understand you somehow need to disambiguate the two constructors, but I'm not sure this is the best way of doing so.",X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/25865,644281600,2021-06-02T20:02:46Z,src/core/lib/security/security_connector/ssl_utils.cc,"@@ -70,9 +70,6 @@ static const char* cipher_suites = nullptr; // All cipher suites for default are compliant with HTTP2. GPR_GLOBAL_CONFIG_DEFINE_STRING(     grpc_ssl_cipher_suites,-    ""TLS_AES_128_GCM_SHA256:""",Is it okay to drop this? gRPC Java still use it though. https://github.com/netty/netty/blob/d34212439068091bcec29a8fad4df82f0a82c638/codec-http2/src/main/java/io/netty/handler/codec/http2/Http2SecurityUtil.java#L69,
303201,JamesNK,https://api.github.com/repos/grpc/grpc/pulls/26407,644355347,2021-06-02T22:15:43Z,src/csharp/Grpc.Core.Api/CallOptions.cs,"@@ -47,14 +47,34 @@ public struct CallOptions         /// <param name=""credentials"">Credentials to use for this call.</param>         public CallOptions(Metadata headers = null, DateTime? deadline = null, CancellationToken cancellationToken = default(CancellationToken),                            WriteOptions writeOptions = null, ContextPropagationToken propagationToken = null, CallCredentials credentials = null)+            : this(waitForReady: false, headers, deadline, cancellationToken, writeOptions, propagationToken, credentials)+        {+        }++        /// <summary>+        /// Creates a new instance of <c>CallOptions</c> struct.+        /// </summary>+        /// <param name=""headers"">Headers to be sent with the call.</param>+        /// <param name=""deadline"">Deadline for the call to finish. null means no deadline.</param>+        /// <param name=""cancellationToken"">Can be used to request cancellation of the call.</param>+        /// <param name=""writeOptions"">Write options that will be used for this call.</param>+        /// <param name=""propagationToken"">Context propagation token obtained from <see cref=""ServerCallContext""/>.</param>+        /// <param name=""credentials"">Credentials to use for this call.</param>+        /// <param name=""waitForReady"">+        /// If <c>true</c> and channel is in <c>ChannelState.TransientFailure</c>, the call will attempt waiting for the channel to recover+        /// instead of failing immediately (which is the default ""FailFast"" semantics).+        /// Note: experimental API that can change or be removed without any prior notice.+        /// </param>+        public CallOptions(bool waitForReady, Metadata headers = null, DateTime? deadline = null, CancellationToken cancellationToken = default(CancellationToken),","As mentioned in the top comment, it needs to be non-optional to disambiguate which method is being called.And it needs to be first because non-optional parameters must come before optional parameters.It is ugly, but I don't know of an alternative.",X
109690,davidben,https://api.github.com/repos/grpc/grpc/pulls/25865,644827553,2021-06-03T14:11:04Z,src/core/lib/security/security_connector/ssl_utils.cc,"@@ -70,9 +70,6 @@ static const char* cipher_suites = nullptr; // All cipher suites for default are compliant with HTTP2. GPR_GLOBAL_CONFIG_DEFINE_STRING(     grpc_ssl_cipher_suites,-    ""TLS_AES_128_GCM_SHA256:""","Java  and OpenSSL don't have the same API. Cipher suite configuration is a TLS implementation thing, not a TLS protocol thing. The TLS protocol doesn't have opinions on which part is configurable by callers. (Making them configurable was probably a mistake, but so it goes.)So it depends on what this list is for. I see it's not defined in an OpenSSL-specific file, so gRPC seems to think it's a generic cross-TLS-implementation thing? Yet it uses OpenSSL-specific cipher suite names (the odd shortened spelling with hyphens is unique to OpenSSL), so there seems to be something strange going on here.Anyway, the short answer is yes, they're unnecessary *in the OpenSSL API*. The long answer is:* In BoringSSL, we do not let you configure TLS 1.3 cipher suites. That configurability was a mistake. So any you pass into `SSL_CTX_set_cipher_list` are just ignored.* In OpenSSL 1.0.2 and 1.1.0, TLS 1.3 is not supported, so any you pass into `SSL_CTX_set_cipher_list` are just ignored.* In OpenSSL 1.1.1, `SSL_CTX_set_cipher_list` does not impact TLS 1.3 cipher suites, so any you pass into `SSL_CTX_set_cipher_list` are irrelevant. If you call `SSL_CTX_set_ciphersuites`, the TLS 1.3 cipher suites do matter. (This is necessary for compatibility so TLS 1.3 doesn't break in old applications that only configure TLS 1.2 ciphers.)However, all of this is specific to the OpenSSL API. If this is meant to be a cross-TLS thing, you probably shouldn't make that assumption. But the format you all use isn't suitable for a cross-TLS thing so I dunno.",X
52979934,matthewstevenson88,https://api.github.com/repos/grpc/grpc/pulls/25865,644871904,2021-06-03T15:02:25Z,src/core/lib/security/security_connector/ssl_utils.cc,"@@ -70,9 +70,6 @@ static const char* cipher_suites = nullptr; // All cipher suites for default are compliant with HTTP2. GPR_GLOBAL_CONFIG_DEFINE_STRING(     grpc_ssl_cipher_suites,-    ""TLS_AES_128_GCM_SHA256:""","Thanks for clarifying David! The only use of this cipher list is to pass in to the `SSL_CTX_set_cipher_list` API. However, this is poorly documented and this list is used very far away from where it is defined. To be safe, I've removed this change from this PR and will send a follow-up PR that changes the cipher list and adds documentation.",
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/26427,645184900,2021-06-03T23:04:25Z,tools/run_tests/performance/loadtest_config.py,"@@ -65,9 +65,6 @@ def now_string() -> str:  def validate_loadtest_name(name: str) -> None:     """"""Validates that a LoadTest name is in the expected format.""""""-    if len(name) > 63:","I would keep the test, replacing the value 63 with 253 (in case someone specifies a really long prefix).See https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names.",
22675398,SameerMahajan-GSLab,https://api.github.com/repos/grpc/grpc/pulls/26216,645275611,2021-06-04T04:09:17Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -506,6 +506,14 @@ async def test_maximum_concurrent_rpcs(self):         await channel.close()         await server.stop(0) +    async def test_is_active(self):+        if (self._server.context.is_active()):+            pass++    async def test_set_callback(self):+        def callback(call):+            return+        self._server.context.add_callback(callback)",Thanks @lidizheng for the feedback. Are there similar tests elsewhere on which I can model my tests that you might be able to help me point to?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26216,645741387,2021-06-04T17:33:07Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -506,6 +506,14 @@ async def test_maximum_concurrent_rpcs(self):         await channel.close()         await server.stop(0) +    async def test_is_active(self):+        if (self._server.context.is_active()):+            pass++    async def test_set_callback(self):+        def callback(call):+            return+        self._server.context.add_callback(callback)",I think the most similar one would be https://github.com/grpc/grpc/blob/master/src/python/grpcio_tests/tests_aio/unit/done_callback_test.py.I recommend to use Bazel to debug:```sh# Test all casesbazel test grpc/src/python/grpcio_tests/tests_aio/...```,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/26433,645892988,2021-06-04T22:48:01Z,src/core/ext/filters/client_channel/resolver/google_c2p/google_c2p_resolver.cc,"@@ -298,7 +298,7 @@ void GoogleCloud2ProdResolver::IPv6QueryDone(bool ipv6_supported) { void GoogleCloud2ProdResolver::StartXdsResolver() {   // Construct bootstrap JSON.   Json::Object node = {-      {""id"", ""C2P""},+      {""id"", absl::StrCat(""C2P-"", rand())},","Even though we use `rand()` in a lot of other places in core, it doesn't seem completely correct to use it given:- AFAIK `rand()` isn't generally guaranteed to be thread safe- it's not guaranteed to have been seededThe backoff code [uses it's own random number generator](https://github.com/grpc/grpc/blob/5ea57ec9052bf970ac0714c3ba354c3044754a49/src/core/lib/backoff/backoff.cc#L33) which I believe solves this - this seems like the correct approach to me, any reason not to do what the backoff code is doing instead of using `rand()` ?",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/26433,645897446,2021-06-04T23:04:31Z,src/core/ext/filters/client_channel/resolver/google_c2p/google_c2p_resolver.cc,"@@ -298,7 +298,7 @@ void GoogleCloud2ProdResolver::IPv6QueryDone(bool ipv6_supported) { void GoogleCloud2ProdResolver::StartXdsResolver() {   // Construct bootstrap JSON.   Json::Object node = {-      {""id"", ""C2P""},+      {""id"", absl::StrCat(""C2P-"", rand())},","The broken case I can imagine here binaries which aren't calling `srand` at the top of main. They would all generate the same random number in their node IDs and so defeat the purpose of the random number.Another alternative to `rand` may be to use the absl random library, which takes care of seeding.That said, since this is used in many other places, this is a wider discussion from this PR so doesn't need to block",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26430,646326901,2021-06-07T07:14:24Z,tools/run_tests/artifacts/artifact_targets.py,"@@ -388,10 +395,10 @@ def targets():         PythonArtifact('manylinux2010', 'x86', 'cp37-cp37m'),         PythonArtifact('manylinux2010', 'x86', 'cp38-cp38'),         PythonArtifact('manylinux2010', 'x86', 'cp39-cp39'),-        PythonArtifact('manylinux2014', 'aarch64', 'cp36-cp36m'),-        PythonArtifact('manylinux2014', 'aarch64', 'cp37-cp37m'),-        PythonArtifact('manylinux2014', 'aarch64', 'cp38-cp38'),-        PythonArtifact('manylinux2014', 'aarch64', 'cp39-cp39'),+        PythonArtifact('manylinux_2_24', 'aarch64', 'cp36-cp36m'),","I think it's better to stay consistent and leave all the aarch64 wheels as manylinux_2_24.- staying consistent is better- it seems that something's wrong with libstdc++ in the dockross manylinux image and we don't know what exactly it is, and it could simply be that with python3.6 the reproduction we have happens not to crash, but things could crash under slighly different conditions. it's better to avoid that.- specialcasing python3.6 doesn't seem to make sense since python3.6 will go end-of-life later this year.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26433,646788057,2021-06-07T17:09:59Z,src/core/ext/filters/client_channel/resolver/google_c2p/google_c2p_resolver.cc,"@@ -298,7 +298,7 @@ void GoogleCloud2ProdResolver::IPv6QueryDone(bool ipv6_supported) { void GoogleCloud2ProdResolver::StartXdsResolver() {   // Construct bootstrap JSON.   Json::Object node = {-      {""id"", ""C2P""},+      {""id"", absl::StrCat(""C2P-"", rand())},","Hmm, actually, I think you're right: having every client report the same random number will kind of defeat the purpose here.  Our other uses of `rand()` inside of C-core are not visible outside of our code, so it doesn't matter as much if there's a little bit of determinism.Unfortunately, we can't yet use the absl random library, as per https://github.com/grpc/grpc/blob/master/third_party/ABSEIL_MANUAL.md.  I've asked @veblush to look into what's blocking us from using it.  If we can get that resolved quickly, I can change this code to use that.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26433,647709607,2021-06-08T18:49:28Z,src/core/ext/filters/client_channel/resolver/google_c2p/google_c2p_resolver.cc,"@@ -298,7 +298,7 @@ void GoogleCloud2ProdResolver::IPv6QueryDone(bool ipv6_supported) { void GoogleCloud2ProdResolver::StartXdsResolver() {   // Construct bootstrap JSON.   Json::Object node = {-      {""id"", ""C2P""},+      {""id"", absl::StrCat(""C2P-"", rand())},","For now, I've changed this to use the C++ random library.  PTAL.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26469,650231388,2021-06-11T19:53:45Z,tools/run_tests/xds_k8s_test_driver/framework/test_app/base_runner.py,"@@ -13,16 +13,26 @@ # limitations under the License. import contextlib import logging+import os import pathlib from typing import Optional+import time+import threading  import mako.template import yaml  from framework.infrastructure import k8s+from kubernetes import client, utils, watch+import kubernetes.client.rest+  logger = logging.getLogger(__name__) +# TODO: Change Kokoro config to look inside the grpc directory.+_TEST_LOG_BASE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)),+                                  '../../../../../../artifacts')","There is an env var for artifacts folder `KOKORO_ARTIFACTS_DIR`. And we have Kokoro config for accepting log files:```build_file: ""grpc/tools/internal_ci/linux/grpc_run_tests_matrix.sh""action {  define_artifacts {    regex: ""**/*sponge_log.*""    regex: ""github/grpc/reports/**""  }}timeout_mins: 40env_vars {  key: ""RUN_TESTS_FLAGS""  value: ""-f basictests linux sanity --inner_jobs 16 -j 1 --internal_ci""}```We can try to make the generated log legit artifacts for the Kokoro job. (This is optional, since unexpected artifacts will be listed in Sponge as well.)",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/26469,650289015,2021-06-11T22:11:52Z,tools/run_tests/xds_k8s_test_driver/framework/test_app/base_runner.py,"@@ -35,14 +45,83 @@ class KubernetesBaseRunner:     def __init__(self,                  k8s_namespace,                  namespace_template=None,-                 reuse_namespace=False):+                 reuse_namespace=False,+                 case_name=None,+                 pod_label_selector=None):         # Kubernetes namespaced resources manager         self.k8s_namespace: k8s.KubernetesNamespace = k8s_namespace         self.reuse_namespace = reuse_namespace         self.namespace_template = namespace_template or 'namespace.yaml'+        self.case_name: str = case_name or """"","> Can we change the case_name variable to be ephemeral?I'm not sure what you mean by this.> This will make more sense if a server pod is going to be reused by multiple test cases. E.g., `pod_name`, `pod_name_suffix`.It was my impression that we didn't reuse pods between test cases. The reason I used the test case name here is because it gives a unique identifier that someone can use to figure out why a particular test case failed.I suppose the alternative is for each test case that instantiates a deployment to explicitly give it a name. I thought this change might be controversial. I'll do this though if both you and @sergiitk think it is the right move. I might need some input on what to name the existing clients and servers that we spin up in each test.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26469,650344717,2021-06-12T04:38:41Z,tools/run_tests/xds_k8s_test_driver/framework/test_app/base_runner.py,"@@ -35,14 +45,83 @@ class KubernetesBaseRunner:     def __init__(self,                  k8s_namespace,                  namespace_template=None,-                 reuse_namespace=False):+                 reuse_namespace=False,+                 case_name=None,+                 pod_label_selector=None):         # Kubernetes namespaced resources manager         self.k8s_namespace: k8s.KubernetesNamespace = k8s_namespace         self.reuse_namespace = reuse_namespace         self.namespace_template = namespace_template or 'namespace.yaml'+        self.case_name: str = case_name or """"","If we move the `case_name` variable to the `run` method, we could reuse the Runner and instantiates the deployment for each case with a more related name. Let's see what @sergiitk think about the naming.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26469,651086172,2021-06-14T16:10:18Z,tools/run_tests/xds_k8s_test_driver/framework/test_app/base_runner.py,"@@ -35,14 +50,113 @@ class KubernetesBaseRunner:     def __init__(self,                  k8s_namespace,                  namespace_template=None,-                 reuse_namespace=False):+                 reuse_namespace=False,+                 case_name=None,+                 pod_label_selector=None):         # Kubernetes namespaced resources manager         self.k8s_namespace: k8s.KubernetesNamespace = k8s_namespace         self.reuse_namespace = reuse_namespace         self.namespace_template = namespace_template or 'namespace.yaml'+        self.case_name: str = case_name or """"+        self.pod_label_selector: str = pod_label_selector or """"          # Mutable state         self.namespace: Optional[k8s.V1Namespace] = None+        self._log_stop_event: threading.Event = threading.Event()++    def _await_container_ready(self, core: client.CoreV1Api, pod_name: str,+                               container_name: str, namespace: str) -> None:+        while not self._log_stop_event.is_set():+            try:+                pod_status = core.read_namespaced_pod_status(+                    pod_name, namespace).status+                candidate_containers = [+                    container for container in pod_status.container_statuses+                    if container.name == container_name+                ]+                if not candidate_containers:+                    time.sleep(_BACKOFF_SECONDS)+                    continue+                container = candidate_containers[0]+                if not container.ready:+                    time.sleep(_BACKOFF_SECONDS)+                else:+                    break+            except kubernetes.client.rest.ApiException:+                time.sleep(_BACKOFF_SECONDS)++    def _log_container(self, core: client.CoreV1Api, pod_name: str,+                       container_name: str, namespace: str) -> None:+        query_restarted = False+        logfile = os.path.join(+            _TEST_LOG_BASE_DIR,+            f""{self.case_name}.{pod_name}.{container_name}.sponge_log.log"")++        self._await_container_ready(core, pod_name, container_name, namespace)++        with open(logfile, ""w"") as f:+            while not self._log_stop_event.is_set():+                try:+                    if query_restarted:+                        f.write(+                            ""Restarted log fetching. Attempting to read from the beginning, but truncation may have occurred.\n""+                        )+                    w = watch.Watch()+                    for msg in w.stream(core.read_namespaced_pod_log,+                                        name=pod_name,+                                        namespace=namespace,+                                        container=container_name,+                                        follow=True):+                        f.write(msg)+                        f.write(""\n"")+                except kubernetes.client.rest.ApiException as e:+                    f.write(f""Exception fetching logs: {e}\n"")+                    query_restarted = True+                    time.sleep(_BACKOFF_SECONDS)++    def _start_logging_container(self, core: client.CoreV1Api, pod_name: str,+                                 container_name: str, namespace: str) -> None:+        t = threading.Thread(target=self._log_container,+                             args=(core, pod_name, container_name, namespace),+                             daemon=True)+        t.start()++    def _start_logging_pod(self, core: client.CoreV1Api, pod_name: str,+                           namespace: str) -> None:+        retryer = tenacity.Retrying(retry=tenacity.retry_if_exception_type(+            kubernetes.client.rest.ApiException),+                                    wait=tenacity.wait_fixed(_BACKOFF_SECONDS),+                                    reraise=True)+        pod = retryer(core.read_namespaced_pod, pod_name, namespace)+        for container in pod.spec.containers:+            self._start_logging_container(core, pod_name, container.name,+                                          namespace)++    def _start_logging_deployment(self, deployment_name: str):+        core = self.k8s_namespace.api.core+        apps = self.k8s_namespace.api.apps++        pod_names = None+        namespace = self.k8s_namespace.name++        def _get_deployment_pods():+            pods_all_namespaces = core.list_pod_for_all_namespaces(+                label_selector=self.pod_label_selector).items+            pods = [+                pod for pod in pods_all_namespaces+                if pod.metadata.namespace == namespace+            ]+            return [pod.metadata.name for pod in pods]",nit: I think generator should work too```suggestion            return (pod.metadata.name for pod in pods)```,
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26469,651103147,2021-06-14T16:33:24Z,tools/run_tests/xds_k8s_test_driver/framework/test_app/base_runner.py,"@@ -35,14 +50,113 @@ class KubernetesBaseRunner:     def __init__(self,                  k8s_namespace,                  namespace_template=None,-                 reuse_namespace=False):+                 reuse_namespace=False,+                 case_name=None,+                 pod_label_selector=None):         # Kubernetes namespaced resources manager         self.k8s_namespace: k8s.KubernetesNamespace = k8s_namespace         self.reuse_namespace = reuse_namespace         self.namespace_template = namespace_template or 'namespace.yaml'+        self.case_name: str = case_name or """"+        self.pod_label_selector: str = pod_label_selector or """"","If we're making this an argument, this should become the single source of truth => we should template it into the manifest instead of `app: ${deployment_name}`: https://github.com/grpc/grpc/blob/156edd687ea6782ed023688ae350eb074c5f4382/tools/run_tests/xds_k8s_test_driver/kubernetes-manifests/server.deployment.yaml#L7-L8I'm also OK to not introduce it as an argument for now, and just assume `app: ${deployment_name}` will always be present in the manifest.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26469,651109949,2021-06-14T16:43:10Z,tools/run_tests/xds_k8s_test_driver/framework/test_app/base_runner.py,"@@ -35,14 +50,113 @@ class KubernetesBaseRunner:     def __init__(self,                  k8s_namespace,                  namespace_template=None,-                 reuse_namespace=False):+                 reuse_namespace=False,+                 case_name=None,+                 pod_label_selector=None):         # Kubernetes namespaced resources manager         self.k8s_namespace: k8s.KubernetesNamespace = k8s_namespace         self.reuse_namespace = reuse_namespace         self.namespace_template = namespace_template or 'namespace.yaml'+        self.case_name: str = case_name or """"+        self.pod_label_selector: str = pod_label_selector or """"          # Mutable state         self.namespace: Optional[k8s.V1Namespace] = None+        self._log_stop_event: threading.Event = threading.Event()++    def _await_container_ready(self, core: client.CoreV1Api, pod_name: str,+                               container_name: str, namespace: str) -> None:+        while not self._log_stop_event.is_set():+            try:+                pod_status = core.read_namespaced_pod_status(+                    pod_name, namespace).status+                candidate_containers = [+                    container for container in pod_status.container_statuses+                    if container.name == container_name+                ]+                if not candidate_containers:+                    time.sleep(_BACKOFF_SECONDS)+                    continue+                container = candidate_containers[0]+                if not container.ready:+                    time.sleep(_BACKOFF_SECONDS)+                else:+                    break+            except kubernetes.client.rest.ApiException:+                time.sleep(_BACKOFF_SECONDS)++    def _log_container(self, core: client.CoreV1Api, pod_name: str,+                       container_name: str, namespace: str) -> None:+        query_restarted = False+        logfile = os.path.join(+            _TEST_LOG_BASE_DIR,+            f""{self.case_name}.{pod_name}.{container_name}.sponge_log.log"")++        self._await_container_ready(core, pod_name, container_name, namespace)++        with open(logfile, ""w"") as f:+            while not self._log_stop_event.is_set():+                try:+                    if query_restarted:+                        f.write(+                            ""Restarted log fetching. Attempting to read from the beginning, but truncation may have occurred.\n""+                        )+                    w = watch.Watch()+                    for msg in w.stream(core.read_namespaced_pod_log,+                                        name=pod_name,+                                        namespace=namespace,+                                        container=container_name,+                                        follow=True):+                        f.write(msg)+                        f.write(""\n"")+                except kubernetes.client.rest.ApiException as e:+                    f.write(f""Exception fetching logs: {e}\n"")+                    query_restarted = True+                    time.sleep(_BACKOFF_SECONDS)++    def _start_logging_container(self, core: client.CoreV1Api, pod_name: str,+                                 container_name: str, namespace: str) -> None:+        t = threading.Thread(target=self._log_container,+                             args=(core, pod_name, container_name, namespace),+                             daemon=True)+        t.start()++    def _start_logging_pod(self, core: client.CoreV1Api, pod_name: str,+                           namespace: str) -> None:+        retryer = tenacity.Retrying(retry=tenacity.retry_if_exception_type(+            kubernetes.client.rest.ApiException),+                                    wait=tenacity.wait_fixed(_BACKOFF_SECONDS),+                                    reraise=True)+        pod = retryer(core.read_namespaced_pod, pod_name, namespace)+        for container in pod.spec.containers:+            self._start_logging_container(core, pod_name, container.name,+                                          namespace)++    def _start_logging_deployment(self, deployment_name: str):+        core = self.k8s_namespace.api.core+        apps = self.k8s_namespace.api.apps++        pod_names = None+        namespace = self.k8s_namespace.name++        def _get_deployment_pods():+            pods_all_namespaces = core.list_pod_for_all_namespaces(","Why not `core.list_namespaced_pod(namespace, label_selector=pod_label_selector)`?",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26469,651112010,2021-06-14T16:46:20Z,tools/run_tests/xds_k8s_test_driver/framework/test_app/base_runner.py,"@@ -35,14 +50,113 @@ class KubernetesBaseRunner:     def __init__(self,                  k8s_namespace,                  namespace_template=None,-                 reuse_namespace=False):+                 reuse_namespace=False,+                 case_name=None,+                 pod_label_selector=None):         # Kubernetes namespaced resources manager         self.k8s_namespace: k8s.KubernetesNamespace = k8s_namespace         self.reuse_namespace = reuse_namespace         self.namespace_template = namespace_template or 'namespace.yaml'+        self.case_name: str = case_name or """"+        self.pod_label_selector: str = pod_label_selector or """"          # Mutable state         self.namespace: Optional[k8s.V1Namespace] = None+        self._log_stop_event: threading.Event = threading.Event()++    def _await_container_ready(self, core: client.CoreV1Api, pod_name: str,+                               container_name: str, namespace: str) -> None:+        while not self._log_stop_event.is_set():+            try:+                pod_status = core.read_namespaced_pod_status(+                    pod_name, namespace).status+                candidate_containers = [+                    container for container in pod_status.container_statuses+                    if container.name == container_name+                ]+                if not candidate_containers:+                    time.sleep(_BACKOFF_SECONDS)+                    continue+                container = candidate_containers[0]+                if not container.ready:+                    time.sleep(_BACKOFF_SECONDS)+                else:+                    break+            except kubernetes.client.rest.ApiException:+                time.sleep(_BACKOFF_SECONDS)++    def _log_container(self, core: client.CoreV1Api, pod_name: str,+                       container_name: str, namespace: str) -> None:+        query_restarted = False+        logfile = os.path.join(+            _TEST_LOG_BASE_DIR,+            f""{self.case_name}.{pod_name}.{container_name}.sponge_log.log"")++        self._await_container_ready(core, pod_name, container_name, namespace)++        with open(logfile, ""w"") as f:+            while not self._log_stop_event.is_set():+                try:+                    if query_restarted:+                        f.write(+                            ""Restarted log fetching. Attempting to read from the beginning, but truncation may have occurred.\n""+                        )+                    w = watch.Watch()+                    for msg in w.stream(core.read_namespaced_pod_log,+                                        name=pod_name,+                                        namespace=namespace,+                                        container=container_name,+                                        follow=True):+                        f.write(msg)+                        f.write(""\n"")+                except kubernetes.client.rest.ApiException as e:+                    f.write(f""Exception fetching logs: {e}\n"")+                    query_restarted = True+                    time.sleep(_BACKOFF_SECONDS)++    def _start_logging_container(self, core: client.CoreV1Api, pod_name: str,+                                 container_name: str, namespace: str) -> None:+        t = threading.Thread(target=self._log_container,+                             args=(core, pod_name, container_name, namespace),+                             daemon=True)+        t.start()++    def _start_logging_pod(self, core: client.CoreV1Api, pod_name: str,+                           namespace: str) -> None:+        retryer = tenacity.Retrying(retry=tenacity.retry_if_exception_type(+            kubernetes.client.rest.ApiException),+                                    wait=tenacity.wait_fixed(_BACKOFF_SECONDS),+                                    reraise=True)+        pod = retryer(core.read_namespaced_pod, pod_name, namespace)+        for container in pod.spec.containers:+            self._start_logging_container(core, pod_name, container.name,+                                          namespace)++    def _start_logging_deployment(self, deployment_name: str):+        core = self.k8s_namespace.api.core+        apps = self.k8s_namespace.api.apps++        pod_names = None+        namespace = self.k8s_namespace.name++        def _get_deployment_pods():+            pods_all_namespaces = core.list_pod_for_all_namespaces(+                label_selector=self.pod_label_selector).items+            pods = [+                pod for pod in pods_all_namespaces+                if pod.metadata.namespace == namespace+            ]+            return [pod.metadata.name for pod in pods]++        retryer = tenacity.Retrying(retry=tenacity.retry_if_exception_type(","Consider using `framework.helpers.retryers.constant_retryer`, example: https://github.com/grpc/grpc/blob/0c5808603f6495f3a365377c748652a7db8f3c12/tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py#L121-L123",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/26289,651900181,2021-06-15T15:21:42Z,doc/xds-test-descriptions.md,"@@ -621,3 +621,110 @@ There are four sub-tests:       `rpc-behavior: sleep-4`.    1. Test driver asserts client recieves ~100% status `OK` for EmptyCall       and ~100% status `DEADLINE_EXCEEDED` for UnaryCall.++### api_listener+The test case verifies a specific use case where it creates a second TD API +listener using the same name as the existing one and then delete the old one. +The test driver verifies this is a safe way to update the API listener +configuration while keep using the existing name.++Client parameters:++1.  --num_channels=1+1.  --qps=100++Load balancer configuration:++1.  One MIG with two backends.++Assert:++The test driver configuration steps:+1. The test driver creates the first set of forwarding rule + target proxy + +URL map with a test host name.+1. Then the test driver creates a second set of forwarding rule + target proxy + +URL map with the same test host name.+1. The test driver deletes the first set of configurations in step 1.++The test driver verifies, at each configuration step, the traffic is always able +to reach the designated hosts.++### metadata_filter+This test case verifies that metadata filter configurations in URL map match +rule are effective at Traffic Director for routing selection against downstream+node metadata.++Client parameters:++1.  --num_channels=1+1.  --qps=100++Load balancer configuration:++1.  Two MIGs in the same zone, each having two backends.++There are four test sub-cases:+1. Test `MATCH_ALL` metadata filter criteria.+1. Test `MATCH_ANY` metadata filter criteria.+1. Test mixed `MATCH_ALL` and `MATCH_ANY` metadata filter criteria.+1. Test when multiple match rules with metadata filter all match.++Assert:++At each test sub-case described above, the test driver configures+and verifies:++1. Set default URL map, and verify traffic goes to the original backend hosts. +1. Then patch URL map to update the match rule with metadata filter ","It seems this should say ""_add_ a match rule _for the alternate MIG_"". And (1) should say ""Set a default URL map pointing to one of the MIGs""It isn't at all clear what configuration is expected here. Is this only positive testing? As in, let's say that the server ignored the metadata match criteria (essentially becoming wildcard), wouldn't this test still pass? What filter labels are to be used? Only a single label, or multiple labels? Is it okay if all labels match for the MATCH_ANY?",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26471,652113754,2021-06-15T20:06:13Z,test/core/security/grpc_tls_certificate_provider_test.cc,"@@ -495,6 +495,29 @@ TEST_F(GrpcTlsCertificateProviderTest,   CancelWatch(watcher_state_1); } +TEST_F(GrpcTlsCertificateProviderTest, CertifyServer0CredentialsPairMatch) {","I am not sure what Server0 means here? The test name should be general, and shouldn't include test variables.Suggest using ValidPrivateKeyCertChainPair or PrivateKeyCertChainSuccessfulMatch or something similarPlease update the other tests too.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/26481,653108082,2021-06-16T22:56:08Z,src/core/ext/filters/client_channel/lb_policy.h,"@@ -419,6 +434,33 @@ class LoadBalancingPolicy : public InternallyRefCounted<LoadBalancingPolicy> {   std::unique_ptr<ChannelControlHelper> channel_control_helper_; }; +//+// Implementation+//++template <typename T>","I copy/pasted some fragments from the promises CL to make this:https://godbolt.org/z/h8d9v347cSuggest we move in the Overload type, and probably the associated Match function as a way of generalizing this part of the CL.I can do so tomorrow if you'd like. (edit: changed the link to a version with slightly better type deduction)",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/26481,653108807,2021-06-16T22:57:59Z,src/core/ext/filters/client_channel/lb_policy.cc,"@@ -91,6 +91,29 @@ LoadBalancingPolicy::UpdateArgs& LoadBalancingPolicy::UpdateArgs::operator=(   return *this; } +//+// LoadBalancingPolicy::PickResult+//++LoadBalancingPolicy::PickResult LoadBalancingPolicy::PickResult::Complete(","Question: do we still want these API's, given that we can now unambiguously construct a type with the right fields?",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26471,653766983,2021-06-17T17:09:28Z,test/core/security/grpc_tls_certificate_provider_test.cc,"@@ -495,8 +495,54 @@ TEST_F(GrpcTlsCertificateProviderTest,   CancelWatch(watcher_state_1); } -}  // namespace testing+TEST_F(GrpcTlsCertificateProviderTest, EmptyPrivateKeyString) {+  std::string empty_string;+  absl::Status match_status =+      PrivateKeyPublicKeyMatches(empty_string, cert_chain_);","Here you can avoid empty_string variable.The function call could be`absl::Status status = PrivateKeyPublicKeyMatches(/*private_key_string=*/"""", cert_chain_);`Similarly for cert_chain below",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26499,653772381,2021-06-17T17:14:40Z,src/python/grpcio/grpc/aio/_base_call.py,"@@ -86,15 +86,15 @@ class Call(RpcContext, metaclass=ABCMeta):     """"""The abstract base class of an RPC on the client-side.""""""      @abstractmethod-    async def initial_metadata(self) -> Metadata:+    async def initial_metadata(self) -> Optional[Metadata]:","Maybe we can use `MetadataType` in `_typing.py`? And update the metadata type to support both kind of metadata.```pythonMetadataType = Union[Iterable[MetadatumType], Metadata]```",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26471,654000769,2021-06-17T22:14:41Z,test/core/security/grpc_tls_certificate_provider_test.cc,"@@ -495,8 +495,47 @@ TEST_F(GrpcTlsCertificateProviderTest,   CancelWatch(watcher_state_1); } -}  // namespace testing+TEST_F(GrpcTlsCertificateProviderTest, EmptyPrivateKeyString) {+  absl::Status status =+      PrivateKeyPublicKeyMatches(/*private_key_string=*/"""", cert_chain_);+  EXPECT_EQ(status.code(), absl::StatusCode::kInvalidArgument);+  EXPECT_EQ(status.message(), ""Private key string is empty."");+}++TEST_F(GrpcTlsCertificateProviderTest, EmptyCertificateString) {+  absl::Status status =+      PrivateKeyPublicKeyMatches(private_key_2_, /*cert_string=*/"""");+  EXPECT_EQ(status.code(), absl::StatusCode::kInvalidArgument);+  EXPECT_EQ(status.message(), ""Certificate string is empty."");+} +TEST_F(GrpcTlsCertificateProviderTest, FailedStringToX509Conversion) {","When naming the test, try to describe the behavior that you are testing against. For example, this can be `FailedKeyCertMatchOnInvalidCertFormat`.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26472,654234000,2021-06-18T08:13:41Z,src/csharp/Grpc.Core.Tests/ChannelCredentialsTest.cs,"@@ -30,6 +30,12 @@ public void InsecureCredentials_IsNonComposable()             Assert.IsFalse(ChannelCredentials.Insecure.IsComposable);         } +        [Test]+        public void SecureCredentials_IsNonComposable()+        {+            Assert.IsFalse(ChannelCredentials.Secure.IsComposable);",ssl credentials are actually composable: https://github.com/grpc/grpc/blob/fd3bd70939fb4239639fbd26143ec416366e4157/src/csharp/Grpc.Core.Api/SslCredentials.cs#L118 (since they can be composed with callCredentials: https://github.com/grpc/grpc/blob/fd3bd70939fb4239639fbd26143ec416366e4157/src/csharp/Grpc.Core.Api/ChannelCredentials.cs#L61),X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26472,654244900,2021-06-18T08:29:55Z,src/csharp/Grpc.Core.Api/ChannelCredentials.cs,"@@ -51,6 +52,23 @@ public static ChannelCredentials Insecure             }         } +        /// <summary>+        /// Returns instance of credentials that provides SSL security.+        /// <para>+        /// When used with Grpc.Core, these credentials will load from a","Let's rephrase this paragraph to say that using ChannelCredentials.Secure has the same effect as using a default instance of parameterless SslCredentials() (the ""default"" SSL credentials).Also add a note that for Grpc.Core, one should avoid using this shortcut if want to specify extra configuration, such as custom trust roots, mutual TLS or specify CallCredentials.Copypasting the documentation of SslCredentials() parameterless constructor here is not necessary.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26472,654248374,2021-06-18T08:34:56Z,src/csharp/Grpc.Core.Api/ChannelCredentials.cs,"@@ -51,6 +52,23 @@ public static ChannelCredentials Insecure             }         } +        /// <summary>+        /// Returns instance of credentials that provides SSL security.+        /// <para>+        /// When used with Grpc.Core, these credentials will load from a+        /// disk file pointed to by the GRPC_DEFAULT_SSL_ROOTS_FILE_PATH environment variable.+        /// If that fails, gets the roots certificates from a well known place on disk.+        /// Use <see cref=""SslCredentials""/> to customize the credentials.+        /// </para>+        /// </summary>+        public static ChannelCredentials Secure","I was also thinking of names such as `SecureSsl` or `SecureTls`, but than rejected `SecureTls` for naming consistency with pre-existing APIs and other gRPC implementations (even though SslCredentials actually create TLS connections).I'm undecided between `SecureSsl` and `Secure`.  Would `SecureSsl` make sense in the context of grpc-dotnet? If so, that's probably a better name since with gRPC, ""secure"" doesn't always mean SSL/TLS (there are other transports that support other auth mechanisms than SSL/TLS).",X
28025951,HannahShiSFB,https://api.github.com/repos/grpc/grpc/pulls/26509,654687378,2021-06-18T21:39:31Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -827,9 +827,11 @@ void ClientChannel::ExternalConnectivityWatcher::Notify(   // automatically remove all watchers in that case.   if (state != GRPC_CHANNEL_SHUTDOWN) {     chand_->work_serializer_->Run(-        [this]() ABSL_EXCLUSIVE_LOCKS_REQUIRED(chand_->work_serializer_) {-          RemoveWatcherLocked();-        },+        [pThis = Ref()]()","The logs clearly show that for watcher `0x7f528c0050e0 `, `ExternalConnectivityWatcher::Notify` was scheduled to run after the destructor `~ExternalConnectivityWatcher`.I believe it happens because of the SHUTDOWN caused watchers being removed from the state tracker at https://github.com/grpc/grpc/blob/master/src/core/lib/transport/connectivity_state.cc#L173",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/26505,655568620,2021-06-21T17:18:55Z,tools/run_tests/performance/loadtest_config.py,"@@ -383,7 +383,17 @@ def main() -> None:     if args.runs_per_test < 1:         argp.error('runs_per_test must be greater than zero.') -    substitutions = parse_key_value_args(args.substitutions)+    # Config generation ignores environment variables that are passed by the+    # controller at runtime.+    substitutions = {","Nit: It looks like there's a coupling here between this list and the environment variables specified by the driver. That is, when someone adds a new environment variable to the controller, they also have to know to add that environment variable here. This seems like a difficult problem to solve considering that the controller lives in a separate repo, but if you can think of a simple solution, it might be good to use it.",
4652122,MHDante,https://api.github.com/repos/grpc/grpc/pulls/26499,656307307,2021-06-22T14:58:22Z,src/python/grpcio/grpc/aio/_base_call.py,"@@ -86,15 +86,15 @@ class Call(RpcContext, metaclass=ABCMeta):     """"""The abstract base class of an RPC on the client-side.""""""      @abstractmethod-    async def initial_metadata(self) -> Metadata:+    async def initial_metadata(self) -> Optional[Metadata]:","While I think that this would lead to greater flexibility, I would like to mention that since this is the `_base_call` of the AIO package, it (from what I understand) should always return `Metadata`. Perhaps it would be useful to define a shared `typing.Protocol` across the AIO and threading implementations for classes that are meant to be used concurrently.",
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/26501,656339966,2021-06-22T15:32:53Z,tools/internal_ci/linux/grpc_e2e_performance_v2.sh,"@@ -78,7 +72,7 @@ buildConfigs() {     shift 2     tools/run_tests/performance/loadtest_config.py ""$@"" \         -t ./tools/run_tests/performance/templates/loadtest_template_prebuilt_all_languages.yaml \-        -s driver_pool=""${DRIVER_POOL}"" -s driver_image=""${DRIVER_IMAGE}"" \+        -s driver_pool=""${DRIVER_POOL}"" -s driver_image= \","It comes from the same place as worker images come from, when no image is specified. There is a set of images (driver image plus worker images for each language) that are built with each release of the test infrastructure and are used when no image is specified. That driver image uses the driver binary from the latest gRPC release, and specify only a run container; the worker images specify clone, build and run containers for the workers (in contrast to the ones used in continuous runs, which are prebuilt by the prepare_prebuilt_workers script and specify only a run container).",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26499,656454019,2021-06-22T17:55:04Z,src/python/grpcio/grpc/aio/_base_call.py,"@@ -86,15 +86,15 @@ class Call(RpcContext, metaclass=ABCMeta):     """"""The abstract base class of an RPC on the client-side.""""""      @abstractmethod-    async def initial_metadata(self) -> Metadata:+    async def initial_metadata(self) -> Optional[Metadata]:","> While I think that this would lead to greater flexibility, I would like to mention that since this is the _base_call of the AIO package, it (from what I understand) should always return Metadata.My bad. I mistaken this PR to be changing the input metadata typing. For metadata provided by the AIO library, it should be `Metadata`.> typing.Protocol?We want to support 3.6, so we can't use 3.8 features yet.---In that case, can you be more specific about why you want to add the `Optional` annotation here? In your description, you mentioned this gives interceptors freedom to return optional Metadata. Generally, the interceptors can also cope to the existing type annotation by returning an empty `Metadata` object.If you think this type annotation is important, we need to do this correctly with some extra steps:- Write a gRFC (can be short, but need to tell a complete story) to propose the API change;- Wait for 14 days for comments and approvals;- Update the docstring to state under what condition will this method return `None`;- Update the AIO implementation to return `None`;- Add tests to guard this behavior.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26550,658314194,2021-06-24T22:06:19Z,tools/run_tests/xds_k8s_test_driver/run.sh,"@@ -68,4 +68,8 @@ fi  cd ""${XDS_K8S_DRIVER_DIR}"" export PYTHONPATH=""${XDS_K8S_DRIVER_DIR}""-exec python ""$@"" --flagfile=""${XDS_K8S_CONFIG}""+# Split path to python file from the rest of the args.+readonly PY_FILE=""$1""+shift+# Append args after --flagfile, so they take higher priority.+exec python ""${PY_FILE}"" --flagfile=""${XDS_K8S_CONFIG}"" ""$@""","TIL. Though, using `shift` is a common pattern too",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26468,659386496,2021-06-27T22:45:32Z,src/core/lib/security/authorization/sdk_server_authz_filter.cc,"@@ -0,0 +1,224 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include ""src/core/lib/security/authorization/sdk_server_authz_filter.h""++#include ""src/core/lib/security/authorization/authorization_policy_provider.h""+#include ""src/core/lib/security/authorization/evaluate_args.h""+#include ""src/core/lib/transport/transport.h""++namespace {++class ChannelData {+ public:+  ChannelData(+      grpc_core::RefCountedPtr<grpc_auth_context> auth_context,+      grpc_endpoint* endpoint,+      grpc_core::RefCountedPtr<grpc_authorization_policy_provider> provider)+      : auth_context_(std::move(auth_context)),+        channel_args_(grpc_core::EvaluateArgs::PerChannelArgs(+            auth_context_.get(), endpoint)),+        provider_(std::move(provider)) {}++  static grpc_error_handle Init(grpc_channel_element* elem,+                                grpc_channel_element_args* args);+  static void Destroy(grpc_channel_element* elem);++  grpc_core::RefCountedPtr<grpc_auth_context> auth_context_;+  grpc_core::EvaluateArgs::PerChannelArgs channel_args_;+  grpc_core::RefCountedPtr<grpc_authorization_policy_provider> provider_;+};++class CallData {+ public:+  CallData(grpc_call_element* elem, const grpc_call_element_args& args)+      : call_combiner_(args.call_combiner) {+    GRPC_CLOSURE_INIT(&recv_initial_metadata_ready_, RecvInitialMetadataReady,+                      elem, grpc_schedule_on_exec_ctx);+    GRPC_CLOSURE_INIT(&recv_trailing_metadata_ready_, RecvTrailingMetadataReady,+                      elem, grpc_schedule_on_exec_ctx);+  }++  ~CallData() { GRPC_ERROR_UNREF(recv_initial_metadata_error_); }++  static void StartTransportStreamOpBatch(+      grpc_call_element* elem, grpc_transport_stream_op_batch* batch);+  static grpc_error_handle Init(grpc_call_element* elem,+                                const grpc_call_element_args* args);+  static void Destroy(grpc_call_element* elem,+                      const grpc_call_final_info* /*final_info*/,+                      grpc_closure* /*ignored*/);++ private:+  static void RecvInitialMetadataReady(void* arg, grpc_error_handle error);+  static void RecvTrailingMetadataReady(void* user_data, grpc_error_handle err);++  grpc_core::CallCombiner* call_combiner_;+  grpc_transport_stream_op_batch* recv_initial_metadata_batch_;+  grpc_closure* original_recv_initial_metadata_ready_;+  grpc_closure recv_initial_metadata_ready_;+  grpc_error_handle recv_initial_metadata_error_ = GRPC_ERROR_NONE;+  grpc_closure* original_recv_trailing_metadata_ready_;+  grpc_closure recv_trailing_metadata_ready_;+  grpc_error_handle recv_trailing_metadata_error_;+  bool seen_recv_trailing_metadata_ready_ = false;+};++bool IsAuthorized(ChannelData* chand, grpc_transport_stream_op_batch* batch) {+  grpc_core::EvaluateArgs args(+      batch->payload->recv_initial_metadata.recv_initial_metadata,+      &chand->channel_args_);+  if (chand->provider_->deny_engine() != nullptr) {+    grpc_core::AuthorizationEngine::Decision decision =+        chand->provider_->deny_engine()->Evaluate(args);+    if (decision.type ==+        grpc_core::AuthorizationEngine::Decision::Type::kDeny) {+      gpr_log(GPR_DEBUG, ""Request denied. Matching policy name: %s."",+              decision.matching_policy_name.c_str());+      return false;+    }+  }+  if (chand->provider_->allow_engine() != nullptr) {+    grpc_core::AuthorizationEngine::Decision decision =+        chand->provider_->allow_engine()->Evaluate(args);+    if (decision.type ==+        grpc_core::AuthorizationEngine::Decision::Type::kAllow) {+      gpr_log(GPR_DEBUG, ""Request allowed. Matching policy name: %s."",+              decision.matching_policy_name.c_str());+      return true;+    }+  }+  gpr_log(GPR_DEBUG, ""Request denied. No match found."");+  return false;+}++void CallData::RecvInitialMetadataReady(void* arg, grpc_error_handle error) {+  grpc_call_element* elem = static_cast<grpc_call_element*>(arg);+  ChannelData* chand = static_cast<ChannelData*>(elem->channel_data);+  CallData* calld = static_cast<CallData*>(elem->call_data);+  grpc_transport_stream_op_batch* batch = calld->recv_initial_metadata_batch_;+  if (error == GRPC_ERROR_NONE) {+    if (!IsAuthorized(chand, batch)) {","@markdroth I had a doubt here. So if the request is authorized, we go ahead and execute the next filter right?For ex. in xds enabled servers, where we may have xds filters after sdk filter in filter chain.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26591,662505084,2021-07-01T18:20:49Z,BUILD,"@@ -543,7 +562,9 @@ grpc_cc_library(         ""include/grpc/census.h"",     ],     deps = [-        ""grpc_base"",+        ""gpr_base"",+        ""grpc_base_c"",",Why do we have `grpc_base` vs. `grpc_base_c` in the first place?  It looks like the former is just the latter plus the lame_client filter.  Is there any reason why that is split out?I'm guessing that this is a leftover from the fact that the lame_client filter was the very first piece of code we converted from C to C++.  It's probably not necessary anymore.  I suggest just moving the lame_client filter into `grpc_base_c` and renaming it to `grpc_base`.,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/26591,662555296,2021-07-01T19:56:30Z,bazel/grpc_deps.bzl,"@@ -44,6 +49,11 @@ def grpc_deps():         actual = ""@upb//:json"",     ) +    native.bind(+        name = ""upb_generated_code_support__only_for_generated_code_do_not_use__i_give_permission_to_break_me"",+        actual = ""@upb//:generated_code_support__only_for_generated_code_do_not_use__i_give_permission_to_break_me"",",Note that we have decided to have this dependency anyway by virtue of checking in the generated code.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26591,662560143,2021-07-01T20:06:30Z,bazel/grpc_deps.bzl,"@@ -44,6 +49,11 @@ def grpc_deps():         actual = ""@upb//:json"",     ) +    native.bind(+        name = ""upb_generated_code_support__only_for_generated_code_do_not_use__i_give_permission_to_break_me"",+        actual = ""@upb//:generated_code_support__only_for_generated_code_do_not_use__i_give_permission_to_break_me"",","Right, but as an implicit dependency, it could be changed without requiring changes on our end.  This way, it might, so I want to make sure that we're not opening the door to maintenance problems.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26621,665514377,2021-07-07T16:07:33Z,src/core/lib/security/credentials/jwt/json_token.cc,"@@ -182,7 +182,15 @@ static char* encoded_jwt_claim(const grpc_auth_json_key* json_key,    if (scope != nullptr) {     object[""scope""] = scope;-    if (!clear_audience) {+    // This code path is for the self-signed jwt token used to access+    // google API's, and it needs to satisfy the requirement specified","This code is not specific to Google APIs; it is used generically for any JWT token.  Is this feature specific to Google APIs, or is it something generally useful?  If the former, does it really belong here?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26612,665520307,2021-07-07T16:15:08Z,src/core/ext/xds/xds_api.cc,"@@ -3069,6 +3071,10 @@ grpc_error_handle CdsResponseParse(             resource_names_failed->insert(cluster_name);             continue;           }+        } else {","Same comment here.  Let's handle the error case right next the condition that causes it, so that we can de-indent the bulk of the code here.",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/26621,665588596,2021-07-07T17:51:39Z,src/core/lib/security/credentials/jwt/json_token.cc,"@@ -182,7 +182,15 @@ static char* encoded_jwt_claim(const grpc_auth_json_key* json_key,    if (scope != nullptr) {     object[""scope""] = scope;-    if (!clear_audience) {+    // This code path is for the self-signed jwt token used to access+    // google API's, and it needs to satisfy the requirement specified","I agree the code is not specific to Google API, but also do not think it is for a general JWT token for the following reasons:1) In the current JWT implementation, it sets the ""iss"" field with the client's email address which is consistent to the Google API requirement (https://google.aip.dev/auth/4111) but I did not see such a requirement in the JWT RFC - https://datatracker.ietf.org/doc/html/rfc7519. 2) I do not know the history of `grpc_service_account_jwt_access_credentials`, but the name also indicates that the API is not for a general JWT token. Since the `grpc_service_account_jwt_access_credentials` API does not take a ""sub"" field as an argument (when it is designed), I think the API is designed for a self-signed JWT token that should have the same value for ""iss"" and ""sub"" fields. Although another possibility is to have an empty ""sub"" field, which is the case when ""scope"" is specified (per original implementation), I do not think it will be a problem to have both ""scope"" and non-empty ""sub"" fields because it is up to the application (verifier) to decide whether to consume the ""sub"" field or not when ""scope"" is present. Also note that all current call sites of `grpc_service_account_jwt_access_credentials` API are generating JWT tokens having both ""sub"" and ""iss"" fields (i.e., both set to the same value). So I suggest updating the current PR to always set the ""sub"" field to be the same as ""iss"" field. This behavior is not specific to google API, but more of a feature for a self-signed JWT token which I believe this API is designed for. WDYT?  ",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26632,666257106,2021-07-08T14:39:53Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -1548,6 +1548,10 @@ void ClientChannel::UpdateServiceConfigInDataPlaneLocked() {     // Process calls that were queued waiting for the resolver result.     for (ResolverQueuedCall* call = resolver_queued_calls_; call != nullptr;          call = call->next) {+      // InvalidateNow to avoid getting stuck re-initializing this timer","This comment is a little confusing as currently written.  There is no timer here, so it's not clear what ""this timer"" refers to.  And the issue in #26079 was actually different than the one that we're solving here -- they both have to do with caching the value of ""now"", but this one isn't related to the WorkSerializer or timers that reschedule themselves.I suggest changing this comment to say the following:""""""If there are a lot of queued calls here, resuming them all may cause us to stay inside C-core for a long period of time.  All of that work would be done using the same ExecCtx instance and therefore the same cached value of ""now"".  The longer it takes to finish all of this work and exit from C-core, the more stale the cached value of ""now"" may become.  This can cause problems whereby (e.g.) we calculate a timer deadline based on the stale value, which results in the timer firing too early.  To avoid this, we invalidate the cached value for each call we process.""""""",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26640,666527371,2021-07-08T21:23:26Z,test/core/gprpp/match_test.cc,"@@ -0,0 +1,45 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/lib/gprpp/match.h""+#include <gtest/gtest.h>++namespace grpc_core {+namespace testing {++TEST(MatchTest, Test) {","If I'm reading the code correctly, it looks like it should allow the functions to modify the value in the variant by taking their parameters as a reference.  Is that right?  If so, how about adding a test that covers that case?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26640,666528572,2021-07-08T21:25:36Z,src/core/lib/gprpp/overload.h,"@@ -0,0 +1,49 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_GPRPP_OVERLOAD_H+#define GRPC_CORE_LIB_GPRPP_OVERLOAD_H++namespace grpc_core {++template <typename... Cases>+struct OverloadType;+// Compose one overload with N more -- use inheritance to leverage using and the+// empty base class optimization.+template <typename Case, typename... Cases>+struct OverloadType<Case, Cases...> : public Case,+                                      public OverloadType<Cases...> {+  explicit OverloadType(Case c, Cases... cases)+      : Case(std::move(c)), OverloadType<Cases...>(std::move(cases)...) {}+  using Case::operator();+  using OverloadType<Cases...>::operator();+};+// Overload of a single case is just that case itself+template <typename Case>+struct OverloadType<Case> : public Case {+  explicit OverloadType(Case c) : Case(std::move(c)) {}+  using Case::operator();+};++// For callables A, B, C, ..., with different argument lists, return a callable+// that is callable with all argument lists by building the sum type of A, B,","What does ""all argument lists"" mean here?  The wording is a little unclear, but I *think* it means that the resulting overload object can be invoked with the arguments of any of its component callables, in which case it will just call the component callable whose arguments match the arguments that overload was invoked with.  Is that right?",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/26640,667011426,2021-07-09T14:54:40Z,test/core/gprpp/match_test.cc,"@@ -0,0 +1,45 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/lib/gprpp/match.h""+#include <gtest/gtest.h>++namespace grpc_core {+namespace testing {++TEST(MatchTest, Test) {","I'd left that open and it turned out to be a terrible idea... so I removed it.If we end up wanting it, let's re-add it later in a principled fashion (maybe `MutatingMatch(variant<>*, cases...)`)",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26671,668307909,2021-07-12T22:52:56Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_test_resources.py,"@@ -132,16 +132,22 @@ class GcpResourceManager(metaclass=_MetaSingletonAndAbslFlags):     (except the client K8s deployment).     """""" -    def __init__(self, absl_flags: Mapping[str, Any]):-        for key in absl_flags:-            setattr(self, key, absl_flags[key])+    # This class dynamically set, so disable ""no-member"" check.+    # pylint: disable=no-member++    def __init__(self, absl_flags: Mapping[str, Any] = None):+        if absl_flags is not None:+            for key in absl_flags:+                setattr(self, key, absl_flags[key])         # API managers         self.k8s_api_manager = k8s.KubernetesApiManager(self.kube_context)         self.gcp_api_manager = gcp.api.GcpApiManager()         self.td = traffic_director.TrafficDirectorManager(             self.gcp_api_manager,             self.project,+            # TODO(lidiz): replace namespace with resource prefix/suffix.             resource_prefix=self.namespace,","To make this work, we need to update this as well: https://github.com/grpc/grpc/blob/ee4f6854bd6d56a16187923421c940e5a47a5a49/tools/internal_ci/linux/grpc_xds_url_map.sh#L96",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26670,668403903,2021-07-13T03:42:29Z,test/core/util/tls_utils.cc,"@@ -71,6 +72,68 @@ std::string GetFileContents(const char* path) {   return credential; } +absl::Status CheckPrivateKey(absl::string_view private_key) {+  if (private_key.empty()) {+    return absl::InvalidArgumentError(""Private key string is empty."");+  }+  BIO* private_key_bio =+      BIO_new_mem_buf(private_key.data(), private_key.size());+  if (private_key_bio == nullptr) {+    return absl::InvalidArgumentError(+        ""Conversion from private key string to BIO failed."");+  }+  EVP_PKEY* private_evp_pkey =+      PEM_read_bio_PrivateKey(private_key_bio, nullptr, nullptr, nullptr);+  if (private_evp_pkey == nullptr) {+    BIO_free(private_key_bio);+    return absl::InvalidArgumentError(""Invalid private key string."");+  }+  int pkey_type = EVP_PKEY_id(private_evp_pkey);+  bool is_key_type_defined = true;+  switch (pkey_type) {+    case EVP_PKEY_NONE:+      is_key_type_defined = false;+      break;+    case EVP_PKEY_RSA:+    case EVP_PKEY_RSA_PSS:+      // The cases that lead here represent currently supported key types.+      break;+    default:+      gpr_log(GPR_ERROR, ""Key type currently not supported."");+  }+  BIO_free(private_key_bio);+  EVP_PKEY_free(private_evp_pkey);+  return is_key_type_defined+             ? absl::OkStatus()+             : absl::InvalidArgumentError(""Undefined key type."");+}++absl::Status CheckCertChain(absl::string_view cert_chain) {+  if (cert_chain.empty()) {+    return absl::InvalidArgumentError(""Certificate chain string is empty."");+  }+  BIO* cert_chain_bio = BIO_new_mem_buf(cert_chain.data(), cert_chain.size());+  STACK_OF(X509_INFO)* cert_stack =+      PEM_X509_INFO_read_bio(cert_chain_bio, nullptr, nullptr, nullptr);+  int num_certs = sk_X509_INFO_num(cert_stack);+  gpr_log(GPR_ERROR, ""Num of certs: %d"", num_certs);+  bool is_null = false;+  bool are_all_certs_invalid = num_certs == 0;","If the cert_chain is empty, i.e. num_certs is zero, please return early. Then we won't need are_all_certs_invalid variable.",X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26670,668406542,2021-07-13T03:51:47Z,test/core/util/tls_utils.h,"@@ -43,6 +43,16 @@ PemKeyCertPairList MakeCertKeyPairs(absl::string_view private_key,  std::string GetFileContents(const char* path); +//  Checks that the string `private_key` is that of a key of supported type and+//  in PKCS #8 syntax. Returns an OK status if so or a not-OK status otherwise.+//  Logs supported but unexpected types.+absl::Status CheckPrivateKey(absl::string_view private_key);",I feel we should choose a more specific naming as to what these functions do. CheckPrivateKey or CheckCertChain seems too vague.,X
84942730,itsemmanuelfrancis,https://api.github.com/repos/grpc/grpc/pulls/26670,668883289,2021-07-13T15:36:45Z,test/core/util/tls_utils.h,"@@ -43,6 +43,16 @@ PemKeyCertPairList MakeCertKeyPairs(absl::string_view private_key,  std::string GetFileContents(const char* path); +//  Checks that the string `private_key` is that of a key of supported type and+//  in PKCS #8 syntax. Returns an OK status if so or a not-OK status otherwise.+//  Logs supported but unexpected types.+absl::Status CheckPrivateKey(absl::string_view private_key);++//  Checks that the string `cert_chain` is that of a certificate chain+//  containing certificates of supported type and PEM-encoded.",Changed it to```//  Checks that the string `cert_chain` is that of a certificate chain//  containing PEM-encoded X.509 certificates.//  Returns an OK status if so or a not-OK status otherwise.```,X
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/26676,669266242,2021-07-14T04:11:56Z,tools/internal_ci/helper_scripts/prepare_build_macos_rc,"@@ -135,12 +135,13 @@ fi  if [ ""${PREPARE_BUILD_INSTALL_DEPS_PHP}"" == ""true"" ] then-  # Install PHP 7.2 explictly to address missing php header files and+  # Install PHP 7.3 explictly to address missing php header files and   # to work well with the pre-installed phpunit 8.4-  brew install php@7.2-  export LDFLAGS=""-L/usr/local/opt/php@7.2/lib $(LDFLAGS)""-  export CPPFLAGS=""-I/usr/local/opt/php@7.2/include $(CPPFLAGS)""-  export PATH=""/usr/local/opt/php@7.2/bin:/usr/local/opt/php@7.2/sbin:$PATH""+  brew update","This is needed because if not, brew's metadata is not up to date and would have picked a too-old version of `nghttp2` (a dependency somewhere).",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,670811753,2021-07-15T21:18:05Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -413,6 +493,21 @@ absl::StatusOr<bool> PrivateKeyAndCertificateMatch(   return result; } +grpc_core::PemKeyCertPairList GetValidKeyCertPairList(+    const grpc_core::PemKeyCertPairList& pair_list) {+  if (pair_list.empty()) {","Is this check necessary? The for loop won't execute, and empty pair_list would be returned.",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,670833511,2021-07-15T22:01:06Z,test/core/security/grpc_tls_certificate_provider_test.cc,"@@ -536,13 +536,169 @@ TEST_F(GrpcTlsCertificateProviderTest, SuccessfulKeyCertMatch) {   EXPECT_TRUE(*status); } +TEST_F(GrpcTlsCertificateProviderTest, SuccessfulKeyMultipleCertMatch) {+  absl::StatusOr<bool> status = PrivateKeyAndCertificateMatch(+      private_key_2_, /*cert_chain*/ cert_chain_2_ + cert_chain_);+  EXPECT_TRUE(status.ok());",Suggest using ASSERT_TRUE here for ok status. Since that is a precondition for the below expectation. Similarly for other test cases.,X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,670833658,2021-07-15T22:01:26Z,test/core/security/grpc_tls_certificate_provider_test.cc,"@@ -536,13 +536,169 @@ TEST_F(GrpcTlsCertificateProviderTest, SuccessfulKeyCertMatch) {   EXPECT_TRUE(*status); } +TEST_F(GrpcTlsCertificateProviderTest, SuccessfulKeyMultipleCertMatch) {+  absl::StatusOr<bool> status = PrivateKeyAndCertificateMatch(+      private_key_2_, /*cert_chain*/ cert_chain_2_ + cert_chain_);",nit: ``` /*cert_chain=*/```,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,670839800,2021-07-15T22:15:29Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -34,7 +34,7 @@ StaticDataCertificateProvider::StaticDataCertificateProvider(     grpc_core::PemKeyCertPairList pem_key_cert_pairs)     : distributor_(MakeRefCounted<grpc_tls_certificate_distributor>()),       root_certificate_(std::move(root_certificate)),-      pem_key_cert_pairs_(std::move(pem_key_cert_pairs)) {+      pem_key_cert_pairs_(GetValidKeyCertPairList(pem_key_cert_pairs)) {","Currently it takes a reference, you would want to change that, to move pem_key_cert_pairs```grpc_core::PemKeyCertPairList GetValidKeyCertPairList(grpc_core::PemKeyCertPairList pair_list)```",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26690,670847983,2021-07-15T22:35:41Z,src/core/lib/security/credentials/tls/tls_utils.h,"@@ -24,6 +24,10 @@ #include <string> #include <vector> +#include ""openssl/evp.h""+#include ""openssl/bio.h""+#include ""openssl/x509.h""","If the only call site  is `grpc_tls_certificate_provider.cc`, you probably don't want to make it in the file now. You can put everything in an anonymous namespace before `PrivateKeyAndCertificateMatch`, like:```namespace {    struct EVP_PKEYDeleter {    }    using OwnedEVP_PKEY = ...}  // namespace```Also I felt like `tls_utils.h` might not be the right place for this...originally I was considering creating a new file for helper functions to make better use of OpenSSL/BoringSSL data structures.",X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26690,670857402,2021-07-15T22:59:51Z,src/core/lib/security/credentials/tls/tls_utils.h,"@@ -46,6 +50,20 @@ absl::string_view GetAuthPropertyValue(grpc_auth_context* context, std::vector<absl::string_view> GetAuthPropertyArray(grpc_auth_context* context,                                                     const char* property_name); +struct EVP_PKEYDeleter {+  void operator()(EVP_PKEY* pkey) const { EVP_PKEY_free(pkey); }+};+struct BIO_Deleter {+  void operator()(BIO* bio) const { BIO_free(bio); }+};+struct X509_Deleter {+  void operator()(X509* x509) const { X509_free(x509); }+};","As we add more features interacting with OpenSSL/BoringSSL, I think there is a need for creating some helper functions/data structures for OpenSSL/BoringSSL. This could be a good starting point.On the other side, I am still not sure if a change like this would be an overkill for such kind of task. In gRPC, creating an object and deleting it later is a common pattern, e.g. `gpr_malloc` and `gpr_free`. Looking at the code again, it seems remembering to `free` the object shouldn't be too bad, and changes like this will cause unnecessary indirections while reading the code. We can always keep it simple, and define such helper functions when deleting such objects really becomes a problem.@markdroth may I know your opinion about this change?",
109690,davidben,https://api.github.com/repos/grpc/grpc/pulls/26690,670865988,2021-07-15T23:25:12Z,src/core/lib/security/credentials/tls/tls_utils.h,"@@ -46,6 +50,20 @@ absl::string_view GetAuthPropertyValue(grpc_auth_context* context, std::vector<absl::string_view> GetAuthPropertyArray(grpc_auth_context* context,                                                     const char* property_name); +struct EVP_PKEYDeleter {+  void operator()(EVP_PKEY* pkey) const { EVP_PKEY_free(pkey); }+};+struct BIO_Deleter {+  void operator()(BIO* bio) const { BIO_free(bio); }+};+struct X509_Deleter {+  void operator()(X509* x509) const { X509_free(x509); }+};","I imagine the commonness of a manual freeing comes from gRPC originally being in C. RAII  is probably the single biggest benefit C++ goes over C. While it's true that, in simple functions, it's not too bad, it's a huge benefit across the codebase to be able to do it automatically. Code becomes dramatically less error-prone when you don't need to manually free everything on ever return path.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26706,671376293,2021-07-16T16:18:13Z,src/python/grpcio/grpc/aio/_call.py,"@@ -399,18 +399,26 @@ async def _consume_request_iterator(             if inspect.isasyncgen(request_iterator) or hasattr(                     request_iterator, '__aiter__'):                 async for request in request_iterator:-                    await self._write(request)+                    try:+                        await self._write(request)+                    except AioRpcError as rpc_error:+                        _LOGGER.debug('Exception while consuming the request_iterator: %s', rpc_error)+                        return             else:                 for request in request_iterator:-                    await self._write(request)+                    try:+                        await self._write(request)+                    except AioRpcError as rpc_error:+                        _LOGGER.debug('Exception while consuming the request_iterator: %s', rpc_error)+                        return              await self._done_writing()-        except AioRpcError as rpc_error:-            # Rpc status should be exposed through other API. Exceptions raised-            # within this Task won't be retrieved by another coroutine. It's-            # better to suppress the error than spamming users' screen.-            _LOGGER.debug('Exception while consuming the request_iterator: %s',-                          rpc_error)+        except Exception as client_iterator_error:+            # Client iterators can raise exceptions, which we should handle by+            # cancelling the RPC and logging the client's error. No exceptions+            # should escape this function.+            _LOGGER.debug('Client request_iterator raised exception: %s', client_iterator_error)","nit: maybe we should also log the type and stack trace in debug, e.g., `traceback.format_exception`. Since this is part of the application, if we can give more detail information, it will help them debug.",X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26709,671520210,2021-07-16T20:54:57Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -198,13 +198,38 @@ void FileWatcherCertificateProvider::ForceUpdate() {       root_certificate_ = """";     }   }+  absl::StatusOr<bool> keyCertMatch = false;",Please update the variable names. They should follow (lowercase) words separated by underscore pattern.https://google.github.io/styleguide/cppguide.html#Variable_Names,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26690,671532255,2021-07-16T21:22:51Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -365,6 +366,10 @@ FileWatcherCertificateProvider::ReadIdentityKeyCertPairFromFiles(   return absl::nullopt; } +using OwnedEVP_PKEY = std::unique_ptr<EVP_PKEY, EVP_PKEYDeleter>;","You might want to follow the naming conventions for types: https://google.github.io/styleguide/cppguide.html#Type_NamesAlso, when naming types, better to stress on the use of those types, while hiding the corresponding OpenSSL type details(e.g. callers might get confused about what is a ""EVP_PKEY"").It might be something like `OwnedOpenSslPrivateKey`. Same for the other two types.",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26690,671541458,2021-07-16T21:48:49Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -373,44 +378,36 @@ absl::StatusOr<bool> PrivateKeyAndCertificateMatch(   if (cert_chain.empty()) {     return absl::InvalidArgumentError(""Certificate string is empty."");   }-  BIO* cert_bio = BIO_new_mem_buf(cert_chain.data(), cert_chain.size());-  if (cert_bio == nullptr) {+  OwnedBIO cert_bio(BIO_new_mem_buf(cert_chain.data(), cert_chain.size()));","Is this BIO variable always an intermediate step for getting keys or certificates? If so, does it make sense to replace the `EVP_PKEYDeleter` and `X509_Deleter` with something like```class OwnedOpenSslPrivateKey {  public:    OwnedOpenSslPrivateKey(const char* private_key, ...) {        // create BIO and convert to private_key_    }    ~OwnedOpenSslPrivateKey() {       if (private_key_ != nullptr) {           EVP_PKEY_free(private_key_);       }    }    EVP_PKEY* get_private_key() {return private_key_; }      private:    EVP_PKEY* private_key_ = nullptr;}```",X
84942730,itsemmanuelfrancis,https://api.github.com/repos/grpc/grpc/pulls/26709,671543150,2021-07-16T21:53:44Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -198,13 +198,38 @@ void FileWatcherCertificateProvider::ForceUpdate() {       root_certificate_ = """";     }   }+  absl::StatusOr<bool> keyCertMatch = false;+  //check whether the present credentials are valid.+  if (!pem_key_cert_pairs_.empty()) {+    for (int i = 0; i < pem_key_cert_pairs_.size(); i++) {+      keyCertMatch =+          PrivateKeyAndCertificateMatch(pem_key_cert_pairs_.at(i).private_key(),+                                        pem_key_cert_pairs_.at(i).cert_chain());+      if (!(keyCertMatch.ok() && *keyCertMatch)) {+        break;+      }+    }+  }   const bool identity_cert_changed =       (!pem_key_cert_pairs.has_value() && !pem_key_cert_pairs_.empty()) ||       (pem_key_cert_pairs.has_value() &&        pem_key_cert_pairs_ != *pem_key_cert_pairs);   if (identity_cert_changed) {     if (pem_key_cert_pairs.has_value()) {-      pem_key_cert_pairs_ = std::move(*pem_key_cert_pairs);+      for (int i = 0; i < pem_key_cert_pairs->size(); i++) {+        keyCertMatch = PrivateKeyAndCertificateMatch(",This is because `size()` returns a type larger than `int`,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26690,671543734,2021-07-16T21:55:22Z,src/core/lib/security/credentials/tls/openssl_utils.h,"@@ -0,0 +1,27 @@+//+// Created by itsemmanuel on 7/16/21.+// TODO: Replace with appropriate comments.+//++#ifndef SRC_CORE_TSI_OPENSSL_UTILS_H_+#define SRC_CORE_TSI_OPENSSL_UTILS_H_++#include ""openssl/bio.h""+#include ""openssl/evp.h""+#include ""openssl/x509.h""++namespace grpc_core {++struct EVP_PKEYDeleter {","Structs defined here are in public headers, so we will think twice about these things:1. conform to the naming conventions as mentioned elsewhere2. add a comment for the use of each of them3. consider adding proper prefix to avoid naming conflicts. Maybe `OpenSsl`?If those type alias(those `using` statements) are going to be used by multiple places, it might make sense to put them here as well, but that will also make them part of the public headers, and they are subject to the above rules. You might find this useful: https://google.github.io/styleguide/cppguide.html#Aliases",X
84942730,itsemmanuelfrancis,https://api.github.com/repos/grpc/grpc/pulls/26690,671549919,2021-07-16T22:15:03Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -373,44 +378,36 @@ absl::StatusOr<bool> PrivateKeyAndCertificateMatch(   if (cert_chain.empty()) {     return absl::InvalidArgumentError(""Certificate string is empty."");   }-  BIO* cert_bio = BIO_new_mem_buf(cert_chain.data(), cert_chain.size());-  if (cert_bio == nullptr) {+  OwnedBIO cert_bio(BIO_new_mem_buf(cert_chain.data(), cert_chain.size()));","Making ""owner"" classes sounds great!",X
84942730,itsemmanuelfrancis,https://api.github.com/repos/grpc/grpc/pulls/26690,671565344,2021-07-16T23:12:31Z,src/core/lib/security/credentials/tls/openssl_utils.h,"@@ -0,0 +1,27 @@+//+// Created by itsemmanuel on 7/16/21.+// TODO: Replace with appropriate comments.+//++#ifndef SRC_CORE_TSI_OPENSSL_UTILS_H_+#define SRC_CORE_TSI_OPENSSL_UTILS_H_++#include ""openssl/bio.h""+#include ""openssl/evp.h""+#include ""openssl/x509.h""++namespace grpc_core {++struct EVP_PKEYDeleter {",I'm using classes now so this doesn't apply.,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,671582617,2021-07-17T00:47:24Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -365,6 +365,86 @@ FileWatcherCertificateProvider::ReadIdentityKeyCertPairFromFiles(   return absl::nullopt; } +DataWatcherCertificateProvider::DataWatcherCertificateProvider(+    std::string root_certificate,+    grpc_core::PemKeyCertPairList pem_key_cert_pairs)+    : StaticDataCertificateProvider(std::move(root_certificate), std::move(pem_key_cert_pairs)) {}++absl::Status DataWatcherCertificateProvider::ReloadRootCertificate(+    const std::string& root_certificate) {+  if (root_certificate.empty()) {+    return absl::InvalidArgumentError(""Root Certificate string is empty."");+  }+  grpc_core::MutexLock lock(&mu_);+  if (root_certificate_ == root_certificate) {+    return absl::InvalidArgumentError(""Root Certificate has not changed."");+  }+  root_certificate_ = root_certificate;+  ExecCtx exec_ctx;+  grpc_error_handle root_cert_error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(+      ""Unable to get latest root certificates."");+  for (const auto& p : watcher_info_) {+    const std::string& cert_name = p.first;+    const WatcherInfo& info = p.second;+    absl::optional<std::string> root_to_report;+    if (info.root_being_watched) {+      root_to_report = root_certificate_;+    }+    if (root_to_report.has_value()) {+      distributor_->SetKeyMaterials(cert_name, std::move(root_to_report),+                                    absl::nullopt);+    }+    if (info.root_being_watched && root_certificate_.empty()) {","May I know when will this condition be reached?From what I understood, if root_certificate_ were empty, it would fail before it reaches here.",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,671584161,2021-07-17T00:58:34Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -365,6 +365,86 @@ FileWatcherCertificateProvider::ReadIdentityKeyCertPairFromFiles(   return absl::nullopt; } +DataWatcherCertificateProvider::DataWatcherCertificateProvider(+    std::string root_certificate,+    grpc_core::PemKeyCertPairList pem_key_cert_pairs)+    : StaticDataCertificateProvider(std::move(root_certificate), std::move(pem_key_cert_pairs)) {}++absl::Status DataWatcherCertificateProvider::ReloadRootCertificate(+    const std::string& root_certificate) {+  if (root_certificate.empty()) {+    return absl::InvalidArgumentError(""Root Certificate string is empty."");+  }+  grpc_core::MutexLock lock(&mu_);+  if (root_certificate_ == root_certificate) {+    return absl::InvalidArgumentError(""Root Certificate has not changed."");+  }+  root_certificate_ = root_certificate;+  ExecCtx exec_ctx;+  grpc_error_handle root_cert_error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(+      ""Unable to get latest root certificates."");+  for (const auto& p : watcher_info_) {+    const std::string& cert_name = p.first;+    const WatcherInfo& info = p.second;+    absl::optional<std::string> root_to_report;+    if (info.root_being_watched) {+      root_to_report = root_certificate_;+    }+    if (root_to_report.has_value()) {+      distributor_->SetKeyMaterials(cert_name, std::move(root_to_report),+                                    absl::nullopt);+    }+    if (info.root_being_watched && root_certificate_.empty()) {+      distributor_->SetErrorForCert(cert_name, GRPC_ERROR_REF(root_cert_error),+                                    absl::nullopt);+    }+  }+  GRPC_ERROR_UNREF(root_cert_error);+  return absl::OkStatus();+}++absl::Status DataWatcherCertificateProvider::ReloadKeyCertificatePair(+    grpc_core::PemKeyCertPairList pem_key_cert_pairs) {+  if (pem_key_cert_pairs.empty()) {+    return absl::InvalidArgumentError(""Empty Key-Cert pair list."");+  }+  if (pem_key_cert_pairs == pem_key_cert_pairs_) {+    return absl::InvalidArgumentError(+        ""The Key-Cert pair list has not changed."");+  }+  for (unsigned long i = 0; i < pem_key_cert_pairs.size(); i++) {",Why use unsigned long? Please use size_t or even better use range based loop.Also prefer using ++i over i++https://google.github.io/styleguide/cppguide.html#Preincrement_and_Predecrement,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,671588241,2021-07-17T01:30:14Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -365,6 +365,86 @@ FileWatcherCertificateProvider::ReadIdentityKeyCertPairFromFiles(   return absl::nullopt; } +DataWatcherCertificateProvider::DataWatcherCertificateProvider(+    std::string root_certificate,+    grpc_core::PemKeyCertPairList pem_key_cert_pairs)+    : StaticDataCertificateProvider(std::move(root_certificate), std::move(pem_key_cert_pairs)) {}++absl::Status DataWatcherCertificateProvider::ReloadRootCertificate(+    const std::string& root_certificate) {+  if (root_certificate.empty()) {+    return absl::InvalidArgumentError(""Root Certificate string is empty."");+  }+  grpc_core::MutexLock lock(&mu_);+  if (root_certificate_ == root_certificate) {+    return absl::InvalidArgumentError(""Root Certificate has not changed."");+  }+  root_certificate_ = root_certificate;+  ExecCtx exec_ctx;+  grpc_error_handle root_cert_error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(+      ""Unable to get latest root certificates."");+  for (const auto& p : watcher_info_) {+    const std::string& cert_name = p.first;+    const WatcherInfo& info = p.second;+    absl::optional<std::string> root_to_report;+    if (info.root_being_watched) {+      root_to_report = root_certificate_;+    }+    if (root_to_report.has_value()) {+      distributor_->SetKeyMaterials(cert_name, std::move(root_to_report),+                                    absl::nullopt);+    }+    if (info.root_being_watched && root_certificate_.empty()) {+      distributor_->SetErrorForCert(cert_name, GRPC_ERROR_REF(root_cert_error),+                                    absl::nullopt);+    }+  }+  GRPC_ERROR_UNREF(root_cert_error);+  return absl::OkStatus();+}++absl::Status DataWatcherCertificateProvider::ReloadKeyCertificatePair(+    grpc_core::PemKeyCertPairList pem_key_cert_pairs) {+  if (pem_key_cert_pairs.empty()) {+    return absl::InvalidArgumentError(""Empty Key-Cert pair list."");+  }+  if (pem_key_cert_pairs == pem_key_cert_pairs_) {+    return absl::InvalidArgumentError(+        ""The Key-Cert pair list has not changed."");+  }+  for (unsigned long i = 0; i < pem_key_cert_pairs.size(); i++) {","Can we reuse GetValidKeyCertPairList function here instead? So we won't need to repeat code here.This means we will have to update the function signature of GetValidKeyCertPairList, make it return a status and accept pair list as a reference. If the function returns an OK status the input was valid, and we just use that. If we get a non-OK status we return that status.In StaticDataCertificateProvider, if GetValidKeyCertPairList returns not-OK status, we initialize to an empty list. For an OK status, we move pem_key_cert_pairs to member variable.Wdyt?",X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,671588509,2021-07-17T01:32:43Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.h,"@@ -134,12 +133,34 @@ class FileWatcherCertificateProvider final   std::map<std::string, WatcherInfo> watcher_info_; }; +// A provider class that initializes its credentials from strings and supports+// reloading said credentials from memory.+class DataWatcherCertificateProvider : public StaticDataCertificateProvider {+ public:+  DataWatcherCertificateProvider(+      std::string root_certificate,+      grpc_core::PemKeyCertPairList pem_key_cert_pairs);++  // Reloads the root_certificate and updates the distributor.+  absl::Status ReloadRootCertificate(const std::string& root_certificate);++  // Reloads the key-cert pair list and updates the distributor.+  absl::Status ReloadKeyCertificatePair(+      grpc_core::PemKeyCertPairList pem_key_cert_pairs);+};+ //  Checks if the private key matches the certificate's public key. //  Returns a not-OK status on failure, or a bool indicating //  whether the key/cert pair matches. absl::StatusOr<bool> PrivateKeyAndCertificateMatch(     absl::string_view private_key, absl::string_view cert_chain); +//  Checks if the private key and leaf cert for all pairs in the list match.+//  Returns the passed pair list if the match is successful and an empty one+//  otherwise.+grpc_core::PemKeyCertPairList GetValidKeyCertPairList(",Based on my recommendation in previous commentThis function would look like```absl::Status CheckValidKeyCertPairList(const PemKeyCertPairList& pair_list)```,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,671589753,2021-07-17T01:43:36Z,test/core/security/grpc_tls_certificate_provider_test.cc,"@@ -532,17 +532,173 @@ TEST_F(GrpcTlsCertificateProviderTest, TEST_F(GrpcTlsCertificateProviderTest, SuccessfulKeyCertMatch) {   absl::StatusOr<bool> status =       PrivateKeyAndCertificateMatch(private_key_2_, cert_chain_2_);-  EXPECT_TRUE(status.ok());+  ASSERT_TRUE(status.ok());+  EXPECT_TRUE(*status);+}++TEST_F(GrpcTlsCertificateProviderTest, SuccessfulKeyMultipleCertMatch) {+  absl::StatusOr<bool> status = PrivateKeyAndCertificateMatch(+      private_key_2_, /*cert_chain=*/ cert_chain_2_ + cert_chain_);",This would have failed if it were cert_chain_+cert_chain_2_ right?Then do we need both test cases single cert and multiple cert. The function anyway ignores all certs after leaf certificate. Maybe just the multiple test case is enough.,X
85572583,johann1000,https://api.github.com/repos/grpc/grpc/pulls/26709,672427380,2021-07-19T15:56:52Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -198,13 +198,38 @@ void FileWatcherCertificateProvider::ForceUpdate() {       root_certificate_ = """";     }   }+  absl::StatusOr<bool> keyCertMatch = false;+  //check whether the present credentials are valid.+  if (!pem_key_cert_pairs_.empty()) {+    for (int i = 0; i < pem_key_cert_pairs_.size(); i++) {+      keyCertMatch =+          PrivateKeyAndCertificateMatch(pem_key_cert_pairs_.at(i).private_key(),+                                        pem_key_cert_pairs_.at(i).cert_chain());+      if (!(keyCertMatch.ok() && *keyCertMatch)) {+        break;+      }+    }+  }   const bool identity_cert_changed =       (!pem_key_cert_pairs.has_value() && !pem_key_cert_pairs_.empty()) ||","I think that's to enable deletion of credentials, or am I missing something here?PS: I didn't change the conditions for identity_cert_changed, they've always been like that",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26722,672429501,2021-07-19T15:59:29Z,tools/distrib/check_copyright.py,"@@ -69,6 +70,31 @@     'BUILD': r'#\s*', } +LICENSE_PREFIX_TEXT = {+    '.bat': (None, '@rem', None),",nit: We could use a named tuple or dataclass or comments to describe the usage of each item here.,X
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/26722,672492414,2021-07-19T17:27:29Z,tools/distrib/check_copyright.py,"@@ -173,7 +214,12 @@ def log(cond, why, filename):     if m:         pass     elif 'DO NOT EDIT' not in text:-        log(1, 'copyright missing', filename)+        if args.fix:+            text = license_text + '\n\n' + text","We're using `'\n'.join()` before so two are needed for a blank line.Anyway, this is no longer relevant in the updated CL.",X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,672536278,2021-07-19T18:33:58Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -413,6 +494,20 @@ absl::StatusOr<bool> PrivateKeyAndCertificateMatch(   return result; } +absl::Status IsKeyCertPairListValid(const PemKeyCertPairList& pair_list) {+  if (pair_list.empty()) {",Should this be an error? We could have an empty pair list right?,X
84942730,itsemmanuelfrancis,https://api.github.com/repos/grpc/grpc/pulls/26663,672560953,2021-07-19T19:12:14Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -33,8 +33,12 @@ StaticDataCertificateProvider::StaticDataCertificateProvider(     std::string root_certificate,     grpc_core::PemKeyCertPairList pem_key_cert_pairs)     : distributor_(MakeRefCounted<grpc_tls_certificate_distributor>()),-      root_certificate_(std::move(root_certificate)),-      pem_key_cert_pairs_(std::move(pem_key_cert_pairs)) {+      root_certificate_(std::move(root_certificate)) {+  if (IsKeyCertPairListValid(pem_key_cert_pairs).ok()) {+    pem_key_cert_pairs_ = std::move(pem_key_cert_pairs);",I tried that and got a `initializer list cannot be used on the right hand side of operator :` error so I just went with if-else.,
84942730,itsemmanuelfrancis,https://api.github.com/repos/grpc/grpc/pulls/26663,672565055,2021-07-19T19:18:31Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -33,8 +33,12 @@ StaticDataCertificateProvider::StaticDataCertificateProvider(     std::string root_certificate,     grpc_core::PemKeyCertPairList pem_key_cert_pairs)     : distributor_(MakeRefCounted<grpc_tls_certificate_distributor>()),-      root_certificate_(std::move(root_certificate)),-      pem_key_cert_pairs_(std::move(pem_key_cert_pairs)) {+      root_certificate_(std::move(root_certificate)) {+  if (IsKeyCertPairListValid(pem_key_cert_pairs).ok()) {+    pem_key_cert_pairs_ = std::move(pem_key_cert_pairs);",Similar situation: https://stackoverflow.com/questions/9982681/ternary-operator-c11-constructor-from-initializer-list,
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,672572617,2021-07-19T19:30:26Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -413,6 +486,19 @@ absl::StatusOr<bool> PrivateKeyAndCertificateMatch(   return result; } +absl::Status IsKeyCertPairsListValid(const PemKeyCertPairList& pair_list) {+  if (!pair_list.empty()) {",This check is not required. For loop won't execute if list is empty.,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26709,674183789,2021-07-21T17:19:07Z,test/core/security/grpc_tls_certificate_provider_test.cc,"@@ -495,6 +495,67 @@ TEST_F(GrpcTlsCertificateProviderTest,   CancelWatch(watcher_state_1); } +TEST_F(GrpcTlsCertificateProviderTest,+       FileWatcherCertificateProviderWithInvalidCertificateKeyPairOnRefresh) {+  // Create temporary files and copy cert data into them.+  TmpFile tmp_root_cert(root_cert_);+  TmpFile tmp_identity_key(private_key_);+  TmpFile tmp_identity_cert(cert_chain_);+  // Create FileWatcherCertificateProvider.+  FileWatcherCertificateProvider provider(tmp_identity_key.name(),+                                          tmp_identity_cert.name(),+                                          tmp_root_cert.name(), 1);+  WatcherState* watcher_state_1 =+      MakeWatcher(provider.distributor(), kCertName, kCertName);+  // Expect to see the credential data.+  EXPECT_THAT(watcher_state_1->GetCredentialQueue(),+              ::testing::ElementsAre(CredentialInfo(+                  root_cert_, MakeCertKeyPairs(private_key_.c_str(),+                                               cert_chain_.c_str()))));+  // Copy new data to files.+  // TODO(ZhenLian): right now it is not completely atomic. Use the real atomic+  // update when the directory renaming is added in gpr.","@markdroth Can we delete the TODO here? Because I think now we don't need the atomic update. Early on we need this because there could be the time when the credentials are updated half way in the middle while our refreshing thread is reading.Now since we have the key-cert mismatch check, even if that happens, the current content will be disregarded, and picked up again next time(as the test logic says). ",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26709,674192388,2021-07-21T17:31:34Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -198,13 +198,38 @@ void FileWatcherCertificateProvider::ForceUpdate() {       root_certificate_ = """";     }   }+  absl::StatusOr<bool> keyCertMatch = false;+  //check whether the present credentials are valid.+  if (!pem_key_cert_pairs_.empty()) {+    for (int i = 0; i < pem_key_cert_pairs_.size(); i++) {+      keyCertMatch =+          PrivateKeyAndCertificateMatch(pem_key_cert_pairs_.at(i).private_key(),+                                        pem_key_cert_pairs_.at(i).cert_chain());+      if (!(keyCertMatch.ok() && *keyCertMatch)) {+        break;+      }+    }+  }   const bool identity_cert_changed =       (!pem_key_cert_pairs.has_value() && !pem_key_cert_pairs_.empty()) ||","That's an interesting topic. At first(before the always-failing handshaker went out), I thought we better do the check of empty creds, since that are blatantly misuse of our API and we'd better avoid our program to crash.With the always-failing handshaker, if that happens, our program won't crash - the server would only drop any connections it takes. That makes me wonder if there could be a certain period of time when users want to clear their creds and make use of the always-failing handshaker. Though I would say that case might be rare and probably might never exist.But the point is to not ""help"" users to make the decision. If they at some point clear their cred files, then we shall just clear our data as well. Users might see this as unexpected, but that's because they clear the files in the first place - which is already a mis-behavior.The same logic applies to the `DataWatcherCertificateProvider` as well.",X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26690,674225856,2021-07-21T18:22:54Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -373,44 +373,24 @@ absl::StatusOr<bool> PrivateKeyAndCertificateMatch(   if (cert_chain.empty()) {     return absl::InvalidArgumentError(""Certificate string is empty."");   }-  BIO* cert_bio = BIO_new_mem_buf(cert_chain.data(), cert_chain.size());-  if (cert_bio == nullptr) {-    return absl::InvalidArgumentError(-        ""Conversion from certificate string to BIO failed."");-  }-  // Reads the first cert from the cert_chain which is expected to be the leaf-  // cert-  X509* x509 = PEM_read_bio_X509(cert_bio, nullptr, nullptr, nullptr);-  BIO_free(cert_bio);-  if (x509 == nullptr) {+  OwnedOpenSslX509 x509(cert_chain.data(), cert_chain.size());+  if (!x509.get_x509()) {     return absl::InvalidArgumentError(         ""Conversion from PEM string to X509 failed."");   }-  EVP_PKEY* public_evp_pkey = X509_get_pubkey(x509);-  X509_free(x509);-  if (public_evp_pkey == nullptr) {+  OwnedOpenSslPrivateKey public_evp_pkey(X509_get_pubkey(x509.get_x509()));",Oh so this can also include public key. Let's change the class name to `OwnedOpenSslPKey` then.Also I feel like there is no need to call it `Owned*`. Maybe just `OpenSslX509` and `OpenSslPKey` would be sufficient.,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26690,674232672,2021-07-21T18:33:34Z,src/core/tsi/openssl_utils.h,"@@ -0,0 +1,58 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#ifndef SRC_CORE_TSI_OPENSSL_UTILS_H_+#define SRC_CORE_TSI_OPENSSL_UTILS_H_++#include ""openssl/bio.h""+#include ""openssl/evp.h""+#include ""openssl/x509.h""++namespace grpc_core {++// A class for managing openssl `EVP_PKEY` structures.+class OwnedOpenSslPrivateKey {+ public:+  explicit OwnedOpenSslPrivateKey(const char* private_key, int size);",No need to be `explicit`. `explicit` will only be useful when the ctor taking exactly one parameter,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26714,674369266,2021-07-21T21:50:00Z,src/core/lib/channel/context.h,"@@ -32,6 +32,9 @@ typedef enum {   /// Value is a \a census_context.   GRPC_CONTEXT_TRACING, +  /// Value is a CallTracer object.+  GRPC_CONTEXT_CALL_TRACER,","I'd rather not change the API used by wrapped languages until we have an approved design for the longer-term stats changes.  For now, this API is needed only between the census filter and code below it, so let's not expose it publicly if we don't have to.",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26690,674398523,2021-07-21T22:58:32Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -373,44 +373,23 @@ absl::StatusOr<bool> PrivateKeyAndCertificateMatch(   if (cert_chain.empty()) {     return absl::InvalidArgumentError(""Certificate string is empty."");   }-  BIO* cert_bio = BIO_new_mem_buf(cert_chain.data(), cert_chain.size());-  if (cert_bio == nullptr) {-    return absl::InvalidArgumentError(-        ""Conversion from certificate string to BIO failed."");-  }-  // Reads the first cert from the cert_chain which is expected to be the leaf-  // cert-  X509* x509 = PEM_read_bio_X509(cert_bio, nullptr, nullptr, nullptr);-  BIO_free(cert_bio);-  if (x509 == nullptr) {+  OpenSslX509 x509(cert_chain.data(), cert_chain.size());",Suggest in the ctor of `OpenSslX509` we directly take `absl::string_view` as the parameter.,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26690,674399195,2021-07-21T23:00:26Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -373,44 +373,23 @@ absl::StatusOr<bool> PrivateKeyAndCertificateMatch(   if (cert_chain.empty()) {     return absl::InvalidArgumentError(""Certificate string is empty."");   }-  BIO* cert_bio = BIO_new_mem_buf(cert_chain.data(), cert_chain.size());-  if (cert_bio == nullptr) {-    return absl::InvalidArgumentError(-        ""Conversion from certificate string to BIO failed."");-  }-  // Reads the first cert from the cert_chain which is expected to be the leaf-  // cert-  X509* x509 = PEM_read_bio_X509(cert_bio, nullptr, nullptr, nullptr);-  BIO_free(cert_bio);-  if (x509 == nullptr) {+  OpenSslX509 x509(cert_chain.data(), cert_chain.size());+  if (x509.get_x509() == nullptr) {     return absl::InvalidArgumentError(         ""Conversion from PEM string to X509 failed."");   }-  EVP_PKEY* public_evp_pkey = X509_get_pubkey(x509);-  X509_free(x509);-  if (public_evp_pkey == nullptr) {+  OpenSslPKey public_evp_pkey(X509_get_pubkey(x509.get_x509()));+  if (public_evp_pkey.get_private_key() == nullptr) {     return absl::InvalidArgumentError(         ""Extraction of public key from x.509 certificate failed."");   }-  BIO* private_key_bio =-      BIO_new_mem_buf(private_key.data(), private_key.size());-  if (private_key_bio == nullptr) {-    EVP_PKEY_free(public_evp_pkey);-    return absl::InvalidArgumentError(-        ""Conversion from private key string to BIO failed."");-  }-  EVP_PKEY* private_evp_pkey =-      PEM_read_bio_PrivateKey(private_key_bio, nullptr, nullptr, nullptr);-  BIO_free(private_key_bio);-  if (private_evp_pkey == nullptr) {-    EVP_PKEY_free(public_evp_pkey);+  OpenSslPKey private_evp_pkey(private_key.data(), private_key.size());",Same: suggest taking `absl::string_view` as the parameter.,X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/26714,674440560,2021-07-22T01:12:24Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -2784,38 +2863,79 @@ void ClientChannel::LoadBalancedCall::StartTransportStreamOpBatch(   } } -void ClientChannel::LoadBalancedCall::-    RecvTrailingMetadataReadyForLoadBalancingPolicy(void* arg,-                                                    grpc_error_handle error) {+void ClientChannel::LoadBalancedCall::SendInitialMetadataOnComplete(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<LoadBalancedCall*>(arg);+  self->call_attempt_tracer_->RecordOnDoneSendInitialMetadata(+      self->peer_string_);+  Closure::Run(DEBUG_LOCATION,+               self->original_send_initial_metadata_on_complete_,+               GRPC_ERROR_REF(error));+}++void ClientChannel::LoadBalancedCall::RecvInitialMetadataReady(+    void* arg, grpc_error_handle error) {   auto* self = static_cast<LoadBalancedCall*>(arg);-  if (self->lb_recv_trailing_metadata_ready_ != nullptr) {-    // Set error if call did not succeed.-    grpc_error_handle error_for_lb = GRPC_ERROR_NONE;+  self->call_attempt_tracer_->RecordReceivedInitialMetadata(+      self->recv_initial_metadata_, *self->recv_initial_metadata_flags_);+  Closure::Run(DEBUG_LOCATION, self->original_recv_initial_metadata_ready_,+               GRPC_ERROR_REF(error));+}++void ClientChannel::LoadBalancedCall::RecvMessageReady(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<LoadBalancedCall*>(arg);+  self->call_attempt_tracer_->RecordReceivedMessage(**self->recv_message_);+  Closure::Run(DEBUG_LOCATION, self->original_recv_message_ready_,+               GRPC_ERROR_REF(error));+}++void ClientChannel::LoadBalancedCall::RecvTrailingMetadataReady(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<LoadBalancedCall*>(arg);+  // Check if we have a tracer or an LB callback to invoke.+  if (self->call_attempt_tracer_ != nullptr ||+      self->lb_recv_trailing_metadata_ready_ != nullptr) {+    // Get the call's status.+    absl::Status status;     if (error != GRPC_ERROR_NONE) {-      error_for_lb = error;+      // Get status from error.+      grpc_status_code code;+      grpc_slice message = grpc_empty_slice();+      grpc_error_get_status(error, self->deadline_, &code, &message,+                            /*http_error=*/nullptr, /*error_string=*/nullptr);+      status = absl::Status(static_cast<absl::StatusCode>(code),+                            StringViewFromSlice(message));     } else {+      // Get status from headers.       const auto& fields = self->recv_trailing_metadata_->idx.named;       GPR_ASSERT(fields.grpc_status != nullptr);-      grpc_status_code status =+      grpc_status_code code =           grpc_get_status_code_from_metadata(fields.grpc_status->md);-      std::string msg;-      if (status != GRPC_STATUS_OK) {-        error_for_lb = grpc_error_set_int(-            GRPC_ERROR_CREATE_FROM_STATIC_STRING(""call failed""),-            GRPC_ERROR_INT_GRPC_STATUS, status);+      if (code != GRPC_STATUS_OK) {+        absl::string_view message;         if (fields.grpc_message != nullptr) {-          error_for_lb = grpc_error_set_str(-              error_for_lb, GRPC_ERROR_STR_GRPC_MESSAGE,-              grpc_slice_ref_internal(GRPC_MDVALUE(fields.grpc_message->md)));+          message = StringViewFromSlice(GRPC_MDVALUE(fields.grpc_message->md));         }+        status = absl::Status(static_cast<absl::StatusCode>(code), message);       }     }-    // Invoke callback to LB policy.-    Metadata trailing_metadata(self, self->recv_trailing_metadata_);-    LbCallState lb_call_state(self);-    self->lb_recv_trailing_metadata_ready_(error_for_lb, &trailing_metadata,-                                           &lb_call_state);-    if (error == GRPC_ERROR_NONE) GRPC_ERROR_UNREF(error_for_lb);+    // If we have a tracer, notify it.+    if (self->call_attempt_tracer_ != nullptr) {+      self->call_attempt_tracer_->RecordReceivedTrailingMetadata(+          status, self->recv_trailing_metadata_,+          *self->transport_stream_stats_);",I don't think this works. We want to record `transport_stream_stats_` irrespective of whether the status had an error or not. I suggest passing it in `RecordEnd()` along with the latency.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26714,675048001,2021-07-22T18:00:15Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -2584,7 +2597,17 @@ ClientChannel::LoadBalancedCall::~LoadBalancedCall() {   for (size_t i = 0; i < GPR_ARRAY_SIZE(pending_batches_); ++i) {     GPR_ASSERT(pending_batches_[i] == nullptr);   }-  if (on_call_destruction_complete_ != nullptr) {+  // Compute latency and report it to the tracer.+  if (call_attempt_tracer_ != nullptr) {+    gpr_timespec latency =+        gpr_cycle_counter_sub(gpr_get_cycle_counter(), lb_call_start_time_);","This is exactly the same code used to compute the latency today:https://github.com/grpc/grpc/blob/6995dcd95877099415c0817537cb8e1fab8bc887/src/core/lib/surface/call.cc#L573I suspect the reason for this is that we want the additional precision offered by the cycle timer.In any case, if we want to make a change here, let's do it as part of a separate PR.  This is consistent with how it already works.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26714,675049396,2021-07-22T18:02:11Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -2784,38 +2863,79 @@ void ClientChannel::LoadBalancedCall::StartTransportStreamOpBatch(   } } -void ClientChannel::LoadBalancedCall::-    RecvTrailingMetadataReadyForLoadBalancingPolicy(void* arg,-                                                    grpc_error_handle error) {+void ClientChannel::LoadBalancedCall::SendInitialMetadataOnComplete(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<LoadBalancedCall*>(arg);+  self->call_attempt_tracer_->RecordOnDoneSendInitialMetadata(+      self->peer_string_);+  Closure::Run(DEBUG_LOCATION,+               self->original_send_initial_metadata_on_complete_,+               GRPC_ERROR_REF(error));+}++void ClientChannel::LoadBalancedCall::RecvInitialMetadataReady(+    void* arg, grpc_error_handle error) {   auto* self = static_cast<LoadBalancedCall*>(arg);-  if (self->lb_recv_trailing_metadata_ready_ != nullptr) {-    // Set error if call did not succeed.-    grpc_error_handle error_for_lb = GRPC_ERROR_NONE;+  self->call_attempt_tracer_->RecordReceivedInitialMetadata(+      self->recv_initial_metadata_, *self->recv_initial_metadata_flags_);+  Closure::Run(DEBUG_LOCATION, self->original_recv_initial_metadata_ready_,+               GRPC_ERROR_REF(error));+}++void ClientChannel::LoadBalancedCall::RecvMessageReady(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<LoadBalancedCall*>(arg);+  self->call_attempt_tracer_->RecordReceivedMessage(**self->recv_message_);+  Closure::Run(DEBUG_LOCATION, self->original_recv_message_ready_,+               GRPC_ERROR_REF(error));+}++void ClientChannel::LoadBalancedCall::RecvTrailingMetadataReady(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<LoadBalancedCall*>(arg);+  // Check if we have a tracer or an LB callback to invoke.+  if (self->call_attempt_tracer_ != nullptr ||+      self->lb_recv_trailing_metadata_ready_ != nullptr) {+    // Get the call's status.+    absl::Status status;     if (error != GRPC_ERROR_NONE) {-      error_for_lb = error;+      // Get status from error.+      grpc_status_code code;+      grpc_slice message = grpc_empty_slice();+      grpc_error_get_status(error, self->deadline_, &code, &message,+                            /*http_error=*/nullptr, /*error_string=*/nullptr);+      status = absl::Status(static_cast<absl::StatusCode>(code),+                            StringViewFromSlice(message));     } else {+      // Get status from headers.       const auto& fields = self->recv_trailing_metadata_->idx.named;       GPR_ASSERT(fields.grpc_status != nullptr);-      grpc_status_code status =+      grpc_status_code code =           grpc_get_status_code_from_metadata(fields.grpc_status->md);-      std::string msg;-      if (status != GRPC_STATUS_OK) {-        error_for_lb = grpc_error_set_int(-            GRPC_ERROR_CREATE_FROM_STATIC_STRING(""call failed""),-            GRPC_ERROR_INT_GRPC_STATUS, status);+      if (code != GRPC_STATUS_OK) {+        absl::string_view message;         if (fields.grpc_message != nullptr) {-          error_for_lb = grpc_error_set_str(-              error_for_lb, GRPC_ERROR_STR_GRPC_MESSAGE,-              grpc_slice_ref_internal(GRPC_MDVALUE(fields.grpc_message->md)));+          message = StringViewFromSlice(GRPC_MDVALUE(fields.grpc_message->md));         }+        status = absl::Status(static_cast<absl::StatusCode>(code), message);       }     }-    // Invoke callback to LB policy.-    Metadata trailing_metadata(self, self->recv_trailing_metadata_);-    LbCallState lb_call_state(self);-    self->lb_recv_trailing_metadata_ready_(error_for_lb, &trailing_metadata,-                                           &lb_call_state);-    if (error == GRPC_ERROR_NONE) GRPC_ERROR_UNREF(error_for_lb);+    // If we have a tracer, notify it.+    if (self->call_attempt_tracer_ != nullptr) {+      self->call_attempt_tracer_->RecordReceivedTrailingMetadata(+          status, self->recv_trailing_metadata_,+          *self->transport_stream_stats_);",We are now invoking `RecordReceivedTrailingMetadata()` regardless of what the call's status is.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26759,675098774,2021-07-22T19:19:58Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_test_resources.py,"@@ -209,8 +220,16 @@ def setup(self, test_case_classes: 'Iterable[XdsUrlMapTestCase]') -> None:         # Health Checks         self.td.create_health_check()         # Backend Services-        self.td.create_backend_service()-        self.td.create_alternative_backend_service()+        #+        # The backend services are created with HTTP2 instead of GRPC (the+        # default) to disable validate-for-proxyless, because the affinity+        # settings are not accepted by RCTH yet.+        #+        # TODO: delete _BackendHTTP2 from the parameters, to use the default","optional: create a tracking issue for yourself, so we won't forget to remove this parameter.If this parameter is going to stay for a long time, we could create an Absl flag to tell whether to create backend service with validation or not. So, it can adapt while the RCTH rollout for staging and prod is in different stage.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26759,675102073,2021-07-22T19:25:27Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_testcase.py,"@@ -217,6 +220,10 @@ class XdsUrlMapTestCase(absltest.TestCase, metaclass=_MetaXdsUrlMapTestCase):     - rpc_distribution_validate: Validates if the routing behavior is correct     """""" +    @staticmethod+    def client_init_config(rpc: str, metadata: str):","nit: Can you add some docstring to describe the expectation here? Especially, what value is `rpc` and `metadata` expecting, and what's the delimiter for multiple items.",X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/26714,675104445,2021-07-22T19:29:13Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -2784,38 +2863,79 @@ void ClientChannel::LoadBalancedCall::StartTransportStreamOpBatch(   } } -void ClientChannel::LoadBalancedCall::-    RecvTrailingMetadataReadyForLoadBalancingPolicy(void* arg,-                                                    grpc_error_handle error) {+void ClientChannel::LoadBalancedCall::SendInitialMetadataOnComplete(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<LoadBalancedCall*>(arg);+  self->call_attempt_tracer_->RecordOnDoneSendInitialMetadata(+      self->peer_string_);+  Closure::Run(DEBUG_LOCATION,+               self->original_send_initial_metadata_on_complete_,+               GRPC_ERROR_REF(error));+}++void ClientChannel::LoadBalancedCall::RecvInitialMetadataReady(+    void* arg, grpc_error_handle error) {   auto* self = static_cast<LoadBalancedCall*>(arg);-  if (self->lb_recv_trailing_metadata_ready_ != nullptr) {-    // Set error if call did not succeed.-    grpc_error_handle error_for_lb = GRPC_ERROR_NONE;+  self->call_attempt_tracer_->RecordReceivedInitialMetadata(+      self->recv_initial_metadata_, *self->recv_initial_metadata_flags_);+  Closure::Run(DEBUG_LOCATION, self->original_recv_initial_metadata_ready_,+               GRPC_ERROR_REF(error));+}++void ClientChannel::LoadBalancedCall::RecvMessageReady(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<LoadBalancedCall*>(arg);+  self->call_attempt_tracer_->RecordReceivedMessage(**self->recv_message_);+  Closure::Run(DEBUG_LOCATION, self->original_recv_message_ready_,+               GRPC_ERROR_REF(error));+}++void ClientChannel::LoadBalancedCall::RecvTrailingMetadataReady(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<LoadBalancedCall*>(arg);+  // Check if we have a tracer or an LB callback to invoke.+  if (self->call_attempt_tracer_ != nullptr ||+      self->lb_recv_trailing_metadata_ready_ != nullptr) {+    // Get the call's status.+    absl::Status status;     if (error != GRPC_ERROR_NONE) {-      error_for_lb = error;+      // Get status from error.+      grpc_status_code code;+      grpc_slice message = grpc_empty_slice();+      grpc_error_get_status(error, self->deadline_, &code, &message,+                            /*http_error=*/nullptr, /*error_string=*/nullptr);+      status = absl::Status(static_cast<absl::StatusCode>(code),+                            StringViewFromSlice(message));     } else {+      // Get status from headers.       const auto& fields = self->recv_trailing_metadata_->idx.named;       GPR_ASSERT(fields.grpc_status != nullptr);-      grpc_status_code status =+      grpc_status_code code =           grpc_get_status_code_from_metadata(fields.grpc_status->md);-      std::string msg;-      if (status != GRPC_STATUS_OK) {-        error_for_lb = grpc_error_set_int(-            GRPC_ERROR_CREATE_FROM_STATIC_STRING(""call failed""),-            GRPC_ERROR_INT_GRPC_STATUS, status);+      if (code != GRPC_STATUS_OK) {+        absl::string_view message;         if (fields.grpc_message != nullptr) {-          error_for_lb = grpc_error_set_str(-              error_for_lb, GRPC_ERROR_STR_GRPC_MESSAGE,-              grpc_slice_ref_internal(GRPC_MDVALUE(fields.grpc_message->md)));+          message = StringViewFromSlice(GRPC_MDVALUE(fields.grpc_message->md));         }+        status = absl::Status(static_cast<absl::StatusCode>(code), message);       }     }-    // Invoke callback to LB policy.-    Metadata trailing_metadata(self, self->recv_trailing_metadata_);-    LbCallState lb_call_state(self);-    self->lb_recv_trailing_metadata_ready_(error_for_lb, &trailing_metadata,-                                           &lb_call_state);-    if (error == GRPC_ERROR_NONE) GRPC_ERROR_UNREF(error_for_lb);+    // If we have a tracer, notify it.+    if (self->call_attempt_tracer_ != nullptr) {+      self->call_attempt_tracer_->RecordReceivedTrailingMetadata(+          status, self->recv_trailing_metadata_,+          *self->transport_stream_stats_);",Is the idea that recv_trailing_metadata will only be valid if the status is OK?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26714,675117515,2021-07-22T19:50:56Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -2784,38 +2863,79 @@ void ClientChannel::LoadBalancedCall::StartTransportStreamOpBatch(   } } -void ClientChannel::LoadBalancedCall::-    RecvTrailingMetadataReadyForLoadBalancingPolicy(void* arg,-                                                    grpc_error_handle error) {+void ClientChannel::LoadBalancedCall::SendInitialMetadataOnComplete(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<LoadBalancedCall*>(arg);+  self->call_attempt_tracer_->RecordOnDoneSendInitialMetadata(+      self->peer_string_);+  Closure::Run(DEBUG_LOCATION,+               self->original_send_initial_metadata_on_complete_,+               GRPC_ERROR_REF(error));+}++void ClientChannel::LoadBalancedCall::RecvInitialMetadataReady(+    void* arg, grpc_error_handle error) {   auto* self = static_cast<LoadBalancedCall*>(arg);-  if (self->lb_recv_trailing_metadata_ready_ != nullptr) {-    // Set error if call did not succeed.-    grpc_error_handle error_for_lb = GRPC_ERROR_NONE;+  self->call_attempt_tracer_->RecordReceivedInitialMetadata(+      self->recv_initial_metadata_, *self->recv_initial_metadata_flags_);+  Closure::Run(DEBUG_LOCATION, self->original_recv_initial_metadata_ready_,+               GRPC_ERROR_REF(error));+}++void ClientChannel::LoadBalancedCall::RecvMessageReady(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<LoadBalancedCall*>(arg);+  self->call_attempt_tracer_->RecordReceivedMessage(**self->recv_message_);+  Closure::Run(DEBUG_LOCATION, self->original_recv_message_ready_,+               GRPC_ERROR_REF(error));+}++void ClientChannel::LoadBalancedCall::RecvTrailingMetadataReady(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<LoadBalancedCall*>(arg);+  // Check if we have a tracer or an LB callback to invoke.+  if (self->call_attempt_tracer_ != nullptr ||+      self->lb_recv_trailing_metadata_ready_ != nullptr) {+    // Get the call's status.+    absl::Status status;     if (error != GRPC_ERROR_NONE) {-      error_for_lb = error;+      // Get status from error.+      grpc_status_code code;+      grpc_slice message = grpc_empty_slice();+      grpc_error_get_status(error, self->deadline_, &code, &message,+                            /*http_error=*/nullptr, /*error_string=*/nullptr);+      status = absl::Status(static_cast<absl::StatusCode>(code),+                            StringViewFromSlice(message));     } else {+      // Get status from headers.       const auto& fields = self->recv_trailing_metadata_->idx.named;       GPR_ASSERT(fields.grpc_status != nullptr);-      grpc_status_code status =+      grpc_status_code code =           grpc_get_status_code_from_metadata(fields.grpc_status->md);-      std::string msg;-      if (status != GRPC_STATUS_OK) {-        error_for_lb = grpc_error_set_int(-            GRPC_ERROR_CREATE_FROM_STATIC_STRING(""call failed""),-            GRPC_ERROR_INT_GRPC_STATUS, status);+      if (code != GRPC_STATUS_OK) {+        absl::string_view message;         if (fields.grpc_message != nullptr) {-          error_for_lb = grpc_error_set_str(-              error_for_lb, GRPC_ERROR_STR_GRPC_MESSAGE,-              grpc_slice_ref_internal(GRPC_MDVALUE(fields.grpc_message->md)));+          message = StringViewFromSlice(GRPC_MDVALUE(fields.grpc_message->md));         }+        status = absl::Status(static_cast<absl::StatusCode>(code), message);       }     }-    // Invoke callback to LB policy.-    Metadata trailing_metadata(self, self->recv_trailing_metadata_);-    LbCallState lb_call_state(self);-    self->lb_recv_trailing_metadata_ready_(error_for_lb, &trailing_metadata,-                                           &lb_call_state);-    if (error == GRPC_ERROR_NONE) GRPC_ERROR_UNREF(error_for_lb);+    // If we have a tracer, notify it.+    if (self->call_attempt_tracer_ != nullptr) {+      self->call_attempt_tracer_->RecordReceivedTrailingMetadata(+          status, self->recv_trailing_metadata_,+          *self->transport_stream_stats_);","I think `recv_trailing_metadata` should be a valid metadata batch in any case.  But if (e.g.) the call failed without actually receiving any trailing metadata from the server, the batch may be empty.Also, there is one other caveat here, which is that if the call is cancelled before the application starts the `recv_initial_metadata` op, then there's no guarantee that `RecordReceivedTrailingMetadata()` will ever be called.  But in that case, the tracer should have seen `RecordCancel()` instead.",X
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/26759,675181297,2021-07-22T21:41:36Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_test_resources.py,"@@ -209,8 +220,16 @@ def setup(self, test_case_classes: 'Iterable[XdsUrlMapTestCase]') -> None:         # Health Checks         self.td.create_health_check()         # Backend Services-        self.td.create_backend_service()-        self.td.create_alternative_backend_service()+        #+        # The backend services are created with HTTP2 instead of GRPC (the+        # default) to disable validate-for-proxyless, because the affinity+        # settings are not accepted by RCTH yet.+        #+        # TODO: delete _BackendHTTP2 from the parameters, to use the default","This won't stay long. And I don't think a tracking issue is useful (I'm not keeping a list of these issues).This will be removed after the RCTH fix is releases, and that's tracked in the Friday spreadsheet, so we won't forget this.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26691,675225991,2021-07-22T23:33:52Z,tools/distrib/python/grpcio_tools/setup.py,"@@ -55,12 +55,7 @@ CLASSIFIERS = [     'Development Status :: 5 - Production/Stable',     'Programming Language :: Python',-    'Programming Language :: Python :: 2',-    'Programming Language :: Python :: 2.7',     'Programming Language :: Python :: 3',-    'Programming Language :: Python :: 3.4',-    'Programming Language :: Python :: 3.5',",I found we keep forgetting to add newer Python versions to this `setup.py`. Simply state we are supporting for all non-EOL Python3 version is more accurate and less burdensome in the long run.,
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26764,675820172,2021-07-23T19:26:11Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_test_resources.py,"@@ -139,6 +140,12 @@ def __init__(self, absl_flags: Mapping[str, Any] = None):         if absl_flags is not None:             for key in absl_flags:                 setattr(self, key, absl_flags[key])+        # Pick a client_namespace_suffix if not set+        if self.resource_suffix is None:+            self.resource_suffix = """"+            self.client_namespace_suffix = str(int(time.time()))","Just curious, why not something similar to https://github.com/grpc/grpc/blob/2dc2ef02c373ab71ddbf21e567cd83a5387df0a5/tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py#L199-L209It produces quite readable `date-time-hash` suffix: `20210723-1101-4bp0g`",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26764,675821026,2021-07-23T19:28:06Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_test_resources.py,"@@ -139,6 +140,12 @@ def __init__(self, absl_flags: Mapping[str, Any] = None):         if absl_flags is not None:             for key in absl_flags:                 setattr(self, key, absl_flags[key])+        # Pick a client_namespace_suffix if not set+        if self.resource_suffix is None:+            self.resource_suffix = """"+            self.client_namespace_suffix = str(int(time.time()))+        else:+            self.client_namespace_suffix = self.resource_suffix",I found it really useful to log generated suffix/prefix: https://github.com/grpc/grpc/blob/2dc2ef02c373ab71ddbf21e567cd83a5387df0a5/tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py#L134-L135,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26764,675840901,2021-07-23T20:10:04Z,tools/run_tests/xds_k8s_test_driver/config/url-map.cfg,"@@ -1,6 +1,5 @@---resource_suffix=interop-psm-url-map+--resource_prefix=interop-psm-url-map","No worry. As long as xDS port stays the same, we are good.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26753,675885825,2021-07-23T22:00:02Z,src/core/ext/filters/client_channel/subchannel.cc,"@@ -295,6 +295,47 @@ void SubchannelCall::IncrementRefCount(   GRPC_CALL_STACK_REF(SUBCHANNEL_CALL_TO_CALL_STACK(this), reason); } +//+// SubchannelConnector::Args+//++SubchannelConnector::Args::Args(const Args& other) {+  interested_parties = other.interested_parties;","These fields can actually be initialized via an initializer list:```SubchannelConnector::Args::Args(const Args& other)    : interested_parties(other.interested_parties),      deadline(other.deadline),      channel_args(grpc_channel_args_copy(other.channel_args)) {}```",X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/26342,675901862,2021-07-23T22:56:33Z,src/core/ext/transport/chttp2/transport/flow_control.h,"@@ -309,7 +309,7 @@ class TransportFlowControl final : public TransportFlowControlBase {     return action;   } -  const grpc_chttp2_transport* const t_;","Is this really that much though? Internally, I've seen 100 as the number being thrown around as the configuration for maximum number of concurrent streams?If it is an issue, we could probably say something like - the initial window size can only grow as much as half the maximum flow control window and a similar restriction on the window delta.",
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/26747,676612973,2021-07-26T13:39:56Z,examples/android/binder/java/io/grpc/binder/cpp/example/native.cc,"@@ -0,0 +1,30 @@+// Copyright 2021 gRPC authors.+// +// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+// +//     http://www.apache.org/licenses/LICENSE-2.0+// +// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+++#include <android/log.h>+#include <jni.h>++extern ""C"" JNIEXPORT jstring JNICALL+Java_io_grpc_binder_cpp_example_ButtonPressHandler_native_1entry(","Oh my bad, underscore in function name should be followed by a 1.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26753,676861559,2021-07-26T18:54:07Z,src/core/ext/filters/client_channel/connector.h,"@@ -45,10 +45,38 @@ class SubchannelConnector : public InternallyRefCounted<SubchannelConnector> {      Args() = default;     ~Args() { grpc_channel_args_destroy(channel_args); }-    Args(const Args& other);-    Args(Args&& other) noexcept;-    Args& operator=(const Args& other);-    Args& operator=(Args&& other) noexcept;++    Args(const Args& other) : interested_parties(other.interested_parties),+      deadline(other.deadline), channel_args(grpc_channel_args_copy(other.channel_args)) {}++    Args(Args&& other) noexcept {+      interested_parties = other.interested_parties;",These first two parameters can be set via an initializer list as well.,X
85572583,johann1000,https://api.github.com/repos/grpc/grpc/pulls/26709,676966841,2021-07-26T21:47:04Z,include/grpc/grpc_security.h,"@@ -1117,17 +1117,17 @@ grpc_authorization_policy_provider_static_data_create( GRPCAPI void grpc_authorization_policy_provider_release(     grpc_authorization_policy_provider* provider); -struct grpc_status_or_bool{-  grpc_status_code status;-  bool boolean;-};+//struct grpc_status_or_bool{+//  grpc_status_code status;+//  bool boolean;+//};  /**  * Checks if the private key matches the certificate's public key.  * Returns a not-OK status on failure, or a bool indicating  * whether the key/cert pair matches.  */-GRPCAPI grpc_status_or_bool* grpc_tls_certificate_key_match(+GRPCAPI grpc_status_code* grpc_tls_certificate_key_match(     const char* private_key, const char* cert_chain);","Why expose it? Well as we noted earlier, the certificate-key match functionality could be highly useful in other situations/classes (not just the FileWatcherCertificateProvider). This should allow for it to be used where needed. I am not sure exactly if Zhen already has a use in mind, but I can recall Ashitha mentioning a few spots where she actually wanted me to implement it.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/26788,676982193,2021-07-26T22:19:33Z,src/core/ext/transport/binder/utils/transport_stream_receiver.h,"@@ -0,0 +1,61 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_EXT_TRANSPORT_BINDER_UTILS_TRANSPORT_STREAM_RECEIVER_INTERFACE_H_+#define GRPC_CORE_EXT_TRANSPORT_BINDER_UTILS_TRANSPORT_STREAM_RECEIVER_INTERFACE_H_++#include <grpc/impl/codegen/port_platform.h>++#include <functional>+#include <string>+#include <vector>++#include ""src/core/ext/transport/binder/wire_format/transaction.h""++namespace binder_transport {++typedef int StreamIdentifier;++class TransportStreamReceiver {+ public:+  virtual ~TransportStreamReceiver() = default;++  // Only handles single time invocation. Callback object will be deleted.+  // The callback should be valid until invocation or unregister.+  virtual void RegisterRecvInitialMd(",could we rename these RegisterRecvInitialMetadata? (I'm finding Md to be a touch too short to read),
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/26700,677055458,2021-07-27T01:43:02Z,test/core/bad_client/tests/large_metadata.cc,"@@ -38,7 +38,7 @@ #define PFX_TOO_MUCH_METADATA_FROM_CLIENT_REQUEST         \   ""\x00\x00\x00\x04\x01\x00\x00\x00\x00""                  \   ""\x00""                                                  \-  ""5{\x01\x05\x00\x00\x00\x01""                            \+  ""\x1aM\x01\x05\x00\x00\x00\x01""                         \","The frame length was incorrect, meaning that this test would simply buffer and wait forever due to an incomplete request.",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26709,677690895,2021-07-27T18:16:50Z,include/grpc/grpc_security.h,"@@ -1117,8 +1117,23 @@ grpc_authorization_policy_provider_static_data_create( GRPCAPI void grpc_authorization_policy_provider_release(     grpc_authorization_policy_provider* provider); +//struct grpc_status_or_bool{+//  grpc_status_code status;+//  bool boolean;+//};++/**+ * Checks if the private key matches the certificate's public key.+ * Returns a not-OK status on failure, or a bool indicating+ * whether the key/cert pair matches.+ */+GRPCAPI grpc_status_code* grpc_tls_certificate_key_match(","Currently, you are only looking at the status for PrivateKeyCertMatch. We also want to check if the match failed or was successful with status OK.Also here instead of only returning the status code, we should propagate the error message too.I would recommend updating this API to```GRPCAPI bool grpc_tls_certificate_key_match(const char* private_key, const char* cert_chain, grpc_status_code* code, const char** error_details);```Please update the definition too. And suggest moving this API to line 850.",X
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/26788,677824782,2021-07-27T21:49:53Z,src/core/ext/transport/binder/utils/transport_stream_receiver_impl.h,"@@ -0,0 +1,80 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_EXT_TRANSPORT_BINDER_UTILS_TRANSPORT_STREAM_RECEIVER_IMPL_H_+#define GRPC_CORE_EXT_TRANSPORT_BINDER_UTILS_TRANSPORT_STREAM_RECEIVER_IMPL_H_++#include <grpc/impl/codegen/port_platform.h>++#include <functional>+#include <map>+#include <queue>+#include <string>+#include <vector>++#include ""absl/types/optional.h""",Let's run iwyu after we import most of our code to clean up them. We might still need to make some dependency change due to codebase difference (GitHub v.s internal),
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/26788,677824792,2021-07-27T21:49:54Z,src/core/ext/transport/binder/utils/BUILD,"@@ -0,0 +1,40 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++load(""//bazel:grpc_build_system.bzl"", ""grpc_cc_library"")++licenses([""notice""])++grpc_cc_library(+    name = ""transport_stream_receiver"",+    srcs = [+        ""transport_stream_receiver_impl.cc"",+    ],+    hdrs = [+        ""transport_stream_receiver.h"",+        ""transport_stream_receiver_impl.h"",+    ],+    external_deps = [+        ""absl/strings"",+        ""absl/synchronization"",+        ""absl/types:optional"",",Let's run buildifier after we import most of our code to clean up them. We might still need to make some dependency change due to codebase difference (GitHub v.s internal),
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26690,678526902,2021-07-28T17:50:09Z,src/core/tsi/openssl_utils.cc,"@@ -0,0 +1,39 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <absl/strings/string_view.h>+#include ""openssl/bio.h""+#include ""openssl/evp.h""+#include ""openssl/pem.h""+#include ""openssl/x509.h""++#include ""src/core/tsi/openssl_utils.h""++namespace grpc_core {++OpenSslPKey::OpenSslPKey(absl::string_view private_key) {+  BIO* bio = BIO_new_mem_buf(private_key.data(), private_key.size());+  p_key_ = PEM_read_bio_PrivateKey(bio, nullptr, nullptr, nullptr);+  BIO_free(bio);+}++OpenSslX509::OpenSslX509(absl::string_view cert) {",nit: Would it make sense to use cert_chain as variable name here instead of cert?,X
84942730,itsemmanuelfrancis,https://api.github.com/repos/grpc/grpc/pulls/26690,678540329,2021-07-28T18:09:17Z,src/core/tsi/openssl_utils.cc,"@@ -0,0 +1,39 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <absl/strings/string_view.h>+#include ""openssl/bio.h""+#include ""openssl/evp.h""+#include ""openssl/pem.h""+#include ""openssl/x509.h""++#include ""src/core/tsi/openssl_utils.h""++namespace grpc_core {++OpenSslPKey::OpenSslPKey(absl::string_view private_key) {+  BIO* bio = BIO_new_mem_buf(private_key.data(), private_key.size());+  p_key_ = PEM_read_bio_PrivateKey(bio, nullptr, nullptr, nullptr);+  BIO_free(bio);+}++OpenSslX509::OpenSslX509(absl::string_view cert) {","I used `cert` cos `OpenSslX509` only contains a single certificate. Even when passed a cert chain, it just uses the first cert.",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26690,678559573,2021-07-28T18:37:23Z,src/core/tsi/openssl_utils.cc,"@@ -0,0 +1,39 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <absl/strings/string_view.h>+#include ""openssl/bio.h""+#include ""openssl/evp.h""+#include ""openssl/pem.h""+#include ""openssl/x509.h""++#include ""src/core/tsi/openssl_utils.h""++namespace grpc_core {++OpenSslPKey::OpenSslPKey(absl::string_view private_key) {+  BIO* bio = BIO_new_mem_buf(private_key.data(), private_key.size());+  p_key_ = PEM_read_bio_PrivateKey(bio, nullptr, nullptr, nullptr);+  BIO_free(bio);+}++OpenSslX509::OpenSslX509(absl::string_view cert) {","Since that is an internal implementation, I suggest using cert_chain as the parameter name here. ",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26769,678628248,2021-07-28T20:27:32Z,tools/run_tests/xds_k8s_test_driver/tests/app_net_test.py,"@@ -0,0 +1,42 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging++from absl import flags+from absl.testing import absltest++from framework import xds_k8s_testcase++from framework.infrastructure.gcp import network_services++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)+++class AppNetTest(xds_k8s_testcase.AppNetXdsKubernetesTestCase):++    def test_ping_pong(self):+        self.td.create_health_check()+        self.td.create_backend_service()+        self.td.create_grpc_route(self.server_xds_host, self.server_xds_port)+        self.td.create_router()+        test_server: _XdsTestServer = self.startTestServer()+        self.setupServerBackends()+        test_client: _XdsTestClient = self.startTestClient(test_server)","Missed this:```suggestion        test_server: xds_k8s_testcase.XdsTestServer = self.startTestServer()        self.setupServerBackends()        test_client: xds_k8s_testcase.XdsTestClient = self.startTestClient(test_server)```Alternatively, declare type aliases as in https://github.com/grpc/grpc/blob/374e7b2cb4b6ca2861943b198c75dd55bdb9ce03/tools/run_tests/xds_k8s_test_driver/tests/baseline_test.py#L24-L26",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26769,678646593,2021-07-28T20:56:57Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/network_services.py,"@@ -22,9 +23,134 @@  logger = logging.getLogger(__name__) +_ComputeV1 = gcp.compute.ComputeV1+GcpResource = _ComputeV1.GcpResource+  class NetworkServicesV1Alpha1(gcp.api.GcpStandardCloudApiResource):     ENDPOINT_CONFIG_SELECTORS = 'endpointConfigSelectors'+    GRPC_ROUTES = 'grpcRoutes'+    ROUTERS = 'routers'++    @dataclasses.dataclass(frozen=True)+    class Router:++        name: str+        url: str+        type: str+        network: Optional[str]+        routes: Optional[List[str]]++        @staticmethod+        def from_dict(name: str, d: dict) -> 'Router':+            return NetworkServicesV1Alpha1.Router(+                name=name,+                url=d[""name""],+                type=d[""type""],+                network=d.get(""network""),+                routes=list(d[""routes""]) if ""routes"" in d else None,+            )++    @dataclasses.dataclass(frozen=True)+    class GrpcRoute:++        @dataclasses.dataclass(frozen=True)+        class MethodMatch:+            type: Optional[str]+            grpc_service: Optional[str]+            grpc_method: Optional[str]+            case_sensitive: Optional[bool]++            @staticmethod+            def from_dict(d: dict) -> 'MethodMatch':+                return MethodMatch(","Hm, my pycharm/intellij complaining about this:![image](https://user-images.githubusercontent.com/672669/127394722-25af21cc-7734-4419-9175-afc6dc99f1ec.png)Consider `@classmethod` and `return cls(`",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26769,678650461,2021-07-28T21:03:33Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/network_services.py,"@@ -22,9 +23,134 @@  logger = logging.getLogger(__name__) +_ComputeV1 = gcp.compute.ComputeV1+GcpResource = _ComputeV1.GcpResource+  class NetworkServicesV1Alpha1(gcp.api.GcpStandardCloudApiResource):     ENDPOINT_CONFIG_SELECTORS = 'endpointConfigSelectors'+    GRPC_ROUTES = 'grpcRoutes'+    ROUTERS = 'routers'++    @dataclasses.dataclass(frozen=True)+    class Router:++        name: str+        url: str+        type: str+        network: Optional[str]+        routes: Optional[List[str]]++        @staticmethod+        def from_dict(name: str, d: dict) -> 'Router':+            return NetworkServicesV1Alpha1.Router(+                name=name,+                url=d[""name""],+                type=d[""type""],+                network=d.get(""network""),+                routes=list(d[""routes""]) if ""routes"" in d else None,+            )++    @dataclasses.dataclass(frozen=True)+    class GrpcRoute:++        @dataclasses.dataclass(frozen=True)+        class MethodMatch:+            type: Optional[str]+            grpc_service: Optional[str]+            grpc_method: Optional[str]+            case_sensitive: Optional[bool]++            @staticmethod+            def from_dict(d: dict) -> 'MethodMatch':+                return MethodMatch(+                    type=d.get(""type""),+                    grpc_service=d.get(""grpcService""),+                    grpc_method=d.get(""grpcMethod""),+                    case_sensitive=d.get(""caseSensitive""),+                )++        @dataclasses.dataclass(frozen=True)+        class HeaderMatch:+            type: Optional[str]+            key: str+            value: str++            @staticmethod+            def from_dict(d: dict) -> 'HeaderMatch':","Note that the return type, even when in quotes, should be resolvable in the current context:![image](https://user-images.githubusercontent.com/672669/127395192-b43188ec-9a82-4405-8f20-6c0e20a278e1.png)So this should be `NetworkServicesV1Alpha1.GrpcRoute.HeaderMatch`.I know how annoying this may be to repeat `NetworkServicesV1Alpha1` everywhere.  Feel free to move these dataclasses to the top level, I ended up doing the same in the lates IAM API:https://github.com/grpc/grpc/blob/c472a2a58c18e08f8ae8e6710cb071eaf631e98c/tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/iam.py#L59-L66",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26769,678652764,2021-07-28T21:07:24Z,tools/run_tests/xds_k8s_test_driver/tests/app_net_test.py,"@@ -0,0 +1,42 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging++from absl import flags+from absl.testing import absltest++from framework import xds_k8s_testcase++from framework.infrastructure.gcp import network_services++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)+++class AppNetTest(xds_k8s_testcase.AppNetXdsKubernetesTestCase):++    def test_ping_pong(self):+        self.td.create_health_check()","Suggestion: depending on what you want, you may choose to report individual operations as subtests, baseline_test-style:https://github.com/grpc/grpc/blob/374e7b2cb4b6ca2861943b198c75dd55bdb9ce03/tools/run_tests/xds_k8s_test_driver/tests/baseline_test.py#L32-L36This results in nice granular reports on the testgrid:![image](https://user-images.githubusercontent.com/672669/127396003-b881c6e3-9533-472c-a8b8-5e72a8a37e37.png)But again, depending on what you're trying to achieve.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26769,678725884,2021-07-28T23:50:51Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/network_services.py,"@@ -22,9 +23,134 @@  logger = logging.getLogger(__name__) +_ComputeV1 = gcp.compute.ComputeV1+GcpResource = _ComputeV1.GcpResource++@dataclasses.dataclass(frozen=True)+class Router:++    name: str+    url: str+    type: str+    network: Optional[str]+    routes: Optional[List[str]]++    @classmethod+    def from_dict(cls, name: str, d: dict) -> 'Router':+        return cls(+            name=name,+            url=d[""name""],+            type=d[""type""],+            network=d.get(""network""),+            routes=list(d[""routes""]) if ""routes"" in d else None,+        )++@dataclasses.dataclass(frozen=True)+class GrpcRoute:++    @dataclasses.dataclass(frozen=True)+    class MethodMatch:+        type: Optional[str]+        grpc_service: Optional[str]+        grpc_method: Optional[str]+        case_sensitive: Optional[bool]++        @classmethod+        def from_dict(cls, d: dict) -> 'MethodMatch':+            return cls(+                type=d.get(""type""),+                grpc_service=d.get(""grpcService""),+                grpc_method=d.get(""grpcMethod""),+                case_sensitive=d.get(""caseSensitive""),+            )++    @dataclasses.dataclass(frozen=True)+    class HeaderMatch:+        type: Optional[str]+        key: str+        value: str++        @classmethod+        def from_dict(cls, d: dict) -> 'HeaderMatch':+            return cls(+                type=d.get(""type""),+                key=d[""key""],+                value=d[""value""],+            )++    @dataclasses.dataclass(frozen=True)+    class RouteMatch:+        method: Optional['MethodMatch']+        headers: Tuple['HeaderMatch']++        @classmethod+        def from_dict(cls, d: dict) -> 'RouteMatch':+            return cls(+                method=MethodMatch.from_dict(+                    d[""method""]) if ""method"" in d else None,+                headers=tuple(+                    HeaderMatch.from_dict(h)+                    for h in d[""headers""]) if ""headers"" in d else (),+            )++    @dataclasses.dataclass(frozen=True)+    class Destination:+        service_name: str+        weight: Optional[int]++        @classmethod+        def from_dict(cls, d: dict) -> 'Destination':+            return cls(+                service_name=d[""serviceName""],+                weight=d.get(""weight""),+            )++    @dataclasses.dataclass(frozen=True)+    class RouteAction:+        destination: Optional['Destination']+        drop: Optional[int]++        @classmethod+        def from_dict(cls, d: dict) -> 'RouteAction':+            return cls(+                destination=Destination.from_dict(+                    d[""destination""]) if ""destination"" in d else None,+                drop=d.get(""drop""),+            )++    @dataclasses.dataclass(frozen=True)+    class RouteRule:+        match: Optional['RouteMatch']+        action: 'RouteAction'++        @classmethod+        def from_dict(cls, d: dict) -> 'RouteRule':+            return cls(+                match=RouteMatch.from_dict(+                    d[""match""]) if ""match"" in d else """",+                action=RouteAction.from_dict(+                    d[""action""]),+            )++    name: str+    url: str+    hostnames: Tuple[str]+    rules: Tuple['RouteRule']++    @classmethod+    def from_dict(cls, name: str, d: dict) -> 'RouteRule':+        return cls(+            name=name,+            url=d[""name""],+            hostnames=tuple(d[""hostnames""]),+            rules=tuple(d[""rules""]),+        )",Missing a newline (two blank lines required between classes),X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26769,678728660,2021-07-28T23:59:13Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/network_services.py,"@@ -22,9 +23,134 @@  logger = logging.getLogger(__name__) +_ComputeV1 = gcp.compute.ComputeV1+GcpResource = _ComputeV1.GcpResource++@dataclasses.dataclass(frozen=True)+class Router:++    name: str+    url: str+    type: str+    network: Optional[str]+    routes: Optional[List[str]]++    @classmethod+    def from_dict(cls, name: str, d: dict) -> 'Router':+        return cls(+            name=name,+            url=d[""name""],+            type=d[""type""],+            network=d.get(""network""),+            routes=list(d[""routes""]) if ""routes"" in d else None,+        )++@dataclasses.dataclass(frozen=True)+class GrpcRoute:++    @dataclasses.dataclass(frozen=True)+    class MethodMatch:+        type: Optional[str]+        grpc_service: Optional[str]+        grpc_method: Optional[str]+        case_sensitive: Optional[bool]++        @classmethod+        def from_dict(cls, d: dict) -> 'MethodMatch':","Here and elsewhere: could you please change the method signature to `def from_response(cls, response: Dict[str, Any]) `, so it's consistent with similar methods in `iam` module: https://github.com/grpc/grpc/blob/ee4f6854bd6d56a16187923421c940e5a47a5a49/tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/iam.py#L77P.S. I was sure I asked this in the prev review cycle, but somehow I can't find my comment. Maybe I accidentally deleted it. Sorry.",
5067076,ericgribkoff,https://api.github.com/repos/grpc/grpc/pulls/26360,679758501,2021-07-30T08:45:22Z,tools/run_tests/xds_k8s_test_driver/tests/failover_test.py,"@@ -0,0 +1,128 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging+from typing import List++from absl import flags+from absl.testing import absltest++from framework import xds_k8s_testcase+from framework.infrastructure import k8s+from framework.test_app import server_app++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient+++class FailoverTest(xds_k8s_testcase.RegularXdsKubernetesTestCase):+    REPLICA_COUNT = 3+    MAX_RATE_PER_ENDPOINT = 100++    def setUp(self):+        super().setUp()+        self.secondary_server_runner = server_app.KubernetesServerRunner(+            k8s.KubernetesNamespace(self.secondary_k8s_api_manager,+                                    self.server_namespace),+            deployment_name=self.server_name + '-alt',","@sergiitk So this proved fraught. Originally I had the suffix here as `-alternate-region`, which didn't show any problems in any of the operations of the framework (as far as I could determine from the logs, but I could easily have missed something) but the deployment in this runner was never moving to ready. Eventually I was able to discover:```$ kubectl get svcneg -o yaml     message: 'googleapi: Error 400: Invalid value for field ''networkEndpointGroup'':        ''xds-k8s-security-server-20210729-0938-k5ad1-psm-grpc-server-alternate-region''.        Must be a match of regex ''(?:[a-z](?:[-a-z0-9]{0,61}[a-z0-9])?)|[1-9][0-9]{0,19}'',```But this took me (admittedly very new to k8s) quite a while of digging around to even know to check this. I think I eventually noticed that there was no NEG for the deployment, and that led to the above kubectl check that revealed the error. But I couldn't identify a particular operation of the framework that actually failed, so it wasn't clear to me if/where we could put a check to help avoid future-me from running into this same too-long-of-a-name problem again and not knowing what the underlying problem is. Curious if you might have a better idea of what step actually led to this and were it could be checked for.",X
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/26788,680052394,2021-07-30T16:19:22Z,src/core/ext/transport/binder/utils/transport_stream_receiver_impl.h,"@@ -0,0 +1,80 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_EXT_TRANSPORT_BINDER_UTILS_TRANSPORT_STREAM_RECEIVER_IMPL_H_+#define GRPC_CORE_EXT_TRANSPORT_BINDER_UTILS_TRANSPORT_STREAM_RECEIVER_IMPL_H_++#include <grpc/impl/codegen/port_platform.h>++#include <functional>+#include <map>+#include <queue>+#include <string>+#include <vector>++#include ""absl/types/optional.h""+#include ""src/core/ext/transport/binder/utils/transport_stream_receiver.h""+#include ""src/core/lib/gprpp/sync.h""++namespace binder_transport {","Since our transport class is named as grpc_binder_transport (following naming convention of other transports!), we renamed the namespace to grpc_binder",
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/26788,680052465,2021-07-30T16:19:26Z,src/core/ext/transport/binder/utils/transport_stream_receiver.h,"@@ -0,0 +1,61 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_EXT_TRANSPORT_BINDER_UTILS_TRANSPORT_STREAM_RECEIVER_INTERFACE_H_+#define GRPC_CORE_EXT_TRANSPORT_BINDER_UTILS_TRANSPORT_STREAM_RECEIVER_INTERFACE_H_++#include <grpc/impl/codegen/port_platform.h>++#include <functional>+#include <string>+#include <vector>++#include ""src/core/ext/transport/binder/wire_format/transaction.h""++namespace binder_transport {++typedef int StreamIdentifier;++class TransportStreamReceiver {+ public:+  virtual ~TransportStreamReceiver() = default;++  // Only handles single time invocation. Callback object will be deleted.+  // The callback should be valid until invocation or unregister.+  virtual void RegisterRecvInitialMd(",Oh I thought Md is gRPC's naming convention (I see it used everywhere and was confused by it for a moment). Renamed Md appearance to MetaData,
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26360,680269899,2021-07-30T23:46:43Z,tools/run_tests/xds_k8s_test_driver/tests/failover_test.py,"@@ -0,0 +1,128 @@+# Copyright 2020 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging+from typing import List++from absl import flags+from absl.testing import absltest++from framework import xds_k8s_testcase+from framework.infrastructure import k8s+from framework.test_app import server_app++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient+++class FailoverTest(xds_k8s_testcase.RegularXdsKubernetesTestCase):+    REPLICA_COUNT = 3+    MAX_RATE_PER_ENDPOINT = 100++    def setUp(self):+        super().setUp()+        self.secondary_server_runner = server_app.KubernetesServerRunner(+            k8s.KubernetesNamespace(self.secondary_k8s_api_manager,+                                    self.server_namespace),+            deployment_name=self.server_name + '-alt',","Ooof, I see how this could've been frustrating. 1. One hard-to-predict thing that changed with adding the ""multiple framework runs at the same time"" feature (#26542) is added resource suffix, with date-time and nonce, f.e. `-20210729-0938-k5ad1`. This is needed to make each run unique. However this added 20 chars to each resource name. And different resource types have different char limits. You can see in the error you're getting, there's a 61 char limit on NEG name: `{0,61}`. In many cases these char limits are implicit, and the only way to know they exist, is to just run the script, and check for errors related to the name field. That's the reason I tried to cut down on char limit, and replaced `-alternate` with `-alt` in all places I found.2. The problem with debugging NEGs is that they are a [custom k8s extension](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) made by GCP for GCP. There's a bunch on opaque machinery invisible to k8s. That's also why it takes forever to delete a k8s namespace, when there are live GCP resources using a NEG associated with a service in the namespace (f.e. this happens when cleanup is done in incorrect order)3. My recent life learning is that most cases I get stuck in debugging, it could've spared myself some time by asking someone who worked on that thing I'm debugging. I'm still pretty bad at applying this lesson though 😄 ",X
4943593,bdhess,https://api.github.com/repos/grpc/grpc/pulls/26834,681385001,2021-08-03T02:19:54Z,src/core/tsi/ssl_transport_security.cc,"@@ -490,21 +490,35 @@ static tsi_result peer_from_x509(X509* cert, int include_certificate_type,   return result; } -/* Logs the SSL error stack. */-static void log_ssl_error_stack(void) {-  unsigned long err;-  while ((err = ERR_get_error()) != 0) {-    char details[256];-    ERR_error_string_n(static_cast<uint32_t>(err), details, sizeof(details));-    gpr_log(GPR_ERROR, ""%s"", details);+/* Retrieves the SSL error stack as a string. */+static std::string ssl_error_stack_to_string(void) {+  BIO* bio = BIO_new(BIO_s_mem());+  ERR_print_errors(bio);","[ERR_print_errors](https://commondatastorage.googleapis.com/chromium-boringssl-docs/bio.h.html#ERR_print_errors) prints to the BIO, not to stderr or stdout.  In this case, we created the BIO with [BIO_s_mem](https://commondatastorage.googleapis.com/chromium-boringssl-docs/bio.h.html#BIO_s_mem), so it refers to an in-memory buffer.  Which is then copied to a `std::string` for subsequent logging with `gpr_log()`.",X
1535036,tamird,https://api.github.com/repos/grpc/grpc/pulls/26866,682046130,2021-08-03T19:35:46Z,include/grpcpp/impl/codegen/sync.h,"@@ -143,13 +143,6 @@ class CondVar {  #endif  // GPR_ABSEIL_SYNC -template <typename Predicate>-static void WaitUntil(CondVar* cv, Mutex* mu, Predicate pred) {","I can restore this function, if you think that's preferable. Should I mark it deprecated?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26871,682698192,2021-08-04T14:58:00Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -838,6 +839,7 @@ class AdsServiceImpl : public std::enable_shared_from_this<AdsServiceImpl> {                         SubscriptionMap* subscription_map,                         SentState* sent_state,                         absl::optional<DiscoveryResponse>* response) {+      grpc_core::MutexLock lock(&parent_->ads_mu_);","This doesn't look right to me.  `ProcessRequest()` is called while already holding the lock, so I think this will deadlock.I think you just need to tag this method with `ABSL_EXCLUSIVE_LOCKS_REQUIRED`.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26871,682698446,2021-08-04T14:58:18Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -941,7 +943,9 @@ class AdsServiceImpl : public std::enable_shared_from_this<AdsServiceImpl> {       gpr_log(GPR_INFO, ""ADS[%p]: Received update for type=%s name=%s"", this,               resource_type.c_str(), resource_name.c_str());       auto& subscription_name_map = (*subscription_map)[resource_type];+      parent_->ads_mu_.Lock();","Same problem here: `ProcessUpdate()` is called when already holding the lock, so this should deadlock.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26871,682943652,2021-08-04T20:40:33Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -644,6 +644,7 @@ class AdsServiceImpl : public std::enable_shared_from_this<AdsServiceImpl> {   }    void NotifyDoneWithAdsCallLocked() {+    grpc_core::MutexLock lock(&ads_mu_);","Looks like this one is also called only when holding the lock, so this is also a deadline.  This method also needs `ABSL_EXCLUSIVE_LOCKS_REQUIRED()`.",
86852568,napathome,https://api.github.com/repos/grpc/grpc/pulls/26862,683079578,2021-08-05T02:19:01Z,src/core/ext/transport/binder/java/io/grpc/binder/cpp/NativeConnectionHelper.java,"@@ -0,0 +1,37 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//      http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++package io.grpc.binder.cpp;++import android.content.Context;+import android.os.IBinder;+import android.os.Parcel;++/** Provide interfaces for native code to invoke */","Could you document how user should use this helper class? For example, after user call tryEstablishConnection, when should user call getServiceBinder?",
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/26862,683115796,2021-08-05T04:16:08Z,src/core/ext/transport/binder/java/io/grpc/binder/cpp/NativeConnectionHelper.java,"@@ -0,0 +1,37 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//      http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++package io.grpc.binder.cpp;++import android.content.Context;+import android.os.IBinder;+import android.os.Parcel;++/** Provide interfaces for native code to invoke */","> Could you document how user should use this helper class? For example, after user call tryEstablishConnection, when should user call getServiceBinder?Added some comments. Actually user will not call this class. This class is only invoked from our implementation in gRPC> Since tryEstablishConnection could fail, should we return something to let user know?Added a TODO. For now we just assume it always success in our prototype.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26883,683604479,2021-08-05T16:21:39Z,tools/internal_ci/linux/grpc_xds_k8s_insecure.sh,"@@ -0,0 +1,144 @@+#!/usr/bin/env bash+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex -o igncr || set -ex++# Constants+readonly GITHUB_REPOSITORY_NAME=""grpc""+# GKE Cluster+readonly GKE_CLUSTER_NAME=""interop-test-psm-sec-v2-us-central1-a""","I thought we were considering a separate cluster for non-security tests. I'm OK with this as a temporary measure, but then we'd have to backport cluster change to all branches created from now on.cc @lidizheng ",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26883,683636668,2021-08-05T17:05:45Z,tools/internal_ci/linux/grpc_xds_k8s_insecure.sh,"@@ -0,0 +1,144 @@+#!/usr/bin/env bash+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex -o igncr || set -ex++# Constants+readonly GITHUB_REPOSITORY_NAME=""grpc""+# GKE Cluster+readonly GKE_CLUSTER_NAME=""interop-test-psm-sec-v2-us-central1-a""","It's not that we worry about a breaking change, it's that PSM Sec cluster is configured differently, and comes with an overhead for security-related services running along with kubernetes. Using a separate cluster for regular tests just seems simpler. @sanjaypujare what do you think?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26889,683680282,2021-08-05T18:09:17Z,src/cpp/ext/filters/census/client_filter.cc,"@@ -56,11 +56,25 @@ grpc_error_handle CensusClientCallData::Init( // OpenCensusCallTracer::OpenCensusCallAttemptTracer // +OpenCensusCallTracer::OpenCensusCallAttemptTracer::OpenCensusCallAttemptTracer(+    OpenCensusCallTracer* parent, uint64_t attempt_num,+    bool is_transparent_retry, bool arena_allocated)+    : parent_(parent),+      arena_allocated_(arena_allocated),+      context_(CensusContext(absl::StrCat(""Attempt."", parent_->method_),",I don't think you need the `CensusContext()` here -- the parameters will automatically be passed to the ctor of the type of the field you're initializing.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26889,683703448,2021-08-05T18:44:02Z,src/cpp/ext/filters/census/client_filter.cc,"@@ -56,11 +56,25 @@ grpc_error_handle CensusClientCallData::Init( // OpenCensusCallTracer::OpenCensusCallAttemptTracer // +OpenCensusCallTracer::OpenCensusCallAttemptTracer::OpenCensusCallAttemptTracer(+    OpenCensusCallTracer* parent, uint64_t attempt_num,+    bool is_transparent_retry, bool arena_allocated)+    : parent_(parent),+      arena_allocated_(arena_allocated),+      context_(CensusContext(absl::StrCat(""Attempt."", parent_->method_),+                             &parent_->context_.Span(),+                             parent_->context_.tags())),+      start_time_(absl::Now()) {+  GPR_DEBUG_ASSERT(parent_->context_.Context().IsValid());","That's not quite optimal, since someone could later change the code to instantiate `OpenCensusCallAttemptTracer` from somewhere else, and they might not remember to add the assertion.Is `CensusContext` movable?  If so, you can use a helper function like this:```namespace {CensusContext CreateCensusContextForCallAttempt(    absl::string_view method, const CensusContext& parent_context) {  GPR_DEBUG_ASSERT(parent_context.Context().IsValid());  return CensusContext(method, &parent_context.Span(), parent_context.tags());}}  // namespace```Then in the initializer list, you can do this:```       context_(CreateCensusContextForCallAttempt(                    absl::StrCat(""Attempt."", parent_->method_),                    parent_->context_)),```",X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26709,683827746,2021-08-05T22:32:36Z,include/grpc/grpc_security.h,"@@ -775,6 +775,15 @@ typedef struct grpc_tls_certificate_provider grpc_tls_certificate_provider;  */ typedef struct grpc_tls_identity_pairs grpc_tls_identity_pairs; +/**+ * Status generated for certificate-key match.+ */+typedef struct grpc_tls_status_or_bool {","I don't think we need this for just one function. I would prefer just adding to the function parameters. That way we won't require the release API just for error_details. It would be the caller's responsibility to free error_details.```GRPCAPI bool grpc_tls_certificate_key_match(const char* private_key, const char* cert_chain, grpc_status_code* code, const char** error_details)```",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26709,683836396,2021-08-05T22:55:00Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -452,3 +474,27 @@ void grpc_tls_certificate_provider_release(   grpc_core::ExecCtx exec_ctx;   if (provider != nullptr) provider->Unref(); }++grpc_tls_status_or_bool grpc_tls_certificate_key_match(+    const char* private_key, const char* cert_chain){+  grpc_core::ExecCtx exec_ctx;+  absl::Status match_status = grpc_core::PrivateKeyAndCertificateMatch(private_key, cert_chain).status();+  grpc_status_code code = GRPC_STATUS_OK;+  const char* error_details = nullptr;+  bool match_bool;+  if (!match_status.ok()){+    code = static_cast<grpc_status_code>(match_status.code());+    error_details = gpr_strdup(std::string(match_status.message()).c_str());+    match_bool = false;","If we update the API, this function would then be```bool grpc_tls_certificate_key_match( const char* private_key, const char* cert_chain, grpc_status_code* code, const char** error_details) {...  absl::StatusOr<bool> matched_or = PrivateKeyCertificateMatch(...);  if(!matched_or.ok()) {    *code = static_cast<grpc_status_code>(matched_or.status().code());    *error_details = gpr_strdup(std::string(matched_or.status().message().c_str()));    return false;  }  *code = GRPC_STATUS_OK;  *error_details = nullptr;  return *matched_or;}```",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,683842476,2021-08-05T23:11:32Z,include/grpc/grpc_security.h,"@@ -768,6 +767,12 @@ typedef struct grpc_tls_credentials_options grpc_tls_credentials_options;  */ typedef struct grpc_tls_certificate_provider grpc_tls_certificate_provider; +/* Status generated while performing TLS handshakes. */","Do we really need this struct? Why not just append the code and error details to function signature?```GRPCAPI void grpc_tls_certificate_provider_data_watcher_set_key_cert_pairs(grpc_tls_certificate_provider* provider, grpc_tls_identity_pairs* pem_key_cert_pairs, grpc_status_code* code, const char** error_details)```This way we can make it the caller's responsibility to free error_details, and we don't have to create a release API just to free error_details. The current structure would have been useful if we had plans on expanding grpc_tls_status, but I don't think that's the case.@itsemmanuelfrancis @ZhenLian Wdyt?@johann1000 FYI Since I made a similar comment in certificate-key match PR.",X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26663,683847343,2021-08-05T23:25:53Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -365,6 +371,85 @@ FileWatcherCertificateProvider::ReadIdentityKeyCertPairFromFiles(   return absl::nullopt; } +DataWatcherCertificateProvider::DataWatcherCertificateProvider(+    std::string root_certificate,+    grpc_core::PemKeyCertPairList pem_key_cert_pairs)+    : StaticDataCertificateProvider(std::move(root_certificate),+                                    std::move(pem_key_cert_pairs)) {}++absl::Status DataWatcherCertificateProvider::SetRootCertificate(+    const std::string& root_certificate) {+  grpc_core::MutexLock lock(&mu_);+  if (root_certificate_ == root_certificate) {+    return absl::InvalidArgumentError(""Root Certificate has not changed."");+  }+  root_certificate_ = root_certificate;+  ExecCtx exec_ctx;+  grpc_error_handle root_cert_error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(+      ""Root Certificates are watched, while their contents are empty."");+  for (const auto& p : watcher_info_) {+    const std::string& cert_name = p.first;+    const WatcherInfo& info = p.second;+    absl::optional<std::string> root_to_report;+    if (info.root_being_watched) {+      root_to_report = root_certificate_;+    }+    if (root_to_report.has_value() && !root_to_report.value().empty()) {+      distributor_->SetKeyMaterials(cert_name, std::move(root_to_report),+                                    absl::nullopt);+    }+    if (info.root_being_watched && root_certificate_.empty()) {+      distributor_->SetErrorForCert(cert_name, GRPC_ERROR_REF(root_cert_error),+                                    absl::nullopt);+    }+  }+  GRPC_ERROR_UNREF(root_cert_error);+  return absl::OkStatus();+}++absl::Status DataWatcherCertificateProvider::SetKeyCertificatePairs(+    const PemKeyCertPairList& pem_key_cert_pairs) {+  grpc_core::MutexLock lock(&mu_);+  if (pem_key_cert_pairs == pem_key_cert_pairs_) {+    return absl::InvalidArgumentError(+        ""The Key-Cert pair list has not changed."");+  }+  absl::StatusOr<bool> all_matched_or =+      PrivateKeyAndCertificateMatch(pem_key_cert_pairs);+  if (!all_matched_or.ok()) {+    std::string error_message = std::string(all_matched_or.status().message());+    gpr_log(GPR_ERROR, ""Key-Cert pair list error: %s"", error_message.c_str());","We can avoid error_message variable here, and the code can be written asgpr_log(GPR_ERROR, ""Key-Cert pair list error: %s"", std::string(all_matched_or.status().message()).c_str());I wasn't sure, do we need these logs, aren't we anyway returning the error back to application?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26871,684314228,2021-08-06T15:15:34Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -837,7 +837,17 @@ class AdsServiceImpl : public std::enable_shared_from_this<AdsServiceImpl> {                         UpdateQueue* update_queue,                         SubscriptionMap* subscription_map,                         SentState* sent_state,-                        absl::optional<DiscoveryResponse>* response) {+                        absl::optional<DiscoveryResponse>* response)+        ABSL_EXCLUSIVE_LOCKS_REQUIRED(parent_->ads_mu_) {+      // We already told annotalysis that this function requires the lock, but","If the problem is that the compiler isn't able to translate the lock annotations through the template, is there a way that we can make it work via a proxy object *inside* the template?  For example, maybe we could create a new mutex class inside the template that just delegates all methods to `parent_->ads_mu_`, and then we can have the lock annotations refer to that lock instead of referring directly to `parent_->ads_mu_`?",
16571023,ananda1066,https://api.github.com/repos/grpc/grpc/pulls/26888,684351427,2021-08-06T16:12:21Z,test/cpp/qps/server_async.cc,"@@ -90,7 +90,7 @@ class AsyncQpsServerTest final : public grpc::testing::Server {      int num_threads = config.async_server_threads();     if (num_threads <= 0) {  // dynamic sizing-      num_threads = cores();+      num_threads = std::min(64, cores());","The maximum cap is machine-dependent. On Forge, we only get up to 12 GB of memory by default which can handle around 100 threads. I discussed with @ctiller and we decided to make this 64 threads by default since there isn't really anything new being tested by scaling it further. ",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/26709,684417014,2021-08-06T18:07:18Z,include/grpc/grpc_security.h,"@@ -775,6 +775,15 @@ typedef struct grpc_tls_certificate_provider grpc_tls_certificate_provider;  */ typedef struct grpc_tls_identity_pairs grpc_tls_identity_pairs; +/**+ * Status generated for certificate-key match.+ */+typedef struct grpc_tls_status_or_bool {","Hmm, I don't see much benefit because instead of freeing error details pointer, user has to make sure to invoke the release API. Similar structure is also used in in-memory PR, resulting in code replication. I would still push for my recommendation, but if current structure is what we decide to use, I think we should at-least make it a general structure, used by both key-match and in-memory (and more), and even the release API should have a general naming.@ZhenLian Wdyt?",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26709,684507298,2021-08-06T21:13:55Z,include/grpc/grpc_security.h,"@@ -775,6 +775,15 @@ typedef struct grpc_tls_certificate_provider grpc_tls_certificate_provider;  */ typedef struct grpc_tls_identity_pairs grpc_tls_identity_pairs; +/**+ * Status generated for certificate-key match.+ */+typedef struct grpc_tls_status_or_bool {","Yeah, we can have a general structure in the core API - that's actually what I've planned for. Whichever PR goes in first can add the struct and the latter can just reuse it. No need to create a separate struct for that.The reason why I didn't choose to put `code` and `error_details` as input parameter but instead making them a return value is that, considering how users would use this API:```grpc_status_code status;char* error_details = nullptr; // Not entirely sure if it works. maybe we need gpr_strdup() here to create a dummy string?bool result = grpc_tls_certificate_key_match(..., &status, &error_details);if (error_details != nullptr) {  gpr_free(error_details);  ​}```I think the condition doesn't get better, because callers still need to remember to free the error message at the end. It actually makes it a bit worse we will ask callers to explicitly free it  using `gpr_free`, not something else, because we will need to use `gpr_strdup` inside. This is an interesting problem, and I agree this applies to in-memory PR as well. Let me ask the gRPC team for advice and update here.",X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26709,684530805,2021-08-06T22:23:17Z,include/grpc/grpc_security.h,"@@ -775,6 +775,15 @@ typedef struct grpc_tls_certificate_provider grpc_tls_certificate_provider;  */ typedef struct grpc_tls_identity_pairs grpc_tls_identity_pairs; +/**+ * Status generated for certificate-key match.+ */+typedef struct grpc_tls_status_or_bool {","OK I got replies from gRPC team and they actually prefer something like this:```GRPCAPI grpc_status_code grpc_tls_certificate_key_match(..., char** error_details)```We will need to make sure in the document very clear that the caller will need to take ownership of the error message string and free it via gpr_free().@itsemmanuelfrancis we will need similar changes for in-memory provider as well.",
1535036,tamird,https://api.github.com/repos/grpc/grpc/pulls/26942,685256323,2021-08-09T14:42:50Z,src/core/lib/iomgr/timer_generic.cc,"@@ -223,7 +223,7 @@ static void validate_non_pending_timer(grpc_timer* t) {  * has last-seen. This is an optimization to prevent the thread from checking","the comment above this block is slightly out of date now; let me know if you'd like me to update it. The incorrect part is ""intptr_t"" -- the real limitation is that TLS types must not be larger than a machine word.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26883,685294700,2021-08-09T15:26:30Z,tools/internal_ci/linux/grpc_xds_k8s_lb_python.sh,"@@ -0,0 +1,154 @@+#!/usr/bin/env bash+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -eo pipefail++# Constants+readonly GITHUB_REPOSITORY_NAME=""grpc""+# GKE Cluster+readonly GKE_CLUSTER_NAME=""interop-test-psm-sec-v2-us-central1-a""+readonly GKE_CLUSTER_ZONE=""us-central1-a""+readonly SECONDARY_GKE_CLUSTER_NAME=""interop-test-psm-sec-v2-us-west1-b""+readonly SECONDARY_GKE_CLUSTER_ZONE=""us-west1-b""+## xDS test client Docker images+readonly CLIENT_IMAGE_NAME=""gcr.io/grpc-testing/xds-interop/python-client""+readonly FORCE_IMAGE_BUILD=""${FORCE_IMAGE_BUILD:-0}""+readonly BUILD_APP_PATH=""interop-testing/build/install/grpc-interop-testing""+readonly LANGUAGE_NAME=""Python""++#######################################+# Builds test app Docker images and pushes them to GCR+# Globals:+#   BUILD_APP_PATH+#   CLIENT_IMAGE_NAME: Test client Docker image name+#   GIT_COMMIT: SHA-1 of git commit being built+# Arguments:+#   None+# Outputs:+#   Writes the output of `gcloud builds submit` to stdout, stderr+#######################################+build_test_app_docker_images() {+  echo ""Building ${LANGUAGE_NAME} xDS interop test app Docker images""++  pushd ""${SRC_DIR}""+  docker build \+    -f src/python/grpcio_tests/tests_py3_only/interop/Dockerfile.client \+    -t ""${CLIENT_IMAGE_NAME}:${GIT_COMMIT}"" \+    .++  popd++  gcloud -q auth configure-docker++  docker push ""${CLIENT_IMAGE_NAME}:${GIT_COMMIT}""+}++#######################################+# Builds test app and its docker images unless they already exist+# Globals:+#   CLIENT_IMAGE_NAME: Test client Docker image name+#   GIT_COMMIT: SHA-1 of git commit being built+#   FORCE_IMAGE_BUILD+# Arguments:+#   None+# Outputs:+#   Writes the output to stdout, stderr+#######################################+build_docker_images_if_needed() {+  # Check if images already exist+  client_tags=""$(gcloud_gcr_list_image_tags ""${CLIENT_IMAGE_NAME}"" ""${GIT_COMMIT}"")""+  printf ""Client image: %s:%s\n"" ""${CLIENT_IMAGE_NAME}"" ""${GIT_COMMIT}""+  echo ""${client_tags:-Client image not found}""++  # Build if any of the images are missing, or FORCE_IMAGE_BUILD=1+  if [[ ""${FORCE_IMAGE_BUILD}"" == ""1"" || -z ""${client_tags}"" ]]; then+    build_test_app_docker_images+  else+    echo ""Skipping ${LANGUAGE_NAME} test app build""+  fi+}++#######################################+# Executes the test case+# Globals:+#   TEST_DRIVER_FLAGFILE: Relative path to test driver flagfile+#   KUBE_CONTEXT: The name of kubectl context with GKE cluster access+#   SECONDARY_KUBE_CONTEXT: The name of kubectl context with secondary GKE cluster access, if any+#   TEST_XML_OUTPUT_DIR: Output directory for the test xUnit XML report+#   CLIENT_IMAGE_NAME: Test client Docker image name+#   GIT_COMMIT: SHA-1 of git commit being built+# Arguments:+#   Test case name+# Outputs:+#   Writes the output of test execution to stdout, stderr+#   Test xUnit report to ${TEST_XML_OUTPUT_DIR}/${test_name}/sponge_log.xml+#######################################+run_test() {+  # Test driver usage:+  # https://github.com/grpc/grpc/tree/master/tools/run_tests/xds_k8s_test_driver#basic-usage+  local test_name=""${1:?Usage: run_test test_name}""+  set -x+  python -m ""tests.${test_name}"" \+    --flagfile=""${TEST_DRIVER_FLAGFILE}"" \+    --kube_context=""${KUBE_CONTEXT}"" \+    --secondary_kube_context=""${SECONDARY_KUBE_CONTEXT}"" \+    --client_image=""${CLIENT_IMAGE_NAME}:${GIT_COMMIT}"" \+    --xml_output_file=""${TEST_XML_OUTPUT_DIR}/${test_name}/sponge_log.xml"" \+    --flagfile=""config/url-map.cfg""+  set +x+}++#######################################+# Main function: provision software necessary to execute tests, and run them+# Globals:+#   KOKORO_ARTIFACTS_DIR+#   GITHUB_REPOSITORY_NAME+#   SRC_DIR: Populated with absolute path to the source repo+#   TEST_DRIVER_REPO_DIR: Populated with the path to the repo containing+#                         the test driver+#   TEST_DRIVER_FULL_DIR: Populated with the path to the test driver source code+#   TEST_DRIVER_FLAGFILE: Populated with relative path to test driver flagfile+#   TEST_XML_OUTPUT_DIR: Populated with the path to test xUnit XML report+#   GIT_ORIGIN_URL: Populated with the origin URL of git repo used for the build+#   GIT_COMMIT: Populated with the SHA-1 of git commit being built+#   GIT_COMMIT_SHORT: Populated with the short SHA-1 of git commit being built+#   KUBE_CONTEXT: Populated with name of kubectl context with GKE cluster access+#   SECONDARY_KUBE_CONTEXT: Populated with name of kubectl context with secondary GKE cluster access, if any+# Arguments:+#   None+# Outputs:+#   Writes the output of test execution to stdout, stderr+#######################################+main() {+  local script_dir+  script_dir=""$(dirname ""$0"")""+  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh+  source ""${script_dir}/grpc_xds_k8s_install_test_driver.sh""+  set -x+  if [[ -n ""${KOKORO_ARTIFACTS_DIR}"" ]]; then+    kokoro_setup_test_driver ""${GITHUB_REPOSITORY_NAME}""+  else+    local_setup_test_driver ""${script_dir}""+  fi+  build_docker_images_if_needed+  # Run tests+  cd ""${TEST_DRIVER_FULL_DIR}""+  run_test change_backend_service_test+  run_test failover_test+  run_test remove_neg_test+  run_test round_robin_test","Keep in mind this will exit after first test failure, and the rest of the tests won't run. It's easy to make it continue, but then the exit code of this script won't be reported should _any_ of the tests fail:```sh  set +e  run_test change_backend_service_test  run_test failover_test  run_test remove_neg_test  run_test round_robin_test```If we want to report the code, we need to do something like```sh#!/usr/bin/env bashset -eo pipefailrun_test() {  echo $1  if [[ $1 == ""good_test"" ]]; then    python -c 'import sys; print(""success""); sys.exit(0)'  else    python -c 'import sys; sys.exit(""fail"")'  fi}main() {  errors=0  for test in good_test bad_test good_test bad_test  do    local exit_code=0    run_test $test || (( errors++ ))  done  echo ""Failed test suites: ${errors}""  if (( errors > 0 )); then    exit 1  fi}main ""$@""```But in this case `python` must be the last call in `run_test`, so `set` command on line 110 must be deleted.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/26942,685376726,2021-08-09T17:14:24Z,src/core/lib/gpr/tls.h,"@@ -21,35 +21,36 @@  #include <grpc/support/port_platform.h> +#include <type_traits>++template <typename T, typename = typename std::enable_if<(+                          std::is_trivially_destructible<T>::value &&+                          sizeof(T) <= sizeof(void*) &&+                          alignof(void*) % alignof(T) == 0)>::type>+class Wrapper {","I know I typed Wrapper in the comment thread, but let's give it a more supportable name long term...```namespace grpc_core {template <...> class TlsTypeWrapper { ... };```",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26709,685561464,2021-08-09T22:25:34Z,include/grpc/grpc_security.h,"@@ -848,6 +848,13 @@ grpc_tls_certificate_provider_file_watcher_create( GRPCAPI void grpc_tls_certificate_provider_release(     grpc_tls_certificate_provider* provider); +/**+ * Checks if the private key matches the certificate's public key.",Let's be more accurate about this: `Checks if the private key matches the public key on the first certificate in the certificate chain`,X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26709,685594721,2021-08-09T23:50:32Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -413,6 +420,20 @@ absl::StatusOr<bool> PrivateKeyAndCertificateMatch(   return result; } +absl::StatusOr<bool> PrivateKeyAndCertificateMatch("," I think if the list is empty, while the key is present, the function should return error. When neither of them are present, we shall return error as well, since it's a blatantly violation of our API. After you made the change, suggest Add a comment describing the function. like ""Checks if the private key and certificate chain for all pairs in the list match. Only Returns OK if both key and certificate chain is present and they match. Otherwise, ....""",X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26709,685599852,2021-08-10T00:05:15Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -413,6 +420,20 @@ absl::StatusOr<bool> PrivateKeyAndCertificateMatch(   return result; } +absl::StatusOr<bool> PrivateKeyAndCertificateMatch(","Oh sorry, I was thinking about the wrong function. This one is checking the whole list.Then just add a comment above ""Checks if the private key and certificate chain for all pairs in the list match. Returns OK if matched or |pair_list| is empty. Otherwise,... """,
85572583,johann1000,https://api.github.com/repos/grpc/grpc/pulls/26709,685600381,2021-08-10T00:07:04Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -413,6 +420,20 @@ absl::StatusOr<bool> PrivateKeyAndCertificateMatch(   return result; } +absl::StatusOr<bool> PrivateKeyAndCertificateMatch(","There are already checks for empty key and/or certificate chain`if (private_key.empty()) {` `   return absl::InvalidArgumentError(""Private key string is empty."");` ` }` ` if (cert_chain.empty()) {` `   return absl::InvalidArgumentError(""Certificate string is empty."");` ` }`And in the header file `src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.h` there is already a similar description for the function. Is there anything else that needs to be added?",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26709,686189207,2021-08-10T17:42:06Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -452,3 +473,23 @@ void grpc_tls_certificate_provider_release(   grpc_core::ExecCtx exec_ctx;   if (provider != nullptr) provider->Unref(); }++grpc_status_code grpc_tls_certificate_key_match(+    const char* private_key, const char* cert_chain, const char** error_details){+  grpc_core::ExecCtx exec_ctx;+  grpc_status_code code;+  absl::StatusOr<bool> match_or = grpc_core::PrivateKeyAndCertificateMatch(private_key, cert_chain);+  if (!match_or.status().ok()) {+    code = static_cast<grpc_status_code>(match_or.status().code());+    *error_details = gpr_strdup(std::string(match_or.status().message()).c_str());+  } else {+    if (match_or.value()) {+      code = GRPC_STATUS_OK;+      *error_details = """";+    } else {+      code = static_cast<grpc_status_code>(absl::StatusCode::kInvalidArgument);+      *error_details = ""Certificate-key mismatch"";","suggest rephrase this to ""The private key doesn't match the public key on the first certificate of the certificate chain.""",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26709,686189617,2021-08-10T17:42:40Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.h,"@@ -140,6 +140,12 @@ class FileWatcherCertificateProvider final absl::StatusOr<bool> PrivateKeyAndCertificateMatch(     absl::string_view private_key, absl::string_view cert_chain); +//  Checks if the private key and leaf cert for all pairs in the list match.",s/leaf cert/certificate chain,
85572583,johann1000,https://api.github.com/repos/grpc/grpc/pulls/26709,686204045,2021-08-10T18:02:34Z,test/cpp/server/credentials_test.cc,"@@ -20,34 +20,64 @@ #include <grpcpp/security/server_credentials.h> #include <grpcpp/security/tls_credentials_options.h> #include <gtest/gtest.h>+#include <stdio.h>+#include <string.h>  #include <memory>  #include ""src/cpp/client/secure_credentials.h"" #include ""test/core/util/port.h"" #include ""test/core/util/test_config.h""+#include ""include/grpc/grpc_security.h""+#include ""test/core/util/tls_utils.h""+#include ""src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.h""","I included ""test/core/util/tls_utils.h"" because I needd to use GetFileContents() to get the credentials I needed for testing. I will remove the other one, it's actually useless",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26670,686236416,2021-08-10T18:49:49Z,src/core/tsi/openssl_utils.h,"@@ -0,0 +1,94 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#ifndef GRPC_CORE_TSI_OPENSSL_UTILS_H+#define GRPC_CORE_TSI_OPENSSL_UTILS_H++#include <grpc/support/port_platform.h>++#include ""absl/strings/string_view.h""++#include <openssl/evp.h>+#include <openssl/ssl.h>+#include <openssl/x509.h>++namespace grpc_core {++// A class for managing OpenSSL |EVP_PKEY| structures.+class OpenSslPKey {+ public:+  explicit OpenSslPKey(absl::string_view private_key);++  explicit OpenSslPKey(EVP_PKEY* pkey) { p_key_ = pkey; }++  ~OpenSslPKey() { EVP_PKEY_free(p_key_); }++  EVP_PKEY* get_p_key() { return p_key_; }++ private:+  EVP_PKEY* p_key_ = nullptr;+};++// A class for managing OpenSSL |X509| structures.+class OpenSslX509 {+ public:+  explicit OpenSslX509(absl::string_view cert_chain);++  explicit OpenSslX509(X509* x509) { x_509_ = x509; }++  ~OpenSslX509() { X509_free(x_509_); }++  X509* get_x509() { return x_509_; }++ private:+  X509* x_509_ = nullptr;+};++// A class for managing openssl `STACK_OF(X509_INFO)` structures.+class OpenSslX509InfoStack {","I think it would be too much for us to create a management class for every OpenSSL structure...Let's keep this for now, but I will need to think through how we can make it better...",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26942,686241322,2021-08-10T18:56:54Z,src/core/lib/gpr/tls.h,"@@ -21,52 +21,121 @@  #include <grpc/support/port_platform.h> -/** Thread local storage.--   A minimal wrapper that should be implementable across many compilers,-   and implementable efficiently across most modern compilers.+#include <type_traits> -   Thread locals have type intptr_t.+/** Thread local storage. -   Declaring a thread local variable 'foo':-     GPR_TLS_DECL(foo);-   Thread locals always have static scope.+   Usage is the same as C++ thread_local. Declaring a thread local:+     static GPR_THREAD_LOCAL(uint32_t) foo; -   Declaring a thread local class variable 'foo':-     GPR_TLS_CLASS_DECL(foo);+   Initializing a thread local (must be done at library initialization time):+     gpr_tls_init(foo); -   Defining the thread local class variable:-     GPR_TLS_CLASS_DEF(foo);+   Destroying a thread local:+     gpr_tls_destroy(foo); -   Initializing a thread local (must be done at library initialization-   time):-     gpr_tls_init(&foo);+   ALL functions here may be implemented as macros. */ -   Destroying a thread local:-     gpr_tls_destroy(&foo);+namespace grpc_core { -   Setting a thread local (returns new_value):-     gpr_tls_set(&foo, new_value);+template <typename T, typename = typename std::enable_if<(+                          std::is_trivially_destructible<T>::value &&+                          sizeof(T) <= sizeof(void*) &&+                          alignof(void*) % alignof(T) == 0)>::type>+class TlsTypeWrapper {+  static_assert(std::is_trivially_destructible<T>::value &&+                    sizeof(T) <= sizeof(void*) &&+                    alignof(void*) % alignof(T) == 0,+                ""unsupported type for TLS""); -   Accessing a thread local:-     current_value = gpr_tls_get(&foo);+ public:+  using Type = T;+}; -   ALL functions here may be implemented as macros. */+}  // namespace grpc_core  #ifdef GPR_STDCPP_TLS-#include ""src/core/lib/gpr/tls_stdcpp.h""+#define GPR_THREAD_LOCAL(type) \+  thread_local grpc_core::TlsTypeWrapper<type>::Type #endif  #ifdef GPR_GCC_TLS-#include ""src/core/lib/gpr/tls_gcc.h""+#define GPR_THREAD_LOCAL(type) __thread grpc_core::TlsTypeWrapper<type>::Type #endif  #ifdef GPR_MSVC_TLS-#include ""src/core/lib/gpr/tls_msvc.h""+#define GPR_THREAD_LOCAL(type) \+  __declspec(thread) grpc_core::TlsTypeWrapper<type>::Type #endif  #ifdef GPR_PTHREAD_TLS-#include ""src/core/lib/gpr/tls_pthread.h""+#include <grpc/support/log.h> /* for GPR_ASSERT */+#include <pthread.h>++namespace grpc_core {++template <typename T>+class pthreadTlsWrapper : TlsTypeWrapper<T> {","Please call this `PthreadTlsWrapper` (capitalized first letter), as per style guide naming rules.https://google.github.io/styleguide/cppguide.html#Type_Names",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26670,686242757,2021-08-10T18:58:54Z,src/core/tsi/openssl_utils.h,"@@ -0,0 +1,94 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#ifndef GRPC_CORE_TSI_OPENSSL_UTILS_H+#define GRPC_CORE_TSI_OPENSSL_UTILS_H++#include <grpc/support/port_platform.h>++#include ""absl/strings/string_view.h""++#include <openssl/evp.h>+#include <openssl/ssl.h>+#include <openssl/x509.h>++namespace grpc_core {++// A class for managing OpenSSL |EVP_PKEY| structures.+class OpenSslPKey {+ public:+  explicit OpenSslPKey(absl::string_view private_key);++  explicit OpenSslPKey(EVP_PKEY* pkey) { p_key_ = pkey; }++  ~OpenSslPKey() { EVP_PKEY_free(p_key_); }++  EVP_PKEY* get_p_key() { return p_key_; }++ private:+  EVP_PKEY* p_key_ = nullptr;+};++// A class for managing OpenSSL |X509| structures.+class OpenSslX509 {+ public:+  explicit OpenSslX509(absl::string_view cert_chain);++  explicit OpenSslX509(X509* x509) { x_509_ = x509; }++  ~OpenSslX509() { X509_free(x_509_); }++  X509* get_x509() { return x_509_; }++ private:+  X509* x_509_ = nullptr;+};++// A class for managing openssl `STACK_OF(X509_INFO)` structures.+class OpenSslX509InfoStack {+ public:+  explicit OpenSslX509InfoStack(absl::string_view cert_chain);++  explicit OpenSslX509InfoStack(STACK_OF(X509_INFO) * sk) { info_stack_ = sk; }++  ~OpenSslX509InfoStack() {+    sk_X509_INFO_pop_free(info_stack_, X509_INFO_free);+  }++  STACK_OF(X509_INFO) * get_stack() { return info_stack_; }++ private:+  STACK_OF(X509_INFO) * info_stack_ = nullptr;+};++// A class for managing openssl `SSL_CTX` structures.+class OpenSslConnectionConfig {",Rename to `OpenSslContext`. CTX represents `context` not config. Same for everything else in this class.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26942,686244842,2021-08-10T19:01:59Z,src/core/lib/gpr/tls.h,"@@ -21,52 +21,121 @@  #include <grpc/support/port_platform.h> -/** Thread local storage.--   A minimal wrapper that should be implementable across many compilers,-   and implementable efficiently across most modern compilers.+#include <type_traits> -   Thread locals have type intptr_t.+/** Thread local storage. -   Declaring a thread local variable 'foo':-     GPR_TLS_DECL(foo);-   Thread locals always have static scope.+   Usage is the same as C++ thread_local. Declaring a thread local:+     static GPR_THREAD_LOCAL(uint32_t) foo; -   Declaring a thread local class variable 'foo':-     GPR_TLS_CLASS_DECL(foo);+   Initializing a thread local (must be done at library initialization time):+     gpr_tls_init(foo); -   Defining the thread local class variable:-     GPR_TLS_CLASS_DEF(foo);+   Destroying a thread local:+     gpr_tls_destroy(foo); -   Initializing a thread local (must be done at library initialization-   time):-     gpr_tls_init(&foo);+   ALL functions here may be implemented as macros. */ -   Destroying a thread local:-     gpr_tls_destroy(&foo);+namespace grpc_core { -   Setting a thread local (returns new_value):-     gpr_tls_set(&foo, new_value);+template <typename T, typename = typename std::enable_if<(+                          std::is_trivially_destructible<T>::value &&+                          sizeof(T) <= sizeof(void*) &&+                          alignof(void*) % alignof(T) == 0)>::type>+class TlsTypeWrapper {+  static_assert(std::is_trivially_destructible<T>::value &&+                    sizeof(T) <= sizeof(void*) &&+                    alignof(void*) % alignof(T) == 0,+                ""unsupported type for TLS""); -   Accessing a thread local:-     current_value = gpr_tls_get(&foo);+ public:+  using Type = T;+}; -   ALL functions here may be implemented as macros. */+}  // namespace grpc_core  #ifdef GPR_STDCPP_TLS-#include ""src/core/lib/gpr/tls_stdcpp.h""+#define GPR_THREAD_LOCAL(type) \+  thread_local grpc_core::TlsTypeWrapper<type>::Type #endif  #ifdef GPR_GCC_TLS-#include ""src/core/lib/gpr/tls_gcc.h""+#define GPR_THREAD_LOCAL(type) __thread grpc_core::TlsTypeWrapper<type>::Type #endif  #ifdef GPR_MSVC_TLS-#include ""src/core/lib/gpr/tls_msvc.h""+#define GPR_THREAD_LOCAL(type) \+  __declspec(thread) grpc_core::TlsTypeWrapper<type>::Type #endif  #ifdef GPR_PTHREAD_TLS-#include ""src/core/lib/gpr/tls_pthread.h""+#include <grpc/support/log.h> /* for GPR_ASSERT */+#include <pthread.h>++namespace grpc_core {++template <typename T>+class pthreadTlsWrapper : TlsTypeWrapper<T> {+ public:+  pthreadTlsWrapper() = default;+  pthreadTlsWrapper(const pthreadTlsWrapper&) = delete;+  pthreadTlsWrapper& operator=(const pthreadTlsWrapper&) = delete;++  void Init() { GPR_ASSERT(0 == pthread_key_create(&key_, nullptr)); }++  void Destroy() { GPR_ASSERT(0 == pthread_key_delete(key_)); }++  operator T() const { return Cast<T>(pthread_getspecific(key_)); }++  T operator=(T t) {+    GPR_ASSERT(0 == pthread_setspecific(key_, Cast<T>(t)));+    return t;+  }++ private:+  // TODO(C++17): Replace these helpers with constexpr if statements inline.+  template <typename V>+  static typename std::enable_if<std::is_pointer<V>::value, V>::type Cast(+      void* object) {+    return static_cast<V>(object);+  }++  template <typename V>+  static typename std::enable_if<+      !std::is_pointer<V>::value && std::is_integral<V>::value, V>::type+  Cast(void* object) {+    return reinterpret_cast<uintptr_t>(object);+  }++  template <typename V>+  static void* Cast(+      typename std::enable_if<std::is_pointer<V>::value, V>::type t) {+    return t;+  }++  template <typename V>+  static void* Cast(typename std::enable_if<!std::is_pointer<V>::value &&+                                                std::is_integral<V>::value,+                                            V>::type t) {+    return reinterpret_cast<void*>(uintptr_t(t));+  }++  pthread_key_t key_;+};++}  // namespace grpc_core++#define GPR_THREAD_LOCAL(type) grpc_core::pthreadTlsWrapper<type>","It looks like there are a number of cases where we use a global thread-local variable.  Note that the style guide prohibits global variables that are not trivially destructible:https://google.github.io/styleguide/cppguide.html#Static_and_Global_VariablesIs the `pthreadTlsWrapper` class trivially destructible?  If not, this won't work.  If so, please add a static assertion that guarantees that property, with a comment as to why it's there.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/26955,686255476,2021-08-10T19:19:05Z,tools/internal_ci/linux/grpc_xds_k8s.sh,"@@ -45,6 +45,9 @@ build_test_app_docker_images() {   gcloud -q auth configure-docker   docker push ""${CLIENT_IMAGE_NAME}:${GIT_COMMIT}""   docker push ""${SERVER_IMAGE_NAME}:${GIT_COMMIT}""+  branch_name=$(echo ""$KOKORO_JOB_NAME"" | sed -E 's|^grpc/core/(.+)/linux/grpc_xds_k8s$|\1|')",I think the regexp won't work with branches. The pattern of jobs from master and jobs from branches is different:1. master: `grpc/core/master/linux/grpc_xds_k8s`2. branches: `grpc/core/<branch_name>/branch/linux/grpc_xds_k8s`,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/26955,686264755,2021-08-10T19:33:39Z,tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh,"@@ -361,3 +361,26 @@ local_setup_test_driver() {   mkdir -p ""${TEST_XML_OUTPUT_DIR}"" } +#######################################+# Tag and push the given Docker image+# Globals:+#   INITIATOR: the initiator of this test script+# Arguments:+#   The Docker image name+#   The Docker image original tag name+#   The Docker image new tag name+# Outputs:+#   Writes the output to stdout, stderr, files+#######################################+tag_and_push_docker_image() {+  local image_name=""$1""+  local from_tag=""$2""+  local to_tag=""$3""++  if [[ ${INITIATOR} == ""kokoro"" ]]; then",Removed.I worried about experimental runs might create unwanted Docker images. This condition is used to protect un-approved code being executed in our interop tests.,
1535036,tamird,https://api.github.com/repos/grpc/grpc/pulls/26942,686270369,2021-08-10T19:42:22Z,src/core/lib/gpr/tls.h,"@@ -21,52 +21,121 @@  #include <grpc/support/port_platform.h> -/** Thread local storage.--   A minimal wrapper that should be implementable across many compilers,-   and implementable efficiently across most modern compilers.+#include <type_traits> -   Thread locals have type intptr_t.+/** Thread local storage. -   Declaring a thread local variable 'foo':-     GPR_TLS_DECL(foo);-   Thread locals always have static scope.+   Usage is the same as C++ thread_local. Declaring a thread local:+     static GPR_THREAD_LOCAL(uint32_t) foo; -   Declaring a thread local class variable 'foo':-     GPR_TLS_CLASS_DECL(foo);+   Initializing a thread local (must be done at library initialization time):+     gpr_tls_init(foo); -   Defining the thread local class variable:-     GPR_TLS_CLASS_DEF(foo);+   Destroying a thread local:+     gpr_tls_destroy(foo); -   Initializing a thread local (must be done at library initialization-   time):-     gpr_tls_init(&foo);+   ALL functions here may be implemented as macros. */ -   Destroying a thread local:-     gpr_tls_destroy(&foo);+namespace grpc_core { -   Setting a thread local (returns new_value):-     gpr_tls_set(&foo, new_value);+template <typename T, typename = typename std::enable_if<(+                          std::is_trivially_destructible<T>::value &&+                          sizeof(T) <= sizeof(void*) &&+                          alignof(void*) % alignof(T) == 0)>::type>+class TlsTypeWrapper {+  static_assert(std::is_trivially_destructible<T>::value &&+                    sizeof(T) <= sizeof(void*) &&+                    alignof(void*) % alignof(T) == 0,+                ""unsupported type for TLS""); -   Accessing a thread local:-     current_value = gpr_tls_get(&foo);+ public:+  using Type = T;+}; -   ALL functions here may be implemented as macros. */+}  // namespace grpc_core  #ifdef GPR_STDCPP_TLS-#include ""src/core/lib/gpr/tls_stdcpp.h""+#define GPR_THREAD_LOCAL(type) \+  thread_local grpc_core::TlsTypeWrapper<type>::Type #endif  #ifdef GPR_GCC_TLS-#include ""src/core/lib/gpr/tls_gcc.h""+#define GPR_THREAD_LOCAL(type) __thread grpc_core::TlsTypeWrapper<type>::Type #endif  #ifdef GPR_MSVC_TLS-#include ""src/core/lib/gpr/tls_msvc.h""+#define GPR_THREAD_LOCAL(type) \+  __declspec(thread) grpc_core::TlsTypeWrapper<type>::Type #endif  #ifdef GPR_PTHREAD_TLS-#include ""src/core/lib/gpr/tls_pthread.h""+#include <grpc/support/log.h> /* for GPR_ASSERT */+#include <pthread.h>++namespace grpc_core {++template <typename T>+class pthreadTlsWrapper : TlsTypeWrapper<T> {+ public:+  pthreadTlsWrapper() = default;+  pthreadTlsWrapper(const pthreadTlsWrapper&) = delete;+  pthreadTlsWrapper& operator=(const pthreadTlsWrapper&) = delete;++  void Init() { GPR_ASSERT(0 == pthread_key_create(&key_, nullptr)); }++  void Destroy() { GPR_ASSERT(0 == pthread_key_delete(key_)); }++  operator T() const { return Cast<T>(pthread_getspecific(key_)); }++  T operator=(T t) {+    GPR_ASSERT(0 == pthread_setspecific(key_, Cast<T>(t)));+    return t;+  }++ private:+  // TODO(C++17): Replace these helpers with constexpr if statements inline.+  template <typename V>+  static typename std::enable_if<std::is_pointer<V>::value, V>::type Cast(+      void* object) {+    return static_cast<V>(object);+  }++  template <typename V>+  static typename std::enable_if<+      !std::is_pointer<V>::value && std::is_integral<V>::value, V>::type+  Cast(void* object) {+    return reinterpret_cast<uintptr_t>(object);+  }++  template <typename V>+  static void* Cast(+      typename std::enable_if<std::is_pointer<V>::value, V>::type t) {+    return t;+  }++  template <typename V>+  static void* Cast(typename std::enable_if<!std::is_pointer<V>::value &&+                                                std::is_integral<V>::value,+                                            V>::type t) {+    return reinterpret_cast<void*>(uintptr_t(t));+  }++  pthread_key_t key_;+};++}  // namespace grpc_core++#define GPR_THREAD_LOCAL(type) grpc_core::pthreadTlsWrapper<type>++#define gpr_tls_init(tls) (tls).Init()",Because there's no wrapper class *being instantiated* in the not-pthread case. What would there be a method on?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26942,686290075,2021-08-10T20:14:25Z,src/core/lib/gprpp/tls.h,"@@ -0,0 +1,156 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_LIB_GPR_TLS_H+#define GRPC_CORE_LIB_GPR_TLS_H++#include <grpc/support/port_platform.h>++#include <type_traits>++/** Thread local storage.++   Usage is the same as C++ thread_local. Declaring a thread local:+     static GPR_THREAD_LOCAL(uint32_t) foo;++   Initializing a thread local (must be done at library initialization time):+     gpr_tls_init(foo);++   Destroying a thread local:+     gpr_tls_destroy(foo);++   ALL functions here may be implemented as macros. */++namespace grpc_core {++template <typename T, typename = typename std::enable_if<(+                          std::is_trivially_destructible<T>::value &&+                          sizeof(T) <= sizeof(void*) &&+                          alignof(void*) % alignof(T) == 0)>::type>+class TlsTypeWrapper {+  static_assert(std::is_trivially_destructible<T>::value &&+                    sizeof(T) <= sizeof(void*) &&+                    alignof(void*) % alignof(T) == 0,+                ""unsupported type for TLS"");++ public:+  using Type = T;+};++}  // namespace grpc_core++#ifdef GPR_STDCPP_TLS+#define GPR_THREAD_LOCAL(type) \+  thread_local grpc_core::TlsTypeWrapper<type>::Type+#endif++#ifdef GPR_GCC_TLS+#define GPR_THREAD_LOCAL(type) __thread grpc_core::TlsTypeWrapper<type>::Type+#endif++#ifdef GPR_MSVC_TLS+#define GPR_THREAD_LOCAL(type) \+  __declspec(thread) grpc_core::TlsTypeWrapper<type>::Type+#endif++#ifdef GPR_PTHREAD_TLS+#include <grpc/support/log.h> /* for GPR_ASSERT */+#include <pthread.h>++namespace grpc_core {++template <typename T>+class PthreadTlsWrapper : TlsTypeWrapper<T> {+ public:+  PthreadTlsWrapper() = default;+  PthreadTlsWrapper(const PthreadTlsWrapper&) = delete;+  PthreadTlsWrapper& operator=(const PthreadTlsWrapper&) = delete;++  void Init() { GPR_ASSERT(0 == pthread_key_create(&key_, nullptr)); }++  void Destroy() { GPR_ASSERT(0 == pthread_key_delete(key_)); }++  operator T() const { return Cast<T>(pthread_getspecific(key_)); }++  T operator=(T t) {+    GPR_ASSERT(0 == pthread_setspecific(key_, Cast<T>(t)));+    return t;+  }++ private:+  // TODO(C++17): Replace these helpers with constexpr if statements inline.+  template <typename V>+  static typename std::enable_if<std::is_pointer<V>::value, V>::type Cast(+      void* object) {+    return static_cast<V>(object);+  }++  template <typename V>+  static typename std::enable_if<+      !std::is_pointer<V>::value && std::is_integral<V>::value, V>::type+  Cast(void* object) {+    return reinterpret_cast<uintptr_t>(object);+  }++  template <typename V>+  static void* Cast(+      typename std::enable_if<std::is_pointer<V>::value, V>::type t) {+    return t;+  }++  template <typename V>+  static void* Cast(typename std::enable_if<!std::is_pointer<V>::value &&+                                                std::is_integral<V>::value,+                                            V>::type t) {+    return reinterpret_cast<void*>(uintptr_t(t));+  }++  pthread_key_t key_;+};++template <typename T>+class TriviallyDestructibleAsserter {","Instead of creating this separate class, how about moving the assertion directly into `TlsTypeWrapper`?Actually, given your point that the class is never actually instantiated, this assertion is probably not needed to begin with.",
1535036,tamird,https://api.github.com/repos/grpc/grpc/pulls/26942,686361475,2021-08-10T22:25:23Z,src/core/lib/gprpp/tls.h,"@@ -0,0 +1,156 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_LIB_GPR_TLS_H+#define GRPC_CORE_LIB_GPR_TLS_H++#include <grpc/support/port_platform.h>++#include <type_traits>++/** Thread local storage.++   Usage is the same as C++ thread_local. Declaring a thread local:+     static GPR_THREAD_LOCAL(uint32_t) foo;++   Initializing a thread local (must be done at library initialization time):+     gpr_tls_init(foo);++   Destroying a thread local:+     gpr_tls_destroy(foo);++   ALL functions here may be implemented as macros. */++namespace grpc_core {++template <typename T, typename = typename std::enable_if<(+                          std::is_trivially_destructible<T>::value &&+                          sizeof(T) <= sizeof(void*) &&+                          alignof(void*) % alignof(T) == 0)>::type>+class TlsTypeWrapper {+  static_assert(std::is_trivially_destructible<T>::value &&+                    sizeof(T) <= sizeof(void*) &&+                    alignof(void*) % alignof(T) == 0,+                ""unsupported type for TLS"");++ public:+  using Type = T;+};++}  // namespace grpc_core++#ifdef GPR_STDCPP_TLS+#define GPR_THREAD_LOCAL(type) \+  thread_local grpc_core::TlsTypeWrapper<type>::Type+#endif++#ifdef GPR_GCC_TLS+#define GPR_THREAD_LOCAL(type) __thread grpc_core::TlsTypeWrapper<type>::Type+#endif++#ifdef GPR_MSVC_TLS+#define GPR_THREAD_LOCAL(type) \+  __declspec(thread) grpc_core::TlsTypeWrapper<type>::Type+#endif++#ifdef GPR_PTHREAD_TLS+#include <grpc/support/log.h> /* for GPR_ASSERT */+#include <pthread.h>++namespace grpc_core {++template <typename T>+class PthreadTlsWrapper : TlsTypeWrapper<T> {+ public:+  PthreadTlsWrapper() = default;+  PthreadTlsWrapper(const PthreadTlsWrapper&) = delete;+  PthreadTlsWrapper& operator=(const PthreadTlsWrapper&) = delete;++  void Init() { GPR_ASSERT(0 == pthread_key_create(&key_, nullptr)); }++  void Destroy() { GPR_ASSERT(0 == pthread_key_delete(key_)); }++  operator T() const { return Cast<T>(pthread_getspecific(key_)); }++  T operator=(T t) {+    GPR_ASSERT(0 == pthread_setspecific(key_, Cast<T>(t)));+    return t;+  }++ private:+  // TODO(C++17): Replace these helpers with constexpr if statements inline.+  template <typename V>+  static typename std::enable_if<std::is_pointer<V>::value, V>::type Cast(+      void* object) {+    return static_cast<V>(object);+  }++  template <typename V>+  static typename std::enable_if<+      !std::is_pointer<V>::value && std::is_integral<V>::value, V>::type+  Cast(void* object) {+    return reinterpret_cast<uintptr_t>(object);+  }++  template <typename V>+  static void* Cast(+      typename std::enable_if<std::is_pointer<V>::value, V>::type t) {+    return t;+  }++  template <typename V>+  static void* Cast(typename std::enable_if<!std::is_pointer<V>::value &&+                                                std::is_integral<V>::value,+                                            V>::type t) {+    return reinterpret_cast<void*>(uintptr_t(t));+  }++  pthread_key_t key_;+};++template <typename T>+class TriviallyDestructibleAsserter {","This assertion is being applied to `PthreadTlsWrapper`, not `TlsTypeWrapper`.Unlike `TlsTypeWrapper`, `PthreadTlsWrapper` *is* instantiated, so this assertion is necessary.",
84942730,itsemmanuelfrancis,https://api.github.com/repos/grpc/grpc/pulls/26670,686463705,2021-08-11T03:26:05Z,src/core/tsi/openssl_utils.h,"@@ -0,0 +1,94 @@+//+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#ifndef GRPC_CORE_TSI_OPENSSL_UTILS_H+#define GRPC_CORE_TSI_OPENSSL_UTILS_H++#include <grpc/support/port_platform.h>++#include ""absl/strings/string_view.h""++#include <openssl/evp.h>+#include <openssl/ssl.h>+#include <openssl/x509.h>++namespace grpc_core {++// A class for managing OpenSSL |EVP_PKEY| structures.+class OpenSslPKey {+ public:+  explicit OpenSslPKey(absl::string_view private_key);++  explicit OpenSslPKey(EVP_PKEY* pkey) { p_key_ = pkey; }++  ~OpenSslPKey() { EVP_PKEY_free(p_key_); }++  EVP_PKEY* get_p_key() { return p_key_; }++ private:+  EVP_PKEY* p_key_ = nullptr;+};++// A class for managing OpenSSL |X509| structures.+class OpenSslX509 {+ public:+  explicit OpenSslX509(absl::string_view cert_chain);++  explicit OpenSslX509(X509* x509) { x_509_ = x509; }++  ~OpenSslX509() { X509_free(x_509_); }++  X509* get_x509() { return x_509_; }++ private:+  X509* x_509_ = nullptr;+};++// A class for managing openssl `STACK_OF(X509_INFO)` structures.+class OpenSslX509InfoStack {",We could use templated classes where you have to pass in the freeing function but that means none of the nice intialising with `absl::string_view`,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26942,686956252,2021-08-11T15:45:25Z,src/core/lib/gpr/tls.h,"@@ -21,52 +21,143 @@  #include <grpc/support/port_platform.h> -/** Thread local storage.--   A minimal wrapper that should be implementable across many compilers,-   and implementable efficiently across most modern compilers.+#include <type_traits> -   Thread locals have type intptr_t.+/** Thread local storage. -   Declaring a thread local variable 'foo':-     GPR_TLS_DECL(foo);-   Thread locals always have static scope.+   Usage is the same as C++ thread_local. Declaring a thread local:+     static GPR_THREAD_LOCAL(uint32_t) foo; -   Declaring a thread local class variable 'foo':-     GPR_TLS_CLASS_DECL(foo);+   Initializing a thread local (must be done at library initialization time):+     gpr_tls_init(foo); -   Defining the thread local class variable:-     GPR_TLS_CLASS_DEF(foo);+   Destroying a thread local:+     gpr_tls_destroy(foo); -   Initializing a thread local (must be done at library initialization-   time):-     gpr_tls_init(&foo);+   ALL functions here may be implemented as macros. */ -   Destroying a thread local:-     gpr_tls_destroy(&foo);+namespace grpc_core { -   Setting a thread local (returns new_value):-     gpr_tls_set(&foo, new_value);+// This class is never instantiated. It exists to statically ensure that all+// TLS usage is compatible with the most restrictive implementation, allowing+// developers to write correct code regardless of the platform they develop on.+template <typename T, typename = typename std::enable_if<(+                          std::is_trivially_destructible<T>::value &&+                          sizeof(T) <= sizeof(void*) &&+                          alignof(void*) % alignof(T) == 0)>::type>+class TlsTypeConstrainer {+  static_assert(std::is_trivially_destructible<T>::value &&+                    sizeof(T) <= sizeof(void*) &&+                    alignof(void*) % alignof(T) == 0,+                ""unsupported type for TLS""); -   Accessing a thread local:-     current_value = gpr_tls_get(&foo);+ public:+  using Type = T;+}; -   ALL functions here may be implemented as macros. */+}  // namespace grpc_core  #ifdef GPR_STDCPP_TLS-#include ""src/core/lib/gpr/tls_stdcpp.h""+#define GPR_THREAD_LOCAL(type) \+  thread_local grpc_core::TlsTypeConstrainer<type>::Type #endif  #ifdef GPR_GCC_TLS-#include ""src/core/lib/gpr/tls_gcc.h""+#define GPR_THREAD_LOCAL(type) \+  __thread grpc_core::TlsTypeConstrainer<type>::Type #endif  #ifdef GPR_MSVC_TLS-#include ""src/core/lib/gpr/tls_msvc.h""+#define GPR_THREAD_LOCAL(type) \+  __declspec(thread) grpc_core::TlsTypeConstrainer<type>::Type #endif  #ifdef GPR_PTHREAD_TLS-#include ""src/core/lib/gpr/tls_pthread.h""+#include <grpc/support/log.h> /* for GPR_ASSERT */+#include <pthread.h>++namespace grpc_core {++template <typename T>+class PthreadTlsImpl : TlsTypeConstrainer<T> {+ public:+  PthreadTlsImpl() = default;+  PthreadTlsImpl(const PthreadTlsImpl&) = delete;+  PthreadTlsImpl& operator=(const PthreadTlsImpl&) = delete;++  void Init() { GPR_ASSERT(0 == pthread_key_create(&key_, nullptr)); }++  void Destroy() { GPR_ASSERT(0 == pthread_key_delete(key_)); }++  operator T() const { return Cast<T>(pthread_getspecific(key_)); }++  T operator=(T t) {+    GPR_ASSERT(0 == pthread_setspecific(key_, Cast<T>(t)));+    return t;+  }++ private:+  // TODO(C++17): Replace these helpers with constexpr if statements inline.+  template <typename V>+  static typename std::enable_if<std::is_pointer<V>::value, V>::type Cast(+      void* object) {+    return static_cast<V>(object);+  }++  template <typename V>+  static typename std::enable_if<+      !std::is_pointer<V>::value && std::is_integral<V>::value, V>::type+  Cast(void* object) {+    return reinterpret_cast<uintptr_t>(object);+  }++  template <typename V>+  static void* Cast(+      typename std::enable_if<std::is_pointer<V>::value, V>::type t) {+    return t;+  }++  template <typename V>+  static void* Cast(typename std::enable_if<!std::is_pointer<V>::value &&+                                                std::is_integral<V>::value,+                                            V>::type t) {+    return reinterpret_cast<void*>(uintptr_t(t));+  }++  pthread_key_t key_;+};++// This class is never instantiated. It exists to statically ensure that all+// TLS usage is compatible with the most restrictive implementation, allowing+// developers to write correct code regardless of the platform they develop on.+template <typename T>+class TriviallyDestructibleAsserter {+  // This type is often used as a global; since the type itself is hidden by the+  // macros, enforce compliance with the style guide here rather than at the+  // caller. See+  // https://google.github.io/styleguide/cppguide.html#Static_and_Global_Variables.+  static_assert(std::is_trivially_destructible<T>::value,","Can this assertion be moved into the `PthreadTlsImpl` ctor, so that we don't need a separate class here?",
1535036,tamird,https://api.github.com/repos/grpc/grpc/pulls/26942,686957517,2021-08-11T15:46:55Z,src/core/lib/gpr/tls.h,"@@ -21,52 +21,143 @@  #include <grpc/support/port_platform.h> -/** Thread local storage.--   A minimal wrapper that should be implementable across many compilers,-   and implementable efficiently across most modern compilers.+#include <type_traits> -   Thread locals have type intptr_t.+/** Thread local storage. -   Declaring a thread local variable 'foo':-     GPR_TLS_DECL(foo);-   Thread locals always have static scope.+   Usage is the same as C++ thread_local. Declaring a thread local:+     static GPR_THREAD_LOCAL(uint32_t) foo; -   Declaring a thread local class variable 'foo':-     GPR_TLS_CLASS_DECL(foo);+   Initializing a thread local (must be done at library initialization time):+     gpr_tls_init(foo); -   Defining the thread local class variable:-     GPR_TLS_CLASS_DEF(foo);+   Destroying a thread local:+     gpr_tls_destroy(foo); -   Initializing a thread local (must be done at library initialization-   time):-     gpr_tls_init(&foo);+   ALL functions here may be implemented as macros. */ -   Destroying a thread local:-     gpr_tls_destroy(&foo);+namespace grpc_core { -   Setting a thread local (returns new_value):-     gpr_tls_set(&foo, new_value);+// This class is never instantiated. It exists to statically ensure that all+// TLS usage is compatible with the most restrictive implementation, allowing+// developers to write correct code regardless of the platform they develop on.+template <typename T, typename = typename std::enable_if<(+                          std::is_trivially_destructible<T>::value &&+                          sizeof(T) <= sizeof(void*) &&+                          alignof(void*) % alignof(T) == 0)>::type>+class TlsTypeConstrainer {+  static_assert(std::is_trivially_destructible<T>::value &&+                    sizeof(T) <= sizeof(void*) &&+                    alignof(void*) % alignof(T) == 0,+                ""unsupported type for TLS""); -   Accessing a thread local:-     current_value = gpr_tls_get(&foo);+ public:+  using Type = T;+}; -   ALL functions here may be implemented as macros. */+}  // namespace grpc_core  #ifdef GPR_STDCPP_TLS-#include ""src/core/lib/gpr/tls_stdcpp.h""+#define GPR_THREAD_LOCAL(type) \+  thread_local grpc_core::TlsTypeConstrainer<type>::Type #endif  #ifdef GPR_GCC_TLS-#include ""src/core/lib/gpr/tls_gcc.h""+#define GPR_THREAD_LOCAL(type) \+  __thread grpc_core::TlsTypeConstrainer<type>::Type #endif  #ifdef GPR_MSVC_TLS-#include ""src/core/lib/gpr/tls_msvc.h""+#define GPR_THREAD_LOCAL(type) \+  __declspec(thread) grpc_core::TlsTypeConstrainer<type>::Type #endif  #ifdef GPR_PTHREAD_TLS-#include ""src/core/lib/gpr/tls_pthread.h""+#include <grpc/support/log.h> /* for GPR_ASSERT */+#include <pthread.h>++namespace grpc_core {++template <typename T>+class PthreadTlsImpl : TlsTypeConstrainer<T> {+ public:+  PthreadTlsImpl() = default;+  PthreadTlsImpl(const PthreadTlsImpl&) = delete;+  PthreadTlsImpl& operator=(const PthreadTlsImpl&) = delete;++  void Init() { GPR_ASSERT(0 == pthread_key_create(&key_, nullptr)); }++  void Destroy() { GPR_ASSERT(0 == pthread_key_delete(key_)); }++  operator T() const { return Cast<T>(pthread_getspecific(key_)); }++  T operator=(T t) {+    GPR_ASSERT(0 == pthread_setspecific(key_, Cast<T>(t)));+    return t;+  }++ private:+  // TODO(C++17): Replace these helpers with constexpr if statements inline.+  template <typename V>+  static typename std::enable_if<std::is_pointer<V>::value, V>::type Cast(+      void* object) {+    return static_cast<V>(object);+  }++  template <typename V>+  static typename std::enable_if<+      !std::is_pointer<V>::value && std::is_integral<V>::value, V>::type+  Cast(void* object) {+    return reinterpret_cast<uintptr_t>(object);+  }++  template <typename V>+  static void* Cast(+      typename std::enable_if<std::is_pointer<V>::value, V>::type t) {+    return t;+  }++  template <typename V>+  static void* Cast(typename std::enable_if<!std::is_pointer<V>::value &&+                                                std::is_integral<V>::value,+                                            V>::type t) {+    return reinterpret_cast<void*>(uintptr_t(t));+  }++  pthread_key_t key_;+};++// This class is never instantiated. It exists to statically ensure that all+// TLS usage is compatible with the most restrictive implementation, allowing+// developers to write correct code regardless of the platform they develop on.+template <typename T>+class TriviallyDestructibleAsserter {+  // This type is often used as a global; since the type itself is hidden by the+  // macros, enforce compliance with the style guide here rather than at the+  // caller. See+  // https://google.github.io/styleguide/cppguide.html#Static_and_Global_Variables.+  static_assert(std::is_trivially_destructible<T>::value,","No, because the class is not fully defined there.",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,687079488,2021-08-11T18:35:17Z,include/grpcpp/security/tls_credentials_options.h,"@@ -146,6 +146,39 @@ class TlsServerAuthorizationCheckConfig {       server_authorization_check_interface_; }; +/// Configuration for Tls key logging.+struct TlsKeyLoggerConfig {","If we later will add extensions that can include filters such as IP addresses, would it be better to just define a class instead of a struct here?",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,687080914,2021-08-11T18:37:37Z,include/grpcpp/security/tls_credentials_options.h,"@@ -187,6 +220,12 @@ class TlsCredentialsOptions {   //   // @param identity_cert_name the name of identity key-cert pairs being set.   void set_identity_cert_name(const std::string& identity_cert_name);+  // Sets the Tls key logging configuration. If not set, tls key logging is+  // disabled.","Can we please add a comment here saying the feature is used for logging only, and should not be used in a real production environment?I just want to make that super clear, as it will be a significant security problem if people accidentally enable this while in a prod environment.",
7281574,nicolasnoble,https://api.github.com/repos/grpc/grpc/pulls/26923,687164689,2021-08-11T20:25:15Z,src/core/lib/gprpp/capture.h,"@@ -0,0 +1,70 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_GPRPP_CAPTURE_H+#define GRPC_CORE_LIB_GPRPP_CAPTURE_H++#include <grpc/impl/codegen/port_platform.h>++#include <utility>+#include ""absl/utility/utility.h""++namespace grpc_core {++namespace detail {++template <typename F, typename... Captures>+class Capture {+ public:+  explicit Capture(F f, Captures... captures)+      : f_(std::move(f)), captures_(std::move(captures)...) {}++  template <typename... Args>+  decltype(std::declval<F>()(static_cast<Captures*>(nullptr)...,+                             std::declval<Args>()...))+  operator()(Args... args) {+    auto f = &f_;+    return absl::apply(+        [f, &args...](Captures&... captures) {+          return (*f)(&captures..., std::move(args)...);+        },+        captures_);+  }++ private:+  GPR_NO_UNIQUE_ADDRESS F f_;+  GPR_NO_UNIQUE_ADDRESS std::tuple<Captures...> captures_;+};++}  // namespace detail++// C++11 helper - best explained by usage:+//+// BigThing big_thing;+// auto f = Capture(+//             [](BigThing* c, int a, int b) { /*...*/ },","This makes BigThing implicitly mutable within the capture itself. Either we want to disallow this (with forcing to use `const BigThing *`), or we probably need to explicitly document the behavior, which IMHO is a bit dangerous wrt multithreading, maybe?```cBigThing big_thing;auto f = capture(             [](BigThing* c, int a, int b) { c->MutateMe(); },             std::move(big_thing));```",X
7281574,nicolasnoble,https://api.github.com/repos/grpc/grpc/pulls/26923,687173787,2021-08-11T20:37:30Z,test/core/gprpp/capture_test.cc,"@@ -0,0 +1,38 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/lib/gprpp/capture.h""+#include <gtest/gtest.h>++namespace grpc_core {++TEST(CaptureTest, Capture) {+  auto f = Capture([](int* p) { EXPECT_EQ(*p, 42); }, 42);+  f();+}++TEST(CaptureTest, WithArgsAndReturn) {+  int captured = 1;+  auto f =+      Capture([captured](int* p, int arg) { return (captured + *p) * arg; }, 2);","This doesn't compile on Visual Studio with anything less than C++17... (I ran into this before myself)(the tests we have aren't specifying this switch for msvc, which then defaults to ""latest"", and I think this is around C++20 for VS2019) https://godbolt.org/z/GqEv446qz`error C2955: 'Capture': use of class template requires template argument list (x64 msvc v19.latest)`I'm not totally sure if we should care, but maybe that's problematic still.(works with C++17 being selected: https://godbolt.org/z/KrEvjd574)",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/26923,687183201,2021-08-11T20:52:34Z,test/core/gprpp/capture_test.cc,"@@ -0,0 +1,38 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/lib/gprpp/capture.h""+#include <gtest/gtest.h>++namespace grpc_core {++TEST(CaptureTest, Capture) {+  auto f = Capture([](int* p) { EXPECT_EQ(*p, 42); }, 42);+  f();+}++TEST(CaptureTest, WithArgsAndReturn) {+  int captured = 1;+  auto f =+      Capture([captured](int* p, int arg) { return (captured + *p) * arg; }, 2);",https://godbolt.org/z/4x8cGrq45wrapper function was missed on godbolt,
16571023,ananda1066,https://api.github.com/repos/grpc/grpc/pulls/26916,687217491,2021-08-11T21:49:52Z,src/core/lib/promise/detail/promise_like.h,"@@ -47,31 +47,30 @@ namespace grpc_core { namespace promise_detail {",One question - is there a reason this folder is called detail instead of internal? So promise_internal? Might be clearer naming,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26999,688064566,2021-08-12T20:32:38Z,src/core/lib/gpr/tls.h,"@@ -67,13 +61,45 @@ namespace grpc_core { template <typename T> class PthreadTlsImpl : TlsTypeConstrainer<T> {  public:-  PthreadTlsImpl() = default;   PthreadTlsImpl(const PthreadTlsImpl&) = delete;   PthreadTlsImpl& operator=(const PthreadTlsImpl&) = delete; -  void Init() { GPR_ASSERT(0 == pthread_key_create(&key_, nullptr)); }--  void Destroy() { GPR_ASSERT(0 == pthread_key_delete(key_)); }+  // Achtung! This class emulates C++ `thread_local` using pthread keys. Each+  // instance of this class is a stand in for a C++ `thread_local`. Think of+  // each `thread_local` as a *global* pthread_key_t and a type tag. An+  // important consequence of this is that the lifetime of a `pthread_key_t`+  // is precisely the lifetime of an instance of this class. To understand why+  // this is, consider the following scenario given a fictional implementation+  // of this class which creates and destroys its `pthread_key_t` each time+  // a given block of code runs (all actions take place on a single thread):+  //+  // - instance 1 (type tag = T*) is initialized, is assigned `pthread_key_t` 1+  // - instance 2 (type tag = int) is initialized, is assigned `pthread_key_t` 2+  // - instances 1 and 2 store and retrieve values; all is well+  // - instances 1 and 2 are de-initialized; their keys are released to the pool+  //+  // - another run commences+  // - instance 1 receives key 2+  // - a value is read from instance 1, it observes a value of type int, but","I see.Another option to avoid the style guide violation would be to change the global variable from `PthreadTlsImpl<T>` to `PthreadTlsImpl<T>*`.  Then in the places where we were calling `gpr_tls_init()`, we can instead call `new PthreadTlsImpl<T>`, and in the places where we were calling `gpr_tls_destroy()`, we can instead call `delete`.  That way, the `PthreadTlsImpl<T>` itself is not a global variable, only the pointer to it is.",X
1535036,tamird,https://api.github.com/repos/grpc/grpc/pulls/26999,688065620,2021-08-12T20:34:22Z,src/core/lib/gpr/tls.h,"@@ -67,13 +61,45 @@ namespace grpc_core { template <typename T> class PthreadTlsImpl : TlsTypeConstrainer<T> {  public:-  PthreadTlsImpl() = default;   PthreadTlsImpl(const PthreadTlsImpl&) = delete;   PthreadTlsImpl& operator=(const PthreadTlsImpl&) = delete; -  void Init() { GPR_ASSERT(0 == pthread_key_create(&key_, nullptr)); }--  void Destroy() { GPR_ASSERT(0 == pthread_key_delete(key_)); }+  // Achtung! This class emulates C++ `thread_local` using pthread keys. Each+  // instance of this class is a stand in for a C++ `thread_local`. Think of+  // each `thread_local` as a *global* pthread_key_t and a type tag. An+  // important consequence of this is that the lifetime of a `pthread_key_t`+  // is precisely the lifetime of an instance of this class. To understand why+  // this is, consider the following scenario given a fictional implementation+  // of this class which creates and destroys its `pthread_key_t` each time+  // a given block of code runs (all actions take place on a single thread):+  //+  // - instance 1 (type tag = T*) is initialized, is assigned `pthread_key_t` 1+  // - instance 2 (type tag = int) is initialized, is assigned `pthread_key_t` 2+  // - instances 1 and 2 store and retrieve values; all is well+  // - instances 1 and 2 are de-initialized; their keys are released to the pool+  //+  // - another run commences+  // - instance 1 receives key 2+  // - a value is read from instance 1, it observes a value of type int, but","That has the same problem as the situation today. Each instance of this class must exactly model a `thread_local`. In your proposal, the pointer would at different times have different pointees, which might be assigned a different `pthread_key_t` and thus once again lead to undefined behavior.You could solve the problem by never calling `pthread_key_delete`, but that has problems as well.",
14166415,sanjaypujare,https://api.github.com/repos/grpc/grpc/pulls/27001,688158623,2021-08-12T23:59:16Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/traffic_director.py,"@@ -630,8 +630,7 @@ class TrafficDirectorSecureManager(TrafficDirectorManager):     netsec: Optional[_NetworkSecurityV1Alpha1]     SERVER_TLS_POLICY_NAME = ""server-tls-policy""     CLIENT_TLS_POLICY_NAME = ""client-tls-policy""-    # TODO(sergiitk): Rename to ENDPOINT_POLICY_NAME when upgraded to v1beta","So the original comment said ""when upgraded to v1beta"". But I notice the name `NetworkServicesV1Alpha1` is not changed. Should that be changed to `NetworkServicesV1Beta` to be correct? May be not in this PR since we have a deadline",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27002,688622135,2021-08-13T16:01:34Z,src/core/lib/iomgr/event_engine/iomgr.h,"@@ -17,8 +17,16 @@  #include <grpc/event_engine/event_engine.h> -// This can be called anywhere in the EE-based iomgr impl where we need to-// access the global EE instance.+/// This can be called anywhere in the EventEngine-based iomgr impl where we+/// need to access the global EventEngine instance. grpc_event_engine::experimental::EventEngine* grpc_iomgr_event_engine(); +namespace grpc_core {++/// Set the default EventEngine. This engine is shut down along with iomgr.+void SetDefaultEventEngine(+    std::unique_ptr<grpc_event_engine::experimental::EventEngine> event_engine);","For now, unique_ptr<> is fine.I don't yet have a clear picture of what the long-term API will look like for setting the EE instance for a channel or server.  We'll figure that out later.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27002,688623176,2021-08-13T16:03:14Z,src/core/lib/iomgr/event_engine/iomgr.cc,"@@ -44,19 +44,15 @@ using ::grpc_event_engine::experimental::DefaultEventEngineFactory; using ::grpc_event_engine::experimental::EventEngine; using ::grpc_event_engine::experimental::Promise; -// Note: This is a pointer to a shared_ptr, so it's trivially destructible.-std::shared_ptr<EventEngine>* g_event_engine;+EventEngine* g_event_engine = nullptr; -void iomgr_platform_init(void) {-  g_event_engine =-      new std::shared_ptr<EventEngine>(DefaultEventEngineFactory());-}+void iomgr_platform_init(void) { GPR_ASSERT(g_event_engine != nullptr); }","I think this should instantiate a new EE instance if `g_event_engine` is null.  That way, we continue to create our own unless the wrapped language has set one already.If we don't do this, then we're going to require all wrapped languages *except* Python (including C++) to explicitly call `SetDefaultEventEngine()`, which I don't think is what we want.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27002,688649272,2021-08-13T16:47:24Z,src/core/lib/iomgr/event_engine/iomgr.cc,"@@ -44,19 +44,15 @@ using ::grpc_event_engine::experimental::DefaultEventEngineFactory; using ::grpc_event_engine::experimental::EventEngine; using ::grpc_event_engine::experimental::Promise; -// Note: This is a pointer to a shared_ptr, so it's trivially destructible.-std::shared_ptr<EventEngine>* g_event_engine;+EventEngine* g_event_engine = nullptr; -void iomgr_platform_init(void) {-  g_event_engine =-      new std::shared_ptr<EventEngine>(DefaultEventEngineFactory());-}+void iomgr_platform_init(void) { GPR_ASSERT(g_event_engine != nullptr); }","That's a good point. We don't yet have a default version to instantiate, I believe that branch is still operating on the `DefaultEventEngineFactory` concept from a few months back. I think we can change this in the PR that introduces that default EventEngine. Sound ok? If so, I'll leave a TODO here.CC @nicolasnoble Update: I've added the TODO",
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/26969,689312466,2021-08-16T07:52:09Z,src/objective-c/ProtoRPC/ProtoRPC.h,"@@ -97,8 +97,36 @@ NS_ASSUME_NONNULL_BEGIN  @end +/**+ * GRPCProtoCallFlowControllable defines the available interface methods to use when flow control+ * is enabled. Calling these methods when flow control is disabled has no effect.+ */+@protocol GRPCProtoCallFlowControllable <NSObject>",got it. Updated the protocol naming.,
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/26969,689315700,2021-08-16T07:57:11Z,src/objective-c/ProtoRPC/ProtoRPC.m,"@@ -100,25 +106,35 @@ - (instancetype)initWithRequestOptions:(GRPCRequestOptions *)requestOptions  - (void)start {   [_call start];-  [_call receiveNextMessage];","thanks. from what I read (https://github.com/grpc/grpc/blob/master/src/objective-c/GRPCClient/GRPCCall.h#L180-L181)   this method would be no-op if flow control is disabled as message would be received automatically after start.  Our existing unit tests should already be guarding this behavior, e.g. (https://github.com/grpc/grpc/blob/master/src/objective-c/tests/InteropTests/InteropTests.m#L585), as flow control is disabled by default and there is no recieveNextMessage calls needed.   LMK  ",X
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/26969,689337064,2021-08-16T08:29:07Z,src/objective-c/ProtoRPC/ProtoRPC.h,"@@ -97,8 +97,36 @@ NS_ASSUME_NONNULL_BEGIN  @end +/**+ * GRPCProtoCallFlowControllable defines the available interface methods to use when flow control+ * is enabled. Calling these methods when flow control is disabled has no effect.+ */+@protocol GRPCProtoCallFlowControllable <NSObject>++/**","yes, these are APIs for users to call on.  This one wraps two receiveMessage methods in a single protocol so we can share the definition among GRPCUnaryProtoCall and GRPCStreamingProtoCall.  Not sure what you mean by public methods though :  ) Since this protocol is defined in ProtoRPC.h it would already be public visible to callers whoever includes it.   Maybe I missed, or do we have some separate mechanism to mark a method public at the gRPC library level at release steps ?  LMK thanks ! ",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27014,689635854,2021-08-16T15:25:33Z,src/core/lib/config/core_configuration.h,"@@ -0,0 +1,74 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_CONFIG_CORE_CONFIGURATION_H+#define GRPC_CORE_LIB_CONFIG_CORE_CONFIGURATION_H++#include <grpc/impl/codegen/port_platform.h>++#include <atomic>++namespace grpc_core {++// Global singleton that stores library configuration - factories, etc...+// that plugins might choose to extend.+class CoreConfiguration {+ public:+  CoreConfiguration(const CoreConfiguration&) = delete;+  CoreConfiguration& operator=(const CoreConfiguration&) = delete;++  // Builder is passed to plugins, etc... at initialization time to collect+  // their configuration and assemble the published CoreConfiguration.+  class Builder {","It seems like we have two levels of indirection here, one for the `Builder` class and another for the `BuildCoreConfiguration()` function.  What's the purpose of all of this indirection?  Aren't we ultimately going to need all state associated with plugins to be stored in `CoreConfiguration` directly?To say this another way, it seems like in order to add any plugin, we're going to plumb its state through all three places.  Wouldn't it make more sense to just have a single object (`CoreConfiguration`) in which to store that state?",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27014,689677154,2021-08-16T16:18:22Z,src/core/lib/config/core_configuration.h,"@@ -0,0 +1,74 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_CONFIG_CORE_CONFIGURATION_H+#define GRPC_CORE_LIB_CONFIG_CORE_CONFIGURATION_H++#include <grpc/impl/codegen/port_platform.h>++#include <atomic>++namespace grpc_core {++// Global singleton that stores library configuration - factories, etc...+// that plugins might choose to extend.+class CoreConfiguration {+ public:+  CoreConfiguration(const CoreConfiguration&) = delete;+  CoreConfiguration& operator=(const CoreConfiguration&) = delete;++  // Builder is passed to plugins, etc... at initialization time to collect+  // their configuration and assemble the published CoreConfiguration.+  class Builder {","Plugin's won't generally add kinds of state to CoreConfiguration, systems will (like a channel, or a resolver).There's a circular dependence in the whole initialization scheme that we have now, that's preserved here: the initialization system needs to make an upcall to the thing that knows about all the plugins, so that they can make downcalls into the initialization system. That's the purpose of `BuildCoreConfiguration` - it's the global plugin init function of the 2020's. (note: if we switched to dynamic registration of plugins via constructors, we'd still want a function with the role of BuildCoreConfiguration to read through the list of things that should be registered - I think this function's role is fundamental).The question then comes: what interface should be presented to BuildCoreConfiguration, and there are two choices:1. `CoreConfiguration*` - we could have mutable methods on CoreConfiguration to add things to it, and const functions to read from it, and have the convention that we only mutate at initialization time2. `CoreConfiguration::Builder*` - we create a separate object to locate initialization methods on, more strictly defining the phases of usage: there is a build time, and then there is a usage time, and they are expressed via two different interfaces. This also gives us a place to locate configuration finalization, of which we already have some examples: https://github.com/grpc/grpc/blob/master/src/core/lib/surface/channel_init.cc#L77.Since we have use cases for 2, and it more clearly expresses the semantics we want, and it's rare that we add a system that needs a new kind of configuration (so a little extra typing is ok), I went with the `Builder` based API.",X
64815511,yulin-liang,https://api.github.com/repos/grpc/grpc/pulls/26969,689832652,2021-08-16T20:21:40Z,src/objective-c/ProtoRPC/ProtoRPC.h,"@@ -97,8 +97,36 @@ NS_ASSUME_NONNULL_BEGIN  @end +/**+ * GRPCProtoCallFlowControllable defines the available interface methods to use when flow control+ * is enabled. Calling these methods when flow control is disabled has no effect.+ */+@protocol GRPCProtoCallFlowControllable <NSObject>++/**",A little bit weird to wrap APIs in a protocol. Can we just create a base class with these APIs and let `GRPCUnaryProtoCall` and `GRPCStreamingProtoCall` to inhere from this class?,
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/26778,689957420,2021-08-17T01:00:50Z,tools/internal_ci/linux/grpc_e2e_performance_v2.sh,"@@ -77,6 +77,11 @@ buildConfigs() {         -s big_query_table=""${table}"" -s timeout_seconds=900 \         -s prebuilt_image_prefix=""${PREBUILT_IMAGE_PREFIX}"" \         -s prebuilt_image_tag=""${UNIQUE_IDENTIFIER}"" \+        -a buildNumber=""${KOKORO_BUILD_NUMBER}"" \","I would prefix all these annotations with a single prefix, for instance ""ci_"", to indicate that they belong together logically.",
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/26778,689971453,2021-08-17T01:44:09Z,tools/run_tests/performance/bq_upload_result.py,"@@ -157,6 +169,40 @@ def _populate_metadata_inplace(scenario_result):     scenario_result['metadata'] = metadata  +def _populate_metadata_from_file(scenario_result, test_metadata_file):+    with open(test_metadata_file, 'r') as f:+        test_metadata = json.loads(f.read())++    """"""eliminate managedField from metadata set""""""+    if 'managedFields' in test_metadata:+        del test_metadata['managedFields']+    annotations = test_metadata['annotations']++    """"""if use kubectl apply ..., kubectl will append current configuration to+     annotation, the field is deleted since it includes a lot of irrelevant +     information""""""+    if 'kubectl.kubernetes.io/last-applied-configuration' in annotations:+        del annotations['kubectl.kubernetes.io/last-applied-configuration']++    utc_timestamp = str(calendar.timegm(time.gmtime()))+    metadata = {'created': utc_timestamp}++    if 'buildNumber' in metadata:","Here I would define (possibly somewhere else) a dictionary to do these transformations.You could use some logic if you don't want to hardcode it:```python_annotation_to_metadata_key_map =_{  'ci_' + key: key  for key in (    'buildNumber',    'buildUrl',    'jobName',    'gitCommit' ,    'gitActualCommit',  )}```You would then have something like```pythonfor  key, value in _annotation_to_bq_metadata_key_map.items():  if key in annotations:    metadata[value] = annotations[key]```",X
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/26969,690077325,2021-08-17T06:38:59Z,src/objective-c/ProtoRPC/ProtoRPC.h,"@@ -97,8 +97,36 @@ NS_ASSUME_NONNULL_BEGIN  @end +/**+ * GRPCProtoCallFlowControllable defines the available interface methods to use when flow control+ * is enabled. Calling these methods when flow control is disabled has no effect.+ */+@protocol GRPCProtoCallFlowControllable <NSObject>++/**","I see, gotcha. Tried a little bit w/ the base class approach and there are some extra handling there to make sure init/new is not available publicly.   I think to keep it simple and not touching this interface too much, I got rid of the protocol and added these two methods directly to  GRPCUnaryProtoCall. Let me know if this works for u ;  )  Given that we already have similar methods on both interfaces (start / cancel / finish) this might be a good refactoring at some later point.   LMK, thanks !",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,690553278,2021-08-17T16:54:06Z,include/grpcpp/security/tls_credentials_options.h,"@@ -146,6 +146,39 @@ class TlsServerAuthorizationCheckConfig {       server_authorization_check_interface_; }; +/// Configuration for Tls key logging.+struct TlsKeyLoggerConfig {+  // The full path at which the TLS keys would be exported.+  std::string tls_key_log_file_path;++  // The format in which the TLS keys would be exported at this path.+  grpc_tls_key_log_format tls_key_log_format;++  // Future extensions can include filters such as IP addresses etc.++  // Constructor+  explicit TlsKeyLoggerConfig()+      : tls_key_log_file_path(""""), tls_key_log_format(TLS_KEY_LOG_FORMAT_NSS){};++  // Copy ctor+  TlsKeyLoggerConfig(const TlsKeyLoggerConfig& copy) {+    tls_key_log_file_path = copy.tls_key_log_file_path;+    tls_key_log_format = copy.tls_key_log_format;+  };++  // Sets the tls key log file path.+  void set_tls_key_log_file_path(std::string key_log_file_path) {+    tls_key_log_file_path = std::move(key_log_file_path);+  }++  // Sets the tls key logging format. Currently only NSS format is supported.+  void set_tls_key_log_format(grpc_tls_key_log_format tls_key_log_format) {+    tls_key_log_format = tls_key_log_format;+  }+};++typedef struct TlsKeyLoggerConfig TlsKeyLoggerConfig;","Do we expect the API the also exposed to the other wrap languages in the future? If so, we might also need to declare similar APIs to grpc_security.h, as ssl_session_cache already did(https://github.com/grpc/grpc/blob/master/include/grpc/grpc_security.h#L108).Normally, when we design a core API, we declare the actual class, like `TlsKeyLoggerConfig`,  in the core layer(so that it can be benefit all wrap languages), and define interfaces in `grpc_security.h`. And in the wrap languages like C++, we just create wrapper class for the core interfaces. This seems the opposite: we define the actual class directly at C++, and invoke the actual functions down to the core. It's probably fine if we only want to expose it to C++; but I think this is a general feature that could benefit all wrap languages in the future.",X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,690630330,2021-08-17T18:43:08Z,src/core/lib/security/credentials/tls/tls_credentials.cc,"@@ -83,10 +101,17 @@ TlsCredentials::create_security_connector(           static_cast<tsi_ssl_session_cache*>(arg->value.pointer.p);     }   }++  grpc_core::RefCountedPtr<tsi::TlsKeyLoggerContainer> tls_key_logger(nullptr);+  if (TlsKeyLoggingEnabled(options_.get())) {+    tls_key_logger = tsi::TlsKeyLoggerRegistry::CreateTlsKeyLoggerContainer(+        options_->tls_key_logger_config());+  }+   grpc_core::RefCountedPtr<grpc_channel_security_connector> sc =       grpc_core::TlsChannelSecurityConnector::CreateTlsChannelSecurityConnector(           this->Ref(), options_, std::move(call_creds), target_name,-          overridden_target_name, ssl_session_cache);+          overridden_target_name, ssl_session_cache, tls_key_logger);","Instead of adding another parameter here, can we put `tls_key_logger` as an additional configuration field in `options_`? ",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,690659385,2021-08-17T19:26:29Z,src/core/tsi/ssl/key_logging/ssl_key_logging.h,"@@ -0,0 +1,149 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_TSI_SSL_KEY_LOGGING_SSL_KEY_LOGGING_H+#define GRPC_CORE_TSI_SSL_KEY_LOGGING_SSL_KEY_LOGGING_H++#include <grpc/support/port_platform.h>++#include <iostream>++#include <grpc/grpc_security.h>+#include <grpc/slice.h>+#include <grpc/support/sync.h>++extern ""C"" {+#include <openssl/ssl.h>+}++#include ""src/core/lib/gprpp/memory.h""+#include ""src/core/lib/gprpp/ref_counted.h""+#include ""src/core/lib/gprpp/sync.h""++/// Instance to facilitate loging of SSL/TLS session keys to aid debugging.+///+/// Keys logged by an instance of this class help decrypting packet captures+/// with tools like wireshark.+///+/// This class is thread safe and serializes access to keylog files.++namespace tsi {++/// Configuration for key logging.+struct tsi_tls_key_logger_config {+  /// The full path at which the TLS keys would be exported.+  std::string tls_key_log_file_path;+  /// The format in which the TLS keys would be exported.+  grpc_tls_key_log_format key_logging_format;++  /// Future extensions can include filters such as IP addresses etc.++  /// Constructor.+  explicit tsi_tls_key_logger_config()+      : tls_key_log_file_path(""""),+        key_logging_format(grpc_tls_key_log_format::TLS_KEY_LOG_FORMAT_NONE){};++  /// Copy ctor.+  tsi_tls_key_logger_config(const tsi_tls_key_logger_config& copy) {+    tls_key_log_file_path = copy.tls_key_log_file_path;+    key_logging_format = copy.key_logging_format;+  }+};++/// A class which facilitates logging Tls keys into a file. The instance is+/// bound to a file.+class TlsKeyLogger : public grpc_core::RefCounted<TlsKeyLogger> {+ public:+  /// Instantiates a TlsKeyLogger instance bound to a specific path.+  explicit TlsKeyLogger(const std::string& tls_key_log_file_path);+  ~TlsKeyLogger() override;++  /// Not copyable nor assignable.+  TlsKeyLogger(const TlsKeyLogger&) = delete;+  TlsKeyLogger& operator=(const TlsKeyLogger&) = delete;++  /// Writes session keys into the file in the key logging format specified.+  /// The passed string may be modified and logged based on the specified+  /// format.+  /// This is called upon completion of a handshake. The associated ssl_context+  /// is also provided here to support future extensions such as logging+  /// keys only when connections are made by certain IPs etc.+  void LogSessionKeys(SSL_CTX* ssl_context,+                      const tsi_tls_key_logger_config& tls_key_log_config,+                      const std::string& session_keys_info);++ private:+  FILE* fd_;+  grpc_core::Mutex lock_;+  std::string tls_key_log_file_path_;+};++/// A Wrapper class which enables key logging to the a file based on specified+/// configuration. A unique KeyLogger Container is bound to each Tls+/// security connector.+class TlsKeyLoggerContainer",I am a bit confused on why we need a `TlsKeyLoggerContainer` in addition to `TlsKeyLogger`...can we make them into one class?,
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/26913,690661961,2021-08-17T19:30:28Z,src/core/lib/promise/loop.h,"@@ -0,0 +1,95 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_PROMISE_LOOP_H+#define GRPC_CORE_LIB_PROMISE_LOOP_H++#include <grpc/impl/codegen/port_platform.h>++#include ""absl/types/variant.h""+#include ""src/core/lib/promise/detail/promise_factory.h""++namespace grpc_core {++// Special type - signals to loop to take another iteration, instead of+// finishing+struct Continue {};","nit, optional: `::grpc_core::Continue` seems like a very general name to put in such a broadly-used namespace. Consider either naming this something specific to Promises, or having it in a different namespace to avoid ambiguity. caveat: general c++ recommendation is for shallow namespace nesting.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/26913,690869194,2021-08-18T03:17:08Z,src/core/lib/promise/loop.h,"@@ -0,0 +1,95 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_PROMISE_LOOP_H+#define GRPC_CORE_LIB_PROMISE_LOOP_H++#include <grpc/impl/codegen/port_platform.h>++#include ""absl/types/variant.h""+#include ""src/core/lib/promise/detail/promise_factory.h""++namespace grpc_core {++// Special type - signals to loop to take another iteration, instead of+// finishing+struct Continue {};","Ack on it being general, but I think that's a positive here: it's mirroring a language feature that's spelled similarly, in a way that few other types would be.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/26913,690872335,2021-08-18T03:27:21Z,src/core/lib/promise/loop.h,"@@ -0,0 +1,95 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_PROMISE_LOOP_H+#define GRPC_CORE_LIB_PROMISE_LOOP_H++#include <grpc/impl/codegen/port_platform.h>++#include ""absl/types/variant.h""+#include ""src/core/lib/promise/detail/promise_factory.h""++namespace grpc_core {++// Special type - signals to loop to take another iteration, instead of+// finishing+struct Continue {};","Ah, I see. If `::grpc_core::Continue` becomes something we'd reach for in this codebase as much as we'd otherwise reach for `continue`, the namespace/generality fits pretty well.",
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27052,691762539,2021-08-19T04:03:28Z,src/core/ext/transport/binder/transport/binder_transport.cc,"@@ -207,137 +437,79 @@ static void perform_stream_op(grpc_transport* gt, grpc_stream* gs,   }   if (op->recv_initial_metadata) {     gpr_log(GPR_INFO, ""recv_initial_metadata"");-    if (!gbs->cancellation_error.ok()) {-      grpc_core::ExecCtx::Run(-          DEBUG_LOCATION,-          op->payload->recv_initial_metadata.recv_initial_metadata_ready,-          absl_status_to_grpc_error(gbs->cancellation_error));-    } else {-      gbs->recv_initial_metadata_ready =-          op->payload->recv_initial_metadata.recv_initial_metadata_ready;-      gbs->recv_initial_metadata =-          op->payload->recv_initial_metadata.recv_initial_metadata;-      gbt->transport_stream_receiver->RegisterRecvInitialMetadata(-          gbs->tx_code,-          [gbs](absl::StatusOr<grpc_binder::Metadata> initial_metadata) {-            grpc_core::ExecCtx exec_ctx;-            GPR_ASSERT(gbs->recv_initial_metadata);-            GPR_ASSERT(gbs->recv_initial_metadata_ready);-            if (!initial_metadata.ok()) {-              gpr_log(GPR_ERROR, ""Failed to parse initial metadata"");-              grpc_core::ExecCtx::Run(-                  DEBUG_LOCATION, gbs->recv_initial_metadata_ready,-                  absl_status_to_grpc_error(initial_metadata.status()));-              return;-            }-            AssignMetadata(gbs->recv_initial_metadata, gbs->arena,-                           *initial_metadata);-            grpc_core::ExecCtx::Run(DEBUG_LOCATION,-                                    gbs->recv_initial_metadata_ready,-                                    GRPC_ERROR_NONE);-          });-    }+    gbs->recv_initial_metadata_ready =+        op->payload->recv_initial_metadata.recv_initial_metadata_ready;+    gbs->recv_initial_metadata =+        op->payload->recv_initial_metadata.recv_initial_metadata;+    gbs->trailing_metadata_available =+        op->payload->recv_initial_metadata.trailing_metadata_available;+    GRPC_BINDER_STREAM_REF(gbs, ""recv_initial_metadata"");+    gbt->transport_stream_receiver->RegisterRecvInitialMetadata(+        tx_code, [tx_code, gbs,+                  gbt](absl::StatusOr<grpc_binder::Metadata> initial_metadata) {+          grpc_core::ExecCtx exec_ctx;","Hi Craig, we're dealing with combiner and execution context in this PR, and would like your opinion on the following issue here:The `RegisterRecvInitialMetadata` function here would register a callback which will be called once a binder transaction comes. The binder transaction will be coming from a separate thread and therefore this callback will be invoked in a non-gRPC thread as well. When this callback is called, we would like to move the received metadata to `op->payload` and call the corresponding `recv_initial_metadata_ready` callback. As a result, we would need to instantiate an execution context here, because otherwise the call to `ExecCtx::Run()` would fail.We noticed that the HTTP/2 transport does not instantiate any execution context on its own, and also having an execution context in a non-gRPC thread makes the behavior of our transport inconsistent. An example of such inconsistency can be seen from `client_fuzzer.cc`, where we have to join the thread before the assertion that the cancel operation completes, otherwise `grpc_completion_queue_next` might return a timeout event. I'm not 100% sure about whether having an execution context in the transport is the root cause of the assertion failure though, but judging from the log it seems like every time the assertion fails, another thread is pushing a callback to the combiner at the same time.We've tried removing the execution context by implementing something like `read_action_locked` in HTTP/2 transport, but couldn't make it work because we cannot `poll` the incoming binder transactions like HTTP/2 does. What's your opinion on the issue? Is it OK to instantiate execution context in the transport like we did? Or do you have any suggestions on how we might be able to do something similar to polling and have `ready` callbacks be invoked in the main thread?There are certain assumptions or statements above that I'm not fully confident about, so please kindly correct me if I'm missing something here.Thanks!",X
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/27030,692458537,2021-08-19T20:22:55Z,doc/interop-test-descriptions.md,"@@ -1011,16 +1011,49 @@ languages. Therefore they are not part of our interop matrix. #### rpc_soak  The client performs many large_unary RPCs in sequence over the same channel.-The number of RPCs is configured by the experimental flag, `soak_iterations`.++This test is configurable via a few different command line flags:++* `soak_iterations`: controls the number of RPCs to perform.++* `soak_max_failures`: an inclusive upper limit on the number of RPC failures+  that should be tolerated (i.e. after which the test process should+  still exit 0). A failure is considered to be either a non-OK status or an RPC+  whose latency exceeds `soak_per_iteration_max_acceptable_latency_ms`.++* `soak_per_iteration_max_acceptable_latency_ms`: an inclusive upper limit+  on the latency of a single RPC in order for that RPC to be considered+  successful.++* `soak_overall_timeout_seconds`: the overall number of seconds after which+  the test should stop and fail if `soak_iterations` have not yet been+  completed.++The following suggestions are optional but encouraged to improve debuggability:++* Implementations should avoid setting RPC deadlines and should instead",I don't understand this. Why shouldn't an implementation set the deadline based on soak_per_iteration_max_acceptable_latency_ms?,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/27030,692498399,2021-08-19T21:26:26Z,doc/interop-test-descriptions.md,"@@ -1011,16 +1011,49 @@ languages. Therefore they are not part of our interop matrix. #### rpc_soak  The client performs many large_unary RPCs in sequence over the same channel.-The number of RPCs is configured by the experimental flag, `soak_iterations`.++This test is configurable via a few different command line flags:++* `soak_iterations`: controls the number of RPCs to perform.++* `soak_max_failures`: an inclusive upper limit on the number of RPC failures+  that should be tolerated (i.e. after which the test process should+  still exit 0). A failure is considered to be either a non-OK status or an RPC+  whose latency exceeds `soak_per_iteration_max_acceptable_latency_ms`.++* `soak_per_iteration_max_acceptable_latency_ms`: an inclusive upper limit+  on the latency of a single RPC in order for that RPC to be considered+  successful.++* `soak_overall_timeout_seconds`: the overall number of seconds after which+  the test should stop and fail if `soak_iterations` have not yet been+  completed.++The following suggestions are optional but encouraged to improve debuggability:++* Implementations should avoid setting RPC deadlines and should instead+  wait for each RPC to complete.++* Implementations should log the number of milliseconds that each RPC takes.","We have been running this test for C++ continuously, using up to 8K iterations. The log output can indeed be fairly big, but it hasn't really been a problem so far (e.g. test infra is able to handle the size of the resulting logs). The test's summary log being at the very bottom also helps to skip over these quickly at a glance.What do you think about another flag to enable or disable these logs?Tests with extremely large numbers of iterations could that way disable it.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/27030,692498523,2021-08-19T21:26:39Z,doc/interop-test-descriptions.md,"@@ -1011,16 +1011,49 @@ languages. Therefore they are not part of our interop matrix. #### rpc_soak  The client performs many large_unary RPCs in sequence over the same channel.-The number of RPCs is configured by the experimental flag, `soak_iterations`.++This test is configurable via a few different command line flags:++* `soak_iterations`: controls the number of RPCs to perform.++* `soak_max_failures`: an inclusive upper limit on the number of RPC failures+  that should be tolerated (i.e. after which the test process should+  still exit 0). A failure is considered to be either a non-OK status or an RPC+  whose latency exceeds `soak_per_iteration_max_acceptable_latency_ms`.++* `soak_per_iteration_max_acceptable_latency_ms`: an inclusive upper limit+  on the latency of a single RPC in order for that RPC to be considered+  successful.++* `soak_overall_timeout_seconds`: the overall number of seconds after which+  the test should stop and fail if `soak_iterations` have not yet been+  completed.++The following suggestions are optional but encouraged to improve debuggability:++* Implementations should avoid setting RPC deadlines and should instead","This just provides more data for debugging in case of failure.E.g., if we set the deadline and then exceeded, it's not clear if the RPC would have taken a few more milliseconds or another minute.",
5564114,Vignesh2208,https://api.github.com/repos/grpc/grpc/pulls/26812,692610058,2021-08-20T02:17:35Z,src/core/tsi/ssl/key_logging/ssl_key_logging.h,"@@ -0,0 +1,149 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_TSI_SSL_KEY_LOGGING_SSL_KEY_LOGGING_H+#define GRPC_CORE_TSI_SSL_KEY_LOGGING_SSL_KEY_LOGGING_H++#include <grpc/support/port_platform.h>++#include <iostream>++#include <grpc/grpc_security.h>+#include <grpc/slice.h>+#include <grpc/support/sync.h>++extern ""C"" {+#include <openssl/ssl.h>+}++#include ""src/core/lib/gprpp/memory.h""+#include ""src/core/lib/gprpp/ref_counted.h""+#include ""src/core/lib/gprpp/sync.h""++/// Instance to facilitate loging of SSL/TLS session keys to aid debugging.+///+/// Keys logged by an instance of this class help decrypting packet captures+/// with tools like wireshark.+///+/// This class is thread safe and serializes access to keylog files.++namespace tsi {++/// Configuration for key logging.+struct tsi_tls_key_logger_config {+  /// The full path at which the TLS keys would be exported.+  std::string tls_key_log_file_path;+  /// The format in which the TLS keys would be exported.+  grpc_tls_key_log_format key_logging_format;++  /// Future extensions can include filters such as IP addresses etc.++  /// Constructor.+  explicit tsi_tls_key_logger_config()+      : tls_key_log_file_path(""""),+        key_logging_format(grpc_tls_key_log_format::TLS_KEY_LOG_FORMAT_NONE){};++  /// Copy ctor.+  tsi_tls_key_logger_config(const tsi_tls_key_logger_config& copy) {+    tls_key_log_file_path = copy.tls_key_log_file_path;+    key_logging_format = copy.key_logging_format;+  }+};++/// A class which facilitates logging Tls keys into a file. The instance is+/// bound to a file.+class TlsKeyLogger : public grpc_core::RefCounted<TlsKeyLogger> {+ public:+  /// Instantiates a TlsKeyLogger instance bound to a specific path.+  explicit TlsKeyLogger(const std::string& tls_key_log_file_path);+  ~TlsKeyLogger() override;++  /// Not copyable nor assignable.+  TlsKeyLogger(const TlsKeyLogger&) = delete;+  TlsKeyLogger& operator=(const TlsKeyLogger&) = delete;++  /// Writes session keys into the file in the key logging format specified.+  /// The passed string may be modified and logged based on the specified+  /// format.+  /// This is called upon completion of a handshake. The associated ssl_context+  /// is also provided here to support future extensions such as logging+  /// keys only when connections are made by certain IPs etc.+  void LogSessionKeys(SSL_CTX* ssl_context,+                      const tsi_tls_key_logger_config& tls_key_log_config,+                      const std::string& session_keys_info);++ private:+  FILE* fd_;+  grpc_core::Mutex lock_;+  std::string tls_key_log_file_path_;+};++/// A Wrapper class which enables key logging to the a file based on specified+/// configuration. A unique KeyLogger Container is bound to each Tls+/// security connector.+class TlsKeyLoggerContainer",I have renamed the classes here for more clarity. There are two classes: TlsKeyLogFileWriter and TlsKeyLogger. TlsKeyLogger is bound to a tls key log configuration i.e on object of TlsKeyLogger exists for each specified configuration. On the other hand one instance of TlsKeyLogFileWriter exists for each unique log file. So a two TlsKeyLoggers may share the same TlsKeyLogFileWriter instance if their key log configuration specifies the same log file. Having two separate classes like this allows the key log configuration (with addition of new entries) to grow over time while the TlsKeyLogFileWriter class can remain unchanged.,X
5564114,Vignesh2208,https://api.github.com/repos/grpc/grpc/pulls/26812,692611947,2021-08-20T02:23:28Z,include/grpcpp/security/tls_credentials_options.h,"@@ -146,6 +146,39 @@ class TlsServerAuthorizationCheckConfig {       server_authorization_check_interface_; }; +/// Configuration for Tls key logging.+struct TlsKeyLoggerConfig {",Its now defined as a class.,
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/26778,693250224,2021-08-20T22:38:11Z,tools/run_tests/performance/bq_upload_result.py,"@@ -157,6 +173,42 @@ def _populate_metadata_inplace(scenario_result):     scenario_result['metadata'] = metadata  +def _populate_metadata_from_file(scenario_result, test_metadata_file):+    with open(test_metadata_file, 'r') as f:","There is an assumption here that the file always exists. Should there be tests, for instance, only try to read the file if test_metadata_file is not empty? I suppose we can require that the file is readable if the variable is set.",
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/26778,693250972,2021-08-20T22:40:59Z,tools/run_tests/performance/bq_upload_result.py,"@@ -157,6 +173,42 @@ def _populate_metadata_inplace(scenario_result):     scenario_result['metadata'] = metadata  +def _populate_metadata_from_file(scenario_result, test_metadata_file):+    with open(test_metadata_file, 'r') as f:+        test_metadata = json.loads(f.read())++    # eliminate managedFields from metadata set+    if 'managedFields' in test_metadata:+        del test_metadata['managedFields']++    # if use kubectl apply ..., kubectl will append current configuration to+    # annotation, the field is deleted since it includes a lot of irrelevant+    # information+    annotations = test_metadata['annotations']","I would not assume that test_metadata contains annotations, even if we expect that to be true. I would at least do `annotations = test_metadata.get('annotations', {})`",
67486458,wanlin31,https://api.github.com/repos/grpc/grpc/pulls/26778,694158936,2021-08-23T17:12:15Z,tools/run_tests/performance/bq_upload_result.py,"@@ -157,6 +173,42 @@ def _populate_metadata_inplace(scenario_result):     scenario_result['metadata'] = metadata  +def _populate_metadata_from_file(scenario_result, test_metadata_file):+    with open(test_metadata_file, 'r') as f:","Yes, I will update this part, I was assuming the check was done in run.sh, as we decide to upload the data with or without metadata I will add a check here.",
4529021,morganchen12,https://api.github.com/repos/grpc/grpc/pulls/26630,694190160,2021-08-23T17:57:51Z,gRPC-C++.podspec,"@@ -1486,6 +1486,7 @@ Pod::Spec.new do |s|   end    s.prepare_command = <<-END_OF_COMMAND+    if [[ ""$(uname)"" == ""Darwin"" ]]; then","Firebase uses Cloud's [repository gardener](https://github.com/GoogleCloudPlatform/repository-gardener), which runs jobs on Linux kokoro to update dependencies on open-source repositories. The [script](https://github.com/GoogleCloudPlatform/repository-gardener/blob/master/use-latest-deps-ios.sh) we run uses CocoaPods to actually generate the dependency updates via `pod update`, which invokes `pod install`, which currently fails for gRPC-C++ due to differences in Linux and macOS `sed` and `base64`.This PR skips the script portion of gRPC-CC++'s `pod install` on Linux systems to allow the installation to succeed.",X
963826,stanhu,https://api.github.com/repos/grpc/grpc/pulls/25863,694565271,2021-08-24T07:13:02Z,Makefile,"@@ -36,10 +36,11 @@ endif  # Basic machine detection HOST_MACHINE = $(shell uname -m)-ifeq ($(HOST_MACHINE),x86_64)-HOST_IS_X86_64 = true+MACHINE ?= $(HOST_MACHINE)","> It would be good if we could get away without needing extra per-arch compilation flags altogether.>> > Absl says ""ABSL_RANDOM_RANDEN_COPTS blaze copts flags which are required by each environment to build an accelerated RandenHwAes library"" - but do we really need the said `RandenHwAes` library? How do we use it?> https://github.com/abseil/abseil-cpp/blob/b315753c0b8b4aa4e3e1479375eddb518393bab6/absl/copts/configure_copts.bzl#L43I agree! See the issues with introducing the SSE4.1 requirement in https://github.com/grpc/grpc/issues/27095. I think the compiler uses runtime dispatch for `-maes` (https://github.com/abseil/abseil-cpp/issues/643#issuecomment-602643378), but I don't believe that's the case for `-msse4`.Can we just drop `-msse4` to start?",X
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27067,694619974,2021-08-24T08:21:48Z,examples/android/binder/java/io/grpc/binder/cpp/exampleserver/ExportedEndpointService.java,"@@ -0,0 +1,28 @@+package io.grpc.binder.cpp.exampleserver;++import android.app.Service;+import android.os.IBinder;+import android.content.Intent;++/** Exposes gRPC services running in the main process */+public final class ExportedEndpointService extends Service {+  private final IBinder binder;++  static {+    System.loadLibrary(""app"");+  }++  public ExportedEndpointService() {+    init_grpc_server();+    binder = get_endpoint_binder();+  }++  @Override+  public IBinder onBind(Intent intent) {+    return binder;+  }++  public native void init_grpc_server();++  public native IBinder get_endpoint_binder();","Will this helper class be part of the library or the users have to implement this themselves? If it is part of the library, should we put it to the `src/` folder instead?",
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27067,694636637,2021-08-24T08:33:30Z,examples/android/binder/java/io/grpc/binder/cpp/example/native.cc,"@@ -15,6 +15,8 @@ #include <android/log.h> #include <jni.h> #include ""src/core/ext/transport/binder/client/channel_create.h""+#include ""examples/protos/helloworld.pb.h""",Sort include headers. Maybe we can add this folder to the targets list of `clang_format_all_the_things.sh`?,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/26921,695260680,2021-08-24T22:30:20Z,src/core/lib/promise/activity.cc,"@@ -0,0 +1,112 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/impl/codegen/port_platform.h>++#include ""src/core/lib/gprpp/atomic_utils.h""+#include ""src/core/lib/promise/activity.h""++namespace grpc_core {++///////////////////////////////////////////////////////////////////////////////+// GLOBALS++thread_local Activity* Activity::g_current_activity_ = nullptr;+Waker::Unwakeable Waker::unwakeable_;++///////////////////////////////////////////////////////////////////////////////+// HELPER TYPES++// Weak handle to an Activity.+// Handle can persist while Activity goes away.+class Activity::Handle final : public Wakeable {+ public:+  explicit Handle(Activity* activity) : activity_(activity) {}++  // Ref the Handle (not the activity).+  void Ref() { refs_.fetch_add(1, std::memory_order_relaxed); }","Final edit... so Wakeable has a scope over which it's referable, and two ways of dropping that ref - both Drop and Wake imply an Unref, but the type should not be generally copied around. When I was writing this code it seemed sufficiently hard to model that and not have a bespoke refcounting system that I needed to choose one or the other... getting the semantics here is important, as a copied Wakeable is likely not what you'd ever want.",
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27067,695274925,2021-08-24T23:05:46Z,examples/android/binder/java/io/grpc/binder/cpp/exampleserver/ExportedEndpointService.java,"@@ -0,0 +1,28 @@+package io.grpc.binder.cpp.exampleserver;++import android.app.Service;+import android.os.IBinder;+import android.content.Intent;++/** Exposes gRPC services running in the main process */+public final class ExportedEndpointService extends Service {+  private final IBinder binder;++  static {+    System.loadLibrary(""app"");+  }++  public ExportedEndpointService() {+    init_grpc_server();+    binder = get_endpoint_binder();+  }++  @Override+  public IBinder onBind(Intent intent) {+    return binder;+  }++  public native void init_grpc_server();++  public native IBinder get_endpoint_binder();","Yes you are right. I think eventually we want to do that, however at this moment I'm afraid that it will cause unnecessary confusion.There is an ""one artifact rule"" that says one APK should only have single `.so` file containing native code. If we create a Java helper class under `src/` folder, then user will be responsible to make sure their `.so` file contains required symbols (i.e they need to manually depends on one of our `grpc_cc_library` in their `.so` target) and they correctly load the shared library before using our Java helper class.For now I just let user to do all these work so it is clear that they need to handle these things by themselves",X
27047833,ybbbby,https://api.github.com/repos/grpc/grpc/pulls/27113,695392663,2021-08-25T04:57:57Z,tools/run_tests/performance/bq_upload_result.py,"@@ -213,6 +215,30 @@ def _populate_metadata_from_file(scenario_result, test_metadata_file):     scenario_result['metadata'] = metadata  +def _populate_node_metadata_from_file(scenario_result, node_info_file):+    node_metadata = {'driver': {}, 'servers': [], 'clients': []}+    _node_info_to_bq_node_metadata_key_map = {+        'Name': 'name',+        'PodIP': 'podIP',+        'NodeName': 'nodeName',+    }+    with open(node_info_file, 'r') as f:","Hey, thanks for commenting! I assume using ""with open():"" would let it jump out of the flow(execute line 239) if an exception occurs. ",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26002,695469459,2021-08-25T07:21:04Z,tools/line_count/yaml2csv.py,"@@ -1,44 +0,0 @@-#!/usr/bin/env python",note that the line_count kokoro job is still being run continously on master:https://source.cloud.google.com/results/invocations/6c344a8c-70fa-41cc-b433-70226c33ea99/loghttps://fusion.corp.google.com/projectanalysis/summary/KOKORO/prod:grpc%2Fcore%2Fmaster%2Flinux%2Fgrpc_line_countlet's doublecheck that you aren't deleting any scripts that are required for the continuous run:https://github.com/grpc/grpc/blob/5cdaec9a4fb9644f4423b5ee180c52747c14d7d2/tools/internal_ci/linux/grpc_line_count.shlooks like e.g. collect-now.sh is still needed.,X
67486458,wanlin31,https://api.github.com/repos/grpc/grpc/pulls/27113,695935254,2021-08-25T16:50:22Z,tools/run_tests/performance/bq_upload_result.py,"@@ -213,6 +215,30 @@ def _populate_metadata_from_file(scenario_result, test_metadata_file):     scenario_result['metadata'] = metadata  +def _populate_node_metadata_from_file(scenario_result, node_info_file):+    node_metadata = {'driver': {}, 'servers': [], 'clients': []}+    _node_info_to_bq_node_metadata_key_map = {+        'Name': 'name',+        'PodIP': 'podIP',+        'NodeName': 'nodeName',+    }+    with open(node_info_file, 'r') as f:","I might understand this wrong, but I think the with open() helps you close the file no matter what, but if there is an exception (i.e. file non-existing), it will raise the exception. https://colab.sandbox.google.com/drive/1LdHGnqRIeHO5Qu1qh2WiXBPMPW8d4BBQ?resourcekey=0-MuDDSsaWE-zFIpN-wPEdIg",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27134,696144754,2021-08-25T22:03:09Z,tools/run_tests/xds_k8s_test_driver/tests/affinity_test.py,"@@ -0,0 +1,154 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging+import time+from typing import List, Optional++from absl import flags+from absl.testing import absltest+from google.protobuf import json_format++from framework import xds_k8s_testcase+from framework import xds_url_map_testcase+from framework.helpers import retryers+from framework.infrastructure import k8s+from framework.rpc import grpc_channelz+from framework.test_app import server_app++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient+_ChannelzChannelState = grpc_channelz.ChannelState++# Testing metadata consts+_TEST_AFFINITY_METADATA_KEY = 'xds_md'+_TD_PROPAGATE_CHECK_INTERVAL_SEC = 10+++class AffinityTest(xds_k8s_testcase.RegularXdsKubernetesTestCase):++    def test_affinity(self) -> None:+        REPLICA_COUNT = 3++        with self.subTest('00_create_health_check'):+            self.td.create_health_check()++        with self.subTest('01_create_backend_services'):+            self.td.create_backend_service(+                affinity_header=_TEST_AFFINITY_METADATA_KEY)++        with self.subTest('02_create_url_map'):+            self.td.create_url_map(self.server_xds_host, self.server_xds_port)++        with self.subTest('03_create_target_proxy'):+            self.td.create_target_proxy()++        with self.subTest('04_create_forwarding_rule'):+            self.td.create_forwarding_rule(self.server_xds_port)++        with self.subTest('05_start_test_servers'):+            self.test_servers: List[_XdsTestServer] = self.startTestServers(+                replica_count=REPLICA_COUNT)++        with self.subTest('06_add_server_backends_to_backend_services'):+            self.setupServerBackends()++        with self.subTest('07_start_test_client'):+            self.test_client: _XdsTestClient = self.startTestClient(+                self.test_servers[0],+                rpc='EmptyCall',+                metadata='EmptyCall:%s:123' % _TEST_AFFINITY_METADATA_KEY)+            # Validate the number of received endpoints and affinity configs.+            config = self.test_client.csds.fetch_client_status(+                log_level=logging.INFO)+            self.assertIsNotNone(config)+            json_config = json_format.MessageToDict(config)+            parsed = xds_url_map_testcase.DumpedXdsConfig(json_config)+            logging.info('Client received CSDS response: %s', parsed)+            self.assertLen(parsed.endpoints, REPLICA_COUNT)+            self.assertEqual(+                parsed.rds['virtualHosts'][0]['routes'][0]['route']+                ['hashPolicy'][0]['header']['headerName'],+                _TEST_AFFINITY_METADATA_KEY)+            self.assertEqual(parsed.cds[0]['lbPolicy'], 'RING_HASH')++        with self.subTest('08_test_client_xds_config_exists'):+            self.assertXdsConfigExists(self.test_client)++        with self.subTest('09_test_server_received_rpcs_from_test_client'):+            self.assertSuccessfulRpcs(self.test_client)++        with self.subTest('10_first_100_affinity_rpcs_pick_same_backend'):+            num_rpcs = 100+            rpc_stats = self.getClientRpcStats(self.test_client, num_rpcs)+            json_lb_stats = json_format.MessageToDict(rpc_stats)+            rpc_distribution = xds_url_map_testcase.RpcDistributionStats(+                json_lb_stats)+            self.assertEqual(1, rpc_distribution.num_peers)+            self.assertLen(+                self.test_client.find_subchannels_with_state(+                    _ChannelzChannelState.READY),+                1,+            )+            self.assertLen(+                self.test_client.find_subchannels_with_state(+                    _ChannelzChannelState.IDLE),+                2,+            )+            # Remember the backend inuse, and turn it down later.+            self.first_backend_inuse = list(+                rpc_distribution.raw['rpcsByPeer'].keys())[0]++        with self.subTest('11_turn_down_server_in_use'):+            for s in self.test_servers:+                if s.pod_name == self.first_backend_inuse:+                    logging.info('setting backend %s to NOT_SERVING',+                                 s.pod_name)+                    s.set_not_serving()++        with self.subTest('12_wait_for_unhealth_status_propagation'):+            success = False+            for i in range(60):+                config = self.test_client.csds.fetch_client_status(+                    log_level=logging.INFO)+                self.assertIsNotNone(config)+                json_config = json_format.MessageToDict(config)+                parsed = xds_url_map_testcase.DumpedXdsConfig(json_config)+                logging.info('Client received CSDS response: %s', parsed)+                if len(parsed.endpoints) == REPLICA_COUNT - 1:+                    success = True+                    break+                logging.info('retrying after %d seconds',+                             _TD_PROPAGATE_CHECK_INTERVAL_SEC)+                time.sleep(_TD_PROPAGATE_CHECK_INTERVAL_SEC)","nit: Recommend to use a while loop with a deadline instead. With 60 retries, the deadline is implicit. I think UHC propagation deadline is 600s? Then we can get rid of ""success"", it will make the structure simpler.",
67486458,wanlin31,https://api.github.com/repos/grpc/grpc/pulls/27133,696176696,2021-08-25T23:20:06Z,tools/run_tests/performance/bq_upload_result.py,"@@ -222,8 +222,10 @@ def _populate_node_metadata_from_file(scenario_result, node_info_file):         'PodIP': 'podIP',         'NodeName': 'nodeName',     }-    with open(node_info_file, 'r') as f:-        file_metadata = json.loads(f.read())++    if os.access(node_info_file, os.R_OK):","Quick question, if the file is not readable do you have to call this _populate_node_metadata_from_file function at all? I think what you did is absolutely okay, just wondering.  ",X
27047833,ybbbby,https://api.github.com/repos/grpc/grpc/pulls/27133,696180479,2021-08-25T23:30:16Z,tools/run_tests/performance/bq_upload_result.py,"@@ -222,8 +222,10 @@ def _populate_node_metadata_from_file(scenario_result, node_info_file):         'PodIP': 'podIP',         'NodeName': 'nodeName',     }-    with open(node_info_file, 'r') as f:-        file_metadata = json.loads(f.read())++    if os.access(node_info_file, os.R_OK):","Thanks for commenting! Your suggestion is also viable. Considering the pattern of checking the existence of file in the `_populate_*_from_file`, I assume we can keep the check inside the func.",X
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/27134,696209952,2021-08-26T00:56:43Z,tools/run_tests/xds_k8s_test_driver/tests/affinity_test.py,"@@ -0,0 +1,154 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging+import time+from typing import List, Optional++from absl import flags+from absl.testing import absltest+from google.protobuf import json_format++from framework import xds_k8s_testcase+from framework import xds_url_map_testcase+from framework.helpers import retryers+from framework.infrastructure import k8s+from framework.rpc import grpc_channelz+from framework.test_app import server_app++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient+_ChannelzChannelState = grpc_channelz.ChannelState++# Testing metadata consts+_TEST_AFFINITY_METADATA_KEY = 'xds_md'+_TD_PROPAGATE_CHECK_INTERVAL_SEC = 10+++class AffinityTest(xds_k8s_testcase.RegularXdsKubernetesTestCase):++    def test_affinity(self) -> None:+        REPLICA_COUNT = 3++        with self.subTest('00_create_health_check'):+            self.td.create_health_check()++        with self.subTest('01_create_backend_services'):+            self.td.create_backend_service(+                affinity_header=_TEST_AFFINITY_METADATA_KEY)++        with self.subTest('02_create_url_map'):+            self.td.create_url_map(self.server_xds_host, self.server_xds_port)++        with self.subTest('03_create_target_proxy'):+            self.td.create_target_proxy()++        with self.subTest('04_create_forwarding_rule'):+            self.td.create_forwarding_rule(self.server_xds_port)++        with self.subTest('05_start_test_servers'):+            self.test_servers: List[_XdsTestServer] = self.startTestServers(+                replica_count=REPLICA_COUNT)++        with self.subTest('06_add_server_backends_to_backend_services'):+            self.setupServerBackends()++        with self.subTest('07_start_test_client'):+            self.test_client: _XdsTestClient = self.startTestClient(+                self.test_servers[0],+                rpc='EmptyCall',+                metadata='EmptyCall:%s:123' % _TEST_AFFINITY_METADATA_KEY)+            # Validate the number of received endpoints and affinity configs.+            config = self.test_client.csds.fetch_client_status(+                log_level=logging.INFO)+            self.assertIsNotNone(config)+            json_config = json_format.MessageToDict(config)+            parsed = xds_url_map_testcase.DumpedXdsConfig(json_config)+            logging.info('Client received CSDS response: %s', parsed)+            self.assertLen(parsed.endpoints, REPLICA_COUNT)+            self.assertEqual(+                parsed.rds['virtualHosts'][0]['routes'][0]['route']+                ['hashPolicy'][0]['header']['headerName'],+                _TEST_AFFINITY_METADATA_KEY)+            self.assertEqual(parsed.cds[0]['lbPolicy'], 'RING_HASH')++        with self.subTest('08_test_client_xds_config_exists'):+            self.assertXdsConfigExists(self.test_client)++        with self.subTest('09_test_server_received_rpcs_from_test_client'):+            self.assertSuccessfulRpcs(self.test_client)++        with self.subTest('10_first_100_affinity_rpcs_pick_same_backend'):+            num_rpcs = 100+            rpc_stats = self.getClientRpcStats(self.test_client, num_rpcs)+            json_lb_stats = json_format.MessageToDict(rpc_stats)+            rpc_distribution = xds_url_map_testcase.RpcDistributionStats(+                json_lb_stats)+            self.assertEqual(1, rpc_distribution.num_peers)+            self.assertLen(+                self.test_client.find_subchannels_with_state(+                    _ChannelzChannelState.READY),+                1,+            )+            self.assertLen(+                self.test_client.find_subchannels_with_state(+                    _ChannelzChannelState.IDLE),+                2,+            )+            # Remember the backend inuse, and turn it down later.+            self.first_backend_inuse = list(+                rpc_distribution.raw['rpcsByPeer'].keys())[0]++        with self.subTest('11_turn_down_server_in_use'):+            for s in self.test_servers:+                if s.pod_name == self.first_backend_inuse:+                    logging.info('setting backend %s to NOT_SERVING',+                                 s.pod_name)+                    s.set_not_serving()++        with self.subTest('12_wait_for_unhealth_status_propagation'):+            success = False+            for i in range(60):+                config = self.test_client.csds.fetch_client_status(+                    log_level=logging.INFO)+                self.assertIsNotNone(config)+                json_config = json_format.MessageToDict(config)+                parsed = xds_url_map_testcase.DumpedXdsConfig(json_config)+                logging.info('Client received CSDS response: %s', parsed)+                if len(parsed.endpoints) == REPLICA_COUNT - 1:+                    success = True+                    break+                logging.info('retrying after %d seconds',+                             _TD_PROPAGATE_CHECK_INTERVAL_SEC)+                time.sleep(_TD_PROPAGATE_CHECK_INTERVAL_SEC)","Changed to while deadlineBut I don't know how to get rid of ""success""",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27145,696720156,2021-08-26T15:03:50Z,src/core/lib/iomgr/call_combiner.cc,"@@ -114,25 +114,32 @@ void CallCombiner::ScheduleClosure(grpc_closure* closure, void CallCombiner::Start(grpc_closure* closure, grpc_error_handle error,                          DEBUG_ARGS const char* reason) {   GPR_TIMER_SCOPE(""CallCombiner::Start"", 0);-  grpc_call_combiner_trace.Log(-      GPR_INFO,-      ""==> CallCombiner::Start() [%p] closure=%p ["" DEBUG_FMT_STR-      ""%s] error=%s"",-      this, closure DEBUG_FMT_ARGS, reason,-      grpc_error_std_string(error).c_str());",Doh!  Good catch.@drfloob We should audit all of these call sites and see how many of them have parameters that are expensive to construct.,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27145,696775139,2021-08-26T16:05:30Z,src/core/lib/iomgr/call_combiner.cc,"@@ -114,25 +114,32 @@ void CallCombiner::ScheduleClosure(grpc_closure* closure, void CallCombiner::Start(grpc_closure* closure, grpc_error_handle error,                          DEBUG_ARGS const char* reason) {   GPR_TIMER_SCOPE(""CallCombiner::Start"", 0);-  grpc_call_combiner_trace.Log(-      GPR_INFO,-      ""==> CallCombiner::Start() [%p] closure=%p ["" DEBUG_FMT_STR-      ""%s] error=%s"",-      this, closure DEBUG_FMT_ARGS, reason,-      grpc_error_std_string(error).c_str());","It's probably worth noting that this pattern will make it easy to introduce such mistakes in the future, and perhaps we look for a different pattern that's more resilient to accidents before trying this again.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27134,696809736,2021-08-26T16:49:44Z,tools/run_tests/xds_k8s_test_driver/tests/affinity_test.py,"@@ -0,0 +1,154 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import logging+import time+from typing import List, Optional++from absl import flags+from absl.testing import absltest+from google.protobuf import json_format++from framework import xds_k8s_testcase+from framework import xds_url_map_testcase+from framework.helpers import retryers+from framework.infrastructure import k8s+from framework.rpc import grpc_channelz+from framework.test_app import server_app++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient+_ChannelzChannelState = grpc_channelz.ChannelState++# Testing metadata consts+_TEST_AFFINITY_METADATA_KEY = 'xds_md'+_TD_PROPAGATE_CHECK_INTERVAL_SEC = 10+++class AffinityTest(xds_k8s_testcase.RegularXdsKubernetesTestCase):++    def test_affinity(self) -> None:+        REPLICA_COUNT = 3++        with self.subTest('00_create_health_check'):+            self.td.create_health_check()++        with self.subTest('01_create_backend_services'):+            self.td.create_backend_service(+                affinity_header=_TEST_AFFINITY_METADATA_KEY)++        with self.subTest('02_create_url_map'):+            self.td.create_url_map(self.server_xds_host, self.server_xds_port)++        with self.subTest('03_create_target_proxy'):+            self.td.create_target_proxy()++        with self.subTest('04_create_forwarding_rule'):+            self.td.create_forwarding_rule(self.server_xds_port)++        with self.subTest('05_start_test_servers'):+            self.test_servers: List[_XdsTestServer] = self.startTestServers(+                replica_count=REPLICA_COUNT)++        with self.subTest('06_add_server_backends_to_backend_services'):+            self.setupServerBackends()++        with self.subTest('07_start_test_client'):+            self.test_client: _XdsTestClient = self.startTestClient(+                self.test_servers[0],+                rpc='EmptyCall',+                metadata='EmptyCall:%s:123' % _TEST_AFFINITY_METADATA_KEY)+            # Validate the number of received endpoints and affinity configs.+            config = self.test_client.csds.fetch_client_status(+                log_level=logging.INFO)+            self.assertIsNotNone(config)+            json_config = json_format.MessageToDict(config)+            parsed = xds_url_map_testcase.DumpedXdsConfig(json_config)+            logging.info('Client received CSDS response: %s', parsed)+            self.assertLen(parsed.endpoints, REPLICA_COUNT)+            self.assertEqual(+                parsed.rds['virtualHosts'][0]['routes'][0]['route']+                ['hashPolicy'][0]['header']['headerName'],+                _TEST_AFFINITY_METADATA_KEY)+            self.assertEqual(parsed.cds[0]['lbPolicy'], 'RING_HASH')++        with self.subTest('08_test_client_xds_config_exists'):+            self.assertXdsConfigExists(self.test_client)++        with self.subTest('09_test_server_received_rpcs_from_test_client'):+            self.assertSuccessfulRpcs(self.test_client)++        with self.subTest('10_first_100_affinity_rpcs_pick_same_backend'):+            num_rpcs = 100+            rpc_stats = self.getClientRpcStats(self.test_client, num_rpcs)+            json_lb_stats = json_format.MessageToDict(rpc_stats)+            rpc_distribution = xds_url_map_testcase.RpcDistributionStats(+                json_lb_stats)+            self.assertEqual(1, rpc_distribution.num_peers)+            self.assertLen(+                self.test_client.find_subchannels_with_state(+                    _ChannelzChannelState.READY),+                1,+            )+            self.assertLen(+                self.test_client.find_subchannels_with_state(+                    _ChannelzChannelState.IDLE),+                2,+            )+            # Remember the backend inuse, and turn it down later.+            self.first_backend_inuse = list(+                rpc_distribution.raw['rpcsByPeer'].keys())[0]++        with self.subTest('11_turn_down_server_in_use'):+            for s in self.test_servers:+                if s.pod_name == self.first_backend_inuse:+                    logging.info('setting backend %s to NOT_SERVING',+                                 s.pod_name)+                    s.set_not_serving()++        with self.subTest('12_wait_for_unhealth_status_propagation'):+            success = False+            for i in range(60):+                config = self.test_client.csds.fetch_client_status(+                    log_level=logging.INFO)+                self.assertIsNotNone(config)+                json_config = json_format.MessageToDict(config)+                parsed = xds_url_map_testcase.DumpedXdsConfig(json_config)+                logging.info('Client received CSDS response: %s', parsed)+                if len(parsed.endpoints) == REPLICA_COUNT - 1:+                    success = True+                    break+                logging.info('retrying after %d seconds',+                             _TD_PROPAGATE_CHECK_INTERVAL_SEC)+                time.sleep(_TD_PROPAGATE_CHECK_INTERVAL_SEC)","```pydeadline = time.time() + _TD_PROPAGATE_TIMEOUTparsed = Nonetry:    while time.time() < deadline:        config = self.test_client.csds.fetch_client_status(            log_level=logging.INFO)        self.assertIsNotNone(config)        json_config = json_format.MessageToDict(config)        parsed = xds_url_map_testcase.DumpedXdsConfig(json_config)        if len(parsed.endpoints) == _REPLICA_COUNT - 1:            break        logging.info('retrying after %d seconds',                        _TD_PROPAGATE_CHECK_INTERVAL_SEC)        time.sleep(_TD_PROPAGATE_CHECK_INTERVAL_SEC)    else:        self.fail('unhealthy status did not propagate after 600 seconds')finally:    logging.info('Client received CSDS response: %s', parsed)```Above is a potential way of achieving that. I moved the CSDS response log to the `finally` block, so it will print even if we hit CTRL+C. The `success` variable is changed to a while-else.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27141,696812994,2021-08-26T16:53:55Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/api.py,"@@ -259,14 +261,24 @@ class OperationError(Error):      def __init__(self, api_name, operation_response, message=None):         self.api_name = api_name-        operation = json_format.ParseDict(operation_response, Operation())+        operation = json_format.ParseDict(+            operation_response,+            Operation(),+            ignore_unknown_fields=True,+            descriptor_pool=error_details_pb2.DESCRIPTOR.pool)","I think you're right, it should work just with importing the pb2. I thought this could be a good thing to be explicit here. This also explains why this module is imported, otherwise it's technically ""unused"". I can remove this. What would you recommend?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27141,696815092,2021-08-26T16:56:42Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/api.py,"@@ -259,14 +261,24 @@ class OperationError(Error):      def __init__(self, api_name, operation_response, message=None):         self.api_name = api_name-        operation = json_format.ParseDict(operation_response, Operation())+        operation = json_format.ParseDict(+            operation_response,+            Operation(),+            ignore_unknown_fields=True,+            descriptor_pool=error_details_pb2.DESCRIPTOR.pool)","Up to you. Removing it is one line less complexity, keeping it saves the time to re-run the presubmits and being explicit.",
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27094,696970734,2021-08-26T20:53:52Z,src/core/ext/transport/binder/wire_format/transaction.h,"@@ -34,34 +37,69 @@ ABSL_CONST_INIT extern const int kFlagExpectSingleMessage; ABSL_CONST_INIT extern const int kFlagStatusDescription; ABSL_CONST_INIT extern const int kFlagMessageDataIsParcelable; -using Metadata = std::vector<std::pair<std::string, std::string>>;+struct KeyValuePair {",gRPC library don't have any existing utility to represent a pair of grpc_slice?,X
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27036,696991428,2021-08-26T21:30:40Z,src/core/ext/transport/binder/server/binder_server_credentials.h,"@@ -0,0 +1,70 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_EXT_TRANSPORT_BINDER_SERVER_BINDER_SERVER_CREDENTIALS_H+#define GRPC_CORE_EXT_TRANSPORT_BINDER_SERVER_BINDER_SERVER_CREDENTIALS_H++#include <grpc/impl/codegen/port_platform.h>++#include <grpcpp/security/server_credentials.h>++#include <memory>++#include ""src/core/ext/transport/binder/server/binder_server.h""++namespace grpc {+namespace experimental {++#ifdef GPR_ANDROID++// TODO(waynetu): Move this to <grpcpp/security/server_credentials.h>+std::shared_ptr<ServerCredentials> BinderServerCredentials();++#endif  // GPR_ANDROID++}  // namespace experimental++namespace internal {++template <typename T>+class BinderServerCredentialsImpl final : public ServerCredentials {",Unfortunately we need to use template here to swap the underlying layer for testing purpose.I think we should add comment here to explain the template parameter here. And maybe rename the template parameter to `BinderTxReceiver` or something like that,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/26940,697304068,2021-08-27T09:44:51Z,templates/CMakeLists.txt.template,"@@ -170,6 +170,7 @@   option(gRPC_BUILD_CODEGEN ""Build codegen"" ON)   option(gRPC_BUILD_CSHARP_EXT ""Build C# extensions"" ON)   option(gRPC_BACKWARDS_COMPATIBILITY_MODE ""Build libraries that are binary compatible across a larger number of OS and libc versions"" OFF)+  option(gRPC_INSTALL_PLUGINS ""Install protobuf plugins (disable e.g. when building for a pure cross-compilation environment"" ON)","I think in general, this solution makes sense, but I'm worried about the explosion of new cmake build options.Currently we have `gRPC_BUILD_CODEGEN` (with a bit unclear semantics)`gRPC_BUILD_GRPC_CPP_PLUGIN (+ equivalent for other languages, e.g. gRPC_BUILD_GRPC_RUBY_PLUGIN)`and now we're adding yet another option called: `gRPC_INSTALL_PLUGINS`. I'm not saying that new option is not needed, but before introducing it, we should re-think what the use of all the options is, document properly and come up with a solution that addresses the use cases without introducing unnecessary baggage in form of extra options.Btw, in this particular case, would just setting  `gRPC_BUILD_GRPC_*_PLUGIN` to OFF solve the issue?",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/27078,697390534,2021-08-27T12:11:16Z,cmake/cares.cmake,"@@ -39,9 +39,12 @@ if(gRPC_CARES_PROVIDER STREQUAL ""module"")     set(gRPC_INSTALL FALSE)   endif() elseif(gRPC_CARES_PROVIDER STREQUAL ""package"")","the `gRPC_CARES_PROVIDER STREQUAL ""package""` branch you're modifying is for the case where you're building gRPC on a system where C-ares is already pre-installed.Why is it important to support a pre-installed statically linked version of c-ares? AFAICT, this is not a scenario that is an important use case for grpc (why not pre-install c-ares as dynamically linked?), so I don't see why we should make the build more complex to support it.Btw, I started tests on this PR and I think that in the current form of this PR, some of the distribution tests are going to fail - let's see.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27082,697565094,2021-08-27T16:18:08Z,src/core/lib/promise/observable.h,"@@ -0,0 +1,282 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_PROMISE_OBSERVABLE_H+#define GRPC_CORE_LIB_PROMISE_OBSERVABLE_H++#include <grpc/impl/codegen/port_platform.h>++#include <limits>+#include ""absl/types/optional.h""+#include ""src/core/lib/promise/activity.h""+#include ""src/core/lib/promise/wait_set.h""++namespace grpc_core {++namespace promise_detail {++using ObservableVersion = uint64_t;+static constexpr ObservableVersion kTombstoneVersion =+    std::numeric_limits<ObservableVersion>::max();++}  // namespace promise_detail++class WatchCommitter {+ public:+  void Commit() { version_seen_ = promise_detail::kTombstoneVersion; }++ protected:+  promise_detail::ObservableVersion version_seen_ = 0;+};++namespace promise_detail {++// Shared state between Observable and Observer.+template <typename T>+class ObservableState {+ public:+  explicit ObservableState(absl::optional<T> value)+      : value_(std::move(value)) {}++  // Publish that we're closed.+  void Close() {+    mu_.Lock();+    version_ = kTombstoneVersion;+    value_.reset();+    auto wakeup = waiters_.TakeWakeupSet();+    mu_.Unlock();+    wakeup.Wakeup();+  }++  // Synchronously publish a new value, and wake any waiters.+  void Push(T value) {+    mu_.Lock();+    version_++;+    value_ = std::move(value);+    auto wakeup = waiters_.TakeWakeupSet();+    mu_.Unlock();+    wakeup.Wakeup();+  }++  Poll<absl::optional<T>> PollGet(ObservableVersion* version_seen) {+    absl::MutexLock lock(&mu_);+    if (!value_.has_value()) {+      if (version_ != kTombstoneVersion) {+        // We allow initial no-value, which does not indicate closure.+        return waiters_.AddPending(Activity::current()->MakeNonOwningWaker());+      }+    }+    *version_seen = version_;+    return value_;+  }++  Poll<absl::optional<T>> PollNext(ObservableVersion* version_seen) {+    absl::MutexLock lock(&mu_);+    if (!value_.has_value()) {+      if (version_ != kTombstoneVersion) {+        // We allow initial no-value, which does not indicate closure.+        return waiters_.AddPending(Activity::current()->MakeNonOwningWaker());+      }+    }+    if (version_ == *version_seen) {+      return waiters_.AddPending(Activity::current()->MakeNonOwningWaker());+    }+    *version_seen = version_;+    return value_;+  }++  Poll<absl::optional<T>> PollWatch(ObservableVersion* version_seen) {+    if (*version_seen == kTombstoneVersion) {+      return Pending();+    }++    absl::MutexLock lock(&mu_);+    if (!value_.has_value()) {",nit: this code is duplicated 3x. It's generally easier to change/maintain code if there's only one place that a change needs to be made.,
11674202,stanley-cheung,https://api.github.com/repos/grpc/grpc/pulls/27101,697608558,2021-08-27T17:31:43Z,tools/run_tests/artifacts/distribtest_targets.py,"@@ -238,7 +238,7 @@ def __init__(self, platform, arch, docker_suffix=None):         self.platform = platform         self.arch = arch         self.docker_suffix = docker_suffix-        self.labels = ['distribtest', 'php7', platform, arch]+        self.labels = ['distribtest', 'php', 'php7', platform, arch]","Is there any image with just `php` (implying PHP 5, which is deprecated?) anymore? I think we did a sweep a while ago and only leave `php7` images for testing. We no longer should have any testing against PHP 5.",
41809318,franksinankaya,https://api.github.com/repos/grpc/grpc/pulls/27078,697635262,2021-08-27T18:18:25Z,cmake/cares.cmake,"@@ -39,9 +39,12 @@ if(gRPC_CARES_PROVIDER STREQUAL ""module"")     set(gRPC_INSTALL FALSE)   endif() elseif(gRPC_CARES_PROVIDER STREQUAL ""package"")","What you described is correct, c-ares is statically linked on my platform and c-ares requires -DCARES_STATICLIB to be defined by c-ares users when static linked. https://github.com/c-ares/c-ares/blob/main/INSTALL.md#important-static-c-ares-usage-noteThis cflags is only present in packageconfig.Name: c-aresURL: https://c-ares.haxx.se/Description: asynchronous DNS lookup libraryVersion: 1.17.2Requires:Requires.private:Cflags: -I${includedir} -DCARES_STATICLIBLibs: -L${libdir} -lcares",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27169,697661709,2021-08-27T19:06:13Z,test/cpp/qps/qps_benchmark_script.bzl,"@@ -31,10 +31,10 @@ load(""//test/cpp/qps:json_run_localhost_scenarios.bzl"", ""JSON_RUN_LOCALHOST_SCEN  def add_suffix(name):     # NOTE(https://github.com/grpc/grpc/issues/24178): Add the suffix to the name-    # to avoid having the target name that 89 or 90 long.-    m = len(name) - (89 - len(""//test/cpp/qps:""))-    if m == 0 or m == 1:-        return name + ""_"" * (2 - m)+    # to avoid having the target name that 87, 88, 89 or 90 long.+    m = len(name) - (87 - len(""//test/cpp/qps:""))+    if m >= 0 and m <= 3:+        return name + ""_"" * (4 - m)",What's the magic of 91 characters long name?,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/27101,698266516,2021-08-30T07:43:43Z,tools/run_tests/artifacts/build_artifact_php.sh,"@@ -21,12 +21,5 @@ mkdir -p ""${ARTIFACTS_OUT}""  # Build the PHP extension archive (this just zips all the files up) pear package-# Note: the extension compiled by this step is not being used in any-# way, i.e. they are not the pacakge being distributed.-# This is done here to get an early signal for compiling the PHP-# extension in some form.-find . -name ""grpc-*.tgz"" | cut -b3- | xargs pecl install","So this step (trying to install the pecl package) is basically an early distribtests and it's the same thing as what's being done by all the PHP distribtests:https://github.com/grpc/grpc/blob/013a45ccc4f0cc99532cb3148ca6ed3f60996711/test/distrib/php/run_distrib_test.sh#L22So from that perspective the step in build_artifacts jobs is duplicate and it also strictly speaking doesn't belong there. The reason why we were including this step until now is that we didn't have a simple way to run PHP distribtests on PRs, so this comand was added just be able to detect potential distribtest breakages early.This PR solves this problem by making it possible to run all PHP distribtests on PR. Trying to install the pear package as part of artifact build can be removed since it doesn't add any new information and only slows down the artifact build (which is very fast otherwise, since it basically just created a tarball with all the files).",
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27177,698309479,2021-08-30T08:49:37Z,src/core/ext/transport/binder/client/channel_create.cc,"@@ -94,4 +104,20 @@ std::shared_ptr<grpc::Channel> CreateBinderChannel( }  // namespace experimental }  // namespace grpc -#endif  // ANDROID+#else  // !GRPC_SUPPORT_BINDER_TRANSPORT++std::shared_ptr<grpc::Channel> CreateBinderChannel(void*, jobject,",These parameters are all unused.,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/27169,698663841,2021-08-30T17:15:43Z,test/cpp/qps/qps_benchmark_script.bzl,"@@ -31,10 +31,10 @@ load(""//test/cpp/qps:json_run_localhost_scenarios.bzl"", ""JSON_RUN_LOCALHOST_SCEN  def add_suffix(name):     # NOTE(https://github.com/grpc/grpc/issues/24178): Add the suffix to the name-    # to avoid having the target name that 89 or 90 long.-    m = len(name) - (89 - len(""//test/cpp/qps:""))-    if m == 0 or m == 1:-        return name + ""_"" * (2 - m)+    # to avoid having the target name that 87, 88, 89 or 90 long.+    m = len(name) - (87 - len(""//test/cpp/qps:""))+    if m >= 0 and m <= 3:+        return name + ""_"" * (4 - m)",All I have is a wild guess and I suspect that executables with 91 long or longer end up having an UNC path described [here](https://docs.microsoft.com/en-us/windows/win32/fileio/maximum-file-path-limitation?tabs=cmd) in the bazel-rbe environment.,
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27182,698684632,2021-08-30T17:46:57Z,src/core/ext/transport/binder/transport/binder_transport.cc,"@@ -204,137 +434,79 @@ static void perform_stream_op(grpc_transport* gt, grpc_stream* gs,   }   if (op->recv_initial_metadata) {     gpr_log(GPR_INFO, ""recv_initial_metadata"");-    if (!gbs->cancellation_error.ok()) {-      grpc_core::ExecCtx::Run(-          DEBUG_LOCATION,-          op->payload->recv_initial_metadata.recv_initial_metadata_ready,-          absl_status_to_grpc_error(gbs->cancellation_error));-    } else {-      gbs->recv_initial_metadata_ready =-          op->payload->recv_initial_metadata.recv_initial_metadata_ready;-      gbs->recv_initial_metadata =-          op->payload->recv_initial_metadata.recv_initial_metadata;-      gbt->transport_stream_receiver->RegisterRecvInitialMetadata(-          gbs->tx_code,-          [gbs](absl::StatusOr<grpc_binder::Metadata> initial_metadata) {-            grpc_core::ExecCtx exec_ctx;-            GPR_ASSERT(gbs->recv_initial_metadata);-            GPR_ASSERT(gbs->recv_initial_metadata_ready);-            if (!initial_metadata.ok()) {-              gpr_log(GPR_ERROR, ""Failed to parse initial metadata"");-              grpc_core::ExecCtx::Run(-                  DEBUG_LOCATION, gbs->recv_initial_metadata_ready,-                  absl_status_to_grpc_error(initial_metadata.status()));-              return;-            }-            AssignMetadata(gbs->recv_initial_metadata, gbs->arena,-                           *initial_metadata);-            grpc_core::ExecCtx::Run(DEBUG_LOCATION,-                                    gbs->recv_initial_metadata_ready,-                                    GRPC_ERROR_NONE);-          });-    }+    gbs->recv_initial_metadata_ready =+        op->payload->recv_initial_metadata.recv_initial_metadata_ready;+    gbs->recv_initial_metadata =+        op->payload->recv_initial_metadata.recv_initial_metadata;+    gbs->trailing_metadata_available =+        op->payload->recv_initial_metadata.trailing_metadata_available;+    GRPC_BINDER_STREAM_REF(gbs, ""recv_initial_metadata"");+    gbt->transport_stream_receiver->RegisterRecvInitialMetadata(+        tx_code, [tx_code, gbs,+                  gbt](absl::StatusOr<grpc_binder::Metadata> initial_metadata) {+          grpc_core::ExecCtx exec_ctx;","(Hi @ctiller, this comment was originally left in the client fuzzer thread, but since relevant changes are moved to this PR, I copy it here. Please take a look when you have time. Thanks!)Hi Craig, we're dealing with combiner and execution context in this PR, and would like your opinion on the following issue here:The RegisterRecvInitialMetadata function here would register a callback which will be called once a binder transaction comes. The binder transaction will be coming from a separate thread and therefore this callback will be invoked in a non-gRPC thread as well. When this callback is called, we would like to move the received metadata to op->payload and call the corresponding recv_initial_metadata_ready callback. As a result, we would need to instantiate an execution context here, because otherwise the call to ExecCtx::Run() would fail.We noticed that the HTTP/2 transport does not instantiate any execution context on its own, and also having an execution context in a non-gRPC thread makes the behavior of our transport inconsistent. An example of such inconsistency can be seen from client_fuzzer.cc, where we have to join the thread before the assertion that the cancel operation completes, otherwise grpc_completion_queue_next might return a timeout event. I'm not 100% sure about whether having an execution context in the transport is the root cause of the assertion failure though, but judging from the log it seems like every time the assertion fails, another thread is pushing a callback to the combiner at the same time.We've tried removing the execution context by implementing something like read_action_locked in HTTP/2 transport, but couldn't make it work because we cannot poll the incoming binder transactions like HTTP/2 does.What's your opinion on the issue? Is it OK to instantiate execution context in the transport like we did? Or do you have any suggestions on how we might be able to do something similar to polling and have ready callbacks be invoked in the main thread?There are certain assumptions or statements above that I'm not fully confident about, so please kindly correct me if I'm missing something here.Thanks!",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27189,698792894,2021-08-30T20:40:48Z,tools/run_tests/xds_k8s_test_driver/kubernetes-manifests/server.deployment.yaml,"@@ -18,7 +18,9 @@ spec:         app: ${deployment_name}         owner: xds-k8s-interop-test     spec:+      % if service_account_name:","This service account it OK, it's just a K8S internal service account.Instead, do not add this annotation when Workload Identity is enabled.https://github.com/grpc/grpc/blob/b6cc72f1295982472bba38e618c006cc943d2fc8/tools/run_tests/xds_k8s_test_driver/kubernetes-manifests/service-account.yaml#L9-L10",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27189,698796645,2021-08-30T20:46:38Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_flags.py,"@@ -41,9 +41,12 @@     ""debug_use_port_forwarding"",     default=False,     help=""Development only: use kubectl port-forward to connect to test app"")+DISABLE_WORKLOAD_IDENTITY = flags.DEFINE_bool(+    ""disable_workload_identity"",+    default=False,+    help=""Disable the WorkloadIdentity feature simplify permission control"")","nit: To avoid double negatives, I'd recommend to inverse this flag:```suggestionENABLE_WORKLOAD_IDENTITY = flags.DEFINE_bool(    ""enable_workload_identity"",    default=True,    help=""Disable the WorkloadIdentity feature simplify permission control"")```Then it can be turned off with `--noenable_workload_identity`. `--noenable` appears to be a common pattern.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27189,698824298,2021-08-30T21:34:42Z,tools/run_tests/xds_k8s_test_driver/kubernetes-manifests/server.deployment.yaml,"@@ -18,7 +18,9 @@ spec:         app: ${deployment_name}         owner: xds-k8s-interop-test     spec:+      % if service_account_name:","Ah, OK. Technically K8S Service Account can be used for things other than Security and not directly dependent upon Workload Identity, like RBAC https://kubernetes.io/docs/reference/access-authn-authz/rbac/#service-account-permissions.So with time this logic of not creating Service Account when Workload Identity disabled may change.However, I'm OK if you want to keep it as if until then.",
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27036,698929724,2021-08-31T02:09:31Z,include/grpc/grpc.h,"@@ -440,6 +440,12 @@ GRPCAPI void grpc_server_set_config_fetcher( GRPCAPI int grpc_server_add_insecure_http2_port(grpc_server* server,                                                 const char* addr); +/** EXPERIMENTAL.  Add a ""binder port"" to the server that listens to incoming+ *  binder transactions.+ *  Returns 1 on success, 0 on failure (or called in non-Android environments)+ *  REQUIRES: server not started */+GRPCAPI int grpc_server_add_binder_port(grpc_server* server, const char* addr);",I wonder if we still need this C API for us to wrap C++ API around it. According to https://github.com/grpc/grpc/projects/8 it seems no longer relevant.,
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27036,698950357,2021-08-31T03:13:18Z,include/grpcpp/security/server_credentials.h,"@@ -64,6 +64,13 @@ namespace experimental { /// Builds Xds ServerCredentials given fallback credentials std::shared_ptr<ServerCredentials> XdsServerCredentials(     const std::shared_ptr<ServerCredentials>& fallback_credentials);++/// Builds Binder ServerCredentials.+///+/// Calling \a ServerBuilder::AddListeningPort() with Binder ServerCredentials+/// in a non-Android environment will make the subsequent call to+/// \a ServerBuilder::BuildAndStart() returns a null pointer.+std::shared_ptr<ServerCredentials> BinderServerCredentials();","We decided to temporarily move the APIs back to internal headers because we need this PR in the internal testings and the RFC process is likely to take longer than desired. We're drafting a gRFC, and will make the APIs public once the RFC is approved. Thanks!",X
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27228,700913827,2021-09-02T09:29:11Z,src/core/ext/transport/binder/wire_format/wire_reader_impl.h,"@@ -104,11 +104,20 @@ class WireReaderImpl : public WireReader {   // called. Be cautious not to access it afterward.   std::unique_ptr<Binder> other_end_binder_;   absl::flat_hash_map<transaction_code_t, int32_t> expected_seq_num_;+  absl::flat_hash_map<transaction_code_t, std::string> message_buffer_;   std::unique_ptr<TransactionReceiver> tx_receiver_;   bool is_client_;   // When WireReaderImpl gets destructed, call on_destruct_callback_. This is   // mostly for decrementing the reference count of its transport.   std::function<void()> on_destruct_callback_;++  // ACK every 16k bytes.+  static constexpr size_t kFlowControlAckBytes = 16 * 1024;+  size_t num_incoming_bytes_ = 0;",How big is `size_t` in Android?We probably want `int64_t` here to be safe. The wire format is already using 64 bits integers to represent size,
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27228,700919496,2021-09-02T09:37:06Z,src/core/ext/transport/binder/wire_format/wire_reader_impl.cc,"@@ -306,7 +315,13 @@ absl::Status WireReaderImpl::ProcessStreamingTransactionImpl(       RETURN_IF_ERROR(parcel->ReadByteArray(&msg_data));     }     gpr_log(GPR_INFO, ""msg_data = %s"", msg_data.c_str());-    transport_stream_receiver_->NotifyRecvMessage(code, std::move(msg_data));+    message_buffer_[code] += msg_data;",I wonder if there exists a hint somewhere (ex. initial metadata) telling us expected total message size? So that we don't need to resize the message buffer,
14166415,sanjaypujare,https://api.github.com/repos/grpc/grpc/pulls/27234,701249467,2021-09-02T16:34:50Z,tools/internal_ci/linux/grpc_xds_k8s_xlang_java.sh,"@@ -0,0 +1,105 @@+#!/usr/bin/env bash+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex -o igncr || set -ex++# Constants+readonly GITHUB_REPOSITORY_NAME=""grpc""+# GKE Cluster+readonly GKE_CLUSTER_NAME=""interop-test-psm-sec-v2-us-central1-a""+readonly GKE_CLUSTER_ZONE=""us-central1-a""++## xDS test server/client Docker images+readonly COMMON_IMAGE_LOCATION=""gcr.io/grpc-testing/xds-interop""+readonly SERVER_LANG=""cpp go""+readonly CLIENT_LANG=""java""+readonly VERSION_TAG=""v1.40.x""++#######################################+# Executes the test case+# Globals:+#   TEST_DRIVER_FLAGFILE: Relative path to test driver flagfile+#   KUBE_CONTEXT: The name of kubectl context with GKE cluster access+#   TEST_XML_OUTPUT_DIR: Output directory for the test xUnit XML report+#   SERVER_IMAGE_NAME: Test server Docker image name+#   CLIENT_IMAGE_NAME: Test client Docker image name+#   GIT_COMMIT: SHA-1 of git commit being built+# Arguments:+#   Test case name+# Outputs:+#   Writes the output of test execution to stdout, stderr+#   Test xUnit report to ${TEST_XML_OUTPUT_DIR}/${test_name}/sponge_log.xml+#######################################+run_test() {+  # Test driver usage:+  # https://github.com/grpc/grpc/tree/master/tools/run_tests/xds_k8s_test_driver#basic-usage+  local test_name=""${1:?Usage: run_test test_name server_image_name client_image_name}""+  local server_image_name=""${2:?Usage: run_test test_name server_image_name client_image_name}""+  local client_image_name=""${3:?Usage: run_test test_name server_image_name client_image_name}""+  set -x+  python -m ""tests.${test_name}"" \+    --flagfile=""${TEST_DRIVER_FLAGFILE}"" \+    --kube_context=""${KUBE_CONTEXT}"" \+    --server_image=""${server_image_name}"" \+    --client_image=""${client_image_name}"" \+    --xml_output_file=""${TEST_XML_OUTPUT_DIR}/${test_name}/sponge_log.xml"" \+    --force_cleanup \+    --nocheck_local_certs+  set +x+}++#######################################+# Main function: provision software necessary to execute tests, and run them+# Globals:+#   KOKORO_ARTIFACTS_DIR+#   GITHUB_REPOSITORY_NAME+#   SRC_DIR: Populated with absolute path to the source repo+#   TEST_DRIVER_REPO_DIR: Populated with the path to the repo containing+#                         the test driver+#   TEST_DRIVER_FULL_DIR: Populated with the path to the test driver source code+#   TEST_DRIVER_FLAGFILE: Populated with relative path to test driver flagfile+#   TEST_XML_OUTPUT_DIR: Populated with the path to test xUnit XML report+#   GIT_ORIGIN_URL: Populated with the origin URL of git repo used for the build+#   GIT_COMMIT: Populated with the SHA-1 of git commit being built+#   GIT_COMMIT_SHORT: Populated with the short SHA-1 of git commit being built+#   KUBE_CONTEXT: Populated with name of kubectl context with GKE cluster access+# Arguments:+#   None+# Outputs:+#   Writes the output of test execution to stdout, stderr+#######################################+main() {+  local script_dir+  script_dir=""$(dirname ""$0"")""+  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh+  source ""${script_dir}/grpc_xds_k8s_install_test_driver.sh""+  set -x+  if [[ -n ""${KOKORO_ARTIFACTS_DIR}"" ]]; then+    kokoro_setup_test_driver ""${GITHUB_REPOSITORY_NAME}""+    cd ""${TEST_DRIVER_FULL_DIR}""+  else+    local_setup_test_driver ""${script_dir}""+    cd ""${SRC_DIR}/${TEST_DRIVER_PATH}""","I had to insert this otherwise this was trying to run the test driver from `${TEST_DRIVER_FULL_DIR}` which is the cloned source code directory which has 2 problems:- it doesn't find `config/local-dev.cfg` file- it doesn't use my local changes which is the purpose of using the ""local"" mode",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27234,701299088,2021-09-02T17:46:26Z,tools/internal_ci/linux/grpc_xds_k8s_xlang_java.sh,"@@ -0,0 +1,105 @@+#!/usr/bin/env bash+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex -o igncr || set -ex++# Constants+readonly GITHUB_REPOSITORY_NAME=""grpc""+# GKE Cluster+readonly GKE_CLUSTER_NAME=""interop-test-psm-sec-v2-us-central1-a""+readonly GKE_CLUSTER_ZONE=""us-central1-a""++## xDS test server/client Docker images+readonly COMMON_IMAGE_LOCATION=""gcr.io/grpc-testing/xds-interop""+readonly SERVER_LANG=""cpp go""+readonly CLIENT_LANG=""java""+readonly VERSION_TAG=""v1.40.x""++#######################################+# Executes the test case+# Globals:+#   TEST_DRIVER_FLAGFILE: Relative path to test driver flagfile+#   KUBE_CONTEXT: The name of kubectl context with GKE cluster access+#   TEST_XML_OUTPUT_DIR: Output directory for the test xUnit XML report+#   SERVER_IMAGE_NAME: Test server Docker image name+#   CLIENT_IMAGE_NAME: Test client Docker image name+#   GIT_COMMIT: SHA-1 of git commit being built+# Arguments:+#   Test case name+# Outputs:+#   Writes the output of test execution to stdout, stderr+#   Test xUnit report to ${TEST_XML_OUTPUT_DIR}/${test_name}/sponge_log.xml+#######################################+run_test() {+  # Test driver usage:+  # https://github.com/grpc/grpc/tree/master/tools/run_tests/xds_k8s_test_driver#basic-usage+  local test_name=""${1:?Usage: run_test test_name server_image_name client_image_name}""+  local server_image_name=""${2:?Usage: run_test test_name server_image_name client_image_name}""+  local client_image_name=""${3:?Usage: run_test test_name server_image_name client_image_name}""+  set -x+  python -m ""tests.${test_name}"" \+    --flagfile=""${TEST_DRIVER_FLAGFILE}"" \+    --kube_context=""${KUBE_CONTEXT}"" \+    --server_image=""${server_image_name}"" \+    --client_image=""${client_image_name}"" \+    --xml_output_file=""${TEST_XML_OUTPUT_DIR}/${test_name}/sponge_log.xml"" \+    --force_cleanup \+    --nocheck_local_certs+  set +x+}++#######################################+# Main function: provision software necessary to execute tests, and run them+# Globals:+#   KOKORO_ARTIFACTS_DIR+#   GITHUB_REPOSITORY_NAME+#   SRC_DIR: Populated with absolute path to the source repo+#   TEST_DRIVER_REPO_DIR: Populated with the path to the repo containing+#                         the test driver+#   TEST_DRIVER_FULL_DIR: Populated with the path to the test driver source code+#   TEST_DRIVER_FLAGFILE: Populated with relative path to test driver flagfile+#   TEST_XML_OUTPUT_DIR: Populated with the path to test xUnit XML report+#   GIT_ORIGIN_URL: Populated with the origin URL of git repo used for the build+#   GIT_COMMIT: Populated with the SHA-1 of git commit being built+#   GIT_COMMIT_SHORT: Populated with the short SHA-1 of git commit being built+#   KUBE_CONTEXT: Populated with name of kubectl context with GKE cluster access+# Arguments:+#   None+# Outputs:+#   Writes the output of test execution to stdout, stderr+#######################################+main() {+  local script_dir+  script_dir=""$(dirname ""$0"")""+  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh+  source ""${script_dir}/grpc_xds_k8s_install_test_driver.sh""+  set -x+  if [[ -n ""${KOKORO_ARTIFACTS_DIR}"" ]]; then+    kokoro_setup_test_driver ""${GITHUB_REPOSITORY_NAME}""+    cd ""${TEST_DRIVER_FULL_DIR}""+  else+    local_setup_test_driver ""${script_dir}""+    cd ""${SRC_DIR}/${TEST_DRIVER_PATH}""+  fi++  # Run tests+  local client_image_name=""${COMMON_IMAGE_LOCATION}/${CLIENT_LANG}-client:${VERSION_TAG}""+  for LANG in ${SERVER_LANG}",Take a look at the discussion about continuing test on failure here: https://github.com/grpc/grpc/pull/26883#discussion_r685294700Also take a look at Eric's solution:https://github.com/grpc/grpc/blob/dc19d6c1322b6b863fb53a094e3a83771f5717ea/tools/internal_ci/linux/grpc_xds_k8s_lb.sh#L146-L154,X
5564114,Vignesh2208,https://api.github.com/repos/grpc/grpc/pulls/27194,701438707,2021-09-02T21:36:13Z,src/core/lib/iomgr/event_engine/uv/impl.cc,"@@ -0,0 +1,1301 @@+#include <uv.h>++#include <atomic>+#include <functional>+#include <future>+#include <unordered_map>++#include ""absl/strings/str_format.h""++#include ""grpc/event_engine/event_engine.h""+#include ""src/core/lib/address_utils/sockaddr_utils.h""+#include ""src/core/lib/gprpp/host_port.h""+#include ""src/core/lib/gprpp/mpscq.h""+#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/gprpp/thd.h""+#include ""src/core/lib/iomgr/event_engine/promise.h""+#include ""src/core/lib/iomgr/exec_ctx.h""+#include ""src/core/lib/iomgr/socket_utils.h""++extern grpc_core::TraceFlag grpc_tcp_trace;++namespace {++class LibuvDNSResolver;+class LibuvEventEngine;+class LibuvListener;+class LibuvLookupTask;+class LibuvTask;++// Helper to dump network traffic in a legible manner.+// Probably belongs to a different utility file elsewhere, but wasn't sure if+// we even want to keep it eventually.+static void hexdump(const std::string& prefix, const void* data_, size_t size) {+  const uint8_t* data = static_cast<const uint8_t*>(data_);+  char ascii[17];+  ascii[16] = 0;+  memset(ascii, ' ', 16);+  std::string line;+  size_t beginning;+  for (size_t i = 0; i < size; i++) {+    if (i % 16 == 0) {+      line = """";+      beginning = i;+    }+    uint8_t d = data[i];+    line += absl::StrFormat(""%02X "", d);+    ascii[i % 16] = isprint(d) ? d : '.';+    size_t n = i + 1;+    if (((n % 8) == 0) || (n == size)) {+      line += "" "";+      if (((n % 16) != 0) && (n != size)) continue;+      if (n == size) {+        n %= 16;+        for (unsigned p = n; (n != 0) && (p < 16); p++) {+          line += ""   "";+          ascii[p] = ' ';+        }+        if ((n <= 8) && (n != 0)) {+          line += "" "";+        }+      }+      gpr_log(GPR_DEBUG, ""%s %p %04zX  | %s| %s |"", prefix.c_str(),+              data + beginning, beginning, line.c_str(), ascii);+    }+  }+}++constexpr size_t READ_BUFFER_SIZE = 4096;++////////////////////////////////////////////////////////////////////////////////+/// The base class to wrap a LibUV TCP handle. The class hierarchy that stems+/// from it is meant to split the Event Engine objects from the LibUV ones, as+/// their lifespan aren't the same. When an Event Engine object is destroyed,+/// we need to request the destruction into LibUV's API, while keeping LibUV's+/// structures around, which is what we're going to do here.+///+/// It will hold the few common data and functions between the Listener and+/// Endpoint classes that will derive from it.+///+/// The derived classes should hold all of the extra information that has to+/// follow the lifespan of the LibUV base socket, such as timers, callback+/// functors, buffers, etc.+////////////////////////////////////////////////////////////////////////////////+class LibuvWrapperBase {+ public:+  // Returns the LibUV loop object associated with this handle. Used by the+  // LibuvEventEngine class.+  uv_loop_t* GetLoop() { return tcp_.loop; }++  // The rest of the API really should only be used by the derived classes.+ protected:+  LibuvWrapperBase() { tcp_.data = this; }+  virtual ~LibuvWrapperBase() = default;++  // Registers the libuv handle into the libuv loop. This needs to be called+  // from the corresponding libuv Thread.+  void RegisterUnsafe(LibuvEventEngine* engine);++  // Schedules a close of the base socket into the libuv loop. If the derived+  // class has more handles to wait on closing, like timers, then it needs+  // to tell how many in the \a extraCloses argument.+  //+  // NOTE: the derived classes USED to hold extra timers, due to the original+  // API having deadlines, but since those are gone, we're now always calling+  // this with extraCloses=0 at the moment. Should this change again in the+  // future however, this can be used again.+  void CloseUnsafe(int extraCloses) {+    tcp_.data = this;+    to_close_ = 1 + extraCloses;+    uv_close(reinterpret_cast<uv_handle_t*>(&tcp_),+             LibuvWrapperBase::LibuvCloseCB);+  }++  // We keep a counter on how many times this callback needs to be called+  // before we can actually delete the object. One of our derived objects may+  // contain more than one LibUV handle, which all need to have LibUV calling+  // us individually when it's safe to delete each of them. We shouldn't care+  // about the type of each of these handle, as long as they have been closed+  // with their respective API. The idea being we won't have one object per+  // LibUV handle, but rather group all of the needed handles into a single+  // derived class, that we will delete all at once.+  static void LibuvCloseCB(uv_handle_t* handle) {+    LibuvWrapperBase* self = reinterpret_cast<LibuvWrapperBase*>(handle->data);+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvWrapperBase:%p close CB, callbacks pending: %i"",+              self, self->to_close_ - 1);+    }+    if (--self->to_close_ == 0) {+      delete self;+    }+  }++  uv_tcp_t tcp_;++ private:+  int to_close_ = 0;+};++////////////////////////////////////////////////////////////////////////////////+/// The derived class to wrap a LibUV TCP listener handle. Its API will+/// closely match that of the Event Engine listener. Its only consumer should+/// be the LibuvListener class.+////////////////////////////////////////////////////////////////////////////////+class LibuvListenerWrapper final : public LibuvWrapperBase {+  friend class LibuvListener;++  LibuvListenerWrapper(+      grpc_event_engine::experimental::EventEngine::Listener::AcceptCallback+          on_accept,+      grpc_event_engine::experimental::EventEngine::Callback on_shutdown,+      const grpc_event_engine::experimental::EndpointConfig& args,+      std::unique_ptr<grpc_event_engine::experimental::SliceAllocatorFactory>+          slice_allocator_factory)+      : on_accept_(std::move(on_accept)),+        on_shutdown_(std::move(on_shutdown)),+        args_(args),+        slice_allocator_factory_(std::move(slice_allocator_factory)) {+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvListenerWrapper:%p created"", this);+    }+  }++  // Schedules closing of the listener. Should be called from the proxy class'+  // destructor. Once the closing is completed, this object will automatically+  // delf-delete from the base class.+  void Close(LibuvEventEngine* engine);++  // When LibUV is finally done with the listener, this destructor will be+  // called from the base class, which is when we can properly invoke the+  // shutdown callback.+  virtual ~LibuvListenerWrapper() { on_shutdown_(absl::OkStatus()); };++  grpc_event_engine::experimental::EventEngine::Listener::AcceptCallback+      on_accept_;+  grpc_event_engine::experimental::EventEngine::Callback on_shutdown_;+  grpc_event_engine::experimental::EndpointConfig args_;+  std::unique_ptr<grpc_event_engine::experimental::SliceAllocatorFactory>+      slice_allocator_factory_;+};++////////////////////////////////////////////////////////////////////////////////+/// The derived class to wrap a LibUV TCP connected handle. Its API will+/// closely match that of the Event Engine endpoint. Its only consumer should+/// be the LibuvEndpoint class.+////////////////////////////////////////////////////////////////////////////////+class LibuvEndpointWrapper final : public LibuvWrapperBase {+  friend class LibuvEndpoint;++  LibuvEndpointWrapper(+      const grpc_event_engine::experimental::EndpointConfig& args,+      std::unique_ptr<grpc_event_engine::experimental::SliceAllocator>+          slice_allocator);++  virtual ~LibuvEndpointWrapper() {+    GPR_ASSERT(write_bufs_ == nullptr);+    GPR_ASSERT(on_read_ == nullptr);+  };++  const grpc_event_engine::experimental::EndpointConfig args_;+  std::unique_ptr<grpc_event_engine::experimental::SliceAllocator>+      slice_allocator_;+  uv_write_t write_req_;+  uv_buf_t* write_bufs_ = nullptr;+  size_t write_bufs_count_ = 0;+  grpc_event_engine::experimental::SliceBuffer* read_sb_;+  grpc_event_engine::experimental::EventEngine::Callback on_writable_;+  grpc_event_engine::experimental::EventEngine::Callback on_read_;+  grpc_event_engine::experimental::EventEngine::ResolvedAddress peer_address_;+  grpc_event_engine::experimental::EventEngine::ResolvedAddress local_address_;+};++////////////////////////////////////////////////////////////////////////////////+/// This class is only a tiny, very temporary shell around the engine itself,+/// and doesn't hold any state of its own. It only exists at the moment because+/// of the need to implement the DNSResolver class in EventEngine. Maybe the+/// cares implementation will change this into a more fleshed out class.+////////////////////////////////////////////////////////////////////////////////+class LibuvDNSResolver final+    : public grpc_event_engine::experimental::EventEngine::DNSResolver {+ public:+  virtual ~LibuvDNSResolver() override = default;++  LibuvDNSResolver(LibuvEventEngine* engine) : engine_(engine) {}++ private:+  virtual LookupTaskHandle LookupHostname(LookupHostnameCallback on_resolve,+                                          absl::string_view address,+                                          absl::string_view default_port,+                                          absl::Time deadline) override;++  virtual LookupTaskHandle LookupSRV(LookupSRVCallback on_resolve,+                                     absl::string_view name,+                                     absl::Time deadline) override {+    // TODO(nnoble): implement on top of cares+    abort();+  }++  virtual LookupTaskHandle LookupTXT(LookupTXTCallback on_resolve,+                                     absl::string_view name,+                                     absl::Time deadline) override {+    // TODO(nnoble): implement on top of cares+    abort();+  }++  virtual void TryCancelLookup(LookupTaskHandle handle) override;++  LibuvEventEngine* engine_;+};++////////////////////////////////////////////////////////////////////////////////+/// The Event Engine Listener class. Aside from implementing the listener API,+/// it will only hold a pointer to a LibuvListenerWrapper.+///+/// It is a proxy class, and its main reason of existence is to transform its+/// destruction into scheduling a LibUV close() call of the underlying socket.+////////////////////////////////////////////////////////////////////////////////+class LibuvListener final+    : public grpc_event_engine::experimental::EventEngine::Listener {+ public:+  LibuvListener(+      Listener::AcceptCallback on_accept,+      grpc_event_engine::experimental::EventEngine::Callback on_shutdown,+      const grpc_event_engine::experimental::EndpointConfig& args,+      std::unique_ptr<grpc_event_engine::experimental::SliceAllocatorFactory>+          slice_allocator_factory);++  void RegisterUnsafe(LibuvEventEngine* engine) {+    uvTCP_->RegisterUnsafe(engine);+  }++  virtual ~LibuvListener() override;++ private:+  // The LibuvEventEngine pointer is tucked away into the uv_loop_t user data.+  // Retrieve this pointer and cast it back to the LibuvEventEngine pointer.+  LibuvEventEngine* GetEventEngine() {+    return reinterpret_cast<LibuvEventEngine*>(uvTCP_->GetLoop()->data);+  }++  virtual absl::StatusOr<int> Bind(+      const grpc_event_engine::experimental::EventEngine::ResolvedAddress& addr)+      override;++  virtual absl::Status Start() override;++  LibuvListenerWrapper* uvTCP_ = nullptr;+};++////////////////////////////////////////////////////////////////////////////////+/// The Event Engine Endpoint class. Aside from implementing the listener API,+/// it will hold a pointer to a LibuvEndpointWrapper, as well as the temporary+/// connection information. The initial connection information is held in it,+/// because of the limbo state the handle is in between the moment we are+/// requested to initiate the connection, and the moment we are actually+/// connected and giving back the endpoint to the callback. Other than that, it+/// essentially is a proxy class to LibuvEndpointWrapper.+///+/// It's separate from the LibuvEndpointWrapper in order to transform its+/// destruction into scheduling a LibUV close() call of the underlying socket.+////////////////////////////////////////////////////////////////////////////////+class LibuvEndpoint final+    : public grpc_event_engine::experimental::EventEngine::Endpoint {+ public:+  LibuvEndpoint(const grpc_event_engine::experimental::EndpointConfig& args,+                std::unique_ptr<grpc_event_engine::experimental::SliceAllocator>+                    slice_allocator)+      : uvTCP_(new LibuvEndpointWrapper(args, std::move(slice_allocator))) {+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvEndpoint:%p created"", this);+    }+    connect_.data = this;+  }++  absl::Status Connect(+      LibuvEventEngine* engine,+      grpc_event_engine::experimental::EventEngine::OnConnectCallback+          on_connect,+      const grpc_event_engine::experimental::EventEngine::ResolvedAddress&+          addr);++  void RegisterUnsafe(LibuvEventEngine* engine) {+    uvTCP_->RegisterUnsafe(engine);+  }++  // When a listener creates an endpoint for an incoming connection, we need to+  // accept it and register it in the libuv loop. This can only be called from+  // the libuv thread. This function can be expanded into fanning out the+  // incoming connection to multiple libuv loops in the future.+  //+  // Returns true if the accept was successful.+  bool AcceptUnsafe(LibuvEventEngine* engine, uv_stream_t* server) {+    RegisterUnsafe(engine);+    int r;+    r = uv_accept(server, reinterpret_cast<uv_stream_t*>(&uvTCP_->tcp_));+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvEndpoint@%p, accepting new connection: %i"", this,+              r);+    }+    if (r == 0) {+      // Not sure what to do here if we're failing. Maybe just abort? But that+      // seems extreme given it shouldn't be a fatal error.+      PopulateAddressesUnsafe();+      return true;+    }+    return false;+  }++  virtual ~LibuvEndpoint() override;++ private:+  // The LibuvEventEngine pointer is tucked away into the uv_loop_t user data.+  // Retrieve this pointer and cast it back to the LibuvEventEngine pointer.+  LibuvEventEngine* GetEventEngine() {+    return reinterpret_cast<LibuvEventEngine*>(uvTCP_->GetLoop()->data);+  }++  // Fills out the local and peer addresses of the connected socket. Needs to be+  // called from the libuv loop thread. Returns 0 if successful.+  int PopulateAddressesUnsafe() {+    auto populate =+        [this](+            std::function<int(const uv_tcp_t*, struct sockaddr*, int*)> f,+            grpc_event_engine::experimental::EventEngine::ResolvedAddress* a) {+          int namelen;+          sockaddr_storage addr;+          int ret =+              f(&uvTCP_->tcp_, reinterpret_cast<sockaddr*>(&addr), &namelen);+          *a = grpc_event_engine::experimental::EventEngine::ResolvedAddress(+              reinterpret_cast<sockaddr*>(&addr), namelen);+          return ret;+        };+    int r = 0;+    r |= populate(uv_tcp_getsockname, &uvTCP_->local_address_);+    r |= populate(uv_tcp_getpeername, &uvTCP_->peer_address_);+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvEndpoint@%p::populateAddresses, r=%d"", uvTCP_,+              r);+    }+    return r;+  }++  virtual void Read(+      grpc_event_engine::experimental::EventEngine::Callback on_read,+      grpc_event_engine::experimental::SliceBuffer* buffer) override;++  virtual void Write(+      grpc_event_engine::experimental::EventEngine::Callback on_writable,+      grpc_event_engine::experimental::SliceBuffer* data) override;++  virtual const grpc_event_engine::experimental::EventEngine::ResolvedAddress&+  GetPeerAddress() const override {+    return uvTCP_->peer_address_;+  };++  virtual const grpc_event_engine::experimental::EventEngine::ResolvedAddress&+  GetLocalAddress() const override {+    return uvTCP_->local_address_;+  };++  LibuvEndpointWrapper* uvTCP_ = nullptr;+  uv_connect_t connect_;+  grpc_event_engine::experimental::EventEngine::OnConnectCallback on_connect_ =+      nullptr;+};++////////////////////////////////////////////////////////////////////////////////+/// The LibUV Event Engine itself. It implements an EventEngine class.+////////////////////////////////////////////////////////////////////////////////+class LibuvEventEngine final+    : public grpc_event_engine::experimental::EventEngine {+  // Since libuv is single-threaded and not thread-safe, we will be running+  // all operations in a multi-producer/single-consumer manner, where all of the+  // surface API of the EventEngine will only just schedule work to be executed+  // on the libuv thread. This structure holds one of these piece of work to+  // execute. Each ""work"" is just a standard functor that takes the+  // LibuvEventEngine pointer as an argument, in an attempt to lower capture+  // costs.+  struct SchedulingRequest : grpc_core::MultiProducerSingleConsumerQueue::Node {+    typedef std::function<void(LibuvEventEngine*)> functor;+    SchedulingRequest(functor&& f) : f_(std::move(f)) {}+    functor f_;+  };++ public:+  LibuvEventEngine() {+    bool success = false;+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvEventEngine:%p created"", this);+    }+    // Creating the libuv loop thread straight upon construction. We don't need+    // the thread to be joinable, since it'll exit on its own gracefully without+    // needing for us to wait on it.+    grpc_core::Thread::Options options;+    options.set_joinable(false);+    // Why isn't grpc_core::Thread accepting a lambda?+    thread_ = grpc_core::Thread(+        ""uv loop"",+        [](void* arg) {+          LibuvEventEngine* engine = reinterpret_cast<LibuvEventEngine*>(arg);+          engine->Thread();+        },+        this, &success, options);+    thread_.Start();+    GPR_ASSERT(success);+    // This promise will be set to true once the thread has fully started and is+    // operational, so let's wait on it.+    success = ready_.Get();+    GPR_ASSERT(success);+  }++  // Schedules one lambda to be executed on the libuv thread. Our libuv loop+  // will have a special async event which is the only piece of API that's+  // marked as thread-safe.+  void Schedule(SchedulingRequest::functor&& f) {+    SchedulingRequest* request = new SchedulingRequest(std::move(f));+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_ERROR, ""LibuvEventEngine@%p::Schedule, created %p"", this,+              request);+    }+    queue_.Push(request);+    uv_async_send(&kicker_);+  }++  uv_loop_t* GetLoop() { return &loop_; }++ private:+  // This is the callback that libuv will call on its thread once the+  // uv_async_send call above is being processed. The kick is only guaranteed to+  // be called once per loop iteration, even if we sent the event multiple+  // times, so we have to process as many events from the queue as possible.+  void Kicker() {+    bool empty = false;+    while (!empty) {+      SchedulingRequest* node =+          reinterpret_cast<SchedulingRequest*>(queue_.PopAndCheckEnd(&empty));+      if (!node) continue;+      SchedulingRequest::functor f = std::move(node->f_);+      if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+        gpr_log(GPR_ERROR, ""LibuvEventEngine@%p::Kicker, got %p"", this, node);+      }+      delete node;+      f(this);+    }+  }++  virtual bool IsWorkerThread() override {+    return worker_thread_id_ == std::this_thread::get_id();+  }++  void Thread() {+    // Ugh. LibUV doesn't take upon itself to mask SIGPIPE on its own on Unix+    // systems. If a connection gets broken, we will get killed, unless we mask+    // it out. These 4 lines of code likely need to be enclosed in a non-Windows+    // #ifdef check, although I'm not certain what platforms will or will not+    // have the necessary function calls here.+    sigset_t set;+    sigemptyset(&set);+    sigaddset(&set, SIGPIPE);+    pthread_sigmask(SIG_BLOCK, &set, NULL);++    // Setting up the loop.+    int r = 0;+    worker_thread_id_ = std::this_thread::get_id();+    r = uv_loop_init(&loop_);+    loop_.data = this;+    r |= uv_async_init(&loop_, &kicker_, [](uv_async_t* async) {+      LibuvEventEngine* engine =+          reinterpret_cast<LibuvEventEngine*>(async->loop->data);+      engine->Kicker();+    });+    if (r != 0) {+      if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+        gpr_log(GPR_ERROR, ""LibuvEventEngine@%p::Thread, failed to start: %i"",+                this, r);+      }+      ready_.Set(false);+      return;+    }+    ready_.Set(true);++    // The meat of running our event loop. We need the various exec contexts,+    // because some of the callbacks we will call will depend on them existing.+    //+    // Calling uv_run with UV_RUN_ONCE will stall until there is any sort of+    // event to process whatsoever, and then will return 0 if it needs to+    // shutdown. The libuv loop will shutdown naturally when there's no more+    // event to process. Since we created the async event for kick, there will+    // always be at least one event holding the loop, until we explicitly weaken+    // its reference to permit a graceful shutdown.+    //+    // When/if there are no longer any sort of exec contexts needed to flush,+    // then we can simply use UV_RUN instead, which will never return until+    // there's no more events to process, which is even more graceful.+    grpc_core::ApplicationCallbackExecCtx callback_exec_ctx;+    grpc_core::ExecCtx ctx;+    while (uv_run(&loop_, UV_RUN_ONCE) != 0) {+      if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+        gpr_log(GPR_ERROR,+                ""LibuvEventEngine@%p::Thread, uv_run requests a ""+                ""context flush"",+                this);+      }+      ctx.Flush();+    }++    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvEventEngine@%p::Thread, shutting down"", this);+    }+    on_shutdown_complete_(absl::OkStatus());+  }++  virtual absl::StatusOr<std::unique_ptr<Listener>> CreateListener(+      Listener::AcceptCallback on_accept, Callback on_shutdown,+      const grpc_event_engine::experimental::EndpointConfig& args,+      std::unique_ptr<grpc_event_engine::experimental::SliceAllocatorFactory>+          slice_allocator_factory) override {+    std::unique_ptr<LibuvListener> ret = absl::make_unique<LibuvListener>(+        on_accept, on_shutdown, args, std::move(slice_allocator_factory));+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvEventEngine@%p::CreateListener, created %p"",+              this, ret.get());+    }+    // Scheduling should have a guarantee on ordering. We don't need to wait+    // until this is finished to return. Any subsequent call on the returned+    // listener will schedule more work after this one.+    Schedule([&ret](LibuvEventEngine* engine) { ret->RegisterUnsafe(engine); });+    return ret;+  }++  virtual absl::Status Connect(+      OnConnectCallback on_connect, const ResolvedAddress& addr,+      const grpc_event_engine::experimental::EndpointConfig& args,+      std::unique_ptr<grpc_event_engine::experimental::SliceAllocator>+          slice_allocator,+      absl::Time deadline) override {+    LibuvEndpoint* e = new LibuvEndpoint(args, std::move(slice_allocator));+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvEventEngine@%p::Connect, created %p"", this, e);+    }+    return e->Connect(this, std::move(on_connect), addr);+  }++  virtual ~LibuvEventEngine() override {}++  virtual absl::StatusOr<std::unique_ptr<+      grpc_event_engine::experimental::EventEngine::DNSResolver>>+  GetDNSResolver() override {+    return absl::make_unique<LibuvDNSResolver>(this);+  }++  virtual TaskHandle Run(Callback fn, RunOptions opts) override;+  virtual TaskHandle RunAt(absl::Time when, Callback fn,+                           RunOptions opts) override;+  virtual void TryCancel(TaskHandle handle) override;++  virtual void Shutdown(Callback on_shutdown_complete) override {+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvEventEngine@%p::Shutdown"", this);+    }+    on_shutdown_complete_ = on_shutdown_complete;+    Schedule([](LibuvEventEngine* engine) {+      if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+        gpr_log(GPR_DEBUG,+                ""LibuvEventEngine@%p shutting down, unreferencing ""+                ""Kicker now"",+                engine);+      }+      // Shutting down at this point is essentially just this unref call here.+      // After it, the libuv loop will continue working until it has no more+      // events to monitor. It means that scheduling new work becomes+      // essentially undefined behavior, which is in line with our surface API+      // contracts, which stipulate the same thing.+      uv_unref(reinterpret_cast<uv_handle_t*>(&engine->kicker_));+      if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+        // This is an unstable API from libuv that we use for its intended+        // purpose: debugging. This can tell us if there's lingering handles+        // that are still going to hold up the loop at this point.+        uv_walk(+            &engine->loop_,+            [](uv_handle_t* handle, void* arg) {+              uv_handle_type type = uv_handle_get_type(handle);+              const char* name = uv_handle_type_name(type);+              gpr_log(GPR_DEBUG,+                      ""in shutdown, handle %p type %s has references: %s"",+                      handle, name, uv_has_ref(handle) ? ""yes"" : ""no"");+            },+            nullptr);+      }+    });+  }++  // Helpers for the internal Task classes.+  void EraseTask(intptr_t taskKey);+  void EraseLookupTask(intptr_t taskKey);++  uv_loop_t loop_;+  uv_async_t kicker_;+  // This should be set only once to true by the thread when it's done setting+  // itself up.+  grpc_event_engine::experimental::Promise<bool> ready_;+  grpc_core::Thread thread_;+  grpc_core::MultiProducerSingleConsumerQueue queue_;++  // We keep a list of all of the tasks here. The atomics will serve as a simple+  // counter mechanism, with the assumption that if it ever rolls over, the+  // colliding tasks will have long been completed.+  //+  // NOTE: now that we're returning two intptr_t instead of just one for the+  // keys, this can be improved, as we can hold the pointer in one+  // key, and a tag in the other, to avoid the ABA problem. We'll keep the+  // atomics as tags in the second key slot, but we can get rid of the maps.+  //+  // TODO(nnoble): remove the maps, and fold the pointers into the keys,+  // alongside the ABA tag.+  std::atomic<intptr_t> taskKey_;+  std::atomic<intptr_t> lookupTaskKey_;+  std::unordered_map<intptr_t, LibuvTask*> taskMap_;+  std::unordered_map<intptr_t, LibuvLookupTask*> lookupTaskMap_;+  grpc_event_engine::experimental::EventEngine::Callback on_shutdown_complete_;++  // Hopefully temporary until we can solve shutdown from the main grpc code.+  // Used by IsWorkerThread.+  std::thread::id worker_thread_id_;++  friend class LibuvDNSResolver;+  friend class LibuvLookupTask;+  friend class LibuvTask;+};++////////////////////////////////////////////////////////////////////////////////+/// The LibuvTask class will be used for Run and RunAt from LibuvEventEngine,+/// and will be internally what's allocated for the returned TaskHandle.+///+/// Its API is to be used solely by the Run and RunAt functions, while in the+/// libuv loop thread.+////////////////////////////////////////////////////////////////////////////////+class LibuvTask {+ public:+  LibuvTask(LibuvEventEngine* engine,+            grpc_event_engine::experimental::EventEngine::Callback&& fn)+      : fn_(std::move(fn)), key_(engine->taskKey_.fetch_add(1)) {+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvTask@%p, created: key = %"" PRIiPTR, this, key_);+    }+    timer_.data = this;+  }++  void StartUnsafe(LibuvEventEngine* engine, uint64_t timeout) {+    uv_timer_init(&engine->loop_, &timer_);+    uv_timer_start(+        &timer_,+        [](uv_timer_t* timer) {+          LibuvTask* task = reinterpret_cast<LibuvTask*>(timer->data);+          if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+            gpr_log(GPR_DEBUG, ""LibuvTask@%p, triggered: key = %"" PRIiPTR, task,+                    task->Key());+          }+          task->CancelUnsafe();+          task->triggered_ = true;+          task->fn_(absl::OkStatus());+        },+        timeout, 0);+  }++  void TryCancelUnsafe() {+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvTask@%p, cancelled: key = %"" PRIiPTR, this,+              key_);+    }+    CancelUnsafe();+    if (!triggered_) {+      fn_(absl::CancelledError());+    }+  }++  // This one will be used by both TryCancel and the completion callback.+  void CancelUnsafe() {+    if (uv_is_closing(reinterpret_cast<uv_handle_t*>(&timer_))) {+      return;+    }+    uv_timer_stop(&timer_);+    uv_close(reinterpret_cast<uv_handle_t*>(&timer_), [](uv_handle_t* handle) {+      uv_timer_t* timer = reinterpret_cast<uv_timer_t*>(handle);+      LibuvTask* task = reinterpret_cast<LibuvTask*>(timer->data);+      LibuvEventEngine* engine =+          reinterpret_cast<LibuvEventEngine*>(timer->loop->data);+      engine->EraseTask(task->key_);+    });+  }++  intptr_t Key() { return key_; }++ private:+  grpc_event_engine::experimental::EventEngine::Callback fn_;+  bool triggered_ = false;+  uv_timer_t timer_;+  const intptr_t key_;+};++////////////////////////////////////////////////////////////////////////////////+/// The LibuvLookupTask class will be used exclusively by the LibuvEventEngine's+/// resolver functions.+///+/// Its API is to be used solely while in the libuv loop thread.+////////////////////////////////////////////////////////////////////////////////+class LibuvLookupTask {+ public:+  LibuvLookupTask(LibuvEventEngine* engine,+                  grpc_event_engine::experimental::EventEngine::DNSResolver::+                      LookupHostnameCallback on_resolve,+                  absl::string_view address, absl::string_view default_port)+      : key_(engine->lookupTaskKey_.fetch_add(1)),+        on_resolve_(std::move(on_resolve)) {+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LibuvLookupTask@%p, created: key = %"" PRIiPTR, this,+              key_);+    }+    req_.data = this;+    timer_.data = this;+    if (!grpc_core::SplitHostPort(address, &address_, &default_port_)) {+      address_ = std::string(address);+      default_port_ = std::string(default_port);+    }+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LookupHostname for %s:%s"", address_.c_str(),+              default_port_.c_str());+    }+  }++  intptr_t Key() { return key_; }++  void StartUnsafe(LibuvEventEngine* engine, absl::Time deadline) {+    const char* const ccaddress = address_.c_str();+    const char* const ccdefault_port = default_port_.c_str();+    // Start resolving, and if successful, call the main resolver callback code.+    int r = uv_getaddrinfo(+        &engine->loop_, &req_,+        [](uv_getaddrinfo_t* req, int status, struct addrinfo* res) {+          LibuvLookupTask* task = reinterpret_cast<LibuvLookupTask*>(req->data);+          task->LibuvResolverCallback(status, res);+        },+        ccaddress, ccdefault_port, nullptr);+    // If we weren't successful in starting the resolution, our callback will+    // never be called, so we can simply fast-abort now.+    if (r != 0) {+      auto on_resolve = std::move(on_resolve_);+      engine->EraseLookupTask(key_);+      on_resolve(absl::UnknownError(""Resolution error""));+      if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+        gpr_log(GPR_DEBUG, ""LookupHostname for %s:%s failed early with %i"",+                address_.c_str(), default_port_.c_str(), r);+      }+      return;+    }+    // Otherwise, set our timer, and request a cancellation of the lookup. The+    // uv_cancel call will fail if the call already finished or was already+    // canceled, which is fine with us, and we don't need take any further+    // action.+    uv_timer_init(&engine->loop_, &timer_);+    absl::Duration timeout = deadline - absl::Now();+    uv_timer_start(+        &timer_,+        [](uv_timer_t* timer) {+          LibuvLookupTask* task =+              reinterpret_cast<LibuvLookupTask*>(timer->data);+          task->deadline_exceeded_ = true;+          uv_cancel(reinterpret_cast<uv_req_t*>(&task->req_));+        },+        timeout / absl::Milliseconds(1), 0);+  }++  void CancelUnsafe(LibuvEventEngine* engine) {+    uv_timer_stop(&timer_);+    uv_cancel(reinterpret_cast<uv_req_t*>(&req_));+  }++ private:+  std::string address_;+  std::string default_port_;+  uv_getaddrinfo_t req_;+  uv_timer_t timer_;+  bool deadline_exceeded_ = false;+  const intptr_t key_;+  grpc_event_engine::experimental::EventEngine::DNSResolver::+      LookupHostnameCallback on_resolve_;++  // Unlike other callbacks in this code, this one is guaranteed to only be+  // called once by libuv.+  void LibuvResolverCallback(int status, struct addrinfo* res) {+    uv_timer_stop(&timer_);+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG, ""LookupHostname for %s:%s completed with status = %i"",+              address_.c_str(), default_port_.c_str(), status);+    }+    uv_close(reinterpret_cast<uv_handle_t*>(&timer_), [](uv_handle_t* timer) {+      LibuvLookupTask* task = reinterpret_cast<LibuvLookupTask*>(timer->data);+      LibuvEventEngine* engine =+          reinterpret_cast<LibuvEventEngine*>(timer->loop->data);+      engine->EraseLookupTask(task->key_);+    });+    if (status == UV_ECANCELED) {+      if (deadline_exceeded_) {+        on_resolve_(absl::DeadlineExceededError(""Deadline exceeded""));+      } else {+        on_resolve_(absl::CancelledError());+      }+    } else if (status != 0) {+      on_resolve_(+          absl::UnknownError(""uv_getaddrinfo failed with an unknown error""));+    } else {+      struct addrinfo* p;+      std::vector<grpc_event_engine::experimental::EventEngine::ResolvedAddress>+          ret;++      for (p = res; p != nullptr; p = p->ai_next) {+        ret.emplace_back(p->ai_addr, p->ai_addrlen);+      }++      uv_freeaddrinfo(res);+      on_resolve_(std::move(ret));+    }+  }+};++// Some implementation details require circular dependencies between classes, so+// we're implementing them here instead.+absl::Status LibuvEndpoint::Connect(+    LibuvEventEngine* engine,+    grpc_event_engine::experimental::EventEngine::OnConnectCallback on_connect,+    const grpc_event_engine::experimental::EventEngine::ResolvedAddress& addr) {+  on_connect_ = std::move(on_connect);+  engine->Schedule([addr, this](LibuvEventEngine* engine) {+    uvTCP_->RegisterUnsafe(engine);+    int r = uv_tcp_connect(+        &connect_, &uvTCP_->tcp_, addr.address(),+        [](uv_connect_t* req, int status) {+          LibuvEndpoint* epRaw = reinterpret_cast<LibuvEndpoint*>(req->data);+          std::unique_ptr<LibuvEndpoint> ep(epRaw);+          auto on_connect = std::move(ep->on_connect_);+          if (status == 0) {+            ep->PopulateAddressesUnsafe();+            if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+              gpr_log(GPR_DEBUG, ""LibuvEndpoint@%p::Connect, success"",+                      epRaw->uvTCP_);+            }+            on_connect(std::move(ep));+          } else {+            if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+              gpr_log(GPR_INFO, ""LibuvEndpoint@%p::Connect, failed: %i"",+                      epRaw->uvTCP_, status);+            }+            on_connect(absl::UnknownError(+                ""uv_tcp_connect gave us an asynchronous error""));+          }+        });+    // If we fail the call to uv_connect, the handle won't even be monitored by+    // the libuv, and won't need to be closed (it won't be in an opened state).+    // We can bail early here.+    if (r != 0) {+      auto on_connect = std::move(on_connect_);+      if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+        gpr_log(GPR_INFO, ""LibuvEndpoint@%p::Connect, failed: %i"", uvTCP_, r);+      }+      delete this;+      on_connect(absl::UnknownError(""uv_tcp_connect gave us an error""));+    }+  });+  return absl::OkStatus();+}++void LibuvWrapperBase::RegisterUnsafe(LibuvEventEngine* engine) {+  auto success = uv_tcp_init(engine->GetLoop(), &tcp_);+  GPR_ASSERT(success == 0);+}++LibuvEndpointWrapper::LibuvEndpointWrapper(+    const grpc_event_engine::experimental::EndpointConfig& args,+    std::unique_ptr<grpc_event_engine::experimental::SliceAllocator>+        slice_allocator)+    : args_(args), slice_allocator_(std::move(slice_allocator)) {+  write_req_.data = this;+}++LibuvListener::LibuvListener(+    Listener::AcceptCallback on_accept,+    grpc_event_engine::experimental::EventEngine::Callback on_shutdown,+    const grpc_event_engine::experimental::EndpointConfig& args,+    std::unique_ptr<grpc_event_engine::experimental::SliceAllocatorFactory>+        slice_allocator_factory)+    : uvTCP_(new LibuvListenerWrapper(on_accept, on_shutdown, args,+                                      std::move(slice_allocator_factory))) {+  uvTCP_->tcp_.data = uvTCP_;+}++LibuvListener::~LibuvListener() { uvTCP_->Close(GetEventEngine()); }++absl::StatusOr<int> LibuvListener::Bind(","Just leaving this comment here to seek some clarification. As per my understanding, the Bind method can be called multiple times with different addresses but it seems like there is only one uv_tcp_t handle per LibuvListener here. Not sure if libuv internally creates and maintains multiple sockets per uv_tcp_t handle or if there should be a linked list of uv_tcp_t handles for each LibuvListener",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27240,701470886,2021-09-02T22:46:47Z,src/python/grpcio/grpc/aio/_call.py,"@@ -406,7 +406,7 @@ async def _consume_request_iterator(                 async for request in request_iterator:                     try:                         await self._write(request)-                    except AioRpcError as rpc_error:+                    except (AioRpcError, cygrpc.InternalError) as rpc_error:",Can you share more information about why we should catch the exception here? Can you add a test case to demonstrate the fixing scenario?Here is a unit test file that I think might be helpful:  https://github.com/grpc/grpc/blob/999e36fb31ca13ef467f3b7327a282a1bc4353e1/src/python/grpcio_tests/tests_aio/unit/call_test.py#L652,
148994,clayg,https://api.github.com/repos/grpc/grpc/pulls/27240,701924349,2021-09-03T14:13:04Z,src/python/grpcio/grpc/aio/_call.py,"@@ -406,7 +406,7 @@ async def _consume_request_iterator(                 async for request in request_iterator:                     try:                         await self._write(request)-                    except AioRpcError as rpc_error:+                    except (AioRpcError, cygrpc.InternalError) as rpc_error:","the reason it should be caught here is becaues it is almost definately NOT an exception from the iterator - it's an exception from the write.I can't find anything in call_test that accurately represents a server error.  I think it would be useful if we had such a test harness framework - the cloest I found was in server_test, so I can pursue that further if your not able to point me at a better example.The reason we need to catch this here is so that we don't catch it below in the bare except.  When the server errors (i.e. server code hits an Exception like a AttributeError because I'm bad at software), the await call site gets a nice error response with details filled in (good error handling server!) - but the background task here, _async_request_poller, is still running.  As soon as it tries to _write it finds that the server has closed the connection and the cython raises ExecuteBatchError which inhrets from InternalError.In the current code, I'd argue it is NOT appropriate to catch that error below, log it as `Client request_iterator raised exception` and calling `self.cancel()` can lead to a lot of confusion if the coro waiting on the call hadn't already recieved the error response yet (i.e. in a race, _write throwing an exception could happen before the response handler gets the error response, but since we've already called cancel things *really* get messed up).  Instead logging the generic error the same way we'd previously handled the AioRpcError seems benign and in all cases I can functionally test prevents tracebacks on shutdown (we still get the debug error message in logging; but I didn't change that behavior).",
26072277,dfawley,https://api.github.com/repos/grpc/grpc/pulls/27216,702015118,2021-09-03T16:08:17Z,doc/PROTOCOL-HTTP2.md,"@@ -229,6 +239,25 @@ CONNECT_ERROR|INTERNAL ENHANCE_YOUR_CALM|RESOURCE_EXHAUSTED ...with additional error detail provided by runtime to indicate that the exhausted resource is bandwidth. INADEQUATE_SECURITY| PERMISSION_DENIED … with additional detail indicating that permission was denied as protocol is not secure enough for call. +##### Client Cancellation++Client cancellation is performed by the client sending RST_STREAM with CANCEL+error code. The RPC's deadline expiring is considered a client cancellation.+Client implementations are entitled to fail calls with DEADLINE_EXCEEDED at any+point after the deadline expires, even if doing so would throw away messages and+trailers.++gRPC servers track the deadline of the RPC as communicated by **Timeout**. When+the server detects the client's deadline has expired (which implies it hasn't+received the client's RST_STREAM yet) the server should locally cancel the RPC+and send RST_STREAM with CANCEL error code. Since the server is separated from+the client by a round-trip time, the client will generally not need to process+the server's RST_STREAM. However, to deal with client-side races and minor+clock-rate skew, when a call fails with CANCELLED status the client is+encouraged to check whether the deadline has expired and fail the call with+DEADLINE_EXCEEDED instead of CANCELLED. Note that the client must fully-generate+the failure locally, without using any part of the server's failure (i.e.,+response metadata from server must be discarded).","""trailing"" response metadata / trailers?",X
4111145,YifeiZhuang,https://api.github.com/repos/grpc/grpc/pulls/27223,702035479,2021-09-03T16:43:04Z,tools/run_tests/run_xds_tests.py,"@@ -2554,6 +2554,7 @@ def create_url_map(gcp, name, backend_service, host_name):         project=gcp.project, body=config).execute(num_retries=_GCP_API_RETRIES)     wait_for_global_operation(gcp, result['name'])     gcp.url_map = GcpResource(config['name'], result['targetLink'])+    gcp.url_maps.append(gcp.url_map)","Yea I saw your comment. :) I removed gcp.url_map and replace it with gcp.url_maps, same as targe_proxy and forwarding _rules, and allow specifying which dependency resource to use when creating a resource. And we have to be careful about the index because they have to be fully aligned.",
4111145,YifeiZhuang,https://api.github.com/repos/grpc/grpc/pulls/27223,702070197,2021-09-03T17:46:27Z,tools/run_tests/run_xds_tests.py,"@@ -2554,6 +2554,7 @@ def create_url_map(gcp, name, backend_service, host_name):         project=gcp.project, body=config).execute(num_retries=_GCP_API_RETRIES)     wait_for_global_operation(gcp, result['name'])     gcp.url_map = GcpResource(config['name'], result['targetLink'])+    gcp.url_maps.append(gcp.url_map)","Previously there was a set of global `gcp.url_map` `gcp.target_proxy` and `gcp.forwarding_rule` resources that was referenced with each other. If we replace them with a list because we want to track them all, then the first one in the list will be the default one - for the test case that only care about one set global resources, and we allow specifying which dependency resource to be used - for the test cases that need multiple set of urlmap+tp+fr.This is different from backend_services, because they don't get involved in the dependency chain nor do they need to be deleted for the test verification.I actually feel like the previous version had its advantage but whatever.",
303201,JamesNK,https://api.github.com/repos/grpc/grpc/pulls/27216,702214333,2021-09-04T01:18:57Z,doc/PROTOCOL-HTTP2.md,"@@ -229,6 +239,24 @@ CONNECT_ERROR|INTERNAL ENHANCE_YOUR_CALM|RESOURCE_EXHAUSTED ...with additional error detail provided by runtime to indicate that the exhausted resource is bandwidth. INADEQUATE_SECURITY| PERMISSION_DENIED … with additional detail indicating that permission was denied as protocol is not secure enough for call. +##### Client Cancellation","nit: Only the first sentence is about cancellation, with the rest covering deadlines.While they are related, maybe there should be a separate deadline section. Or the first sentence that is specific to cancellation be its own paragraph.",X
303201,JamesNK,https://api.github.com/repos/grpc/grpc/pulls/27216,702215128,2021-09-04T01:26:34Z,doc/PROTOCOL-HTTP2.md,"@@ -205,6 +205,16 @@ All GRPC calls need to specify an internal ID. We will use HTTP2 stream-ids as c ##### Data Frames DATA frame boundaries have no relation to **Length-Prefixed-Message** boundaries and implementations should make no assumptions about their alignment. +##### Early Server Closure++If a server responds with **Trailers** before the client's END_STREAM, then the+server should send RST_STREAM with NO_ERROR error code immediately following+that HEADERS frame as suggested by RFC 7540 Section 8.1 to allow the stream to+be deallocated. Similarly, when a client receives **Trailers** before the+client's END_STREAM, then the client should send RST_STREAM with NO_ERROR error+code to allow the stream to be deallocted. This is done on both client and+server because the remote behavior cannot be guaranteed.","Is the client sending RST_STREAM in this situation correct? Typically the server would have sent RST_STREAM to the client if the server finishes the response without reading to the end of the request stream. A client shouldn't send an RST_STREAM in response to an RST_STREAM (to avoid recursive loop)https://datatracker.ietf.org/doc/html/rfc7540#section-6.4>    The RST_STREAM frame fully terminates the referenced stream and>    causes it to enter the ""closed"" state.  After receiving a RST_STREAM>    on a stream, the receiver MUST NOT send additional frames for that>    stream, with the exception of PRIORITY.  However, after sending the>    RST_STREAM, the sending endpoint MUST be prepared to receive and>    process additional frames sent on the stream that might have been>    sent by the peer prior to the arrival of the RST_STREAM.I think the content covered here is standard HTTP/2 behavior. Probably best to leave it to the HTTP/2 spec to avoid the gRPC spec getting it wrong.",
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27254,702643729,2021-09-06T07:06:56Z,src/core/ext/transport/binder/transport/binder_transport.cc,"@@ -157,21 +157,25 @@ static void perform_stream_op(grpc_transport* gt, grpc_stream* gs,   }   if (op->send_message && gbs->cancellation_error.ok()) {     gpr_log(GPR_INFO, ""send_message"");-    grpc_slice s;-    bool next_result =-        op->payload->send_message.send_message->Next(SIZE_MAX, nullptr);-    gpr_log(GPR_INFO, ""next_result = %d"", static_cast<int>(next_result));-    op->payload->send_message.send_message->Pull(&s);-    auto* p = GRPC_SLICE_START_PTR(s);-    int len = GRPC_SLICE_LENGTH(s);-    std::string message_data(reinterpret_cast<char*>(p), len);+    size_t remaining = op->payload->send_message.send_message->length();+    std::string message_data;+    do {+      grpc_slice message_slice;+      GPR_ASSERT(+          op->payload->send_message.send_message->Next(SIZE_MAX, nullptr));+      op->payload->send_message.send_message->Pull(&message_slice);","> Next() should returns true because we know that there's still messages to comehttps://github.com/grpc/grpc/blob/ca945a58e9ccff69df9815bdb657c4b32e2e6b37/src/core/lib/transport/byte_stream.h#L45-L49 Says ""false if the bytes will be available asynchronously"" so I'm not sure if that case can happen or not> I think remaining should also be positive in the beginning because why sending an empty message?IIRC there is a special test case that tests if the transport supports sending empty message? Change your do-while loop to while loop should be enough to account for that situation",
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27260,703134490,2021-09-07T02:30:11Z,src/core/ext/transport/binder/wire_format/binder_android.cc,"@@ -243,36 +280,18 @@ absl::Status ReadableParcelAndroid::ReadBinder(   return absl::OkStatus(); } -namespace {--bool byte_array_allocator(void* arrayData, int32_t length, int8_t** outBuffer) {-  std::string tmp;-  tmp.resize(length);-  *reinterpret_cast<std::string*>(arrayData) = tmp;-  *outBuffer = reinterpret_cast<int8_t*>(-      &(*reinterpret_cast<std::string*>(arrayData))[0]);-  return true;-}--bool string_allocator(void* stringData, int32_t length, char** outBuffer) {-  if (length > 0) {-    // TODO(mingcl): Don't fix the length of the string-    GPR_ASSERT(length < 100);  // call should preallocate 100 bytes-    *outBuffer = reinterpret_cast<char*>(stringData);-  }-  return true;-}--}  // namespace- absl::Status ReadableParcelAndroid::ReadByteArray(std::string* data) const {-  return AParcel_readByteArray(parcel_, data, byte_array_allocator) == STATUS_OK-             ? absl::OkStatus()-             : absl::InternalError(""AParcel_readByteArray failed"");+  std::vector<uint8_t> vec;+  if (AParcelReadVector(parcel_, &vec) == STATUS_OK) {+    data->resize(vec.size());+    memcpy(&((*data)[0]), &vec[0], vec.size());",Can we use `vec.data()` here?,X
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27259,703149357,2021-09-07T03:22:46Z,examples/android/binder/java/io/grpc/binder/cpp/exampleserver/native.cc,"@@ -60,11 +58,10 @@ Java_io_grpc_binder_cpp_exampleserver_ExportedEndpointService_init_1grpc_1server   grpc::ServerBuilder server_builder;   server_builder.RegisterService(&service); -  // TODO(mingcl): Uncomment this after server interfaces are merged-  //-  // grpc_endpoint_binder_pool_init();-  // server_builder.AddListeningPort(""binder://example.service"",-  // grpc::experimental::BinderServerCredentials());+  grpc_endpoint_binder_pool_init();","> Why is that an issue?What I mean is that if both the mutex and the object are pointers that need to be heap-allocated for the first time we enter the constructor, then how are we gonna handle races that happens while we're initializing these two things? We can use an atomic integer to make sure they are only initialized once, but I'm not sure how to make others that come during the initialization wait until the initialization is over (of course we can have another atomic flag and all other threads just keep reading the flag until it is turned on, but it seems quite bad).",
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27259,703155635,2021-09-07T03:45:35Z,examples/android/binder/java/io/grpc/binder/cpp/exampleserver/native.cc,"@@ -60,11 +58,10 @@ Java_io_grpc_binder_cpp_exampleserver_ExportedEndpointService_init_1grpc_1server   grpc::ServerBuilder server_builder;   server_builder.RegisterService(&service); -  // TODO(mingcl): Uncomment this after server interfaces are merged-  //-  // grpc_endpoint_binder_pool_init();-  // server_builder.AddListeningPort(""binder://example.service"",-  // grpc::experimental::BinderServerCredentials());+  grpc_endpoint_binder_pool_init();",Initialization of block-scoped static variable is thread safe (See https://www.modernescpp.com/index.php/thread-safe-initialization-of-data and https://stackoverflow.com/a/8102145),
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27094,703194206,2021-09-07T05:50:03Z,src/core/ext/transport/binder/wire_format/transaction.h,"@@ -34,34 +37,69 @@ ABSL_CONST_INIT extern const int kFlagExpectSingleMessage; ABSL_CONST_INIT extern const int kFlagStatusDescription; ABSL_CONST_INIT extern const int kFlagMessageDataIsParcelable; -using Metadata = std::vector<std::pair<std::string, std::string>>;+struct KeyValuePair {",We can use the `MetadataMap` introduced in https://github.com/grpc/grpc/pull/27262.,X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27094,703575224,2021-09-07T14:41:49Z,src/core/ext/transport/binder/wire_format/transaction.h,"@@ -34,34 +37,69 @@ ABSL_CONST_INIT extern const int kFlagExpectSingleMessage; ABSL_CONST_INIT extern const int kFlagStatusDescription; ABSL_CONST_INIT extern const int kFlagMessageDataIsParcelable; -using Metadata = std::vector<std::pair<std::string, std::string>>;+struct KeyValuePair {",That's probably the right thing to use (noting grpc_metadata_batch is-a MetadataMap. I'm trying to move our metadata apis fairly radically so my advice would be to try and stick with what's available there and I'll try and iterate you to the new world.,
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/27216,703752982,2021-09-07T18:53:23Z,doc/PROTOCOL-HTTP2.md,"@@ -205,6 +205,16 @@ All GRPC calls need to specify an internal ID. We will use HTTP2 stream-ids as c ##### Data Frames DATA frame boundaries have no relation to **Length-Prefixed-Message** boundaries and implementations should make no assumptions about their alignment. +##### Early Server Closure++If a server responds with **Trailers** before the client's END_STREAM, then the+server should send RST_STREAM with NO_ERROR error code immediately following+that HEADERS frame as suggested by RFC 7540 Section 8.1 to allow the stream to+be deallocated. Similarly, when a client receives **Trailers** before the+client's END_STREAM, then the client should send RST_STREAM with NO_ERROR error+code to allow the stream to be deallocted. This is done on both client and+server because the remote behavior cannot be guaranteed.","> Typically the server would have sent RST_STREAM to the client if the server finishes the response without reading to the end of the request stream.We'd _hope_ that is the server's behavior, but it isn't guaranteed. Quite annoyingly, the RST_STREAM from the server is a _MAY_. RFC 7540 §8.1:> An HTTP response is complete after the server sends — or the client receives — a frame with the END_STREAM flag set (including any CONTINUATION frames needed to complete a header block). A server can send a complete response prior to the client sending an entire request if the response does not depend on any portion of the request that has not been sent and received. When this is true, a server MAY request that the client abort transmission of a request without error by sending a RST_STREAM with an error code of NO_ERROR after sending a complete response (i.e., a frame with the END_STREAM flag).> A client shouldn't send an RST_STREAM in response to an RST_STREAM (to avoid recursive loop)Definitely. But that's not what I said here. I said it is response to ""Trailers"", which implies END_STREAM. That would be before the RST_STREAM is received/processed. It'd be _really nifty_ to avoid the RST_STREAM from the client, but there doesn't seem to be a easy way to send it only when necessary.I agree this is really an HTTP/2 issue, but we know the expected semantics, the RFC really is not clear how it should work, and it has bit each of Java/Go/C. Even with proper grpc servers, proxies still cause problems.",
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/27271,703821902,2021-09-07T20:52:29Z,src/objective-c/tests/InteropTests/InteropTests.m,"@@ -712,8 +715,8 @@ - (void)testConcurrentRPCsWithErrorsWithV2API {      RMTSimpleRequest *request = [RMTSimpleRequest message];     request.responseType = RMTPayloadType_Compressable;-    request.responseSize = 314159;-    request.payload.body = [NSMutableData dataWithLength:271828];+    request.responseSize = SMALL_PAYLOAD_SIZE;","thanks, large size payload should already be covered in two separate tests here ([testLargeUnaryRPCWithV2API](https://github.com/grpc/grpc/blob/fd3bd70939fb4239639fbd26143ec416366e4157/src/objective-c/tests/InteropTests/InteropTests.m#L660) and [testLargeUnaryRPC](https://github.com/grpc/grpc/blob/fd3bd70939fb4239639fbd26143ec416366e4157/src/objective-c/tests/InteropTests/InteropTests.m#L576)).  I think this one is mainly focus on making sure a short burst of unary requests did finish and closing with various state code.  ",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27252,703834643,2021-09-07T21:12:14Z,include/grpc/impl/codegen/atm.h,"@@ -19,6 +19,8 @@ #ifndef GRPC_IMPL_CODEGEN_ATM_H #define GRPC_IMPL_CODEGEN_ATM_H +// IWYU pragma: private, include <grpc/impl/codegen/atm.h>","I'm not sure what this accomplishes, the specified alternative file is the `<...>` way of naming itself. Is this so that IWYU suggests our preferred system-header-style include format?",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27234,703922779,2021-09-07T23:42:29Z,tools/internal_ci/linux/grpc_xds_k8s_xlang_java.sh,"@@ -0,0 +1,120 @@+#!/usr/bin/env bash+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -eo pipefail++# Constants+readonly GITHUB_REPOSITORY_NAME=""grpc""+# GKE Cluster+readonly GKE_CLUSTER_NAME=""interop-test-psm-sec-v2-us-central1-a""+readonly GKE_CLUSTER_ZONE=""us-central1-a""++## xDS test server/client Docker images+readonly IMAGE_REPO=""gcr.io/grpc-testing/xds-interop""+readonly SERVER_LANG=""cpp go java""+readonly CLIENT_LANG=""cpp go java""+readonly VERSION_TAG=""v1.40.x""++#######################################+# Executes the test case+# Globals:+#   TEST_DRIVER_FLAGFILE: Relative path to test driver flagfile+#   KUBE_CONTEXT: The name of kubectl context with GKE cluster access+#   TEST_XML_OUTPUT_DIR: Output directory for the test xUnit XML report+#   SERVER_IMAGE_NAME: Test server Docker image name+#   CLIENT_IMAGE_NAME: Test client Docker image name+#   GIT_COMMIT: SHA-1 of git commit being built+# Arguments:+#   Test case name+# Outputs:+#   Writes the output of test execution to stdout, stderr+#   Test xUnit report to ${TEST_XML_OUTPUT_DIR}/${test_name}/sponge_log.xml+#######################################+run_test() {+  # Test driver usage:+  # https://github.com/grpc/grpc/tree/master/tools/run_tests/xds_k8s_test_driver#basic-usage+  local tag=""${1:?Usage: run_test tag server_lang client_lang}""+  local slang=""${2:?Usage: run_test tag server_lang client_lang}""+  local clang=""${3:?Usage: run_test tag server_lang client_lang}""+  local server_image_name=""${IMAGE_REPO}/${slang}-server:${tag}""+  local client_image_name=""${IMAGE_REPO}/${clang}-client:${tag}""",Add TODO to skip tests when image not found (using `gcloud_gcr_list_image_tags`).,
18282604,UVV-gh,https://api.github.com/repos/grpc/grpc/pulls/26940,704056320,2021-09-08T05:22:59Z,templates/CMakeLists.txt.template,"@@ -170,6 +170,7 @@   option(gRPC_BUILD_CODEGEN ""Build codegen"" ON)   option(gRPC_BUILD_CSHARP_EXT ""Build C# extensions"" ON)   option(gRPC_BACKWARDS_COMPATIBILITY_MODE ""Build libraries that are binary compatible across a larger number of OS and libc versions"" OFF)+  option(gRPC_INSTALL_PLUGINS ""Install protobuf plugins (disable e.g. when building for a pure cross-compilation environment"" ON)","> Btw, in this particular case, would just setting gRPC_BUILD_GRPC_*_PLUGIN to OFF solve the issue?Technically yes, that's like a shortcut for switching off all the plugins. Perhaps, following the semantics, it would be better to use the name `gRPC_BUILD_PLUGINS` then? I'd say having one option switching off all the plugins is better than switching them off one by one, isn't it?",X
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27243,704059489,2021-09-08T05:31:03Z,src/core/ext/transport/binder/wire_format/wire_writer.cc,"@@ -30,60 +30,126 @@ namespace grpc_binder { WireWriterImpl::WireWriterImpl(std::unique_ptr<Binder> binder)     : binder_(std::move(binder)) {} +absl::Status WireWriterImpl::WriteInitialMetadata(const Transaction& tx,+                                                  WritableParcel* parcel) {+  if (tx.IsClient()) {+    // Only client sends method ref.+    RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));+  }+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));+  for (const auto& md : tx.GetPrefixMetadata()) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+  }+  return absl::OkStatus();+}++absl::Status WireWriterImpl::WriteTrailingMetadata(const Transaction& tx,+                                                   WritableParcel* parcel) {+  if (tx.IsServer()) {+    if (tx.GetFlags() & kFlagStatusDescription) {+      RETURN_IF_ERROR(parcel->WriteString(tx.GetStatusDesc()));+    }+    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSuffixMetadata().size()));+    for (const auto& md : tx.GetSuffixMetadata()) {+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+    }+  } else {+    // client suffix currently is always empty according to the wireformat+    if (!tx.GetSuffixMetadata().empty()) {+      gpr_log(GPR_ERROR, ""Got non-empty suffix metadata from client."");+    }+  }+  return absl::OkStatus();+}++const int64_t WireWriterImpl::kBlockSize = 16 * 1024;+const int64_t WireWriterImpl::kFlowControlWindowSize = 128 * 1024;+ absl::Status WireWriterImpl::RpcCall(const Transaction& tx) {   // TODO(mingcl): check tx_code <= last call id   grpc_core::MutexLock lock(&mu_);   GPR_ASSERT(tx.GetTxCode() >= kFirstCallId);-  RETURN_IF_ERROR(binder_->PrepareTransaction());-  WritableParcel* parcel = binder_->GetWritableParcel();-  {-    //  fill parcel+  int& seq = seq_num_[tx.GetTxCode()];+  if ((tx.GetFlags() & kFlagMessageData) == 0 ||+      tx.GetMessageData().size() <= kBlockSize) {+    // Fast path: send data in one transaction.+    RETURN_IF_ERROR(binder_->PrepareTransaction());+    WritableParcel* parcel = binder_->GetWritableParcel();     RETURN_IF_ERROR(parcel->WriteInt32(tx.GetFlags()));-    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSeqNum()));+    RETURN_IF_ERROR(parcel->WriteInt32(seq++));     if (tx.GetFlags() & kFlagPrefix) {-      // prefix set-      if (tx.IsClient()) {-        // Only client sends method ref.-        RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));-      }-      RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));-      for (const auto& md : tx.GetPrefixMetadata()) {-        RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));-        RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));-      }+      RETURN_IF_ERROR(WriteInitialMetadata(tx, parcel));     }     if (tx.GetFlags() & kFlagMessageData) {       RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(tx.GetMessageData()));     }     if (tx.GetFlags() & kFlagSuffix) {-      if (tx.IsServer()) {-        if (tx.GetFlags() & kFlagStatusDescription) {-          RETURN_IF_ERROR(parcel->WriteString(tx.GetStatusDesc()));-        }-        RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSuffixMetadata().size()));-        for (const auto& md : tx.GetSuffixMetadata()) {-          RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));-          RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));-        }-      } else {-        // client suffix currently is always empty according to the wireformat-        if (!tx.GetSuffixMetadata().empty()) {-          gpr_log(GPR_ERROR, ""Got non-empty suffix metadata from client."");-        }+      RETURN_IF_ERROR(WriteTrailingMetadata(tx, parcel));+    }+    // FIXME(waynetu): Construct BinderTransportTxCode from an arbitrary integer+    // is an undefined behavior.+    return binder_->Transact(BinderTransportTxCode(tx.GetTxCode()));+  }+  // Slow path: the message data is too large to fit in one transaction.+  int original_flags = tx.GetFlags();+  GPR_ASSERT(original_flags & kFlagMessageData);+  absl::string_view data = tx.GetMessageData();+  size_t ptr = 0;+  while (ptr < data.size()) {+    while (num_outgoing_bytes_ >=+           num_acknowledged_bytes_ + kFlowControlWindowSize) {+      cv_.Wait(&mu_);","Probably shouldn't block here, it would be pretty hard to debug if we don't receive ACK from the other end for some reason (since the program will just hang).We probably want a helper class or something to handle chunking so that flow control logic (which can be orthogonal to other logic) don't make `WireWriterImpl` become too complexIf that's too much to implement in the same PR, probably add a 10ms deadline to `cv_.Wait()` (and log ERROR if deadline exceeded) and add a TODO",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27252,704516414,2021-09-08T15:10:39Z,include/grpcpp/impl/codegen/sync_stream.h,"@@ -18,7 +18,7 @@ #ifndef GRPCPP_IMPL_CODEGEN_SYNC_STREAM_H #define GRPCPP_IMPL_CODEGEN_SYNC_STREAM_H -// IWYU pragma: private, include <grpcpp/support/sync_stream.h>+// IWYU pragma: private, include <grpc/support/sync.h>","This needs an exception, `sync_stream` is not a `sync` alternative.",X
10470658,donnadionne,https://api.github.com/repos/grpc/grpc/pulls/27276,704595283,2021-09-08T16:43:03Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -7255,12 +7332,54 @@ TEST_P(CdsTest, MultipleBadResources) {   EXPECT_EQ(response_state.state, AdsServiceImpl::ResponseState::NACKED);   EXPECT_THAT(       response_state.error_message,-      ::testing::ContainsRegex(absl::StrCat(kDefaultClusterName,+      ::testing::ContainsRegex(absl::StrCat(kClusterName2,                                             "": validation error.*""                                             ""DiscoveryType is not valid.*"",-                                            kClusterName2,+                                            kClusterName3,                                             "": validation error.*""                                             ""DiscoveryType is not valid"")));+  // RPCs for default cluster should succeed.+  std::vector<std::pair<std::string, std::string>> metadata_default_cluster = {+      {""cluster"", kDefaultClusterName},+  };+  CheckRpcSendOk(+      1, RpcOptions().set_metadata(std::move(metadata_default_cluster)));+  // RPCs for cluster 2 should fail.+  std::vector<std::pair<std::string, std::string>> metadata_cluster_2 = {+      {""cluster"", kClusterName2},+  };+  CheckRpcSendFailure(CheckRpcSendFailureOptions().set_rpc_options(+      RpcOptions().set_metadata(std::move(metadata_default_cluster))));",metadata_default_cluster was moved a second time,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27276,704608623,2021-09-08T16:59:45Z,test/cpp/end2end/xds_end2end_test.cc,"@@ -7255,12 +7332,54 @@ TEST_P(CdsTest, MultipleBadResources) {   EXPECT_EQ(response_state.state, AdsServiceImpl::ResponseState::NACKED);   EXPECT_THAT(       response_state.error_message,-      ::testing::ContainsRegex(absl::StrCat(kDefaultClusterName,+      ::testing::ContainsRegex(absl::StrCat(kClusterName2,                                             "": validation error.*""                                             ""DiscoveryType is not valid.*"",-                                            kClusterName2,+                                            kClusterName3,                                             "": validation error.*""                                             ""DiscoveryType is not valid"")));+  // RPCs for default cluster should succeed.+  std::vector<std::pair<std::string, std::string>> metadata_default_cluster = {+      {""cluster"", kDefaultClusterName},+  };+  CheckRpcSendOk(+      1, RpcOptions().set_metadata(std::move(metadata_default_cluster)));+  // RPCs for cluster 2 should fail.+  std::vector<std::pair<std::string, std::string>> metadata_cluster_2 = {+      {""cluster"", kClusterName2},+  };+  CheckRpcSendFailure(CheckRpcSendFailureOptions().set_rpc_options(+      RpcOptions().set_metadata(std::move(metadata_default_cluster))));","Thanks for catching this!  That's actually a bug in the test; it was supposed to use the second set of metadata, not reuse the first set.Fixed.",X
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27243,704935753,2021-09-09T04:08:08Z,src/core/ext/transport/binder/wire_format/wire_writer.cc,"@@ -30,60 +30,126 @@ namespace grpc_binder { WireWriterImpl::WireWriterImpl(std::unique_ptr<Binder> binder)     : binder_(std::move(binder)) {} +absl::Status WireWriterImpl::WriteInitialMetadata(const Transaction& tx,+                                                  WritableParcel* parcel) {+  if (tx.IsClient()) {+    // Only client sends method ref.+    RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));+  }+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));+  for (const auto& md : tx.GetPrefixMetadata()) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+  }+  return absl::OkStatus();+}++absl::Status WireWriterImpl::WriteTrailingMetadata(const Transaction& tx,+                                                   WritableParcel* parcel) {+  if (tx.IsServer()) {+    if (tx.GetFlags() & kFlagStatusDescription) {+      RETURN_IF_ERROR(parcel->WriteString(tx.GetStatusDesc()));+    }+    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSuffixMetadata().size()));+    for (const auto& md : tx.GetSuffixMetadata()) {+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+    }+  } else {+    // client suffix currently is always empty according to the wireformat+    if (!tx.GetSuffixMetadata().empty()) {+      gpr_log(GPR_ERROR, ""Got non-empty suffix metadata from client."");+    }+  }+  return absl::OkStatus();+}++const int64_t WireWriterImpl::kBlockSize = 16 * 1024;+const int64_t WireWriterImpl::kFlowControlWindowSize = 128 * 1024;+ absl::Status WireWriterImpl::RpcCall(const Transaction& tx) {   // TODO(mingcl): check tx_code <= last call id   grpc_core::MutexLock lock(&mu_);   GPR_ASSERT(tx.GetTxCode() >= kFirstCallId);-  RETURN_IF_ERROR(binder_->PrepareTransaction());-  WritableParcel* parcel = binder_->GetWritableParcel();-  {-    //  fill parcel+  int& seq = seq_num_[tx.GetTxCode()];+  if ((tx.GetFlags() & kFlagMessageData) == 0 ||+      tx.GetMessageData().size() <= kBlockSize) {+    // Fast path: send data in one transaction.+    RETURN_IF_ERROR(binder_->PrepareTransaction());+    WritableParcel* parcel = binder_->GetWritableParcel();     RETURN_IF_ERROR(parcel->WriteInt32(tx.GetFlags()));-    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSeqNum()));+    RETURN_IF_ERROR(parcel->WriteInt32(seq++));     if (tx.GetFlags() & kFlagPrefix) {-      // prefix set-      if (tx.IsClient()) {-        // Only client sends method ref.-        RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));-      }-      RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));-      for (const auto& md : tx.GetPrefixMetadata()) {-        RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));-        RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));-      }+      RETURN_IF_ERROR(WriteInitialMetadata(tx, parcel));     }     if (tx.GetFlags() & kFlagMessageData) {       RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(tx.GetMessageData()));     }     if (tx.GetFlags() & kFlagSuffix) {-      if (tx.IsServer()) {-        if (tx.GetFlags() & kFlagStatusDescription) {-          RETURN_IF_ERROR(parcel->WriteString(tx.GetStatusDesc()));-        }-        RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSuffixMetadata().size()));-        for (const auto& md : tx.GetSuffixMetadata()) {-          RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));-          RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));-        }-      } else {-        // client suffix currently is always empty according to the wireformat-        if (!tx.GetSuffixMetadata().empty()) {-          gpr_log(GPR_ERROR, ""Got non-empty suffix metadata from client."");-        }+      RETURN_IF_ERROR(WriteTrailingMetadata(tx, parcel));+    }+    // FIXME(waynetu): Construct BinderTransportTxCode from an arbitrary integer+    // is an undefined behavior.+    return binder_->Transact(BinderTransportTxCode(tx.GetTxCode()));+  }+  // Slow path: the message data is too large to fit in one transaction.+  int original_flags = tx.GetFlags();+  GPR_ASSERT(original_flags & kFlagMessageData);+  absl::string_view data = tx.GetMessageData();+  size_t ptr = 0;+  while (ptr < data.size()) {+    while (num_outgoing_bytes_ >=+           num_acknowledged_bytes_ + kFlowControlWindowSize) {+      cv_.Wait(&mu_);",How about having a new class like `FlowControl` to take care of the flow control mechanism. The transport now holds a `FlowControl` (which internally holds a `WieWtier` to deal with the wire format) and the transport sends RPC call through `FlowControl` instead of `WireWriter`.What do you think?,X
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27052,705816706,2021-09-10T00:52:39Z,test/core/transport/binder/end2end/fuzzers/client_fuzzer.cc,"@@ -0,0 +1,331 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <fuzzer/FuzzedDataProvider.h>++#include <thread>+#include <utility>++#include ""absl/memory/memory.h""++#include ""src/core/ext/transport/binder/transport/binder_transport.h""+#include ""src/core/ext/transport/binder/wire_format/binder.h""+#include ""src/core/ext/transport/binder/wire_format/wire_reader.h""+#include ""src/core/lib/iomgr/executor.h""+#include ""src/core/lib/surface/channel.h""++// Used in test/core/util/fuzzer_corpus_test.cc+bool squelch = true;+bool leak_check = true;++namespace {++// A WritableParcel implementation that simply does nothing. Don't use+// MockWritableParcel here since capturing calls is expensive.+class NoOpWritableParcel : public grpc_binder::WritableParcel {+ public:+  int32_t GetDataPosition() const override { return 0; }+  absl::Status SetDataPosition(int32_t /*pos*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteInt32(int32_t /*data*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteInt64(int64_t /*data*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteBinder(grpc_binder::HasRawBinder* /*binder*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteString(absl::string_view /*s*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteByteArray(const int8_t* /*buffer*/,+                              int32_t /*length*/) override {+    return absl::OkStatus();+  }+};++// Binder implementation used in fuzzing.+//+// Most of its the functionalities are no-op, except ConstructTxReceiver now+// returns a FuzzedTransactionReceiver.+class FuzzedBinder : public grpc_binder::Binder {","Naming of these classes feels a little bit strange to meInstead of `FuzzedXXX`, would it be more clear if we name them as `XXXForFuzzing`? (Because `XXX` is not the actual thing that we want to fuzz)",
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27052,705818269,2021-09-10T00:57:49Z,test/core/transport/binder/end2end/fuzzers/client_fuzzer.cc,"@@ -0,0 +1,331 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <fuzzer/FuzzedDataProvider.h>++#include <thread>+#include <utility>++#include ""absl/memory/memory.h""++#include ""src/core/ext/transport/binder/transport/binder_transport.h""+#include ""src/core/ext/transport/binder/wire_format/binder.h""+#include ""src/core/ext/transport/binder/wire_format/wire_reader.h""+#include ""src/core/lib/iomgr/executor.h""+#include ""src/core/lib/surface/channel.h""++// Used in test/core/util/fuzzer_corpus_test.cc+bool squelch = true;+bool leak_check = true;++namespace {++// A WritableParcel implementation that simply does nothing. Don't use+// MockWritableParcel here since capturing calls is expensive.+class NoOpWritableParcel : public grpc_binder::WritableParcel {+ public:+  int32_t GetDataPosition() const override { return 0; }+  absl::Status SetDataPosition(int32_t /*pos*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteInt32(int32_t /*data*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteInt64(int64_t /*data*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteBinder(grpc_binder::HasRawBinder* /*binder*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteString(absl::string_view /*s*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteByteArray(const int8_t* /*buffer*/,+                              int32_t /*length*/) override {+    return absl::OkStatus();+  }+};++// Binder implementation used in fuzzing.+//+// Most of its the functionalities are no-op, except ConstructTxReceiver now+// returns a FuzzedTransactionReceiver.+class FuzzedBinder : public grpc_binder::Binder {+ public:+  FuzzedBinder() : input_(absl::make_unique<NoOpWritableParcel>()) {}++  FuzzedBinder(const uint8_t* data, size_t size)+      : data_(data),+        size_(size),+        input_(absl::make_unique<NoOpWritableParcel>()) {}++  void Initialize() override {}+  absl::Status PrepareTransaction() override { return absl::OkStatus(); }++  absl::Status Transact(+      grpc_binder::BinderTransportTxCode /*tx_code*/) override {+    return absl::OkStatus();+  }++  std::unique_ptr<grpc_binder::TransactionReceiver> ConstructTxReceiver(+      grpc_core::RefCountedPtr<grpc_binder::WireReader> wire_reader_ref,+      grpc_binder::TransactionReceiver::OnTransactCb cb) const override;++  grpc_binder::ReadableParcel* GetReadableParcel() const override {+    return nullptr;+  }+  grpc_binder::WritableParcel* GetWritableParcel() const override {+    return input_.get();+  }+  void* GetRawBinder() override { return nullptr; }++ private:+  const uint8_t* data_;+  size_t size_;+  std::unique_ptr<grpc_binder::WritableParcel> input_;+};++// ReadableParcel implementation used in fuzzing.+//+// It consumes a FuzzedDataProvider, and returns fuzzed data upon user's+// requests.+class FuzzedReadableParcel : public grpc_binder::ReadableParcel {+ public:+  FuzzedReadableParcel(FuzzedDataProvider* data_provider,+                       bool is_setup_transport)+      : data_provider_(data_provider),+        is_setup_transport_(is_setup_transport) {}++  absl::Status ReadInt32(int32_t* data) const override {+    if (!is_setup_transport_ && data_provider_->ConsumeBool()) {","The purpose of `is_setup_transport_` and```if (!is_setup_transport_ && data_provider_->ConsumeBool()) {      return absl::InternalError(""error"");}```is not immediately obvious to me when I scan through the code for the first timemaybe worth adding comments?",
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27052,705822359,2021-09-10T01:06:59Z,test/core/transport/binder/end2end/fuzzers/client_fuzzer.cc,"@@ -0,0 +1,331 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <fuzzer/FuzzedDataProvider.h>++#include <thread>+#include <utility>++#include ""absl/memory/memory.h""++#include ""src/core/ext/transport/binder/transport/binder_transport.h""+#include ""src/core/ext/transport/binder/wire_format/binder.h""+#include ""src/core/ext/transport/binder/wire_format/wire_reader.h""+#include ""src/core/lib/iomgr/executor.h""+#include ""src/core/lib/surface/channel.h""++// Used in test/core/util/fuzzer_corpus_test.cc+bool squelch = true;+bool leak_check = true;++namespace {++// A WritableParcel implementation that simply does nothing. Don't use+// MockWritableParcel here since capturing calls is expensive.+class NoOpWritableParcel : public grpc_binder::WritableParcel {+ public:+  int32_t GetDataPosition() const override { return 0; }+  absl::Status SetDataPosition(int32_t /*pos*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteInt32(int32_t /*data*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteInt64(int64_t /*data*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteBinder(grpc_binder::HasRawBinder* /*binder*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteString(absl::string_view /*s*/) override {+    return absl::OkStatus();+  }+  absl::Status WriteByteArray(const int8_t* /*buffer*/,+                              int32_t /*length*/) override {+    return absl::OkStatus();+  }+};++// Binder implementation used in fuzzing.+//+// Most of its the functionalities are no-op, except ConstructTxReceiver now+// returns a FuzzedTransactionReceiver.+class FuzzedBinder : public grpc_binder::Binder {+ public:+  FuzzedBinder() : input_(absl::make_unique<NoOpWritableParcel>()) {}++  FuzzedBinder(const uint8_t* data, size_t size)+      : data_(data),+        size_(size),+        input_(absl::make_unique<NoOpWritableParcel>()) {}++  void Initialize() override {}+  absl::Status PrepareTransaction() override { return absl::OkStatus(); }++  absl::Status Transact(+      grpc_binder::BinderTransportTxCode /*tx_code*/) override {+    return absl::OkStatus();+  }++  std::unique_ptr<grpc_binder::TransactionReceiver> ConstructTxReceiver(+      grpc_core::RefCountedPtr<grpc_binder::WireReader> wire_reader_ref,+      grpc_binder::TransactionReceiver::OnTransactCb cb) const override;++  grpc_binder::ReadableParcel* GetReadableParcel() const override {+    return nullptr;+  }+  grpc_binder::WritableParcel* GetWritableParcel() const override {+    return input_.get();+  }+  void* GetRawBinder() override { return nullptr; }++ private:+  const uint8_t* data_;+  size_t size_;+  std::unique_ptr<grpc_binder::WritableParcel> input_;+};++// ReadableParcel implementation used in fuzzing.+//+// It consumes a FuzzedDataProvider, and returns fuzzed data upon user's+// requests.+class FuzzedReadableParcel : public grpc_binder::ReadableParcel {+ public:+  FuzzedReadableParcel(FuzzedDataProvider* data_provider,+                       bool is_setup_transport)+      : data_provider_(data_provider),+        is_setup_transport_(is_setup_transport) {}++  absl::Status ReadInt32(int32_t* data) const override {+    if (!is_setup_transport_ && data_provider_->ConsumeBool()) {+      return absl::InternalError(""error"");+    }+    *data = data_provider_->ConsumeIntegral<int32_t>();+    return absl::OkStatus();+  }+  absl::Status ReadInt64(int64_t* data) const override {+    if (!is_setup_transport_ && data_provider_->ConsumeBool()) {+      return absl::InternalError(""error"");+    }+    *data = data_provider_->ConsumeIntegral<int64_t>();+    return absl::OkStatus();+  }+  absl::Status ReadBinder(+      std::unique_ptr<grpc_binder::Binder>* binder) const override {+    if (!is_setup_transport_ && data_provider_->ConsumeBool()) {+      return absl::InternalError(""error"");+    }+    *binder = absl::make_unique<FuzzedBinder>();+    return absl::OkStatus();+  }+  absl::Status ReadByteArray(std::string* data) const override {+    if (!is_setup_transport_ && data_provider_->ConsumeBool()) {+      return absl::InternalError(""error"");+    }+    *data = data_provider_->ConsumeRandomLengthString(100);+    return absl::OkStatus();+  }+  absl::Status ReadString(char data[111]) const override {+    if (!is_setup_transport_ && data_provider_->ConsumeBool()) {+      return absl::InternalError(""error"");+    }+    std::string s = data_provider_->ConsumeRandomLengthString(100);+    std::memcpy(data, s.data(), s.size());+    return absl::OkStatus();+  }++ private:+  FuzzedDataProvider* data_provider_;+  bool is_setup_transport_;+};++std::vector<std::thread>* thread_pool = nullptr;++void FuzzingLoop(+    const uint8_t* data, size_t size,+    grpc_core::RefCountedPtr<grpc_binder::WireReader> wire_reader_ref,+    grpc_binder::TransactionReceiver::OnTransactCb callback) {+  FuzzedDataProvider data_provider(data, size);+  std::unique_ptr<grpc_binder::ReadableParcel> parcel =+      absl::make_unique<FuzzedReadableParcel>(&data_provider,+                                              /*is_setup_transport=*/true);+  callback(static_cast<transaction_code_t>(+               grpc_binder::BinderTransportTxCode::SETUP_TRANSPORT),+           parcel.get())+      .IgnoreError();+  while (data_provider.remaining_bytes() > 0) {+    gpr_log(GPR_INFO, ""Fuzzing"");+    bool streaming_call = data_provider.ConsumeBool();+    transaction_code_t tx_code =+        streaming_call+            ? data_provider.ConsumeIntegralInRange<transaction_code_t>(+                  0, static_cast<transaction_code_t>(+                         grpc_binder::BinderTransportTxCode::PING_RESPONSE))+            : data_provider.ConsumeIntegralInRange<transaction_code_t>(+                  0, LAST_CALL_TRANSACTION);+    std::unique_ptr<grpc_binder::ReadableParcel> parcel =+        absl::make_unique<FuzzedReadableParcel>(&data_provider,+                                                /*is_setup_transport=*/false);+    callback(tx_code, parcel.get()).IgnoreError();+  }+  gpr_log(GPR_INFO, ""Fuzzing done"");+  wire_reader_ref = nullptr;+}++// TransactionReceiver implementation used in fuzzing.+//+// When constructed, start sending fuzzed requests to the client. When all the+// bytes are consumed, the reference to WireReader will be released.+class FuzzedTransactionReceiver : public grpc_binder::TransactionReceiver {+ public:+  FuzzedTransactionReceiver(+      const uint8_t* data, size_t size,+      grpc_core::RefCountedPtr<grpc_binder::WireReader> wire_reader_ref,+      grpc_binder::TransactionReceiver::OnTransactCb cb) {+    gpr_log(GPR_INFO, ""Construct FuzzedTransactionReceiver"");+    thread_pool->emplace_back(FuzzingLoop, data, size, wire_reader_ref, cb);+  }++  void* GetRawBinder() override { return nullptr; }+};++std::unique_ptr<grpc_binder::TransactionReceiver>+FuzzedBinder::ConstructTxReceiver(","Code is probably organized better if we simply put this inside the class definition, since all other member functions are implemented inside the class",X
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27243,706532511,2021-09-11T00:45:52Z,src/core/ext/transport/binder/wire_format/wire_writer.cc,"@@ -30,60 +30,160 @@ namespace grpc_binder { WireWriterImpl::WireWriterImpl(std::unique_ptr<Binder> binder)     : binder_(std::move(binder)) {} +absl::Status WireWriterImpl::WriteInitialMetadata(const Transaction& tx,+                                                  WritableParcel* parcel) {+  if (tx.IsClient()) {+    // Only client sends method ref.+    RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));+  }+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));+  for (const auto& md : tx.GetPrefixMetadata()) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+  }+  return absl::OkStatus();+}++absl::Status WireWriterImpl::WriteTrailingMetadata(const Transaction& tx,+                                                   WritableParcel* parcel) {+  if (tx.IsServer()) {+    if (tx.GetFlags() & kFlagStatusDescription) {+      RETURN_IF_ERROR(parcel->WriteString(tx.GetStatusDesc()));+    }+    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSuffixMetadata().size()));+    for (const auto& md : tx.GetSuffixMetadata()) {+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+    }+  } else {+    // client suffix currently is always empty according to the wireformat+    if (!tx.GetSuffixMetadata().empty()) {+      gpr_log(GPR_ERROR, ""Got non-empty suffix metadata from client."");+    }+  }+  return absl::OkStatus();+}++const int64_t WireWriterImpl::kBlockSize = 16 * 1024;+const int64_t WireWriterImpl::kFlowControlWindowSize = 128 * 1024;++bool WireWriterImpl::CanBeSentInOneTransaction(const Transaction& tx) const {+  return (tx.GetFlags() & kFlagMessageData) == 0 ||+         tx.GetMessageData().size() <= kBlockSize;+}++absl::Status WireWriterImpl::RpcCallFastPath(const Transaction& tx) {+  int& seq = seq_num_[tx.GetTxCode()];+  // Fast path: send data in one transaction.+  RETURN_IF_ERROR(binder_->PrepareTransaction());+  WritableParcel* parcel = binder_->GetWritableParcel();+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetFlags()));+  RETURN_IF_ERROR(parcel->WriteInt32(seq++));+  if (tx.GetFlags() & kFlagPrefix) {+    RETURN_IF_ERROR(WriteInitialMetadata(tx, parcel));+  }+  if (tx.GetFlags() & kFlagMessageData) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(tx.GetMessageData()));+  }+  if (tx.GetFlags() & kFlagSuffix) {+    RETURN_IF_ERROR(WriteTrailingMetadata(tx, parcel));+  }+  // FIXME(waynetu): Construct BinderTransportTxCode from an arbitrary integer+  // is an undefined behavior.+  return binder_->Transact(BinderTransportTxCode(tx.GetTxCode()));+}++bool WireWriterImpl::WaitForAcknowledgement() {+  if (num_outgoing_bytes_ < num_acknowledged_bytes_ + kFlowControlWindowSize) {+    return true;+  }+  absl::Time deadline = absl::Now() + absl::Seconds(1);+  do {+    if (cv_.WaitWithDeadline(&mu_, deadline)) {+      return false;+    }+    if (absl::Now() >= deadline) {+      return false;+    }+  } while (num_outgoing_bytes_ >=+           num_acknowledged_bytes_ + kFlowControlWindowSize);+  return true;+}++int WireWriterImpl::GetFlagForCurrentTransaction(int original_flags,+                                                 int sent_length,+                                                 int remaining_length) const {+  int flags = kFlagMessageData;+  if (sent_length == 0) {+    // This is the first transaction. Include initial metadata if there's any.+    if (original_flags & kFlagPrefix) {+      flags |= kFlagPrefix;+    }+  }+  if (remaining_length == 0) {+    // This is the last transaction. Include trailing metadata if there's any.+    if (original_flags & kFlagSuffix) {+      flags |= kFlagSuffix;+    }+  } else {+    // There are more messages to send.+    flags |= kFlagMessageDataIsPartial;+  }+  return flags;+}+ absl::Status WireWriterImpl::RpcCall(const Transaction& tx) {   // TODO(mingcl): check tx_code <= last call id   grpc_core::MutexLock lock(&mu_);   GPR_ASSERT(tx.GetTxCode() >= kFirstCallId);-  RETURN_IF_ERROR(binder_->PrepareTransaction());-  WritableParcel* parcel = binder_->GetWritableParcel();-  {-    //  fill parcel-    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetFlags()));-    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSeqNum()));-    if (tx.GetFlags() & kFlagPrefix) {-      // prefix set-      if (tx.IsClient()) {-        // Only client sends method ref.-        RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));-      }-      RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));-      for (const auto& md : tx.GetPrefixMetadata()) {-        RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));-        RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));-      }+  if (CanBeSentInOneTransaction(tx)) {+    return RpcCallFastPath(tx);+  }+  // Slow path: the message data is too large to fit in one transaction.+  int& seq = seq_num_[tx.GetTxCode()];+  int original_flags = tx.GetFlags();+  GPR_ASSERT(original_flags & kFlagMessageData);+  absl::string_view data = tx.GetMessageData();+  size_t ptr = 0;+  while (ptr < data.size()) {+    if (!WaitForAcknowledgement()) {+      return absl::InternalError(""Timeout waiting for acknowledgement"");     }-    if (tx.GetFlags() & kFlagMessageData) {-      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(tx.GetMessageData()));+    RETURN_IF_ERROR(binder_->PrepareTransaction());+    WritableParcel* parcel = binder_->GetWritableParcel();+    size_t size = std::min(static_cast<size_t>(kBlockSize), data.size() - ptr);+    int flags = GetFlagForCurrentTransaction(+        original_flags, /*sent_length=*/ptr,+        /*remaining_length=*/data.size() - ptr - size);+    RETURN_IF_ERROR(parcel->WriteInt32(flags));+    RETURN_IF_ERROR(parcel->WriteInt32(seq++));+    if (flags & kFlagPrefix) {+      RETURN_IF_ERROR(WriteInitialMetadata(tx, parcel));",Would it makes sense to always send initial metadata and trailing metadata separately? i.e Don't put them and message data in the same transaction.I guess we probably should consider this after the `AParcel_getsize` PR,
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27243,706532873,2021-09-11T00:48:32Z,src/core/ext/transport/binder/wire_format/wire_writer.cc,"@@ -30,60 +30,160 @@ namespace grpc_binder { WireWriterImpl::WireWriterImpl(std::unique_ptr<Binder> binder)     : binder_(std::move(binder)) {} +absl::Status WireWriterImpl::WriteInitialMetadata(const Transaction& tx,+                                                  WritableParcel* parcel) {+  if (tx.IsClient()) {+    // Only client sends method ref.+    RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));+  }+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));+  for (const auto& md : tx.GetPrefixMetadata()) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+  }+  return absl::OkStatus();+}++absl::Status WireWriterImpl::WriteTrailingMetadata(const Transaction& tx,+                                                   WritableParcel* parcel) {+  if (tx.IsServer()) {+    if (tx.GetFlags() & kFlagStatusDescription) {+      RETURN_IF_ERROR(parcel->WriteString(tx.GetStatusDesc()));+    }+    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSuffixMetadata().size()));+    for (const auto& md : tx.GetSuffixMetadata()) {+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+    }+  } else {+    // client suffix currently is always empty according to the wireformat+    if (!tx.GetSuffixMetadata().empty()) {+      gpr_log(GPR_ERROR, ""Got non-empty suffix metadata from client."");+    }+  }+  return absl::OkStatus();+}++const int64_t WireWriterImpl::kBlockSize = 16 * 1024;+const int64_t WireWriterImpl::kFlowControlWindowSize = 128 * 1024;++bool WireWriterImpl::CanBeSentInOneTransaction(const Transaction& tx) const {+  return (tx.GetFlags() & kFlagMessageData) == 0 ||+         tx.GetMessageData().size() <= kBlockSize;+}++absl::Status WireWriterImpl::RpcCallFastPath(const Transaction& tx) {+  int& seq = seq_num_[tx.GetTxCode()];+  // Fast path: send data in one transaction.+  RETURN_IF_ERROR(binder_->PrepareTransaction());+  WritableParcel* parcel = binder_->GetWritableParcel();+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetFlags()));+  RETURN_IF_ERROR(parcel->WriteInt32(seq++));+  if (tx.GetFlags() & kFlagPrefix) {+    RETURN_IF_ERROR(WriteInitialMetadata(tx, parcel));+  }+  if (tx.GetFlags() & kFlagMessageData) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(tx.GetMessageData()));+  }+  if (tx.GetFlags() & kFlagSuffix) {+    RETURN_IF_ERROR(WriteTrailingMetadata(tx, parcel));+  }+  // FIXME(waynetu): Construct BinderTransportTxCode from an arbitrary integer+  // is an undefined behavior.+  return binder_->Transact(BinderTransportTxCode(tx.GetTxCode()));+}++bool WireWriterImpl::WaitForAcknowledgement() {+  if (num_outgoing_bytes_ < num_acknowledged_bytes_ + kFlowControlWindowSize) {+    return true;+  }+  absl::Time deadline = absl::Now() + absl::Seconds(1);+  do {+    if (cv_.WaitWithDeadline(&mu_, deadline)) {+      return false;+    }+    if (absl::Now() >= deadline) {+      return false;+    }+  } while (num_outgoing_bytes_ >=+           num_acknowledged_bytes_ + kFlowControlWindowSize);+  return true;+}++int WireWriterImpl::GetFlagForCurrentTransaction(int original_flags,+                                                 int sent_length,+                                                 int remaining_length) const {+  int flags = kFlagMessageData;+  if (sent_length == 0) {+    // This is the first transaction. Include initial metadata if there's any.+    if (original_flags & kFlagPrefix) {+      flags |= kFlagPrefix;+    }+  }+  if (remaining_length == 0) {+    // This is the last transaction. Include trailing metadata if there's any.+    if (original_flags & kFlagSuffix) {+      flags |= kFlagSuffix;+    }+  } else {+    // There are more messages to send.+    flags |= kFlagMessageDataIsPartial;+  }+  return flags;+}+ absl::Status WireWriterImpl::RpcCall(const Transaction& tx) {   // TODO(mingcl): check tx_code <= last call id   grpc_core::MutexLock lock(&mu_);   GPR_ASSERT(tx.GetTxCode() >= kFirstCallId);-  RETURN_IF_ERROR(binder_->PrepareTransaction());-  WritableParcel* parcel = binder_->GetWritableParcel();-  {-    //  fill parcel-    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetFlags()));-    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSeqNum()));-    if (tx.GetFlags() & kFlagPrefix) {-      // prefix set-      if (tx.IsClient()) {-        // Only client sends method ref.-        RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));-      }-      RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));-      for (const auto& md : tx.GetPrefixMetadata()) {-        RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));-        RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));-      }+  if (CanBeSentInOneTransaction(tx)) {+    return RpcCallFastPath(tx);+  }+  // Slow path: the message data is too large to fit in one transaction.+  int& seq = seq_num_[tx.GetTxCode()];+  int original_flags = tx.GetFlags();+  GPR_ASSERT(original_flags & kFlagMessageData);+  absl::string_view data = tx.GetMessageData();+  size_t ptr = 0;+  while (ptr < data.size()) {+    if (!WaitForAcknowledgement()) {+      return absl::InternalError(""Timeout waiting for acknowledgement"");     }-    if (tx.GetFlags() & kFlagMessageData) {-      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(tx.GetMessageData()));+    RETURN_IF_ERROR(binder_->PrepareTransaction());+    WritableParcel* parcel = binder_->GetWritableParcel();+    size_t size = std::min(static_cast<size_t>(kBlockSize), data.size() - ptr);+    int flags = GetFlagForCurrentTransaction(+        original_flags, /*sent_length=*/ptr,+        /*remaining_length=*/data.size() - ptr - size);+    RETURN_IF_ERROR(parcel->WriteInt32(flags));+    RETURN_IF_ERROR(parcel->WriteInt32(seq++));+    if (flags & kFlagPrefix) {+      RETURN_IF_ERROR(WriteInitialMetadata(tx, parcel));     }-    if (tx.GetFlags() & kFlagSuffix) {-      if (tx.IsServer()) {-        if (tx.GetFlags() & kFlagStatusDescription) {-          RETURN_IF_ERROR(parcel->WriteString(tx.GetStatusDesc()));-        }-        RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSuffixMetadata().size()));-        for (const auto& md : tx.GetSuffixMetadata()) {-          RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));-          RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));-        }-      } else {-        // client suffix currently is always empty according to the wireformat-        if (!tx.GetSuffixMetadata().empty()) {-          gpr_log(GPR_ERROR, ""Got non-empty suffix metadata from client."");-        }-      }+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(data.substr(ptr, size)));+    if (flags & kFlagSuffix) {+      RETURN_IF_ERROR(WriteTrailingMetadata(tx, parcel));     }+    RETURN_IF_ERROR(binder_->Transact(BinderTransportTxCode(tx.GetTxCode())));+    ptr += size;+    // TODO(waynetu): This should be parcel->GetDataSize().+    num_outgoing_bytes_ += size;   }-  // FIXME(waynetu): Construct BinderTransportTxCode from an arbitrary integer-  // is an undefined behavior.-  return binder_->Transact(BinderTransportTxCode(tx.GetTxCode()));+  return absl::OkStatus(); } -absl::Status WireWriterImpl::Ack(int64_t num_bytes) {+absl::Status WireWriterImpl::SendAck(int64_t num_bytes) {   grpc_core::MutexLock lock(&mu_);   RETURN_IF_ERROR(binder_->PrepareTransaction());   WritableParcel* parcel = binder_->GetWritableParcel();   RETURN_IF_ERROR(parcel->WriteInt64(num_bytes));   return binder_->Transact(BinderTransportTxCode::ACKNOWLEDGE_BYTES); } +void WireWriterImpl::RecvAck(int64_t num_bytes) {+  grpc_core::MutexLock lock(&mu_);",there is no deadlock?Is it because `cv_.WaitWithDeadline` allows us to obtain `mu_` here?,
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27243,706532973,2021-09-11T00:49:12Z,src/core/ext/transport/binder/wire_format/wire_writer.h,"@@ -32,18 +32,53 @@ class WireWriter {  public:   virtual ~WireWriter() = default;   virtual absl::Status RpcCall(const Transaction& call) = 0;-  virtual absl::Status Ack(int64_t num_bytes) = 0;+  virtual absl::Status SendAck(int64_t num_bytes) = 0;+  virtual void RecvAck(int64_t num_bytes) = 0;",Consider renaming to `OnAckReceived`,X
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27243,706536341,2021-09-11T01:12:39Z,src/core/ext/transport/binder/wire_format/wire_writer.cc,"@@ -30,60 +30,160 @@ namespace grpc_binder { WireWriterImpl::WireWriterImpl(std::unique_ptr<Binder> binder)     : binder_(std::move(binder)) {} +absl::Status WireWriterImpl::WriteInitialMetadata(const Transaction& tx,+                                                  WritableParcel* parcel) {+  if (tx.IsClient()) {+    // Only client sends method ref.+    RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));+  }+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));+  for (const auto& md : tx.GetPrefixMetadata()) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+  }+  return absl::OkStatus();+}++absl::Status WireWriterImpl::WriteTrailingMetadata(const Transaction& tx,+                                                   WritableParcel* parcel) {+  if (tx.IsServer()) {+    if (tx.GetFlags() & kFlagStatusDescription) {+      RETURN_IF_ERROR(parcel->WriteString(tx.GetStatusDesc()));+    }+    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSuffixMetadata().size()));+    for (const auto& md : tx.GetSuffixMetadata()) {+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+    }+  } else {+    // client suffix currently is always empty according to the wireformat+    if (!tx.GetSuffixMetadata().empty()) {+      gpr_log(GPR_ERROR, ""Got non-empty suffix metadata from client."");+    }+  }+  return absl::OkStatus();+}++const int64_t WireWriterImpl::kBlockSize = 16 * 1024;+const int64_t WireWriterImpl::kFlowControlWindowSize = 128 * 1024;++bool WireWriterImpl::CanBeSentInOneTransaction(const Transaction& tx) const {+  return (tx.GetFlags() & kFlagMessageData) == 0 ||+         tx.GetMessageData().size() <= kBlockSize;+}++absl::Status WireWriterImpl::RpcCallFastPath(const Transaction& tx) {+  int& seq = seq_num_[tx.GetTxCode()];+  // Fast path: send data in one transaction.+  RETURN_IF_ERROR(binder_->PrepareTransaction());+  WritableParcel* parcel = binder_->GetWritableParcel();+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetFlags()));+  RETURN_IF_ERROR(parcel->WriteInt32(seq++));+  if (tx.GetFlags() & kFlagPrefix) {+    RETURN_IF_ERROR(WriteInitialMetadata(tx, parcel));+  }+  if (tx.GetFlags() & kFlagMessageData) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(tx.GetMessageData()));+  }+  if (tx.GetFlags() & kFlagSuffix) {+    RETURN_IF_ERROR(WriteTrailingMetadata(tx, parcel));+  }+  // FIXME(waynetu): Construct BinderTransportTxCode from an arbitrary integer+  // is an undefined behavior.+  return binder_->Transact(BinderTransportTxCode(tx.GetTxCode()));+}++bool WireWriterImpl::WaitForAcknowledgement() {+  if (num_outgoing_bytes_ < num_acknowledged_bytes_ + kFlowControlWindowSize) {+    return true;+  }+  absl::Time deadline = absl::Now() + absl::Seconds(1);+  do {+    if (cv_.WaitWithDeadline(&mu_, deadline)) {","Because after `WaitWithTimeout` we might still not receive a big-enough acknowledgement for us to proceed, so in that case we have to try again if we are not timed out yet.",
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27243,706536453,2021-09-11T01:13:30Z,src/core/ext/transport/binder/wire_format/wire_writer.cc,"@@ -30,60 +30,160 @@ namespace grpc_binder { WireWriterImpl::WireWriterImpl(std::unique_ptr<Binder> binder)     : binder_(std::move(binder)) {} +absl::Status WireWriterImpl::WriteInitialMetadata(const Transaction& tx,+                                                  WritableParcel* parcel) {+  if (tx.IsClient()) {+    // Only client sends method ref.+    RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));+  }+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));+  for (const auto& md : tx.GetPrefixMetadata()) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+  }+  return absl::OkStatus();+}++absl::Status WireWriterImpl::WriteTrailingMetadata(const Transaction& tx,+                                                   WritableParcel* parcel) {+  if (tx.IsServer()) {+    if (tx.GetFlags() & kFlagStatusDescription) {+      RETURN_IF_ERROR(parcel->WriteString(tx.GetStatusDesc()));+    }+    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSuffixMetadata().size()));+    for (const auto& md : tx.GetSuffixMetadata()) {+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+    }+  } else {+    // client suffix currently is always empty according to the wireformat+    if (!tx.GetSuffixMetadata().empty()) {+      gpr_log(GPR_ERROR, ""Got non-empty suffix metadata from client."");+    }+  }+  return absl::OkStatus();+}++const int64_t WireWriterImpl::kBlockSize = 16 * 1024;+const int64_t WireWriterImpl::kFlowControlWindowSize = 128 * 1024;++bool WireWriterImpl::CanBeSentInOneTransaction(const Transaction& tx) const {+  return (tx.GetFlags() & kFlagMessageData) == 0 ||+         tx.GetMessageData().size() <= kBlockSize;+}++absl::Status WireWriterImpl::RpcCallFastPath(const Transaction& tx) {+  int& seq = seq_num_[tx.GetTxCode()];+  // Fast path: send data in one transaction.+  RETURN_IF_ERROR(binder_->PrepareTransaction());+  WritableParcel* parcel = binder_->GetWritableParcel();+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetFlags()));+  RETURN_IF_ERROR(parcel->WriteInt32(seq++));+  if (tx.GetFlags() & kFlagPrefix) {+    RETURN_IF_ERROR(WriteInitialMetadata(tx, parcel));+  }+  if (tx.GetFlags() & kFlagMessageData) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(tx.GetMessageData()));+  }+  if (tx.GetFlags() & kFlagSuffix) {+    RETURN_IF_ERROR(WriteTrailingMetadata(tx, parcel));+  }+  // FIXME(waynetu): Construct BinderTransportTxCode from an arbitrary integer+  // is an undefined behavior.+  return binder_->Transact(BinderTransportTxCode(tx.GetTxCode()));+}++bool WireWriterImpl::WaitForAcknowledgement() {+  if (num_outgoing_bytes_ < num_acknowledged_bytes_ + kFlowControlWindowSize) {+    return true;+  }+  absl::Time deadline = absl::Now() + absl::Seconds(1);+  do {+    if (cv_.WaitWithDeadline(&mu_, deadline)) {+      return false;+    }+    if (absl::Now() >= deadline) {+      return false;+    }+  } while (num_outgoing_bytes_ >=+           num_acknowledged_bytes_ + kFlowControlWindowSize);+  return true;+}++int WireWriterImpl::GetFlagForCurrentTransaction(int original_flags,+                                                 int sent_length,+                                                 int remaining_length) const {+  int flags = kFlagMessageData;+  if (sent_length == 0) {+    // This is the first transaction. Include initial metadata if there's any.+    if (original_flags & kFlagPrefix) {+      flags |= kFlagPrefix;+    }+  }+  if (remaining_length == 0) {+    // This is the last transaction. Include trailing metadata if there's any.+    if (original_flags & kFlagSuffix) {+      flags |= kFlagSuffix;+    }+  } else {+    // There are more messages to send.+    flags |= kFlagMessageDataIsPartial;+  }+  return flags;+}+ absl::Status WireWriterImpl::RpcCall(const Transaction& tx) {   // TODO(mingcl): check tx_code <= last call id   grpc_core::MutexLock lock(&mu_);   GPR_ASSERT(tx.GetTxCode() >= kFirstCallId);-  RETURN_IF_ERROR(binder_->PrepareTransaction());-  WritableParcel* parcel = binder_->GetWritableParcel();-  {-    //  fill parcel-    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetFlags()));-    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSeqNum()));-    if (tx.GetFlags() & kFlagPrefix) {-      // prefix set-      if (tx.IsClient()) {-        // Only client sends method ref.-        RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));-      }-      RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));-      for (const auto& md : tx.GetPrefixMetadata()) {-        RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));-        RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));-      }+  if (CanBeSentInOneTransaction(tx)) {+    return RpcCallFastPath(tx);+  }+  // Slow path: the message data is too large to fit in one transaction.+  int& seq = seq_num_[tx.GetTxCode()];+  int original_flags = tx.GetFlags();+  GPR_ASSERT(original_flags & kFlagMessageData);+  absl::string_view data = tx.GetMessageData();+  size_t ptr = 0;+  while (ptr < data.size()) {+    if (!WaitForAcknowledgement()) {+      return absl::InternalError(""Timeout waiting for acknowledgement"");     }-    if (tx.GetFlags() & kFlagMessageData) {-      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(tx.GetMessageData()));+    RETURN_IF_ERROR(binder_->PrepareTransaction());+    WritableParcel* parcel = binder_->GetWritableParcel();+    size_t size = std::min(static_cast<size_t>(kBlockSize), data.size() - ptr);+    int flags = GetFlagForCurrentTransaction(+        original_flags, /*sent_length=*/ptr,+        /*remaining_length=*/data.size() - ptr - size);+    RETURN_IF_ERROR(parcel->WriteInt32(flags));+    RETURN_IF_ERROR(parcel->WriteInt32(seq++));+    if (flags & kFlagPrefix) {+      RETURN_IF_ERROR(WriteInitialMetadata(tx, parcel));     }-    if (tx.GetFlags() & kFlagSuffix) {-      if (tx.IsServer()) {-        if (tx.GetFlags() & kFlagStatusDescription) {-          RETURN_IF_ERROR(parcel->WriteString(tx.GetStatusDesc()));-        }-        RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSuffixMetadata().size()));-        for (const auto& md : tx.GetSuffixMetadata()) {-          RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));-          RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));-        }-      } else {-        // client suffix currently is always empty according to the wireformat-        if (!tx.GetSuffixMetadata().empty()) {-          gpr_log(GPR_ERROR, ""Got non-empty suffix metadata from client."");-        }-      }+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(data.substr(ptr, size)));+    if (flags & kFlagSuffix) {+      RETURN_IF_ERROR(WriteTrailingMetadata(tx, parcel));     }+    RETURN_IF_ERROR(binder_->Transact(BinderTransportTxCode(tx.GetTxCode())));+    ptr += size;+    // TODO(waynetu): This should be parcel->GetDataSize().+    num_outgoing_bytes_ += size;   }-  // FIXME(waynetu): Construct BinderTransportTxCode from an arbitrary integer-  // is an undefined behavior.-  return binder_->Transact(BinderTransportTxCode(tx.GetTxCode()));+  return absl::OkStatus(); } -absl::Status WireWriterImpl::Ack(int64_t num_bytes) {+absl::Status WireWriterImpl::SendAck(int64_t num_bytes) {   grpc_core::MutexLock lock(&mu_);   RETURN_IF_ERROR(binder_->PrepareTransaction());   WritableParcel* parcel = binder_->GetWritableParcel();   RETURN_IF_ERROR(parcel->WriteInt64(num_bytes));   return binder_->Transact(BinderTransportTxCode::ACKNOWLEDGE_BYTES); } +void WireWriterImpl::RecvAck(int64_t num_bytes) {+  grpc_core::MutexLock lock(&mu_);",Waiting on a conditional variable will release the mutex IIRC.,
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27243,706537363,2021-09-11T01:20:11Z,src/core/ext/transport/binder/wire_format/wire_writer.cc,"@@ -30,60 +30,160 @@ namespace grpc_binder { WireWriterImpl::WireWriterImpl(std::unique_ptr<Binder> binder)     : binder_(std::move(binder)) {} +absl::Status WireWriterImpl::WriteInitialMetadata(const Transaction& tx,+                                                  WritableParcel* parcel) {+  if (tx.IsClient()) {+    // Only client sends method ref.+    RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));+  }+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));+  for (const auto& md : tx.GetPrefixMetadata()) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+  }+  return absl::OkStatus();+}++absl::Status WireWriterImpl::WriteTrailingMetadata(const Transaction& tx,+                                                   WritableParcel* parcel) {+  if (tx.IsServer()) {+    if (tx.GetFlags() & kFlagStatusDescription) {+      RETURN_IF_ERROR(parcel->WriteString(tx.GetStatusDesc()));+    }+    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSuffixMetadata().size()));+    for (const auto& md : tx.GetSuffixMetadata()) {+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+    }+  } else {+    // client suffix currently is always empty according to the wireformat+    if (!tx.GetSuffixMetadata().empty()) {+      gpr_log(GPR_ERROR, ""Got non-empty suffix metadata from client."");+    }+  }+  return absl::OkStatus();+}++const int64_t WireWriterImpl::kBlockSize = 16 * 1024;+const int64_t WireWriterImpl::kFlowControlWindowSize = 128 * 1024;++bool WireWriterImpl::CanBeSentInOneTransaction(const Transaction& tx) const {+  return (tx.GetFlags() & kFlagMessageData) == 0 ||+         tx.GetMessageData().size() <= kBlockSize;+}++absl::Status WireWriterImpl::RpcCallFastPath(const Transaction& tx) {+  int& seq = seq_num_[tx.GetTxCode()];+  // Fast path: send data in one transaction.+  RETURN_IF_ERROR(binder_->PrepareTransaction());+  WritableParcel* parcel = binder_->GetWritableParcel();+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetFlags()));+  RETURN_IF_ERROR(parcel->WriteInt32(seq++));+  if (tx.GetFlags() & kFlagPrefix) {+    RETURN_IF_ERROR(WriteInitialMetadata(tx, parcel));+  }+  if (tx.GetFlags() & kFlagMessageData) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(tx.GetMessageData()));+  }+  if (tx.GetFlags() & kFlagSuffix) {+    RETURN_IF_ERROR(WriteTrailingMetadata(tx, parcel));+  }+  // FIXME(waynetu): Construct BinderTransportTxCode from an arbitrary integer+  // is an undefined behavior.+  return binder_->Transact(BinderTransportTxCode(tx.GetTxCode()));+}++bool WireWriterImpl::WaitForAcknowledgement() {+  if (num_outgoing_bytes_ < num_acknowledged_bytes_ + kFlowControlWindowSize) {+    return true;+  }+  absl::Time deadline = absl::Now() + absl::Seconds(1);+  do {+    if (cv_.WaitWithDeadline(&mu_, deadline)) {+      return false;+    }+    if (absl::Now() >= deadline) {+      return false;+    }+  } while (num_outgoing_bytes_ >=+           num_acknowledged_bytes_ + kFlowControlWindowSize);+  return true;+}++int WireWriterImpl::GetFlagForCurrentTransaction(int original_flags,+                                                 int sent_length,+                                                 int remaining_length) const {+  int flags = kFlagMessageData;+  if (sent_length == 0) {+    // This is the first transaction. Include initial metadata if there's any.+    if (original_flags & kFlagPrefix) {+      flags |= kFlagPrefix;+    }+  }+  if (remaining_length == 0) {+    // This is the last transaction. Include trailing metadata if there's any.+    if (original_flags & kFlagSuffix) {+      flags |= kFlagSuffix;+    }+  } else {+    // There are more messages to send.+    flags |= kFlagMessageDataIsPartial;+  }+  return flags;+}+ absl::Status WireWriterImpl::RpcCall(const Transaction& tx) {   // TODO(mingcl): check tx_code <= last call id   grpc_core::MutexLock lock(&mu_);   GPR_ASSERT(tx.GetTxCode() >= kFirstCallId);-  RETURN_IF_ERROR(binder_->PrepareTransaction());-  WritableParcel* parcel = binder_->GetWritableParcel();-  {-    //  fill parcel-    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetFlags()));-    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSeqNum()));-    if (tx.GetFlags() & kFlagPrefix) {-      // prefix set-      if (tx.IsClient()) {-        // Only client sends method ref.-        RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));-      }-      RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));-      for (const auto& md : tx.GetPrefixMetadata()) {-        RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));-        RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));-      }+  if (CanBeSentInOneTransaction(tx)) {+    return RpcCallFastPath(tx);+  }+  // Slow path: the message data is too large to fit in one transaction.+  int& seq = seq_num_[tx.GetTxCode()];+  int original_flags = tx.GetFlags();+  GPR_ASSERT(original_flags & kFlagMessageData);+  absl::string_view data = tx.GetMessageData();+  size_t ptr = 0;+  while (ptr < data.size()) {+    if (!WaitForAcknowledgement()) {+      return absl::InternalError(""Timeout waiting for acknowledgement"");     }-    if (tx.GetFlags() & kFlagMessageData) {-      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(tx.GetMessageData()));+    RETURN_IF_ERROR(binder_->PrepareTransaction());+    WritableParcel* parcel = binder_->GetWritableParcel();+    size_t size = std::min(static_cast<size_t>(kBlockSize), data.size() - ptr);+    int flags = GetFlagForCurrentTransaction(+        original_flags, /*sent_length=*/ptr,+        /*remaining_length=*/data.size() - ptr - size);+    RETURN_IF_ERROR(parcel->WriteInt32(flags));+    RETURN_IF_ERROR(parcel->WriteInt32(seq++));+    if (flags & kFlagPrefix) {+      RETURN_IF_ERROR(WriteInitialMetadata(tx, parcel));","It would work, but I'm not sure why it will be beneficial to send them separately. Seems like the Java implementation also pack them together if possible: https://github.com/grpc/grpc-java/blob/master/binder/src/main/java/io/grpc/binder/internal/Outbound.java#L222.",X
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27243,706541632,2021-09-11T01:57:40Z,src/core/ext/transport/binder/wire_format/wire_writer.cc,"@@ -30,60 +30,160 @@ namespace grpc_binder { WireWriterImpl::WireWriterImpl(std::unique_ptr<Binder> binder)     : binder_(std::move(binder)) {} +absl::Status WireWriterImpl::WriteInitialMetadata(const Transaction& tx,+                                                  WritableParcel* parcel) {+  if (tx.IsClient()) {+    // Only client sends method ref.+    RETURN_IF_ERROR(parcel->WriteString(tx.GetMethodRef()));+  }+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetPrefixMetadata().size()));+  for (const auto& md : tx.GetPrefixMetadata()) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+  }+  return absl::OkStatus();+}++absl::Status WireWriterImpl::WriteTrailingMetadata(const Transaction& tx,+                                                   WritableParcel* parcel) {+  if (tx.IsServer()) {+    if (tx.GetFlags() & kFlagStatusDescription) {+      RETURN_IF_ERROR(parcel->WriteString(tx.GetStatusDesc()));+    }+    RETURN_IF_ERROR(parcel->WriteInt32(tx.GetSuffixMetadata().size()));+    for (const auto& md : tx.GetSuffixMetadata()) {+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.first));+      RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(md.second));+    }+  } else {+    // client suffix currently is always empty according to the wireformat+    if (!tx.GetSuffixMetadata().empty()) {+      gpr_log(GPR_ERROR, ""Got non-empty suffix metadata from client."");+    }+  }+  return absl::OkStatus();+}++const int64_t WireWriterImpl::kBlockSize = 16 * 1024;+const int64_t WireWriterImpl::kFlowControlWindowSize = 128 * 1024;++bool WireWriterImpl::CanBeSentInOneTransaction(const Transaction& tx) const {+  return (tx.GetFlags() & kFlagMessageData) == 0 ||+         tx.GetMessageData().size() <= kBlockSize;+}++absl::Status WireWriterImpl::RpcCallFastPath(const Transaction& tx) {+  int& seq = seq_num_[tx.GetTxCode()];+  // Fast path: send data in one transaction.+  RETURN_IF_ERROR(binder_->PrepareTransaction());+  WritableParcel* parcel = binder_->GetWritableParcel();+  RETURN_IF_ERROR(parcel->WriteInt32(tx.GetFlags()));+  RETURN_IF_ERROR(parcel->WriteInt32(seq++));+  if (tx.GetFlags() & kFlagPrefix) {+    RETURN_IF_ERROR(WriteInitialMetadata(tx, parcel));+  }+  if (tx.GetFlags() & kFlagMessageData) {+    RETURN_IF_ERROR(parcel->WriteByteArrayWithLength(tx.GetMessageData()));+  }+  if (tx.GetFlags() & kFlagSuffix) {+    RETURN_IF_ERROR(WriteTrailingMetadata(tx, parcel));+  }+  // FIXME(waynetu): Construct BinderTransportTxCode from an arbitrary integer+  // is an undefined behavior.+  return binder_->Transact(BinderTransportTxCode(tx.GetTxCode()));+}++bool WireWriterImpl::WaitForAcknowledgement() {+  if (num_outgoing_bytes_ < num_acknowledged_bytes_ + kFlowControlWindowSize) {+    return true;+  }+  absl::Time deadline = absl::Now() + absl::Seconds(1);+  do {+    if (cv_.WaitWithDeadline(&mu_, deadline)) {","Hmm ok, I was thinking that the loop will run 4 or 8 times at most anyway. But I guess deadline also makes a lot of sense",X
133680,sampajano,https://api.github.com/repos/grpc/grpc/pulls/27338,707762329,2021-09-13T22:30:05Z,test/core/iomgr/ios/CFStreamTests/CFStreamEndpointTests.mm,"@@ -89,17 +89,11 @@ - (BOOL)waitForEvent:(gpr_atm *)event timeout:(int)timeout {   return (gpr_atm_acq_load(event) != -1); } -+ (void)setUp {-  grpc_init();-}--+ (void)tearDown {-  grpc_shutdown();-}- - (void)setUp {   self.continueAfterFailure = NO; +  grpc_init();",I wonder if grpc_init() is generally needed per test?I'm slightly concerned that we might hide some code reentrancy issues when it's done per test. (but leaving to your judgement :)),X
18630764,TaWeiTu,https://api.github.com/repos/grpc/grpc/pulls/27122,709910340,2021-09-16T08:40:22Z,src/core/ext/transport/binder/utils/transport_stream_receiver_impl.cc,"@@ -117,7 +117,7 @@ void TransportStreamReceiverImpl::RegisterRecvTrailingMetadata( void TransportStreamReceiverImpl::NotifyRecvInitialMetadata(     StreamIdentifier id, absl::StatusOr<Metadata> initial_metadata) {   gpr_log(GPR_INFO, ""%s id = %d is_client = %d"", __func__, id, is_client_);-  if (!is_client_ && accept_stream_callback_) {+  if (!is_client_ && accept_stream_callback_ && initial_metadata.ok()) {",We probably don't want to open a new stream for invalid initial metadata. This was inspired by the server fuzzer and since this is so small I included it here. I can also open a separate PR for this.,
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/27363,710488697,2021-09-16T21:10:38Z,src/cpp/README.md,"@@ -19,13 +19,20 @@ We will make our best effort to support them, and we welcome patches for such pl  | Operating System | Architectures | Versions | Support Level | |------------------|---------------|----------|---------------|-| Linux - Debian, Ubuntu, CentOS | x86, x64      | clang 3.4+, GCC 4.9+   | Supported |-| Windows 10+                    | x86, x64      | Visual Studio 2015+    | Supported |-| MacOS                          | x86, x64      | XCode 7.2+             | Supported |-| Linux - Others                 | x86, x64      | clang 3.4+, GCC 4.9+   | Best Effort |-| Linux                          | ARM           |                        | Best Effort |-| iOS                            |               |                        | Best Effort |-| Android                        |               |                        | Best Effort |+| Linux - Debian, Ubuntu, CentOS | x86, x64      | clang 3.4+, GCC 4.9+   | Officially Supported |+| Windows 10+                    | x86, x64      | Visual Studio 2015+    | Officially Supported |+| MacOS                          | x86, x64      | XCode 7.2+             | Officially Supported |+| Linux - Others                 | x86, x64      | clang 3.4+, GCC 4.9+   | Best Effort          |+| Linux                          | ARM           |                        | Best Effort          |+| iOS                            |               |                        | Best Effort          |+| Android                        |               |                        | Best Effort          |+| Asylo                          |               |                        | Best Effort          |+| FreeBSD                        |               |                        | Community Supported  |",Please define `Community Supported` above along with `Supported` and `Best Effort`,
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27363,710537905,2021-09-16T22:21:33Z,src/cpp/README.md,"@@ -19,13 +19,20 @@ We will make our best effort to support them, and we welcome patches for such pl  | Operating System | Architectures | Versions | Support Level | |------------------|---------------|----------|---------------|-| Linux - Debian, Ubuntu, CentOS | x86, x64      | clang 3.4+, GCC 4.9+   | Supported |-| Windows 10+                    | x86, x64      | Visual Studio 2015+    | Supported |-| MacOS                          | x86, x64      | XCode 7.2+             | Supported |-| Linux - Others                 | x86, x64      | clang 3.4+, GCC 4.9+   | Best Effort |-| Linux                          | ARM           |                        | Best Effort |-| iOS                            |               |                        | Best Effort |-| Android                        |               |                        | Best Effort |+| Linux - Debian, Ubuntu, CentOS | x86, x64      | clang 3.4+, GCC 4.9+   | Officially Supported |+| Windows 10+                    | x86, x64      | Visual Studio 2015+    | Officially Supported |+| MacOS                          | x86, x64      | XCode 7.2+             | Officially Supported |+| Linux - Others                 | x86, x64      | clang 3.4+, GCC 4.9+   | Best Effort          |+| Linux                          | ARM           |                        | Best Effort          |+| iOS                            |               |                        | Best Effort          |+| Android                        |               |                        | Best Effort          |+| Asylo                          |               |                        | Best Effort          |+| FreeBSD                        |               |                        | Community Supported  |","Done, PTAL. I've also word-wrapped the list items, there are no significant changes in the other definitions.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27327,710544662,2021-09-16T22:38:20Z,src/core/lib/resource_quota/memory_quota.cc,"@@ -0,0 +1,375 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include ""src/core/lib/resource_quota/memory_quota.h""++#include <thread>++#include ""src/core/lib/promise/loop.h""+#include ""src/core/lib/promise/race.h""+#include ""src/core/lib/promise/seq.h""++namespace grpc_core {++//+// Reclaimer+//++ReclamationSweep::~ReclamationSweep() {+  if (memory_quota_ != nullptr) {+    memory_quota_->FinishReclamation(sweep_token_);+  }+}++//+// MemoryRequest+//++namespace {+size_t RoundUp(size_t size, size_t block_size) {+  return (size + block_size - 1) / block_size * block_size;+}+size_t RoundDown(size_t size, size_t block_size) {+  return size / block_size * block_size;+}+}  // namespace++MemoryRequest MemoryRequest::WithBlockSize(size_t block_size) const {+  MemoryRequest r(*this);+  r.block_size_ = block_size;+  return r;+}++MemoryRequest MemoryRequest::Increase(size_t amount) const {+  MemoryRequest r(min_ + amount, max_ + amount);+  r.block_size_ = block_size_;+  return r;+}++//+// ReclaimerQueue+//++const ReclaimerQueue::Index ReclaimerQueue::kInvalidIndex;",Or we can make it not-class-scoped.,
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27369,711223959,2021-09-17T17:14:28Z,src/core/lib/promise/activity.h,"@@ -170,8 +178,8 @@ class Activity : private Wakeable {   static bool have_current() { return g_current_activity_ != nullptr; }   // Check if we got an internal wakeup since the last time this function was   // called.-  bool got_wakeup() ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_) {-    return absl::exchange(got_wakeup_during_run_, false);+  ActionDuringRun got_action_during_run() ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_) {","nit: since this is not a plain accessor, function names should be mixed case, like `GotActionDuringRun()`. See https://google.github.io/styleguide/cppguide.html#Function_Names. This class has a few such issues, so it can be handled in another cleanup (or argued, it's debatable).",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27369,711225647,2021-09-17T17:17:15Z,src/core/lib/promise/activity.h,"@@ -208,12 +222,20 @@ class Activity : private Wakeable {   bool RefIfNonzero();   // Drop the (proved existing) wait handle.   void DropHandle() ABSL_EXCLUSIVE_LOCKS_REQUIRED(mu_);+  // Set the action that occured during this run.+  // We use max to combine actions so that cancellation overrides wakeups.","I wonder if it's worth commenting about this on the enum class itself, that any newly-added value will override a smaller value when considering the overall result of multiple actions during a run. That's non-intuitive.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27388,711263240,2021-09-17T18:20:25Z,src/core/ext/transport/chttp2/server/chttp2_server.cc,"@@ -692,15 +692,15 @@ Chttp2ServerListener::~Chttp2ServerListener() { void Chttp2ServerListener::Start(     Server* /*server*/, const std::vector<grpc_pollset*>* /* pollsets */) {   if (server_->config_fetcher() != nullptr) {",We seem to be tying too many things together here. For this change I'd expect no changes to chttp2 to be necessary. Suggest undoing this dependency prior to making this change.,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,711390177,2021-09-17T22:53:59Z,src/core/tsi/ssl/key_logging/ssl_key_logging.h,"@@ -0,0 +1,163 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_TSI_SSL_KEY_LOGGING_SSL_KEY_LOGGING_H+#define GRPC_CORE_TSI_SSL_KEY_LOGGING_SSL_KEY_LOGGING_H++#include <grpc/support/port_platform.h>++#include <iostream>++#include <grpc/grpc_security.h>+#include <grpc/slice.h>+#include <grpc/support/sync.h>++extern ""C"" {+#include <openssl/ssl.h>+}++#include ""src/core/lib/gprpp/memory.h""+#include ""src/core/lib/gprpp/ref_counted.h""+#include ""src/core/lib/gprpp/sync.h""+++/// Configuration for tls key logging. Defines the opaque struct specified in+/// grpc_security.h+struct grpc_tls_key_log_config {+ public:+  /// The full path at which the TLS keys would be exported.+  std::string tls_key_log_file_path;+  /// The format in which the TLS keys would be exported.+  grpc_tls_key_log_format key_logging_format;++  /// Future extensions can include filters such as IP addresses etc.++  /// Constructor.+  explicit grpc_tls_key_log_config()+      : tls_key_log_file_path(""""),+        key_logging_format(grpc_tls_key_log_format::GRPC_TLS_KEY_LOG_FORMAT_NSS){};++  /// Copy ctor.+  grpc_tls_key_log_config(const grpc_tls_key_log_config& copy) {+    tls_key_log_file_path = copy.tls_key_log_file_path;+    key_logging_format = copy.key_logging_format;+  }++  /// Assignment operation.+  grpc_tls_key_log_config& operator=(const grpc_tls_key_log_config& other) {","Are these custom constructors actually needed? If not, we can make this struct an aggregate class? That can make other part of the code a bit easier as well.",X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,711391010,2021-09-17T22:56:59Z,src/core/tsi/ssl/key_logging/ssl_key_logging.h,"@@ -0,0 +1,163 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_TSI_SSL_KEY_LOGGING_SSL_KEY_LOGGING_H+#define GRPC_CORE_TSI_SSL_KEY_LOGGING_SSL_KEY_LOGGING_H++#include <grpc/support/port_platform.h>++#include <iostream>++#include <grpc/grpc_security.h>+#include <grpc/slice.h>+#include <grpc/support/sync.h>++extern ""C"" {+#include <openssl/ssl.h>+}++#include ""src/core/lib/gprpp/memory.h""+#include ""src/core/lib/gprpp/ref_counted.h""+#include ""src/core/lib/gprpp/sync.h""+++/// Configuration for tls key logging. Defines the opaque struct specified in+/// grpc_security.h+struct grpc_tls_key_log_config {+ public:+  /// The full path at which the TLS keys would be exported.+  std::string tls_key_log_file_path;+  /// The format in which the TLS keys would be exported.+  grpc_tls_key_log_format key_logging_format;++  /// Future extensions can include filters such as IP addresses etc.++  /// Constructor.+  explicit grpc_tls_key_log_config()+      : tls_key_log_file_path(""""),+        key_logging_format(grpc_tls_key_log_format::GRPC_TLS_KEY_LOG_FORMAT_NSS){};++  /// Copy ctor.+  grpc_tls_key_log_config(const grpc_tls_key_log_config& copy) {+    tls_key_log_file_path = copy.tls_key_log_file_path;+    key_logging_format = copy.key_logging_format;+  }++  /// Assignment operation.+  grpc_tls_key_log_config& operator=(const grpc_tls_key_log_config& other) {+      tls_key_log_file_path = other.tls_key_log_file_path;+      key_logging_format = other.key_logging_format;+      return *this;+  }+};++/// Instance to facilitate logging of SSL/TLS session keys to aid debugging.+///+/// Keys logged by an instance of this class help decrypting packet captures+/// with tools like wireshark.+///+/// This class is thread safe and serializes access to keylog files.",Are these comments supposed to be somewhere else(I think they are comments for a specific class)?,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,711440828,2021-09-18T03:36:19Z,src/core/tsi/ssl/key_logging/ssl_key_logging.h,"@@ -0,0 +1,163 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_TSI_SSL_KEY_LOGGING_SSL_KEY_LOGGING_H+#define GRPC_CORE_TSI_SSL_KEY_LOGGING_SSL_KEY_LOGGING_H++#include <grpc/support/port_platform.h>++#include <iostream>++#include <grpc/grpc_security.h>+#include <grpc/slice.h>+#include <grpc/support/sync.h>++extern ""C"" {+#include <openssl/ssl.h>+}++#include ""src/core/lib/gprpp/memory.h""+#include ""src/core/lib/gprpp/ref_counted.h""+#include ""src/core/lib/gprpp/sync.h""+++/// Configuration for tls key logging. Defines the opaque struct specified in+/// grpc_security.h+struct grpc_tls_key_log_config {+ public:+  /// The full path at which the TLS keys would be exported.+  std::string tls_key_log_file_path;+  /// The format in which the TLS keys would be exported.+  grpc_tls_key_log_format key_logging_format;++  /// Future extensions can include filters such as IP addresses etc.++  /// Constructor.+  explicit grpc_tls_key_log_config()+      : tls_key_log_file_path(""""),+        key_logging_format(grpc_tls_key_log_format::GRPC_TLS_KEY_LOG_FORMAT_NSS){};++  /// Copy ctor.+  grpc_tls_key_log_config(const grpc_tls_key_log_config& copy) {+    tls_key_log_file_path = copy.tls_key_log_file_path;+    key_logging_format = copy.key_logging_format;+  }++  /// Assignment operation.+  grpc_tls_key_log_config& operator=(const grpc_tls_key_log_config& other) {+      tls_key_log_file_path = other.tls_key_log_file_path;+      key_logging_format = other.key_logging_format;+      return *this;+  }+};++/// Instance to facilitate logging of SSL/TLS session keys to aid debugging.+///+/// Keys logged by an instance of this class help decrypting packet captures+/// with tools like wireshark.+///+/// This class is thread safe and serializes access to keylog files.++namespace tsi {++/// Configuration for key logging.+typedef struct grpc_tls_key_log_config TsiTlsKeyLoggerConfig;++/// A helper class which facilitates appending Tls keys into a file.+/// The instance is bound to a file meaning only one instance of this object+/// can ever exist for a given file path.+class TlsKeyLogFileWriter : public grpc_core::RefCounted<TlsKeyLogFileWriter> {","(Responding to the comments of having two classes `TlsKeyLogFileWriter` and `TlsKeyLogger`, as I was unable to reply to the original comments)Instead of declaring a global map in the `.cc` file, have we considered the possibility to make `TlsKeyLogger` a global instance, which keeps a map of all the current used `TlsKeyLogFileWriter` instances? It could be initialized at the construction time, and whenever a particular file name was requested during a connection, we could just access the instance from the map. ",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,711475634,2021-09-18T04:33:18Z,src/core/tsi/ssl/key_logging/ssl_key_logging.cc,"@@ -0,0 +1,158 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include <map>++#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/iomgr/error.h""+#include ""src/core/lib/slice/slice_internal.h""+#include ""src/core/tsi/ssl/key_logging/ssl_key_logging.h""++#include <grpc/support/log.h>+#include <grpc/support/string_util.h>++namespace tsi {++#if OPENSSL_VERSION_NUMBER >= 0x10100000 && !defined(LIBRESSL_VERSION_NUMBER)+grpc_core::Mutex* g_tls_key_logger_registry_mu;+std::map<std::string, TlsKeyLogFileWriter*> g_tls_key_log_file_writer_map+  ABSL_GUARDED_BY(g_tls_key_logger_registry_mu);","This was the map I mentioned in my another comment in `ssl_key_logging.cc`. If possible, I would tend to avoid declaring global variables here, but instead attach them to some classes instead. That in my opinion is more structured and readable.",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,711493269,2021-09-18T05:22:14Z,src/core/tsi/ssl/key_logging/ssl_key_logging.cc,"@@ -0,0 +1,158 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include <map>++#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/iomgr/error.h""+#include ""src/core/lib/slice/slice_internal.h""+#include ""src/core/tsi/ssl/key_logging/ssl_key_logging.h""++#include <grpc/support/log.h>+#include <grpc/support/string_util.h>++namespace tsi {++#if OPENSSL_VERSION_NUMBER >= 0x10100000 && !defined(LIBRESSL_VERSION_NUMBER)+grpc_core::Mutex* g_tls_key_logger_registry_mu;+std::map<std::string, TlsKeyLogFileWriter*> g_tls_key_log_file_writer_map+  ABSL_GUARDED_BY(g_tls_key_logger_registry_mu);+static std::atomic<bool> g_tls_key_logger_registry_initialized(false);+#endif++TlsKeyLogFileWriter::TlsKeyLogFileWriter(+    const std::string& tls_key_log_file_path)+    : fd_(nullptr), tls_key_log_file_path_(tls_key_log_file_path) {+  GPR_ASSERT(!tls_key_log_file_path_.empty());+  fd_ = fopen(tls_key_log_file_path_.c_str(), ""w+"");+  grpc_error_handle error = GRPC_ERROR_NONE;++  if (fd_ == nullptr) {+    error = GRPC_OS_ERROR(errno, ""fopen"");+  }++  if (error != GRPC_ERROR_NONE) {+    gpr_log(GPR_ERROR,+            ""Ignoring TLS Key logging. ERROR Opening TLS Keylog ""+            ""file: %s"", grpc_error_std_string(error).c_str());+  }+};++TlsKeyLogFileWriter::~TlsKeyLogFileWriter() {+  if (fd_ != nullptr) fclose(fd_);+  {+    grpc_core::MutexLock lock(g_tls_key_logger_registry_mu);+    g_tls_key_log_file_writer_map.erase(tls_key_log_file_path_);+  }+}++void TlsKeyLogFileWriter::AppendSessionKeys(+    SSL_CTX* /* ssl_context */,+    const TsiTlsKeyLoggerConfig& tsi_tls_key_log_config,+    const std::string& session_keys_info) {+  grpc_core::MutexLock lock(&lock_);++  if (fd_ == nullptr || session_keys_info.empty()) return;++  switch (tsi_tls_key_log_config.key_logging_format) {+    case GRPC_TLS_KEY_LOG_FORMAT_NSS:+      // Append to key log file under lock+      bool err;++      err = (fwrite(session_keys_info.c_str(), sizeof(char),+                    session_keys_info.length(),+                    fd_) < session_keys_info.length());++      // Append new-line character as well. Doing a separate syscall to avoid+      // dynamic memory allocation and modification of passed session_keys_info+      err |= (fwrite(""\n"", sizeof(char), 1, fd_) != 1);++      if (err) {+        grpc_error_handle error = GRPC_OS_ERROR(errno, ""fwrite"");+        gpr_log(GPR_ERROR, ""Error Appending to TLS Keylog file: %s"",+                grpc_error_std_string(error).c_str());+        fclose(fd_);+        fd_ = nullptr;  // disable future attempts to write to this file+      } else {+        fflush(fd_);+      }","Writing to a file can be a common pattern which might be helpful for other future code as well.  It seems we have helper function for file loading(`src/core/lib/iomgr/load_file.cc`), but not for file writing. As a future enhancement, maybe we can consider adding it.This doesn't have to be in this PR though.",X
294043,AustinSchuh,https://api.github.com/repos/grpc/grpc/pulls/19141,711792667,2021-09-19T19:38:00Z,src/core/lib/compression/message_compress.cc,"@@ -149,7 +149,13 @@ static int copy(grpc_slice_buffer* input, grpc_slice_buffer* output) { }  static int compress_inner(grpc_message_compression_algorithm algorithm,-                          grpc_slice_buffer* input, grpc_slice_buffer* output) {+                          grpc_slice_buffer* input, grpc_slice_buffer* output)+#if defined(__clang__)","If I'm remembering right, there is a test to see what happens when an enum is passed in which is not in the list of accepted enums.  Per the C++ standard, this is undefined behavior, so Clang's ubsan detects it and complains.I spent some time looking around and the best advice I found was to look for the presence of the __clang__ define.  Do you have any better ideas?(I'm happy to drop any pieces of this you want me to as well.  This is what we ran)",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27360,712302866,2021-09-20T15:56:21Z,src/core/lib/security/authorization/matchers.cc,"@@ -19,6 +19,7 @@ #include <grpc/grpc_security_constants.h>  #include ""src/core/lib/address_utils/sockaddr_utils.h""+#include ""src/core/lib/security/security_connector/tls/tls_security_connector.h""","I think this include has implications for the work that @yihuazhang is doing to remove insecure builds.  I think this would mean that the authz policy would depend on the TLS creds implementation, which means it would drag in a dependency on the SSL libraries.  That seems maybe more heavyweight than what we want here.Can we maybe define the transport security type in some other file, so that it doesn't drag in the SSL dependency?",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27327,712377690,2021-09-20T17:36:27Z,src/core/lib/resource_quota/memory_quota.h,"@@ -0,0 +1,322 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_RESOURCE_QUOTA_MEMORY_QUOTA_H+#define GRPC_CORE_LIB_RESOURCE_QUOTA_MEMORY_QUOTA_H++#include <grpc/support/port_platform.h>++#include <algorithm>+#include <cstddef>+#include <queue>+#include <vector>++#include ""src/core/lib/gprpp/dual_ref_counted.h""+#include ""src/core/lib/gprpp/orphanable.h""+#include ""src/core/lib/gprpp/ref_counted_ptr.h""+#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/promise/activity.h""+#include ""src/core/lib/promise/poll.h""++namespace grpc_core {++class Reclaimer;+class MemoryAllocator;+class MemoryQuota;++// Reclamation passes.+// When memory is tight, we start trying to claim some back from memory+// reclaimers. We do this in multiple passes: if there is a less destructive+// operation available, we do that, otherwise we do something more destructive.+enum class ReclamationPass {+  // Non-empty reclamation ought to take index 0, but to simplify API we don't+  // expose that publicly (it's an internal detail), and hence index zero is+  // here unnamed.++  // Benign reclamation is intended for reclamation steps that are not+  // observable outside of gRPC (besides maybe causing an increase in CPU+  // usage).+  // Examples of such reclamation would be resizing buffers to fit the current+  // load needs, rather than whatever was the peak usage requirement.+  kBenign = 1,+  // Idle reclamation is intended for reclamation steps that are observable+  // outside of gRPC, but do not cause application work to be lost.+  // Examples of such reclamation would be dropping channels that are not being+  // used.+  kIdle = 2,+  // Destructive reclamation is our last resort, and is these reclamations are+  // allowed to drop work - such as cancelling in flight requests.+  kDestructive = 3,+};+static constexpr size_t kNumReclamationPasses = 4;++// Reservation request - how much memory do we want to allocate?+class MemoryRequest {+ public:+  // Request a fixed amount of memory.+  // NOLINTNEXTLINE(google-explicit-constructor)+  MemoryRequest(size_t n) : min_(n), max_(n) {}+  // Request a range of memory.+  MemoryRequest(size_t min, size_t max) : min_(std::min(min, max)), max_(max) {}++  // Increase the size by amount+  MemoryRequest Increase(size_t amount) const {+    return MemoryRequest(min_ + amount, max_ + amount);+  }++  size_t min() const { return min_; }+  size_t max() const { return max_; }++ private:+  size_t min_;+  size_t max_;+};++// For each reclamation function run we construct a ReclamationSweep.+// When this object is finally destroyed (it may be moved several times first),+// then that reclamation is complete and we may continue the reclamation loop.+class ReclamationSweep {+ public:+  ReclamationSweep(WeakRefCountedPtr<MemoryQuota> memory_quota,+                   uint64_t sweep_token)+      : memory_quota_(std::move(memory_quota)), sweep_token_(sweep_token) {}+  ~ReclamationSweep();++  ReclamationSweep(const ReclamationSweep&) = delete;+  ReclamationSweep& operator=(const ReclamationSweep&) = delete;+  ReclamationSweep(ReclamationSweep&&) = default;+  ReclamationSweep& operator=(ReclamationSweep&&) = default;++  bool IsSufficient() const;++ private:+  WeakRefCountedPtr<MemoryQuota> memory_quota_;+  uint64_t sweep_token_;+};++using ReclamationFunction = std::function<void(ReclamationSweep)>;++class ReclaimerQueue {+ public:+  using Index = size_t;++  static constexpr Index kInvalidIndex = std::numeric_limits<Index>::max();++  Index Insert(RefCountedPtr<MemoryAllocator> allocator,+               ReclamationFunction reclaimer) ABSL_LOCKS_EXCLUDED(mu_);+  ReclamationFunction Cancel(Index index, MemoryAllocator* allocator)+      ABSL_LOCKS_EXCLUDED(mu_);+  Poll<ReclamationFunction> PollNext() ABSL_LOCKS_EXCLUDED(mu_);++  class NextPromise {+   public:+    explicit NextPromise(ReclaimerQueue* queue) : queue_(queue) {}+    Poll<ReclamationFunction> operator()() { return queue_->PollNext(); }++   private:+    ReclaimerQueue* queue_;+  };+  NextPromise Next() { return NextPromise(this); }++ private:+  Mutex mu_;+  struct Entry {+    Entry(RefCountedPtr<MemoryAllocator> allocator,+          ReclamationFunction reclaimer)+        : allocator(allocator), reclaimer(reclaimer) {}+    RefCountedPtr<MemoryAllocator> allocator;+    ReclamationFunction reclaimer;+  };+  std::vector<Entry> entries_ ABSL_GUARDED_BY(mu_);+  std::vector<size_t> free_entries_ ABSL_GUARDED_BY(mu_);+  std::queue<Index> queue_ ABSL_GUARDED_BY(mu_);+  Waker waker_ ABSL_GUARDED_BY(mu_);+};++// MemoryAllocator grants the owner the ability to allocate memory from an+// underlying resource quota.+class MemoryAllocator final : public InternallyRefCounted<MemoryAllocator> {+ public:+  explicit MemoryAllocator(RefCountedPtr<MemoryQuota> memory_quota);+  ~MemoryAllocator() override;++  void Orphan() override;++  // Rebind -  Swaps the underlying quota for this allocator, taking care to+  // make sure memory allocated is moved to allocations against the new quota.+  void Rebind(RefCountedPtr<MemoryQuota> memory_quota)+      ABSL_LOCKS_EXCLUDED(memory_quota_mu_);++  // Reserve bytes from the quota.+  // If we enter overcommit, reclamation will begin concurrently.+  // Returns the number of bytes reserved.+  size_t Reserve(MemoryRequest request) ABSL_LOCKS_EXCLUDED(memory_quota_mu_);++  // Release some bytes that were previously reserved.+  void Release(size_t n) ABSL_LOCKS_EXCLUDED(memory_quota_mu_) {+    // Add the released memory to our free bytes counter... if this increases+    // from  0 to non-zero, then we have more to do, otherwise, we're actually+    // done.+    if (free_bytes_.fetch_add(n, std::memory_order_release) != 0) return;+    MaybeRegisterReclaimer();+  }++  // Post a reclaimer for some reclamation pass.+  void PostReclaimer(ReclamationPass pass,+                     std::function<void(ReclamationSweep)>);++  // Allocate a new object of type T, with constructor arguments.+  // The returned type is wrapped, and upon destruction the reserved memory+  // will be released to the allocator automatically. As such, T must have a+  // virtual destructor so we can insert the necessary hook.+  template <typename T, typename... Args>+  absl::enable_if_t<std::has_virtual_destructor<T>::value, T*> New(+      Args&&... args) ABSL_LOCKS_EXCLUDED(memory_quota_mu_) {+    // Wrap T such that when it's destroyed, we can release memory back to the+    // allocator.+    class Wrapper final : public T {+     public:+      explicit Wrapper(RefCountedPtr<MemoryAllocator> allocator, Args&&... args)+          : T(std::forward<Args>(args)...), allocator_(std::move(allocator)) {}+      ~Wrapper() override { allocator_->Release(sizeof(*this)); }++     private:+      const RefCountedPtr<MemoryAllocator> allocator_;+    };+    Reserve(sizeof(Wrapper));+    return new Wrapper(Ref(), std::forward<Args>(args)...);+  }++  // Construct a unique ptr immediately.+  template <typename T, typename... Args>+  std::unique_ptr<T> MakeUnique(Args&&... args)+      ABSL_LOCKS_EXCLUDED(memory_quota_mu_) {+    return std::unique_ptr<T>(New<T>(std::forward<Args>(args)...));+  }++ private:+  // Primitive reservation function.+  absl::optional<size_t> TryReserve(MemoryRequest request) GRPC_MUST_USE_RESULT;+  // Replenish bytes from the quota, without blocking, possibly entering+  // overcommit.+  void Replenish() ABSL_LOCKS_EXCLUDED(memory_quota_mu_);+  // If we have not already, register a reclamation function against the quota+  // to sweep any free memory back to that quota.+  void MaybeRegisterReclaimer() ABSL_LOCKS_EXCLUDED(memory_quota_mu_);+  void MaybeRegisterReclaimerLocked()+      ABSL_EXCLUSIVE_LOCKS_REQUIRED(memory_quota_mu_);++  // Amount of memory this allocator has cached for its own use: to avoid quota+  // contention, each MemoryAllocator can keep some memory in addition to what+  // it is immediately using, and the quota can pull it back under memory+  // pressure.+  std::atomic<size_t> free_bytes_{0};+  // Mutex guarding the backing resource quota.+  Mutex memory_quota_mu_;+  // Backing resource quota.+  RefCountedPtr<MemoryQuota> memory_quota_ ABSL_GUARDED_BY(memory_quota_mu_);+  // Amount of memory taken from the quota by this allocator.+  size_t taken_bytes_ ABSL_GUARDED_BY(memory_quota_mu_) = 0;+  // Indices into the various reclaimer queues, used so that we can cancel+  // reclamation should we shutdown or get rebound.+  ReclaimerQueue::Index+      reclamation_indices_[kNumReclamationPasses] ABSL_GUARDED_BY(+          memory_quota_mu_) = {ReclaimerQueue::kInvalidIndex};+};++class AtomicBarrier {",Some light class documentation would be appreciated.,X
294043,AustinSchuh,https://api.github.com/repos/grpc/grpc/pulls/19141,712401595,2021-09-20T18:11:20Z,src/core/lib/compression/message_compress.cc,"@@ -149,7 +149,13 @@ static int copy(grpc_slice_buffer* input, grpc_slice_buffer* output) { }  static int compress_inner(grpc_message_compression_algorithm algorithm,-                          grpc_slice_buffer* input, grpc_slice_buffer* output) {+                          grpc_slice_buffer* input, grpc_slice_buffer* output)+#if defined(__clang__)",That is undefined behavior in C++.  @bsilver8192 knows a lot more about this part of the C++ standard.  He also didn't have any better ideas than looking for `__clang__`https://godbolt.org/z/GGYo7z shows a stripped down example.  Adding a default is still undefined behavior.,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,712585998,2021-09-20T23:28:20Z,src/core/tsi/ssl/key_logging/ssl_key_logging.cc,"@@ -0,0 +1,158 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include <map>++#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/iomgr/error.h""+#include ""src/core/lib/slice/slice_internal.h""+#include ""src/core/tsi/ssl/key_logging/ssl_key_logging.h""++#include <grpc/support/log.h>+#include <grpc/support/string_util.h>++namespace tsi {++#if OPENSSL_VERSION_NUMBER >= 0x10100000 && !defined(LIBRESSL_VERSION_NUMBER)+grpc_core::Mutex* g_tls_key_logger_registry_mu;+std::map<std::string, TlsKeyLogFileWriter*> g_tls_key_log_file_writer_map+  ABSL_GUARDED_BY(g_tls_key_logger_registry_mu);+static std::atomic<bool> g_tls_key_logger_registry_initialized(false);","Why do we want to make this initialization status information atomic? Could there be a possibility that the same code is going to be executed by different threads?And is it possible to encapsulate this(as well as the other information, e.g. the map) into a registry class?",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,712656620,2021-09-21T03:03:25Z,src/core/tsi/ssl_transport_security.cc,"@@ -1952,6 +1989,22 @@ tsi_result tsi_create_ssl_client_handshaker_factory_with_options(     SSL_CTX_set_session_cache_mode(ssl_context, SSL_SESS_CACHE_CLIENT);   } +  if (options->key_logger != nullptr) {+    // Unref is manually called on factory destruction+    impl->key_logger =+        reinterpret_cast<tsi::TlsKeyLogger*>(options->key_logger)->Ref();+    if (options->session_cache == nullptr) {+      // Need to set factory at g_ssl_ctx_ex_factory_index+      SSL_CTX_set_ex_data(ssl_context, g_ssl_ctx_ex_factory_index, impl);","Can we restructure the code to ```if (options->session_cache != nullptr) {    impl->session_cache = ...;    SSL_CTX_sess_set_new_cb(...);    SSL_CTX_set_session_cache_mode(...);}if (options->key_logger != nullptr) {    impl->key_logger = ...;    SSL_CTX_set_keylog_callback(...);}if (options->session_cache != nullptr || options->key_logger != nullptr) {   SSL_CTX_set_ex_data(...);}```We can probably also consider removing the last condition enclosing `SSL_CTX_set_ex_data`, since I think the cost is minimal(setting a pointer).",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,712663060,2021-09-21T03:18:38Z,src/core/tsi/ssl_transport_security.cc,"@@ -1952,6 +1989,22 @@ tsi_result tsi_create_ssl_client_handshaker_factory_with_options(     SSL_CTX_set_session_cache_mode(ssl_context, SSL_SESS_CACHE_CLIENT);   } +  if (options->key_logger != nullptr) {+    // Unref is manually called on factory destruction+    impl->key_logger =+        reinterpret_cast<tsi::TlsKeyLogger*>(options->key_logger)->Ref();+    if (options->session_cache == nullptr) {+      // Need to set factory at g_ssl_ctx_ex_factory_index+      SSL_CTX_set_ex_data(ssl_context, g_ssl_ctx_ex_factory_index, impl);+    }  // Otherwise it is already set++#if OPENSSL_VERSION_NUMBER >= 0x10100000","Is it possible to move the macros defined in `ssl_key_logging.cc` to this file, so that we don't need any additional macros in `ssl_key_logging.cc`(I just wanted to reduce the number of macros if possible, since they affect readability)?I think ideally speaking, `ssl_key_logging.cc` doesn't need to contain any macros, since it contains some self-contained classes that not restricted to specific OpenSSL versions. It was the caller of these classes(ssl_transport_security.cc) that can only work for certain OpenSSL versions.",X
2999225,bsilver8192,https://api.github.com/repos/grpc/grpc/pulls/19141,712670412,2021-09-21T03:32:52Z,src/core/lib/compression/message_compress.cc,"@@ -149,7 +149,13 @@ static int copy(grpc_slice_buffer* input, grpc_slice_buffer* output) { }  static int compress_inner(grpc_message_compression_algorithm algorithm,-                          grpc_slice_buffer* input, grpc_slice_buffer* output) {+                          grpc_slice_buffer* input, grpc_slice_buffer* output)+#if defined(__clang__)","I think [test/core/util/ubsan_suppressions.txt](https://github.com/grpc/grpc/blob/b669a3c521809c9b193a8f39b7d9195014b3f19c/test/core/util/ubsan_suppressions.txt#L7) covers all of these, and dropping these parts of the patch makes the most sense.About why default doesn't help: pre-C++17 the conversion gives an unspecified result, and C++17 downgrades it to undefined behavior. The problem is the underlying type of the enum might only have enough bits to hold all the values in the enum (because it's a non-`enum class` without a specified underlying type), which means converting any values that require more bits to represent isn't well-defined.",
148994,clayg,https://api.github.com/repos/grpc/grpc/pulls/27240,713222118,2021-09-21T16:33:34Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -468,6 +495,24 @@ async def test_error_without_raise_in_stream_stream(self):          self.assertEqual(grpc.StatusCode.INTERNAL, await call.code()) +    async def test_error_in_stream_unary(self):+        stream_unary_call = self._channel.stream_unary(_ERROR_IN_STREAM_UNARY)++        finished = []++        def request_gen():+            for _ in range(_NUM_STREAM_REQUESTS):+                yield _REQUEST+            finished.append(True)++        call = stream_unary_call(request_gen())++        with self.assertRaises(aio.AioRpcError) as exception_context:+            await call","without my change in _call the *server error* causes this await to raise an inscrutible CancelledError```(grpc-test) clayg@banana:~/Workspace/scratch/grpc-test/.scratch/grpc$ pytest src/python/grpcio_tests/tests_aio/unit/server_test.py::TestServer::test_error_in_stream_unary=========================================================================================== test session starts ===========================================================================================platform linux -- Python 3.8.5, pytest-6.2.2, py-1.10.0, pluggy-0.13.1rootdir: /home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio_testscollected 1 item                                                                                                                                                                                          src/python/grpcio_tests/tests_aio/unit/server_test.py F                                                                                                                                             [100%]================================================================================================ FAILURES =================================================================================================__________________________________________________________________________________ TestServer.test_error_in_stream_unary __________________________________________________________________________________src/python/grpcio_tests/tests_aio/unit/_test_base.py:32: in wrapper    return loop.run_until_complete(f(*args, **kwargs))_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _self = <_UnixSelectorEventLoop running=False closed=False debug=True>future = <Task cancelled name='Task-3' coro=<TestServer.test_error_in_stream_unary() done, defined at /home/clayg/Workspace/scr...pc/src/python/grpcio_tests/tests_aio/unit/server_test.py:498> created at /usr/lib/python3.8/asyncio/base_events.py:595>    def run_until_complete(self, future):        """"""Run until the Future is done.            If the argument is a coroutine, it is wrapped in a Task.            WARNING: It would be disastrous to call run_until_complete()        with the same coroutine twice -- it would wrap it in two        different Tasks and that can't be good.            Return the Future's result, or raise its exception.        """"""        self._check_closed()        self._check_running()            new_task = not futures.isfuture(future)        future = tasks.ensure_future(future, loop=self)        if new_task:            # An exception is raised if the future didn't complete, so there            # is no need to log the ""destroy pending task"" message            future._log_destroy_pending = False            future.add_done_callback(_run_until_complete_cb)        try:            self.run_forever()        except:            if new_task and future.done() and not future.cancelled():                # The coroutine raised a BaseException. Consume the exception                # to not log a warning, the caller doesn't have access to the                # local task.                future.exception()            raise        finally:            future.remove_done_callback(_run_until_complete_cb)        if not future.done():            raise RuntimeError('Event loop stopped before Future completed.')    >       return future.result()E       asyncio.exceptions.CancelledError/usr/lib/python3.8/asyncio/base_events.py:616: CancelledError-------------------------------------------------------------------------------------------- Captured log call --------------------------------------------------------------------------------------------ERROR    grpc._cython.cygrpc:events.py:81 Unexpected [ValueError] raised by servicer method [/test/ErrorInStreamUnary]Traceback (most recent call last):  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi"", line 664, in grpc._cython.cygrpc._handle_exceptions    await rpc_coro  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi"", line 777, in _handle_rpc    await _handle_stream_unary_rpc(method_handler,  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi"", line 620, in _handle_stream_unary_rpc    await _finish_handler_with_unary_response(  File ""src/python/grpcio/grpc/_cython/_cygrpc/aio/server.pyx.pxi"", line 386, in _finish_handler_with_unary_response    response_message = await unary_handler(  File ""/home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio_tests/tests_aio/unit/server_test.py"", line 190, in _error_in_stream_unary    raise ValueError('The test server has a bug!')ValueError: The test server has a bug!============================================================================================ warnings summary =============================================================================================tests_aio/unit/server_test.py::TestServer::test_error_in_stream_unary  /home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_server.py:54: DeprecationWarning: The loop argument is deprecated since Python 3.8, and scheduled for removal in Python 3.10.    self._server = cygrpc.AioServer(tests_aio/unit/server_test.py::TestServer::test_error_in_stream_unary  /home/clayg/Workspace/scratch/grpc-test/.scratch/grpc/src/python/grpcio/grpc/aio/_call.py:374: DeprecationWarning: The loop argument is deprecated since Python 3.8, and scheduled for removal in Python 3.10.    self._metadata_sent = asyncio.Event(loop=self._loop)-- Docs: https://docs.pytest.org/en/stable/warnings.html========================================================================================= short test summary info =========================================================================================FAILED src/python/grpcio_tests/tests_aio/unit/server_test.py::TestServer::test_error_in_stream_unary - asyncio.exceptions.CancelledError====================================================================================== 1 failed, 2 warnings in 0.16s ======================================================================================```",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/27268,713341665,2021-09-21T19:08:19Z,bazel/python_rules.bzl,"@@ -212,6 +210,10 @@ _generate_pb2_grpc_src = rule(             cfg = ""host"",             default = Label(""//external:protocol_compiler""),         ),+        ""_grpc_library"": attr.label(+            default = Label(""//src/python/grpcio/grpc:grpcio""),","I believe this PR has been abandoned in favor of [this PR](https://github.com/grpc/grpc/pull/27275) and [its associated gRFC](https://github.com/grpc/proposal/pull/263). Whether or not this will be completed is still uncertain. @mbeards to comment on that.The impact is spelled out in the gRFC, but to excerpt it,> No behavior change should be observed by the user of `py_proto_library` or `py_grpc_library` unless they rely on the (removed) `plugin` attribute.So, unless you1. Use `py_proto_library` in your repo and2. Use the `plugin` attribute in that ruleyou will be unaffected. Based on your `bazel build` command above, it seems likely that you would in fact be unaffected. ",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/27428,713655422,2021-09-22T07:15:00Z,tools/remote_build/rbe_common.bazelrc,"@@ -91,4 +91,4 @@ build:ubsan --test_timeout=60,600,1800,3600 # how to update the bazel toolchain for ubsan: # - check for the latest released version in https://github.com/bazelbuild/bazel-toolchains/tree/master/configs/experimental/ubuntu16_04_clang # - you might need to update the bazel_toolchains dependency in grpc_deps.bzl-build:ubsan --crosstool_top=@bazel_toolchains//configs/experimental/ubuntu16_04_clang/1.3/bazel_0.29.1/ubsan:toolchain+build:ubsan --crosstool_top=@bazel_toolchains//configs/experimental/ubuntu16_04_clang/1.3/bazel_1.0.0/ubsan:toolchain",This line (both the new and the old version is tricky since it prevents us from upgrading the bazel toolchain to anything recent - see https://github.com/grpc/grpc/pull/25001#issuecomment-745559569 for that).Looks like we'll have to spend some time figuring out how to run ubsan on RBE so that :- we can re-enable UBSAN with absl- we can upgrade our bazel toolchain,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/27405,714159337,2021-09-22T17:26:25Z,bazel/grpc_deps.bzl,"@@ -519,3 +520,11 @@ def grpc_test_only_deps():             ],             build_file = ""@com_github_grpc_grpc//third_party:constantly.BUILD"",         )++    if ""com_google_libprotobuf-mutator"" not in native.existing_rules():","nit: for consistency, you can use `com_google_libprotobuf_mutator` as a name (change dash into underscore). That's consistent with naming of other dependencies in this file that also have a dash in their name.",X
148994,clayg,https://api.github.com/repos/grpc/grpc/pulls/27240,714178414,2021-09-22T17:51:33Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -270,6 +281,24 @@ async def test_stream_unary_async_generator(self):         self.assertEqual(_RESPONSE, response)         self.assertEqual(await call.code(), grpc.StatusCode.OK) +    async def test_stream_unary_async_generator_with_request_iter(self):+        stream_unary_call = self._channel.stream_unary(_STREAM_UNARY_ASYNC_GEN)++        finished = False++        def request_gen():+            for _ in range(_NUM_STREAM_REQUESTS):+                yield _REQUEST+            nonlocal finished",are we good with py3-only nonlocal keyword?  I'd been used to doing using a mutable object like a list in py2...,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/27405,714208069,2021-09-22T18:32:26Z,bazel/grpc_deps.bzl,"@@ -519,3 +519,14 @@ def grpc_test_only_deps():             ],             build_file = ""@com_github_grpc_grpc//third_party:constantly.BUILD"",         )++    if ""com_google_libprotobuf_mutator"" not in native.existing_rules():+        http_archive(+            name = ""com_google_libprotobuf_mutator"",+            sha256 = ""b847c71723d8ce0b747aa661d7f3a07f1d16c595bf9c0202f30febc2f9a24a06"",+            urls = [",You'll want to add [a mirror](https://github.com/grpc/grpc/blob/dfae8de8638e25dd212d1783a7cc67a1dc1fc485/bazel/grpc_deps.bzl#L195) as well to  protect us from GitHub outages. [This script](https://github.com/grpc/grpc/blob/dfae8de8638e25dd212d1783a7cc67a1dc1fc485/bazel/update_mirror.sh) will help you with that.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27240,714237875,2021-09-22T19:16:04Z,src/python/grpcio/grpc/aio/_call.py,"@@ -444,6 +444,8 @@ async def _write(self, request: RequestType) -> None:                                                self._request_serializer)         try:             await self._cython_call.send_serialized_message(serialized_request)+        except cygrpc.InternalError:+            await self._raise_for_status()",This change and the original change is similar functional-wise. This is fine. Changing the `_write` function ensures we are catching all InternalError from the send/write message behavior.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27240,714241585,2021-09-22T19:21:32Z,src/python/grpcio_tests/tests_aio/unit/server_test.py,"@@ -270,6 +281,24 @@ async def test_stream_unary_async_generator(self):         self.assertEqual(_RESPONSE, response)         self.assertEqual(await call.code(), grpc.StatusCode.OK) +    async def test_stream_unary_async_generator_with_request_iter(self):+        stream_unary_call = self._channel.stream_unary(_STREAM_UNARY_ASYNC_GEN)++        finished = False++        def request_gen():+            for _ in range(_NUM_STREAM_REQUESTS):+                yield _REQUEST+            nonlocal finished","Yeah, both syntax looks good to me. Depends on you.One alternative is use the `asyncio.Event` to tell whether the request generator reaches the end or not: https://docs.python.org/3/library/asyncio-sync.html#eventThe `asyncio.Event.set` can be called without `async` keyword.",
16571023,ananda1066,https://api.github.com/repos/grpc/grpc/pulls/27417,714317833,2021-09-22T21:17:17Z,src/core/lib/transport/metadata_batch.h,"@@ -185,6 +190,88 @@ class MetadataMap {       GRPC_MUST_USE_RESULT;   void MaybeUnlinkCallout(grpc_linked_mdelem* storage); +  static void assert_valid_list(grpc_mdelem_list* list) {+#ifndef NDEBUG+    grpc_linked_mdelem* l;++    GPR_ASSERT((list->head == nullptr) == (list->tail == nullptr));+    if (!list->head) return;+    GPR_ASSERT(list->head->prev == nullptr);+    GPR_ASSERT(list->tail->next == nullptr);+    GPR_ASSERT((list->head == list->tail) == (list->head->next == nullptr));++    size_t verified_count = 0;+    for (l = list->head; l; l = l->next) {+      GPR_ASSERT(!GRPC_MDISNULL(l->md));+      GPR_ASSERT((l->prev == nullptr) == (l == list->head));+      GPR_ASSERT((l->next == nullptr) == (l == list->tail));+      if (l->next) GPR_ASSERT(l->next->prev == l);+      if (l->prev) GPR_ASSERT(l->prev->next == l);+      verified_count++;+    }+    GPR_ASSERT(list->count == verified_count);+#else+    // Avoid unused-parameter warning for debug-only parameter+    (void)list;+#endif /* NDEBUG */+  }++  static grpc_error_handle GPR_ATTRIBUTE_NOINLINE+  error_with_md(grpc_mdelem md) {+    return grpc_attach_md_to_error(+        GRPC_ERROR_CREATE_FROM_STATIC_STRING(""Unallowed duplicate metadata""),+        md);+  }++  static void link_head(grpc_mdelem_list* list, grpc_linked_mdelem* storage) {+    assert_valid_list(list);+    GPR_DEBUG_ASSERT(!GRPC_MDISNULL(storage->md));+    storage->prev = nullptr;+    storage->next = list->head;+    storage->reserved = nullptr;+    if (list->head != nullptr) {+      list->head->prev = storage;+    } else {+      list->tail = storage;+    }+    list->head = storage;+    list->count++;+    assert_valid_list(list);",I'm curious as to why this second valid check is necessary? How can a list go from valid -> invalid in this function? Same question in below functions,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27417,714386373,2021-09-22T23:50:15Z,src/core/lib/transport/metadata_batch.h,"@@ -185,6 +190,88 @@ class MetadataMap {       GRPC_MUST_USE_RESULT;   void MaybeUnlinkCallout(grpc_linked_mdelem* storage); +  static void assert_valid_list(grpc_mdelem_list* list) {+#ifndef NDEBUG+    grpc_linked_mdelem* l;++    GPR_ASSERT((list->head == nullptr) == (list->tail == nullptr));+    if (!list->head) return;+    GPR_ASSERT(list->head->prev == nullptr);+    GPR_ASSERT(list->tail->next == nullptr);+    GPR_ASSERT((list->head == list->tail) == (list->head->next == nullptr));++    size_t verified_count = 0;+    for (l = list->head; l; l = l->next) {+      GPR_ASSERT(!GRPC_MDISNULL(l->md));+      GPR_ASSERT((l->prev == nullptr) == (l == list->head));+      GPR_ASSERT((l->next == nullptr) == (l == list->tail));+      if (l->next) GPR_ASSERT(l->next->prev == l);+      if (l->prev) GPR_ASSERT(l->prev->next == l);+      verified_count++;+    }+    GPR_ASSERT(list->count == verified_count);+#else+    // Avoid unused-parameter warning for debug-only parameter+    (void)list;+#endif /* NDEBUG */+  }++  static grpc_error_handle GPR_ATTRIBUTE_NOINLINE+  error_with_md(grpc_mdelem md) {+    return grpc_attach_md_to_error(+        GRPC_ERROR_CREATE_FROM_STATIC_STRING(""Unallowed duplicate metadata""),+        md);+  }++  static void link_head(grpc_mdelem_list* list, grpc_linked_mdelem* storage) {+    assert_valid_list(list);+    GPR_DEBUG_ASSERT(!GRPC_MDISNULL(storage->md));+    storage->prev = nullptr;+    storage->next = list->head;+    storage->reserved = nullptr;+    if (list->head != nullptr) {+      list->head->prev = storage;+    } else {+      list->tail = storage;+    }+    list->head = storage;+    list->count++;+    assert_valid_list(list);","I remember having some difficulty getting this linked list code right, and so added these whenever we manipulated the list, as a pre and post condition. That way, if I got it wrong, I'd have a boundary back to where it was last correct to focus my bug search.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27452,714943582,2021-09-23T16:02:25Z,test/core/slice/slice_string_helpers_test.cc,"@@ -60,144 +60,7 @@ static void test_dump_slice(void) {                     GPR_DUMP_HEX | GPR_DUMP_ASCII, ""01 '.'""); } -static void test_strsplit(void) {-  grpc_slice_buffer* parts;-  grpc_slice str;--  LOG_TEST_NAME(""test_strsplit"");--  parts =-      static_cast<grpc_slice_buffer*>(gpr_malloc(sizeof(grpc_slice_buffer)));-  grpc_slice_buffer_init(parts);--  str = grpc_slice_from_copied_string(""one, two, three, four"");-  grpc_slice_split(str, "", "", parts);-  GPR_ASSERT(4 == parts->count);-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[0], ""one""));-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[1], ""two""));-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[2], ""three""));-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[3], ""four""));-  grpc_slice_buffer_reset_and_unref(parts);-  grpc_slice_unref(str);--  /* separator not present in string */-  str = grpc_slice_from_copied_string(""one two three four"");-  grpc_slice_split(str, "", "", parts);-  GPR_ASSERT(1 == parts->count);-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[0], ""one two three four""));-  grpc_slice_buffer_reset_and_unref(parts);-  grpc_slice_unref(str);--  /* separator at the end */-  str = grpc_slice_from_copied_string(""foo,"");-  grpc_slice_split(str, "","", parts);-  GPR_ASSERT(2 == parts->count);-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[0], ""foo""));-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[1], """"));-  grpc_slice_buffer_reset_and_unref(parts);-  grpc_slice_unref(str);--  /* separator at the beginning */-  str = grpc_slice_from_copied_string("",foo"");-  grpc_slice_split(str, "","", parts);-  GPR_ASSERT(2 == parts->count);-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[0], """"));-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[1], ""foo""));-  grpc_slice_buffer_reset_and_unref(parts);-  grpc_slice_unref(str);--  /* standalone separator */-  str = grpc_slice_from_copied_string("","");-  grpc_slice_split(str, "","", parts);-  GPR_ASSERT(2 == parts->count);-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[0], """"));-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[1], """"));-  grpc_slice_buffer_reset_and_unref(parts);-  grpc_slice_unref(str);--  /* empty input */-  str = grpc_slice_from_copied_string("""");-  grpc_slice_split(str, "", "", parts);-  GPR_ASSERT(1 == parts->count);-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[0], """"));-  grpc_slice_buffer_reset_and_unref(parts);-  grpc_slice_unref(str);--  grpc_slice_buffer_destroy(parts);-  gpr_free(parts);-}--static void test_strsplit_nospace(void) {-  grpc_slice_buffer* parts;-  grpc_slice str;--  LOG_TEST_NAME(""test_strsplit_nospace"");--  parts =-      static_cast<grpc_slice_buffer*>(gpr_malloc(sizeof(grpc_slice_buffer)));-  grpc_slice_buffer_init(parts);--  str = grpc_slice_from_copied_string(""one  ,two,   three  , four"");-  grpc_slice_split_without_space(str, "","", parts);-  GPR_ASSERT(4 == parts->count);-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[0], ""one""));-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[1], ""two""));-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[2], ""three""));-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[3], ""four""));-  grpc_slice_buffer_reset_and_unref(parts);-  grpc_slice_unref(str);--  /* separator not present in string */-  str = grpc_slice_from_copied_string(""one two three four "");-  grpc_slice_split_without_space(str, "","", parts);-  GPR_ASSERT(1 == parts->count);-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[0], ""one two three four""));-  grpc_slice_buffer_reset_and_unref(parts);-  grpc_slice_unref(str);--  /* separator at the end */-  str = grpc_slice_from_copied_string(""foo,"");-  grpc_slice_split_without_space(str, "","", parts);-  GPR_ASSERT(2 == parts->count);-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[0], ""foo""));-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[1], """"));-  grpc_slice_buffer_reset_and_unref(parts);-  grpc_slice_unref(str);--  /* separator at the beginning */-  str = grpc_slice_from_copied_string("" , foo"");-  grpc_slice_split_without_space(str, "","", parts);-  GPR_ASSERT(2 == parts->count);-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[0], """"));-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[1], ""foo""));-  grpc_slice_buffer_reset_and_unref(parts);-  grpc_slice_unref(str);--  /* standalone separator */-  str = grpc_slice_from_copied_string("", "");-  grpc_slice_split_without_space(str, "", "", parts);-  GPR_ASSERT(2 == parts->count);-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[0], """"));-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[1], """"));-  grpc_slice_buffer_reset_and_unref(parts);-  grpc_slice_unref(str);--  /* empty input */-  str = grpc_slice_from_copied_string("""");-  grpc_slice_split_without_space(str, "","", parts);-  GPR_ASSERT(1 == parts->count);-  GPR_ASSERT(0 == grpc_slice_str_cmp(parts->slices[0], """"));-  grpc_slice_buffer_reset_and_unref(parts);-  grpc_slice_unref(str);--  grpc_slice_buffer_destroy(parts);-  gpr_free(parts);-}- int main(int argc, char** argv) {",Please fix: unused vars.,
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27454,714949269,2021-09-23T16:09:22Z,src/core/lib/promise/exec_ctx_wakeup_scheduler.h,"@@ -0,0 +1,45 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_PROMISE_EXEC_CTX_WAKEUP_SCHEDULER_H+#define GRPC_CORE_LIB_PROMISE_EXEC_CTX_WAKEUP_SCHEDULER_H++#include <grpc/impl/codegen/port_platform.h>++#include ""src/core/lib/iomgr/exec_ctx.h""++namespace grpc_core {++// A callback scheduler for activities that works by scheduling callbacks on the+// exec ctx.+class ExecCtxWakeupScheduler {+ public:+  template <typename ActivityType>+  void ScheduleWakeup(ActivityType* activity) {+    GRPC_CLOSURE_INIT(+        &closure_,+        [](void* arg, grpc_error_handle hdl) {","`hdl` is unused, compilers will complain.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27417,714957557,2021-09-23T16:19:41Z,src/core/lib/transport/metadata_batch.h,"@@ -185,6 +190,88 @@ class MetadataMap {       GRPC_MUST_USE_RESULT;   void MaybeUnlinkCallout(grpc_linked_mdelem* storage); +  static void assert_valid_list(grpc_mdelem_list* list) {+#ifndef NDEBUG+    grpc_linked_mdelem* l;++    GPR_ASSERT((list->head == nullptr) == (list->tail == nullptr));+    if (!list->head) return;+    GPR_ASSERT(list->head->prev == nullptr);+    GPR_ASSERT(list->tail->next == nullptr);+    GPR_ASSERT((list->head == list->tail) == (list->head->next == nullptr));++    size_t verified_count = 0;+    for (l = list->head; l; l = l->next) {+      GPR_ASSERT(!GRPC_MDISNULL(l->md));+      GPR_ASSERT((l->prev == nullptr) == (l == list->head));+      GPR_ASSERT((l->next == nullptr) == (l == list->tail));+      if (l->next) GPR_ASSERT(l->next->prev == l);+      if (l->prev) GPR_ASSERT(l->prev->next == l);+      verified_count++;+    }+    GPR_ASSERT(list->count == verified_count);+#else+    // Avoid unused-parameter warning for debug-only parameter+    (void)list;+#endif /* NDEBUG */+  }++  static grpc_error_handle GPR_ATTRIBUTE_NOINLINE+  error_with_md(grpc_mdelem md) {+    return grpc_attach_md_to_error(+        GRPC_ERROR_CREATE_FROM_STATIC_STRING(""Unallowed duplicate metadata""),+        md);+  }++  static void link_head(grpc_mdelem_list* list, grpc_linked_mdelem* storage) {+    assert_valid_list(list);+    GPR_DEBUG_ASSERT(!GRPC_MDISNULL(storage->md));+    storage->prev = nullptr;+    storage->next = list->head;+    storage->reserved = nullptr;+    if (list->head != nullptr) {+      list->head->prev = storage;+    } else {+      list->tail = storage;+    }+    list->head = storage;+    list->count++;+    assert_valid_list(list);","It'll be gone soon... towards the end of this transition, but definitely scoped inside of it. Plan is to replace this linked list with something like `InlinedVector<pair<Slice, Slice>>`",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27462,715220968,2021-09-23T23:28:31Z,tools/internal_ci/linux/grpc_xds_k8s.sh,"@@ -17,14 +17,15 @@ set -ex -o igncr || set -ex  # Constants readonly GITHUB_REPOSITORY_NAME=""grpc""-# GKE Cluster-readonly GKE_CLUSTER_NAME=""interop-test-psm-sec-v2-us-central1-a""-readonly GKE_CLUSTER_ZONE=""us-central1-a"" ## xDS test server/client Docker images readonly SERVER_IMAGE_NAME=""gcr.io/grpc-testing/xds-interop/cpp-server"" readonly CLIENT_IMAGE_NAME=""gcr.io/grpc-testing/xds-interop/cpp-client"" readonly FORCE_IMAGE_BUILD=""${FORCE_IMAGE_BUILD:-0}"" readonly BUILD_APP_PATH=""interop-testing/build/install/grpc-interop-testing""+# Test driver+readonly TEST_DRIVER_REPO_URL=""https://github.com/${TEST_DRIVER_REPO_OWNER:-grpc}/grpc.git""+readonly TEST_DRIVER_BRANCH=""${TEST_DRIVER_BRANCH:-master}""+readonly TEST_DRIVER_INSTALL_LIB_PATH=""tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh""","Idea: move this line into `grpc_xds_k8s_clone_driver_repo.sh`, and update `clone_test_driver` to source `grpc_xds_k8s_install_test_driver.sh` as well. Or add another method that sources this script.This should save some code repetitions in all these lang-specific build files.",X
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27410,715420745,2021-09-24T08:30:47Z,bazel/grpc_deps.bzl,"@@ -295,11 +295,11 @@ def grpc_deps():         # list of releases is at https://releases.bazel.build/bazel-toolchains.html         http_archive(             name = ""bazel_toolchains"",-            sha256 = ""0b36eef8a66f39c8dbae88e522d5bbbef49d5e66e834a982402c79962281be10"",-            strip_prefix = ""bazel-toolchains-1.0.1"",+            sha256 = ""179ec02f809e86abf56356d8898c8bd74069f1bd7c56044050c2cd3d79d0e024"",+            strip_prefix = ""bazel-toolchains-4.1.0"",             urls = [-                ""https://mirror.bazel.build/github.com/bazelbuild/bazel-toolchains/archive/1.0.1.tar.gz"",-                ""https://github.com/bazelbuild/bazel-toolchains/releases/download/1.0.1/bazel-toolchains-1.0.1.tar.gz"",+                ""https://mirror.bazel.build/github.com/bazelbuild/bazel-toolchains/archive/4.1.0.tar.gz"",",@jtattermusch I get `WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/bazel-toolchains/archive/4.1.0.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found` when using bazel to build things. Is this expected?,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/27410,715433987,2021-09-24T08:49:50Z,bazel/grpc_deps.bzl,"@@ -295,11 +295,11 @@ def grpc_deps():         # list of releases is at https://releases.bazel.build/bazel-toolchains.html         http_archive(             name = ""bazel_toolchains"",-            sha256 = ""0b36eef8a66f39c8dbae88e522d5bbbef49d5e66e834a982402c79962281be10"",-            strip_prefix = ""bazel-toolchains-1.0.1"",+            sha256 = ""179ec02f809e86abf56356d8898c8bd74069f1bd7c56044050c2cd3d79d0e024"",+            strip_prefix = ""bazel-toolchains-4.1.0"",             urls = [-                ""https://mirror.bazel.build/github.com/bazelbuild/bazel-toolchains/archive/1.0.1.tar.gz"",-                ""https://github.com/bazelbuild/bazel-toolchains/releases/download/1.0.1/bazel-toolchains-1.0.1.tar.gz"",+                ""https://mirror.bazel.build/github.com/bazelbuild/bazel-toolchains/archive/4.1.0.tar.gz"",",Filed https://github.com/bazelbuild/bazel-toolchains/issues/972,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27462,715926074,2021-09-24T22:02:05Z,tools/internal_ci/linux/grpc_xds_k8s_lb_python.sh,"@@ -17,17 +17,16 @@ set -eo pipefail  # Constants readonly GITHUB_REPOSITORY_NAME=""grpc""-# GKE Cluster-readonly GKE_CLUSTER_NAME=""interop-test-psm-sec-v2-us-central1-a""-readonly GKE_CLUSTER_ZONE=""us-central1-a""-readonly SECONDARY_GKE_CLUSTER_NAME=""interop-test-psm-sec-v2-us-west1-b""-readonly SECONDARY_GKE_CLUSTER_ZONE=""us-west1-b"" ## xDS test client Docker images readonly SERVER_IMAGE_NAME=""gcr.io/grpc-testing/xds-interop/cpp-server:13171a8b293837517c0446ec0e149e9d10ea3d10"" readonly CLIENT_IMAGE_NAME=""gcr.io/grpc-testing/xds-interop/python-client"" readonly FORCE_IMAGE_BUILD=""${FORCE_IMAGE_BUILD:-0}"" readonly BUILD_APP_PATH=""interop-testing/build/install/grpc-interop-testing"" readonly LANGUAGE_NAME=""Python""+# Test driver+readonly TEST_DRIVER_REPO_URL=""https://github.com/${TEST_DRIVER_REPO_OWNER:-grpc}/grpc.git""+readonly TEST_DRIVER_BRANCH=""${TEST_DRIVER_BRANCH:-master}""+readonly TEST_DRIVER_INSTALL_LIB_PATH=""tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh""",optional: maybe we can put these branch/repo variables into `tools/internal_ci/linux/grpc_xds_k8s_clone_driver_repo.sh`?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/27464,716920892,2021-09-27T17:58:04Z,tools/run_tests/python_utils/jobset.py,"@@ -240,7 +240,7 @@ def __init__(self):  def read_from_start(f):     f.seek(0)-    return f.read()+    return f.read().decode(""utf8"")","> I think I've seen problems with run_tests.py priting logs for tests that produce logs containing non-ascii charactersNo, I'm trying to address a standard str/bytes Python 2/3 issue here. The `read_from_start` function here is the one source of   data that differs in type between 2 and 3. There's one function that already does a `decode` and it uses `utf8`, so I took the cue from that function call and moved the `decode` earlier in the pipeline, so the rest of the script would get access to it. That way, a single `decode` call could be used instead of many scattered throughout the file.However, as you've noticed, it seems that not all files we read from are in utf8. I'll try again and will make sure to trigger these other jobs before merging.",X
12451183,temawi,https://api.github.com/repos/grpc/grpc/pulls/27462,716936097,2021-09-27T18:20:25Z,tools/internal_ci/linux/grpc_xds_k8s_lb_python.sh,"@@ -133,15 +132,30 @@ run_test() { main() {   local script_dir   script_dir=""$(dirname ""$0"")""-  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh-  source ""${script_dir}/grpc_xds_k8s_install_test_driver.sh""+  +  # Clone the test driver from the master branch using an external script.+  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_clone_driver_repo.sh+  source ""${script_dir}/grpc_xds_k8s_clone_driver_repo.sh""+  clone_test_driver","That was my first thought as well, but Sergii had done some initial work with the clone approach so I went with that. Performance is not really a concern here and it doesn't hurt to have the whole master branch available.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27462,717099427,2021-09-27T22:51:55Z,tools/internal_ci/linux/grpc_xds_k8s_lb_python.sh,"@@ -133,15 +132,30 @@ run_test() { main() {   local script_dir   script_dir=""$(dirname ""$0"")""-  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh-  source ""${script_dir}/grpc_xds_k8s_install_test_driver.sh""+  +  # Clone the test driver from the master branch using an external script.+  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_clone_driver_repo.sh+  source ""${script_dir}/grpc_xds_k8s_clone_driver_repo.sh""+  clone_test_driver","I think by going with a simple curl, we can save managing an extra file in java/go/node repos. This will be a significant improvement for reducing coupling between repos. E.g., in the worst scenario, if there is a small improvement we need to do in the clone logic, we will need to patch all release branches across repos. I agree performance is not the main issue here, my suggestion is more about reducing future maintenance toil.@sergiitk What do you think about using `curl` to download the xds_k8s utility functions?",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27462,717107977,2021-09-27T23:13:28Z,tools/internal_ci/linux/grpc_xds_k8s_lb_python.sh,"@@ -133,15 +132,30 @@ run_test() { main() {   local script_dir   script_dir=""$(dirname ""$0"")""-  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh-  source ""${script_dir}/grpc_xds_k8s_install_test_driver.sh""+  +  # Clone the test driver from the master branch using an external script.+  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_clone_driver_repo.sh+  source ""${script_dir}/grpc_xds_k8s_clone_driver_repo.sh""+  clone_test_driver","Yes, my initial work was from some months ago, and I don't fully remember the reasoning for git clone vs curl. I remember talking to @ejona86 about it, and we concluded on cloning. Now, as it introduces an extra file to manage, I like the idea the curl oneliner.",X
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/27462,717111380,2021-09-27T23:22:32Z,tools/internal_ci/linux/grpc_xds_k8s_lb_python.sh,"@@ -133,15 +132,30 @@ run_test() { main() {   local script_dir   script_dir=""$(dirname ""$0"")""-  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh-  source ""${script_dir}/grpc_xds_k8s_install_test_driver.sh""+  +  # Clone the test driver from the master branch using an external script.+  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_clone_driver_repo.sh+  source ""${script_dir}/grpc_xds_k8s_clone_driver_repo.sh""+  clone_test_driver","I generally prefer downloading the tar.gz/zip, as then I don't need a git dependency. Although the core repo tends to need submodules for lots of tasks, so it normally isn't an option for grpc/grpc, and a lot of the test tooling uses git directly, so nothing is gained there.I don't remember the precise reasoning, but I think it was probably already using a git clone and I may have said something like, ""if you are doing --depth=1, then you are already doing a lot better than most of the other scripts when they are written."" And then we just left it at ""good enough.""Grabbing just the one file we need sounds great, assuming it won't ever want to source other files.",
12451183,temawi,https://api.github.com/repos/grpc/grpc/pulls/27462,717115893,2021-09-27T23:34:50Z,tools/internal_ci/linux/grpc_xds_k8s_lb_python.sh,"@@ -133,15 +132,30 @@ run_test() { main() {   local script_dir   script_dir=""$(dirname ""$0"")""-  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh-  source ""${script_dir}/grpc_xds_k8s_install_test_driver.sh""+  +  # Clone the test driver from the master branch using an external script.+  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_clone_driver_repo.sh+  source ""${script_dir}/grpc_xds_k8s_clone_driver_repo.sh""+  clone_test_driver","I initially liked the clone approach as it would allow us to potentially use other files in the future. But I can't really think what files those would be, so I'll switch to curl and eliminate the new clone script.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27462,717116363,2021-09-27T23:35:57Z,tools/internal_ci/linux/grpc_xds_k8s_lb_python.sh,"@@ -133,15 +132,30 @@ run_test() { main() {   local script_dir   script_dir=""$(dirname ""$0"")""-  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh-  source ""${script_dir}/grpc_xds_k8s_install_test_driver.sh""+  +  # Clone the test driver from the master branch using an external script.+  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_clone_driver_repo.sh+  source ""${script_dir}/grpc_xds_k8s_clone_driver_repo.sh""+  clone_test_driver","I agree that Git clone is still necessary, but it will happen after the downloaded entrypoint script is sourced. It would look like following:```diff  # shellcheck source=buildscripts/kokoro/xds-k8s-install-test-driver.sh- source ""${script_dir}/xds-k8s-install-test-driver.sh""+ source /dev/stdin <<< ""$(curl https://raw.githubusercontent.com/grpc/grpc/master/tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh)""  set -x  if [[ -n ""${KOKORO_ARTIFACTS_DIR}"" ]]; then    kokoro_setup_test_driver ""${GITHUB_REPOSITORY_NAME}""  else    local_setup_test_driver ""${script_dir}""  fi```",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/27493,717772349,2021-09-28T16:54:35Z,tools/run_tests/python_utils/jobset.py,"@@ -357,7 +357,7 @@ def stdout(self=self):                 if measure_cpu_costs:                     m = re.search(                         r'real\s+([0-9.]+)\nuser\s+([0-9.]+)\nsys\s+([0-9.]+)',-                        stdout())","We're definitely using the flag. That's why I ended up making this change. If no one is using this data anymore though, we should consider just ripping it out entirely.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27402,717779851,2021-09-28T17:04:03Z,tools/gource/gource.sh,"@@ -19,7 +19,7 @@ gource                          \   --max-file-lag 0.05           \   --max-files 0                 \   -e 0.01                       \-  --hide filenames,dirnames,mouse,progress     \+  --hide filenames,mouse,progress     \",Various gods I wondered where that script went to...,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/27493,717862353,2021-09-28T18:37:16Z,tools/run_tests/python_utils/jobset.py,"@@ -357,7 +357,7 @@ def stdout(self=self):                 if measure_cpu_costs:                     m = re.search(                         r'real\s+([0-9.]+)\nuser\s+([0-9.]+)\nsys\s+([0-9.]+)',-                        stdout())","[This log](https://source.cloud.google.com/results/invocations/8498d457-2d58-4f4d-8587-fa0b7b0aa2f4/log) includes it:```run_tests_python_linux_dbg_native_default_python_alpine: ""python tools/run_tests/run_tests.py --use_docker -t -j 16 -x run_tests/python_linux_dbg_native_default_python_alpine/sponge_log.xml --report_suite_name python_linux_dbg_native_default_python_alpine -l python -c dbg --iomgr_platform native --arch default --compiler python_alpine --bq_result_table aggregate_results --measure_cpu_costs --report_multi_target""```A quick grep indicates that it's coming from [here](https://github.com/grpc/grpc/blob/91a2db9bc2f0195e649380f5e0f46d5ced8c73a6/tools/run_tests/run_tests_matrix.py#L463).",
12451183,temawi,https://api.github.com/repos/grpc/grpc/pulls/27462,717951875,2021-09-28T20:47:46Z,tools/internal_ci/linux/grpc_xds_k8s_lb_python.sh,"@@ -133,15 +132,30 @@ run_test() { main() {   local script_dir   script_dir=""$(dirname ""$0"")""-  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_install_test_driver.sh-  source ""${script_dir}/grpc_xds_k8s_install_test_driver.sh""+  +  # Clone the test driver from the master branch using an external script.+  # shellcheck source=tools/internal_ci/linux/grpc_xds_k8s_clone_driver_repo.sh+  source ""${script_dir}/grpc_xds_k8s_clone_driver_repo.sh""+  clone_test_driver","The install script is now sourced and I removed the clone script, take another look please.@sergiitk@lidizheng ",X
5564114,Vignesh2208,https://api.github.com/repos/grpc/grpc/pulls/26812,718012462,2021-09-28T22:31:21Z,src/core/tsi/ssl/key_logging/ssl_key_logging.cc,"@@ -0,0 +1,158 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include <map>++#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/iomgr/error.h""+#include ""src/core/lib/slice/slice_internal.h""+#include ""src/core/tsi/ssl/key_logging/ssl_key_logging.h""++#include <grpc/support/log.h>+#include <grpc/support/string_util.h>++namespace tsi {++#if OPENSSL_VERSION_NUMBER >= 0x10100000 && !defined(LIBRESSL_VERSION_NUMBER)+grpc_core::Mutex* g_tls_key_logger_registry_mu;+std::map<std::string, TlsKeyLogFileWriter*> g_tls_key_log_file_writer_map+  ABSL_GUARDED_BY(g_tls_key_logger_registry_mu);",The definition of this map is moved inside TlsSessionKeyLoggerRegistry in the latest commit. It is no a static member of this class.,
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/27509,718732595,2021-09-29T17:21:46Z,src/core/tsi/transport_security.cc,"@@ -251,6 +251,16 @@ tsi_result tsi_handshaker_result_extract_peer(const tsi_handshaker_result* self,   return self->vtable->extract_peer(self, peer); } +tsi_frame_protector_type tsi_handshaker_result_frame_protector_type(","How about adding some sanity check to fail-early mismatched cases e.g., user only implemented zero-copy frame protector but mistakenly let `frame_protector_type` API return `TSI_FRAME_PROTECTOR_NORMAL`?  which makes me wonder if we should let the API return `tsi_result` and have an output parameter with type of `tsi_frame_protector_type*`? ",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26287,718775070,2021-09-29T18:20:41Z,test/core/tsi/crl_ssl_transport_security_test.cc,"@@ -0,0 +1,323 @@+// Copyright 2021 gRPC authors.++// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at++// http://www.apache.org/licenses/LICENSE-2.0+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/tsi/ssl_transport_security.h""++#include <grpc/grpc.h>+#include <grpc/support/alloc.h>+#include <grpc/support/log.h>+#include <grpc/support/string_util.h>+#include <stdbool.h>+#include <stdio.h>+#include <string.h>++#include <gmock/gmock.h>+#include <gtest/gtest.h>++#include ""src/core/lib/iomgr/load_file.h""+#include ""src/core/lib/security/security_connector/security_connector.h""+#include ""src/core/tsi/transport_security.h""+#include ""src/core/tsi/transport_security_interface.h""+#include ""test/core/tsi/transport_security_test_lib.h""+#include ""test/core/util/test_config.h""++extern ""C"" {+#include <openssl/crypto.h>+#include <openssl/pem.h>+}++static const int kSslTsiTestRevokedKeyCertPairsNum = 1;+static const int kSslTsiTestValidKeyCertPairsNum = 1;+const char* const kSslTsiTestCrlSupportedCredentialsDir =+    ""test/core/tsi/test_creds/"";++// Indicates the TLS version used for the test.+static tsi_tls_version test_tls_version = tsi_tls_version::TSI_TLS1_3;++class CrlSslTransportSecurityTest : public ::testing::Test {+ public:+  // Credentials created under the root+  // kSslTsiTestCrlSupportedCredentialsDir/ca.pem+  // The CA root is also configured with KeyUsage cRLSign that the CA root in+  // tsi_test_creds does not contain+  typedef struct ssl_key_cert_lib {",I think in-class struct can be defined as ```struct SslKeyCertLib  {  // ....};```,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27509,718778807,2021-09-29T18:26:13Z,src/core/tsi/transport_security_interface.h,"@@ -64,6 +64,26 @@ typedef enum {   TSI_REQUEST_AND_REQUIRE_CLIENT_CERTIFICATE_AND_VERIFY, } tsi_client_certificate_request_type; +typedef enum {+  // TSI implementation provides a normal frame protector.  The caller+  // should invoke tsi_handshaker_result_create_frame_protector() to+  // generate the frame protector.+  TSI_FRAME_PROTECTOR_NORMAL,+  // TSI implementation provides a zero-copy frame protector.  The caller+  // should invoke tsi_handshaker_result_create_zero_copy_grpc_protector()+  // to generate the frame protector.+  TSI_FRAME_PROTECTOR_ZERO_COPY,+  // TSI implementation provides both normal and zero-copy frame protectors.+  // The caller should invoke either+  // tsi_handshaker_result_create_frame_protector() or+  // tsi_handshaker_result_create_zero_copy_grpc_protector() to generate+  // the frame protector.+  TSI_FRAME_PROTECTOR_NORMAL_OR_ZERO_COPY,+  // TSI implementation does not provide any frame protector.  This means+  // that it is safe for the caller to send bytes unprotected on the wire.+  TSI_FRAME_PROTECTOR_NONE,+} tsi_frame_protector_type;","Right.  The idea here is that (as shown in the security handshaker code) the caller will first call `tsi_handshaker_result_get_frame_protector_type()` to determine what type of frame protector is supported (if any), and then decide whether to call `tsi_handshaker_result_create_zero_copy_grpc_protector()` or `tsi_handshaker_result_create_frame_protector()`.  If `tsi_handshaker_result_get_frame_protector_type()` indicates that a type of frame protector is supported but the function for that frame protector type returns `TSI_UNIMPLEMENTED`, that is considered an error.I could have just added a new method that returned a bool indicating whether a frame protector was required and not affected the choice of which type of frame protector to use.  But it seemed like this was a good opportunity to clean up the semantics around the two types of frame protectors.  And since nothing but gRPC uses the zero-copy frame protector, this change shouldn't actually impact any other caller in practice.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27509,718779135,2021-09-29T18:26:35Z,src/core/tsi/transport_security.cc,"@@ -251,6 +251,16 @@ tsi_result tsi_handshaker_result_extract_peer(const tsi_handshaker_result* self,   return self->vtable->extract_peer(self, peer); } +tsi_frame_protector_type tsi_handshaker_result_frame_protector_type(","Changed the API to return `tsi_result`.I will note that the semantics around which methods must be implemented would be much simpler if this API were converted to C++, since we could just have the base class methods crash.  But that's something we can tackle in the future.",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26287,718784167,2021-09-29T18:33:47Z,test/core/tsi/crl_ssl_transport_security_test.cc,"@@ -0,0 +1,323 @@+// Copyright 2021 gRPC authors.++// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at++// http://www.apache.org/licenses/LICENSE-2.0+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/tsi/ssl_transport_security.h""++#include <grpc/grpc.h>+#include <grpc/support/alloc.h>+#include <grpc/support/log.h>+#include <grpc/support/string_util.h>+#include <stdbool.h>+#include <stdio.h>+#include <string.h>++#include <gmock/gmock.h>+#include <gtest/gtest.h>++#include ""src/core/lib/iomgr/load_file.h""+#include ""src/core/lib/security/security_connector/security_connector.h""+#include ""src/core/tsi/transport_security.h""+#include ""src/core/tsi/transport_security_interface.h""+#include ""test/core/tsi/transport_security_test_lib.h""+#include ""test/core/util/test_config.h""++extern ""C"" {+#include <openssl/crypto.h>+#include <openssl/pem.h>+}++static const int kSslTsiTestRevokedKeyCertPairsNum = 1;+static const int kSslTsiTestValidKeyCertPairsNum = 1;+const char* const kSslTsiTestCrlSupportedCredentialsDir =+    ""test/core/tsi/test_creds/"";++// Indicates the TLS version used for the test.+static tsi_tls_version test_tls_version = tsi_tls_version::TSI_TLS1_3;++class CrlSslTransportSecurityTest : public ::testing::Test {+ public:+  // Credentials created under the root+  // kSslTsiTestCrlSupportedCredentialsDir/ca.pem+  // The CA root is also configured with KeyUsage cRLSign that the CA root in+  // tsi_test_creds does not contain+  typedef struct ssl_key_cert_lib {+    bool use_revoked_server_cert;+    bool use_revoked_client_cert;+    char* root_cert;+    tsi_ssl_root_certs_store* root_store;+    tsi_ssl_pem_key_cert_pair* revoked_pem_key_cert_pairs;+    tsi_ssl_pem_key_cert_pair* valid_pem_key_cert_pairs;+    uint16_t revoked_num_key_cert_pairs;+    uint16_t valid_num_key_cert_pairs;+    const char* crl_directory;+  } ssl_key_cert_lib;++  typedef struct ssl_tsi_test_fixture {",I think the in-class struct and function names need to be in CamelCase. Ditto elsewhere.,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26287,718787805,2021-09-29T18:39:11Z,test/core/tsi/crl_ssl_transport_security_test.cc,"@@ -0,0 +1,323 @@+// Copyright 2021 gRPC authors.++// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at++// http://www.apache.org/licenses/LICENSE-2.0+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/tsi/ssl_transport_security.h""++#include <grpc/grpc.h>+#include <grpc/support/alloc.h>+#include <grpc/support/log.h>+#include <grpc/support/string_util.h>+#include <stdbool.h>+#include <stdio.h>+#include <string.h>++#include <gmock/gmock.h>+#include <gtest/gtest.h>++#include ""src/core/lib/iomgr/load_file.h""+#include ""src/core/lib/security/security_connector/security_connector.h""+#include ""src/core/tsi/transport_security.h""+#include ""src/core/tsi/transport_security_interface.h""+#include ""test/core/tsi/transport_security_test_lib.h""+#include ""test/core/util/test_config.h""++extern ""C"" {+#include <openssl/crypto.h>+#include <openssl/pem.h>+}++static const int kSslTsiTestRevokedKeyCertPairsNum = 1;+static const int kSslTsiTestValidKeyCertPairsNum = 1;+const char* const kSslTsiTestCrlSupportedCredentialsDir =+    ""test/core/tsi/test_creds/"";++// Indicates the TLS version used for the test.+static tsi_tls_version test_tls_version = tsi_tls_version::TSI_TLS1_3;++class CrlSslTransportSecurityTest : public ::testing::Test {+ public:+  // Credentials created under the root+  // kSslTsiTestCrlSupportedCredentialsDir/ca.pem+  // The CA root is also configured with KeyUsage cRLSign that the CA root in+  // tsi_test_creds does not contain+  typedef struct ssl_key_cert_lib {+    bool use_revoked_server_cert;+    bool use_revoked_client_cert;+    char* root_cert;+    tsi_ssl_root_certs_store* root_store;+    tsi_ssl_pem_key_cert_pair* revoked_pem_key_cert_pairs;+    tsi_ssl_pem_key_cert_pair* valid_pem_key_cert_pairs;+    uint16_t revoked_num_key_cert_pairs;+    uint16_t valid_num_key_cert_pairs;+    const char* crl_directory;+  } ssl_key_cert_lib;++  typedef struct ssl_tsi_test_fixture {+    tsi_test_fixture base;+    ssl_key_cert_lib* key_cert_lib;+    char* server_name_indication;+    bool session_reused;+    const char* session_ticket_key;+    size_t session_ticket_key_size;+    tsi_ssl_server_handshaker_factory* server_handshaker_factory;+    tsi_ssl_client_handshaker_factory* client_handshaker_factory;+  } ssl_tsi_test_fixture;++  static void ssl_test_setup_handshakers(tsi_test_fixture* fixture) {","Since this operates on `tsi_test_fixture*`, which is a member of this class - can we make the static function non-static:```void ssl_test_setup_handshakers()``` and just make it do operations based on the current class members? Similar questions for other static functions taking `tsi_test_fixture*` as well...",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27522,718965712,2021-09-30T00:06:27Z,include/grpc/event_engine/memory_allocator.h,"@@ -0,0 +1,242 @@+// Copyright 2021 The gRPC Authors+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+#ifndef GRPC_EVENT_ENGINE_MEMORY_ALLOCATOR_H+#define GRPC_EVENT_ENGINE_MEMORY_ALLOCATOR_H++#include <grpc/impl/codegen/port_platform.h>++#include <algorithm>+#include <memory>+#include <type_traits>+#include <vector>++#include <grpc/slice.h>++// forward-declaring an internal struct, not used publicly.+struct grpc_slice_buffer;++namespace grpc_event_engine {+namespace experimental {++// TODO(nnoble): needs implementation+class SliceBuffer {+ public:+  SliceBuffer() { abort(); }+  explicit SliceBuffer(grpc_slice_buffer*) { abort(); }++  grpc_slice_buffer* RawSliceBuffer() { return slice_buffer_; }++ private:+  grpc_slice_buffer* slice_buffer_;+};++// Reservation request - how much memory do we want to allocate?+class MemoryRequest {+ public:+  // Request a fixed amount of memory.+  // NOLINTNEXTLINE(google-explicit-constructor)+  MemoryRequest(size_t n) : min_(n), max_(n) {}+  // Request a range of memory.+  MemoryRequest(size_t min, size_t max) : min_(std::min(min, max)), max_(max) {}++  // Increase the size by amount+  MemoryRequest Increase(size_t amount) const {+    return MemoryRequest(min_ + amount, max_ + amount);+  }++  size_t min() const { return min_; }+  size_t max() const { return max_; }++ private:+  size_t min_;+  size_t max_;+};++class BasicMemoryAllocator+    : public std::enable_shared_from_this<BasicMemoryAllocator> {+ public:+  BasicMemoryAllocator() {}+  virtual ~BasicMemoryAllocator() {}++  BasicMemoryAllocator(const BasicMemoryAllocator&) = delete;+  BasicMemoryAllocator& operator=(const BasicMemoryAllocator&) = delete;++  // Reserve bytes from the quota.+  // If we enter overcommit, reclamation will begin concurrently.+  // Returns the number of bytes reserved.+  virtual size_t Reserve(MemoryRequest request) = 0;++  // Release some bytes that were previously reserved.+  virtual void Release(size_t n) = 0;++  // Shutdown this allocator.+  // Further usage of Reserve() is undefined behavior.+  virtual void Shutdown() = 0;+};++class MemoryAllocator {+ public:+  // Construct a MemoryAllocator given a BasicMemoryAllocator implementation.+  // The constructed MemoryAllocator will call BasicMemoryAllocator::Shutdown()+  // upon destruction.+  explicit MemoryAllocator(std::shared_ptr<BasicMemoryAllocator> allocator)+      : allocator_(allocator) {}+  ~MemoryAllocator() {+    if (allocator_ != nullptr) allocator_->Shutdown();","Explicitly shutting down a `std::shared_ptr` is unfriendly, no? Why not make it a unique_ptr if this class controls the BaseMemoryAllocator's usable lifetime?",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27522,718966381,2021-09-30T00:08:28Z,include/grpc/event_engine/memory_allocator.h,"@@ -0,0 +1,242 @@+// Copyright 2021 The gRPC Authors+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+#ifndef GRPC_EVENT_ENGINE_MEMORY_ALLOCATOR_H+#define GRPC_EVENT_ENGINE_MEMORY_ALLOCATOR_H++#include <grpc/impl/codegen/port_platform.h>++#include <algorithm>+#include <memory>+#include <type_traits>+#include <vector>++#include <grpc/slice.h>++// forward-declaring an internal struct, not used publicly.+struct grpc_slice_buffer;++namespace grpc_event_engine {+namespace experimental {++// TODO(nnoble): needs implementation+class SliceBuffer {+ public:+  SliceBuffer() { abort(); }+  explicit SliceBuffer(grpc_slice_buffer*) { abort(); }++  grpc_slice_buffer* RawSliceBuffer() { return slice_buffer_; }++ private:+  grpc_slice_buffer* slice_buffer_;+};++// Reservation request - how much memory do we want to allocate?+class MemoryRequest {+ public:+  // Request a fixed amount of memory.+  // NOLINTNEXTLINE(google-explicit-constructor)+  MemoryRequest(size_t n) : min_(n), max_(n) {}+  // Request a range of memory.+  MemoryRequest(size_t min, size_t max) : min_(std::min(min, max)), max_(max) {}++  // Increase the size by amount+  MemoryRequest Increase(size_t amount) const {+    return MemoryRequest(min_ + amount, max_ + amount);+  }++  size_t min() const { return min_; }+  size_t max() const { return max_; }++ private:+  size_t min_;+  size_t max_;+};++class BasicMemoryAllocator+    : public std::enable_shared_from_this<BasicMemoryAllocator> {+ public:+  BasicMemoryAllocator() {}+  virtual ~BasicMemoryAllocator() {}++  BasicMemoryAllocator(const BasicMemoryAllocator&) = delete;+  BasicMemoryAllocator& operator=(const BasicMemoryAllocator&) = delete;++  // Reserve bytes from the quota.+  // If we enter overcommit, reclamation will begin concurrently.+  // Returns the number of bytes reserved.+  virtual size_t Reserve(MemoryRequest request) = 0;++  // Release some bytes that were previously reserved.+  virtual void Release(size_t n) = 0;++  // Shutdown this allocator.+  // Further usage of Reserve() is undefined behavior.+  virtual void Shutdown() = 0;+};++class MemoryAllocator {+ public:+  // Construct a MemoryAllocator given a BasicMemoryAllocator implementation.+  // The constructed MemoryAllocator will call BasicMemoryAllocator::Shutdown()+  // upon destruction.+  explicit MemoryAllocator(std::shared_ptr<BasicMemoryAllocator> allocator)+      : allocator_(allocator) {}+  ~MemoryAllocator() {+    if (allocator_ != nullptr) allocator_->Shutdown();+  }++  MemoryAllocator(const MemoryAllocator&) = delete;+  MemoryAllocator& operator=(const MemoryAllocator&) = delete;++  MemoryAllocator(MemoryAllocator&&) = default;+  MemoryAllocator& operator=(MemoryAllocator&&) = default;++  // Reserve bytes from the quota.+  // If we enter overcommit, reclamation will begin concurrently.+  // Returns the number of bytes reserved.+  size_t Reserve(MemoryRequest request) { return allocator_->Reserve(request); }","Just a note, that if `BasicMemoryAllocator::Shutdown` goes away, and that behavior is replaced either with shared_ptr or unique_ptr deletion semantics, then these two methods make `MemoryAllocator` effectively a `BasicMemoryAllocator` implementation as well.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27522,718970296,2021-09-30T00:19:56Z,include/grpc/event_engine/memory_allocator.h,"@@ -0,0 +1,242 @@+// Copyright 2021 The gRPC Authors+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+#ifndef GRPC_EVENT_ENGINE_MEMORY_ALLOCATOR_H+#define GRPC_EVENT_ENGINE_MEMORY_ALLOCATOR_H++#include <grpc/impl/codegen/port_platform.h>++#include <algorithm>+#include <memory>+#include <type_traits>+#include <vector>++#include <grpc/slice.h>++// forward-declaring an internal struct, not used publicly.+struct grpc_slice_buffer;++namespace grpc_event_engine {+namespace experimental {++// TODO(nnoble): needs implementation+class SliceBuffer {+ public:+  SliceBuffer() { abort(); }+  explicit SliceBuffer(grpc_slice_buffer*) { abort(); }++  grpc_slice_buffer* RawSliceBuffer() { return slice_buffer_; }++ private:+  grpc_slice_buffer* slice_buffer_;+};++// Reservation request - how much memory do we want to allocate?+class MemoryRequest {+ public:+  // Request a fixed amount of memory.+  // NOLINTNEXTLINE(google-explicit-constructor)+  MemoryRequest(size_t n) : min_(n), max_(n) {}+  // Request a range of memory.+  MemoryRequest(size_t min, size_t max) : min_(std::min(min, max)), max_(max) {}++  // Increase the size by amount+  MemoryRequest Increase(size_t amount) const {+    return MemoryRequest(min_ + amount, max_ + amount);+  }++  size_t min() const { return min_; }+  size_t max() const { return max_; }++ private:+  size_t min_;+  size_t max_;+};++class BasicMemoryAllocator+    : public std::enable_shared_from_this<BasicMemoryAllocator> {+ public:+  BasicMemoryAllocator() {}+  virtual ~BasicMemoryAllocator() {}++  BasicMemoryAllocator(const BasicMemoryAllocator&) = delete;+  BasicMemoryAllocator& operator=(const BasicMemoryAllocator&) = delete;++  // Reserve bytes from the quota.+  // If we enter overcommit, reclamation will begin concurrently.+  // Returns the number of bytes reserved.+  virtual size_t Reserve(MemoryRequest request) = 0;++  // Release some bytes that were previously reserved.+  virtual void Release(size_t n) = 0;++  // Shutdown this allocator.+  // Further usage of Reserve() is undefined behavior.+  virtual void Shutdown() = 0;+};++class MemoryAllocator {+ public:+  // Construct a MemoryAllocator given a BasicMemoryAllocator implementation.+  // The constructed MemoryAllocator will call BasicMemoryAllocator::Shutdown()+  // upon destruction.+  explicit MemoryAllocator(std::shared_ptr<BasicMemoryAllocator> allocator)+      : allocator_(allocator) {}+  ~MemoryAllocator() {+    if (allocator_ != nullptr) allocator_->Shutdown();+  }++  MemoryAllocator(const MemoryAllocator&) = delete;+  MemoryAllocator& operator=(const MemoryAllocator&) = delete;++  MemoryAllocator(MemoryAllocator&&) = default;+  MemoryAllocator& operator=(MemoryAllocator&&) = default;++  // Reserve bytes from the quota.+  // If we enter overcommit, reclamation will begin concurrently.+  // Returns the number of bytes reserved.+  size_t Reserve(MemoryRequest request) { return allocator_->Reserve(request); }++  // Release some bytes that were previously reserved.+  void Release(size_t n) { return allocator_->Release(n); }++  //+  // The remainder of this type are helper functions implemented in terms of+  // Reserve/Release.+  //++  // An automatic releasing reservation of memory.+  class Reservation {+   public:+    Reservation() = default;+    Reservation(const Reservation&) = delete;+    Reservation& operator=(const Reservation&) = delete;+    Reservation(Reservation&&) = default;+    Reservation& operator=(Reservation&&) = default;+    ~Reservation() {+      if (allocator_ != nullptr) allocator_->Release(size_);+    }++   private:+    friend class MemoryAllocator;+    Reservation(std::shared_ptr<BasicMemoryAllocator> allocator, size_t size)+        : allocator_(allocator), size_(size) {}++    std::shared_ptr<BasicMemoryAllocator> allocator_ = nullptr;+    size_t size_ = 0;+  };++  // Reserve bytes from the quota and automatically release them when+  // Reservation is destroyed.+  Reservation MakeReservation(MemoryRequest request) {+    return Reservation(allocator_, Reserve(request));+  }++  // Allocate a new object of type T, with constructor arguments.+  // The returned type is wrapped, and upon destruction the reserved memory+  // will be released to the allocator automatically. As such, T must have a+  // virtual destructor so we can insert the necessary hook.+  template <typename T, typename... Args>+  typename std::enable_if<std::has_virtual_destructor<T>::value, T*>::type New(+      Args&&... args) {+    // Wrap T such that when it's destroyed, we can release memory back to the+    // allocator.+    class Wrapper final : public T {+     public:+      explicit Wrapper(std::shared_ptr<BasicMemoryAllocator> allocator,+                       Args&&... args)+          : T(std::forward<Args>(args)...), allocator_(std::move(allocator)) {}+      ~Wrapper() override { allocator_->Release(sizeof(*this)); }++     private:+      const std::shared_ptr<BasicMemoryAllocator> allocator_;+    };+    Reserve(sizeof(Wrapper));+    return new Wrapper(allocator_, std::forward<Args>(args)...);+  }++  // Construct a unique ptr immediately.+  template <typename T, typename... Args>+  std::unique_ptr<T> MakeUnique(Args&&... args) {+    return std::unique_ptr<T>(New<T>(std::forward<Args>(args)...));+  }++  // Allocate a slice, using MemoryRequest to size the number of returned bytes.+  // For a variable length request, check the returned slice length to verify+  // how much memory was allocated.+  // Takes care of reserving memory for any relevant control structures also.+  grpc_slice MakeSlice(MemoryRequest request);++  // A C++ allocator for containers of T.+  template <typename T>+  class Container {+   public:+    using value_type = T;++    // Construct the allocator: \a underlying_allocator is borrowed, and must+    // outlive this object.+    explicit Container(MemoryAllocator* underlying_allocator)+        : underlying_allocator_(underlying_allocator) {}+    template <typename U>+    explicit Container(const Container<U>& other)+        : underlying_allocator_(other.underlying_allocator()) {}++    MemoryAllocator* underlying_allocator() const {+      return underlying_allocator_;+    }++    T* allocate(size_t n) {+      underlying_allocator_->Reserve(n * sizeof(T));+      return static_cast<T*>(::operator new(n * sizeof(T)));",Interesting. What happens if you remove the absolute qualification on `::operator new`? Concerns about ambiguity?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27529,719608370,2021-09-30T17:15:25Z,src/core/ext/filters/client_channel/resolver/binder/binder_resolver.cc,"@@ -0,0 +1,127 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include <grpc/support/alloc.h>+#include <grpc/support/string_util.h>++#include ""src/core/ext/filters/client_channel/resolver_registry.h""+#include ""src/core/ext/filters/client_channel/server_address.h""+#include ""src/core/lib/address_utils/parse_address.h""+#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/gpr/string.h""+#include ""src/core/lib/iomgr/resolve_address.h""+#include ""src/core/lib/iomgr/unix_sockets_posix.h""++namespace grpc_core {+namespace {++class BinderResolver : public Resolver {+ public:+  BinderResolver(ServerAddressList addresses, ResolverArgs args)+      : result_handler_(std::move(args.result_handler)),+        addresses_(std::move(addresses)),+        channel_args_(grpc_channel_args_copy(args.args)) {}++  ~BinderResolver() override { grpc_channel_args_destroy(channel_args_); };++  void StartLocked() override {+    Result result;+    result.addresses = std::move(addresses_);+    result.args = channel_args_;+    channel_args_ = nullptr;+    result_handler_->ReturnResult(std::move(result));+  }++  void ShutdownLocked() override {}++ private:+  std::unique_ptr<ResultHandler> result_handler_;+  ServerAddressList addresses_;+  const grpc_channel_args* channel_args_ = nullptr;+};++grpc_error_handle BinderAddrPopulate(absl::string_view authority,+                                     grpc_resolved_address* resolved_addr) {+  if (std::string(authority).size() + 1 > sizeof(resolved_addr->addr)) {+    return GRPC_ERROR_CREATE_FROM_CPP_STRING(std::string(authority) ++                                             "" is too long to be handled"");+  }+  if (std::find_if(authority.begin(), authority.end(),+                   [](char c) { return !isalnum(c); }) != authority.end()) {+    return GRPC_ERROR_CREATE_FROM_CPP_STRING(std::string(authority) ++                                             "" contains invalid character"");+  }+  strcpy(resolved_addr->addr, std::string(authority).c_str());+  resolved_addr->len = strlen(resolved_addr->addr) + 1;+  return GRPC_ERROR_NONE;+}++bool parse_binder(const grpc_core::URI& uri,+                  grpc_resolved_address* resolved_addr) {+  if (uri.scheme() != ""binder"") {+    gpr_log(GPR_ERROR, ""Expected 'binder' scheme, got '%s'"",+            uri.scheme().c_str());+    return false;+  }+  if (uri.authority().empty()) {+    gpr_log(GPR_ERROR, ""authority missing in binder scheme"");+    return false;+  }+  grpc_error_handle error = BinderAddrPopulate(uri.authority(), resolved_addr);+  if (error != GRPC_ERROR_NONE) {+    gpr_log(GPR_ERROR, ""%s"", grpc_error_std_string(error).c_str());+    GRPC_ERROR_UNREF(error);+    return false;+  }+  return true;+}++bool ParseUri(const URI& uri, ServerAddressList* addresses) {+  grpc_resolved_address addr;+  if (!parse_binder(uri, &addr)) {+    return false;+  }+  if (addresses != nullptr) {+    addresses->emplace_back(addr, nullptr /* args */);+  }+  return true;+}++class BinderResolverFactory : public ResolverFactory {","This class may also need to implement the `GetDefaultAuthority()` method, which determines what authority is used for security negotiation in the backend connections.  If you don't do that, you get the default behavior, which is to use the URI path without its leading `/` character, if any.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27507,719620630,2021-09-30T17:33:24Z,src/python/grpcio_tests/tests/unit/_rpc_part_1_test.py,"@@ -36,6 +37,8 @@ from tests.unit.framework.common import test_constants  +@unittest.skipIf(test_common.running_under_gevent(),+                 ""This test is nondeterministic under gevent."")","optional: The part1 and part2 have large number of cases. Is it possible to have a finer granularity for the skips. E.g., for AsyncIO, the hang/deadlock usually has a pattern, like only happen to a type of RPC, or only happen to non-OK issues.But if it's too time-costly to check, I'm okay to keep it this way.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27507,719624559,2021-09-30T17:39:09Z,bazel/_gevent_test_main.py,"@@ -0,0 +1,56 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import grpc+import unittest+import sys+import os+import pkgutil++import zope+from zope.event import subscribers++class SingleLoader(object):+    def __init__(self, pattern):+        self._pattern = pattern+        self.suite = unittest.TestSuite()+        self._loader = unittest.TestLoader()+        tests = []+        for importer, module_name, is_package in pkgutil.walk_packages([os.path.dirname(os.path.relpath(__file__))]):+            if self._pattern in module_name:+                module = importer.find_module(module_name).load_module(module_name)+                tests.append(self._loader.loadTestsFromModule(module))","optional: Maybe we can use `TestLoader.discover(pattern=f""{module_name}.py"")`?",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/27507,719638836,2021-09-30T17:59:48Z,bazel/grpc_python_deps.bzl,"@@ -49,8 +49,10 @@ def grpc_python_deps():     if ""io_bazel_rules_python"" not in native.existing_rules():         http_archive(             name = ""io_bazel_rules_python"",-            url = ""https://github.com/bazelbuild/rules_python/releases/download/0.0.1/rules_python-0.0.1.tar.gz"",-            sha256 = ""aa96a691d3a8177f3215b14b0edc9641787abaaa30363a080165d06ab65e1161"",+            url = ""https://github.com/bazelbuild/rules_python/releases/download/0.4.0/rules_python-0.4.0.tar.gz"",+            sha256 = ""954aa89b491be4a083304a2cb838019c8b8c3720a7abb9c4cb81ac7a24230cea"",+            patches = [""//third_party:rules_python.patch""],+            patch_args = [""-p1""],         )","Even if we upgrade the RBE environments, I think this patch is still necessary. We need it for environments that don't have Python installed at all (like Windows RBE) and the minimum required version of `rules_python` is a moving target. We might update our RBE workers, but then find that the newest version of `rules_python` requires a version of Python _newer_ than what we have installed on our infrastructure.I think the long term action item here is actually to try to upstream this patch to `rules_python`. But it'd call that a P2. Since this dependency is only in our `WORKSPACE` and not in `grpc_python_deps`, there's no chance of it affecting those who use our repo as a Bazel dependency.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27534,719711618,2021-09-30T19:50:42Z,tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py,"@@ -0,0 +1,119 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import datetime+import logging+from typing import List, Optional++from absl import flags+from absl.testing import absltest++from framework import xds_k8s_testcase+from framework.helpers import retryers+from framework.infrastructure import k8s+from framework.test_app import server_app++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient++_TD_CONFIG_RETRY_WAIT_SEC = 2+++class ApiListenerTest(xds_k8s_testcase.RegularXdsKubernetesTestCase):+    ALTERNATE_RESOURCE_SUFFIX = '2'+    previous_route_config_version: str++    def test_api_listener(self) -> None:+        with self.subTest('00_create_health_check'):+            self.td.create_health_check()++        with self.subTest('01_create_backend_services'):+            self.td.create_backend_service()++        with self.subTest('02_create_url_maps'):+            self.td.create_url_map(self.server_xds_host, self.server_xds_port)+            self.td.create_alternative_url_map(self.server_xds_host,+                                               self.server_xds_port)++        with self.subTest('03_create_target_proxies'):+            self.td.create_target_proxy()+            self.td.create_alternative_target_proxy()++        with self.subTest('04_create_forwarding_rule'):+            self.td.create_forwarding_rule(self.server_xds_port)+            self.td.create_alternative_forwarding_rule(self.server_xds_port,+                                                       ip_address='10.10.10.10')++        with self.subTest('05_start_test_servers'):+            self.test_servers: List[_XdsTestServer] = self.startTestServers()++        with self.subTest('06_add_server_backends_to_backend_services'):+            self.setupServerBackends()++        with self.subTest('07_start_test_client'):+            self.test_client: _XdsTestClient = self.startTestClient(+                self.test_servers[0])++        with self.subTest('08_test_client_xds_config_exists'):+            self.assertXdsConfigExists(self.test_client)++        with self.subTest('09_test_server_received_rpcs_from_test_client'):+            self.assertSuccessfulRpcs(self.test_client)+            self.previous_route_config_version = self.getRouteConfigVersion(+                self.test_client)++        with self.subTest('10_delete_one_url_map_target_proxy_forwarding_rule'):+            self.td.delete_forwarding_rule()+            self.td.delete_target_grpc_proxy()+            self.td.delete_url_map()++        with self.subTest('11_test_server_continues_to_receive_rpcs'):++            class TdPropagationRetryableError(Exception):+                pass++            def verify_route_traffic_continues():+                self.assertSuccessfulRpcs(self.test_client)+                if self.previous_route_config_version == self.getRouteConfigVersion(+                        self.test_client):+                    logger.info('Routing config not propagated yet. Retrying.')+                    raise TdPropagationRetryableError(+                        ""CSDS not get updated routing config corresponding""+                        "" to the second set of url maps"")+                else:+                    self.assertSuccessfulRpcs(self.test_client)+                    logger.info('Success.')++            retryer = retryers.constant_retryer(+                wait_fixed=datetime.timedelta(+                    seconds=_TD_CONFIG_RETRY_WAIT_SEC),+                timeout=datetime.timedelta(+                    seconds=xds_k8s_testcase._TD_CONFIG_MAX_WAIT_SEC),+                retry_on_exceptions=(TdPropagationRetryableError,),+                logger=logging,+                log_level=logging.INFO)+            try:+                retryer(verify_route_traffic_continues)+            except retryers.RetryError as e:",nit: Can we name the exception `retry_error` instead of `e`?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/27507,719824116,2021-09-30T23:00:00Z,bazel/_gevent_test_main.py,"@@ -0,0 +1,56 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import grpc+import unittest+import sys+import os+import pkgutil++import zope+from zope.event import subscribers","Good catch. We need `zope` as a transitive dependency, since `gevent` requires it, but we don't need this import here. I accidentally left this in from a debugging session. Removed.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/27507,719834458,2021-09-30T23:12:16Z,src/python/grpcio_tests/tests/unit/_rpc_part_1_test.py,"@@ -36,6 +37,8 @@ from tests.unit.framework.common import test_constants  +@unittest.skipIf(test_common.running_under_gevent(),+                 ""This test is nondeterministic under gevent."")","I attempted to bisect and found that 90+% of the individual test cases deterministically deadlock. I do intend to make these tests work with the gevent reimplementation, so I'm not giving up on these yet.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27534,719840094,2021-09-30T23:28:14Z,tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py,"@@ -0,0 +1,119 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import datetime+import logging+from typing import List, Optional++from absl import flags+from absl.testing import absltest++from framework import xds_k8s_testcase+from framework.helpers import retryers+from framework.infrastructure import k8s+from framework.test_app import server_app++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient++_TD_CONFIG_RETRY_WAIT_SEC = 2+++class ApiListenerTest(xds_k8s_testcase.RegularXdsKubernetesTestCase):+    ALTERNATE_RESOURCE_SUFFIX = '2'+    previous_route_config_version: str++    def test_api_listener(self) -> None:+        with self.subTest('00_create_health_check'):+            self.td.create_health_check()++        with self.subTest('01_create_backend_services'):+            self.td.create_backend_service()++        with self.subTest('02_create_url_maps'):+            self.td.create_url_map(self.server_xds_host, self.server_xds_port)+            self.td.create_alternative_url_map(self.server_xds_host,+                                               self.server_xds_port)++        with self.subTest('03_create_target_proxies'):+            self.td.create_target_proxy()+            self.td.create_alternative_target_proxy()++        with self.subTest('04_create_forwarding_rule'):+            self.td.create_forwarding_rule(self.server_xds_port)+            self.td.create_alternative_forwarding_rule(self.server_xds_port,+                                                       ip_address='10.10.10.10')++        with self.subTest('05_start_test_servers'):+            self.test_servers: List[_XdsTestServer] = self.startTestServers()++        with self.subTest('06_add_server_backends_to_backend_services'):+            self.setupServerBackends()++        with self.subTest('07_start_test_client'):+            self.test_client: _XdsTestClient = self.startTestClient(+                self.test_servers[0])++        with self.subTest('08_test_client_xds_config_exists'):+            self.assertXdsConfigExists(self.test_client)++        with self.subTest('09_test_server_received_rpcs_from_test_client'):+            self.assertSuccessfulRpcs(self.test_client)+            self.previous_route_config_version = self.getRouteConfigVersion(+                self.test_client)++        with self.subTest('10_delete_one_url_map_target_proxy_forwarding_rule'):+            self.td.delete_forwarding_rule()+            self.td.delete_target_grpc_proxy()+            self.td.delete_url_map()++        with self.subTest('11_test_server_continues_to_receive_rpcs'):++            class TdPropagationRetryableError(Exception):+                pass++            def verify_route_traffic_continues():+                self.assertSuccessfulRpcs(self.test_client)+                if self.previous_route_config_version == self.getRouteConfigVersion(+                        self.test_client):+                    logger.info('Routing config not propagated yet. Retrying.')+                    raise TdPropagationRetryableError(+                        ""CSDS not get updated routing config corresponding""+                        "" to the second set of url maps"")+                else:+                    self.assertSuccessfulRpcs(self.test_client)+                    logger.info('Success.')++            retryer = retryers.constant_retryer(+                wait_fixed=datetime.timedelta(+                    seconds=_TD_CONFIG_RETRY_WAIT_SEC),+                timeout=datetime.timedelta(+                    seconds=xds_k8s_testcase._TD_CONFIG_MAX_WAIT_SEC),+                retry_on_exceptions=(TdPropagationRetryableError,),+                logger=logging,+                log_level=logging.INFO)","This is a sound strategy, thanks.Just to double-check, by ""never see any retry attempt"", do you you mean within 2 seconds after the UrlMap+TP+FR is removed, the version_info in the client-side xDS config is already updated, so there is no retry log at all? (I'm asking because this could be a new data point for TD config propagation, that patch might take minutes, but delete only takes seconds.)",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27507,719842552,2021-09-30T23:35:24Z,src/python/grpcio_tests/tests/unit/BUILD.bazel,"@@ -144,6 +144,7 @@ py2and3_test(     imports = [""../../""],     main = ""_dynamic_stubs_test.py"",     deps = [+        "":test_common"",",I might miss this line in the first pass. Why do we need to add this dependency now?,X
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27529,719922478,2021-10-01T04:07:03Z,src/core/ext/filters/client_channel/resolver/binder/binder_resolver.cc,"@@ -0,0 +1,127 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include <grpc/support/alloc.h>+#include <grpc/support/string_util.h>++#include ""src/core/ext/filters/client_channel/resolver_registry.h""+#include ""src/core/ext/filters/client_channel/server_address.h""+#include ""src/core/lib/address_utils/parse_address.h""+#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/gpr/string.h""+#include ""src/core/lib/iomgr/resolve_address.h""+#include ""src/core/lib/iomgr/unix_sockets_posix.h""++namespace grpc_core {+namespace {++class BinderResolver : public Resolver {+ public:+  BinderResolver(ServerAddressList addresses, ResolverArgs args)+      : result_handler_(std::move(args.result_handler)),+        addresses_(std::move(addresses)),+        channel_args_(grpc_channel_args_copy(args.args)) {}++  ~BinderResolver() override { grpc_channel_args_destroy(channel_args_); };++  void StartLocked() override {+    Result result;+    result.addresses = std::move(addresses_);+    result.args = channel_args_;+    channel_args_ = nullptr;+    result_handler_->ReturnResult(std::move(result));+  }++  void ShutdownLocked() override {}++ private:+  std::unique_ptr<ResultHandler> result_handler_;+  ServerAddressList addresses_;+  const grpc_channel_args* channel_args_ = nullptr;+};++grpc_error_handle BinderAddrPopulate(absl::string_view authority,+                                     grpc_resolved_address* resolved_addr) {+  if (std::string(authority).size() + 1 > sizeof(resolved_addr->addr)) {+    return GRPC_ERROR_CREATE_FROM_CPP_STRING(std::string(authority) ++                                             "" is too long to be handled"");+  }+  if (std::find_if(authority.begin(), authority.end(),+                   [](char c) { return !isalnum(c); }) != authority.end()) {+    return GRPC_ERROR_CREATE_FROM_CPP_STRING(std::string(authority) ++                                             "" contains invalid character"");+  }+  strcpy(resolved_addr->addr, std::string(authority).c_str());","It is actually a `char[]` (and it is `reinterpret_cast`ed to `struct sockaddr` when required)```struct grpc_resolved_address {  char addr[GRPC_MAX_SOCKADDR_SIZE];  socklen_t len;};```In binder transport's case we only need a simple string so I directly store a c-style string in it. When parsing `resolved_addr->addr` (which is null-terminated) we can make sure that we don't accidentally read more than `sizeof(resolved_addr->addr)` bytes, so I think it is not that bad> The PR description says that the SubchannelConnector will know how to deal with this. Where is that code? I'd like to make sure that we have a good story for how this all fits together.Let me clean up the code and link it here later",
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27529,720126527,2021-10-01T10:17:26Z,src/core/ext/filters/client_channel/resolver/binder/binder_resolver.cc,"@@ -0,0 +1,127 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include <grpc/support/alloc.h>+#include <grpc/support/string_util.h>++#include ""src/core/ext/filters/client_channel/resolver_registry.h""+#include ""src/core/ext/filters/client_channel/server_address.h""+#include ""src/core/lib/address_utils/parse_address.h""+#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/gpr/string.h""+#include ""src/core/lib/iomgr/resolve_address.h""+#include ""src/core/lib/iomgr/unix_sockets_posix.h""++namespace grpc_core {+namespace {++class BinderResolver : public Resolver {+ public:+  BinderResolver(ServerAddressList addresses, ResolverArgs args)+      : result_handler_(std::move(args.result_handler)),+        addresses_(std::move(addresses)),+        channel_args_(grpc_channel_args_copy(args.args)) {}++  ~BinderResolver() override { grpc_channel_args_destroy(channel_args_); };++  void StartLocked() override {+    Result result;+    result.addresses = std::move(addresses_);+    result.args = channel_args_;+    channel_args_ = nullptr;+    result_handler_->ReturnResult(std::move(result));+  }++  void ShutdownLocked() override {}++ private:+  std::unique_ptr<ResultHandler> result_handler_;+  ServerAddressList addresses_;+  const grpc_channel_args* channel_args_ = nullptr;+};++grpc_error_handle BinderAddrPopulate(absl::string_view authority,+                                     grpc_resolved_address* resolved_addr) {+  if (std::string(authority).size() + 1 > sizeof(resolved_addr->addr)) {+    return GRPC_ERROR_CREATE_FROM_CPP_STRING(std::string(authority) ++                                             "" is too long to be handled"");+  }+  if (std::find_if(authority.begin(), authority.end(),+                   [](char c) { return !isalnum(c); }) != authority.end()) {+    return GRPC_ERROR_CREATE_FROM_CPP_STRING(std::string(authority) ++                                             "" contains invalid character"");+  }+  strcpy(resolved_addr->addr, std::string(authority).c_str());","1. When user calls `CreateBinderChannel`, we notify Java to start trying to connect to the remote component (identifier is passed to Java)https://github.com/sifmelcara/grpc/blob/21dfd08ef5329c98614e96fd04b38a4624271e9c/src/core/ext/transport/binder/client/channel_create.cc#L89-L93Then we save the identifier in `GRPC_ARG_SERVER_URI` channel arg.2. Java will notify us when the connection is established. We then save the endpoint binder in `EndpointBinderPool`.  https://github.com/sifmelcara/grpc/blob/21dfd08ef5329c98614e96fd04b38a4624271e9c/src/core/ext/transport/binder/client/endpoint_binder_pool.cc3. When `SubchannelConnector::Connect` is called, we get the identifier from `args.address->addr` and use it to obtain the endpoint binder from `EndpointBinderPool` https://github.com/sifmelcara/grpc/blob/21dfd08ef5329c98614e96fd04b38a4624271e9c/src/core/ext/transport/binder/client/binder_connector.cc#L37-L57",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/27537,720260130,2021-10-01T13:44:49Z,templates/tools/dockerfile/php7_deps.include,"@@ -29,17 +29,17 @@ RUN cd /var/local ${'\\'}   && tar -zxvf bison-3.4.2.tar.gz ${'\\'}   && cd /var/local/bison-3.4.2 ${'\\'}   && ./configure ${'\\'}-  && make ${'\\'}+  && make -j4 ${'\\'}","I think $(nproc) is not necessarily better since in many of our jobs, we're running multiple run_tests.py instances (where each one has their own docker container for isolation) in parallel, and having multiple `make -j$(nproc)` might be excessive. `-j4` was fine IMHO.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27523,720332968,2021-10-01T15:13:59Z,src/cpp/ext/filters/census/client_filter.cc,"@@ -44,15 +44,29 @@ constexpr uint32_t  grpc_error_handle CensusClientCallData::Init(     grpc_call_element* /* elem */, const grpc_call_element_args* args) {-  auto tracer = args->arena->New<OpenCensusCallTracer>(args);+  tracer_ = args->arena->New<OpenCensusCallTracer>(args);   GPR_DEBUG_ASSERT(args->context[GRPC_CONTEXT_CALL_TRACER].value == nullptr);-  args->context[GRPC_CONTEXT_CALL_TRACER].value = tracer;+  args->context[GRPC_CONTEXT_CALL_TRACER].value = tracer_;   args->context[GRPC_CONTEXT_CALL_TRACER].destroy = [](void* tracer) {     (static_cast<OpenCensusCallTracer*>(tracer))->~OpenCensusCallTracer();   };   return GRPC_ERROR_NONE; } +void CensusClientCallData::StartTransportStreamOpBatch(+    grpc_call_element* elem, TransportStreamOpBatch* op) {+  // Note that we are generating the overall call context here instead of in+  // the constructor of `OpenCensusCallTracer` due to the semantics of+  // `grpc_census_call_set_context` which allows the application to set the+  // census context for a call anytime before the first call to+  // `grpc_call_start_batch`.+  if (!context_created_) {","Instead of adding a `context_created_` data member, I think you can just trigger this when you see the `send_initial_metadata` op.  That will happen just once at the start of the call.",X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/27523,720445952,2021-10-01T18:05:02Z,src/cpp/ext/filters/census/client_filter.cc,"@@ -44,15 +44,29 @@ constexpr uint32_t  grpc_error_handle CensusClientCallData::Init(     grpc_call_element* /* elem */, const grpc_call_element_args* args) {-  auto tracer = args->arena->New<OpenCensusCallTracer>(args);+  tracer_ = args->arena->New<OpenCensusCallTracer>(args);   GPR_DEBUG_ASSERT(args->context[GRPC_CONTEXT_CALL_TRACER].value == nullptr);-  args->context[GRPC_CONTEXT_CALL_TRACER].value = tracer;+  args->context[GRPC_CONTEXT_CALL_TRACER].value = tracer_;   args->context[GRPC_CONTEXT_CALL_TRACER].destroy = [](void* tracer) {     (static_cast<OpenCensusCallTracer*>(tracer))->~OpenCensusCallTracer();   };   return GRPC_ERROR_NONE; } +void CensusClientCallData::StartTransportStreamOpBatch(+    grpc_call_element* elem, TransportStreamOpBatch* op) {+  // Note that we are generating the overall call context here instead of in+  // the constructor of `OpenCensusCallTracer` due to the semantics of+  // `grpc_census_call_set_context` which allows the application to set the+  // census context for a call anytime before the first call to+  // `grpc_call_start_batch`.+  if (!context_created_) {",I was thinking of doing that but this seems cleaner since we no longer need to make assumptions on the ordering of the ops.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27523,720457270,2021-10-01T18:24:58Z,src/cpp/ext/filters/census/client_filter.cc,"@@ -44,15 +44,29 @@ constexpr uint32_t  grpc_error_handle CensusClientCallData::Init(     grpc_call_element* /* elem */, const grpc_call_element_args* args) {-  auto tracer = args->arena->New<OpenCensusCallTracer>(args);+  tracer_ = args->arena->New<OpenCensusCallTracer>(args);   GPR_DEBUG_ASSERT(args->context[GRPC_CONTEXT_CALL_TRACER].value == nullptr);-  args->context[GRPC_CONTEXT_CALL_TRACER].value = tracer;+  args->context[GRPC_CONTEXT_CALL_TRACER].value = tracer_;   args->context[GRPC_CONTEXT_CALL_TRACER].destroy = [](void* tracer) {     (static_cast<OpenCensusCallTracer*>(tracer))->~OpenCensusCallTracer();   };   return GRPC_ERROR_NONE; } +void CensusClientCallData::StartTransportStreamOpBatch(+    grpc_call_element* elem, TransportStreamOpBatch* op) {+  // Note that we are generating the overall call context here instead of in+  // the constructor of `OpenCensusCallTracer` due to the semantics of+  // `grpc_census_call_set_context` which allows the application to set the+  // census context for a call anytime before the first call to+  // `grpc_call_start_batch`.+  if (!context_created_) {","I think it's safe to assume that send_initial_metadata will always be the first op on the call; it can't proceed without that.  And this way, we are not unnecessarily increasing the per-call memory size (albeit by a small amount).",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27194,720727476,2021-10-02T20:49:51Z,include/grpc/event_engine/slice.h,"@@ -0,0 +1,120 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_EVENT_ENGINE_SLICE_H+#define GRPC_EVENT_ENGINE_SLICE_H++#include <grpc/impl/codegen/port_platform.h>++#include <string>+#include <utility>++#include <grpc/slice.h>++namespace grpc_event_engine {+namespace experimental {++class SliceBuffer;++/// A wrapper around \a grpc_slice.+///+/// A slice represents a contiguous reference counted array of bytes.+/// It is cheap to take references to a slice, and it is cheap to create a+/// slice pointing to a subset of another slice.+class Slice final {+  friend class SliceBuffer;++ public:+  /// Construct an empty slice.+  Slice() : slice_(grpc_empty_slice()) {}+  /// Destructor - drops one reference.+  ~Slice() { grpc_slice_unref(slice_); }++  enum AddRef { ADD_REF };+  /// Construct a slice from \a slice, adding a reference.+  Slice(grpc_slice slice, AddRef) : slice_(grpc_slice_ref(slice)) {}++  enum StealRef { STEAL_REF };+  /// Construct a slice from \a slice, stealing a reference.+  Slice(grpc_slice slice, StealRef) : slice_(slice) {}++  /// Allocate a slice of specified size+  explicit Slice(size_t len) : slice_(grpc_slice_malloc(len)) {}++  /// Construct a slice from a copied buffer+  Slice(const void* buf, size_t len)+      : slice_(grpc_slice_from_copied_buffer(reinterpret_cast<const char*>(buf),+                                             len)) {}++  /// Construct a slice from a copied string+  /* NOLINTNEXTLINE(google-explicit-constructor) */+  Slice(const std::string& str)+      : slice_(grpc_slice_from_copied_buffer(str.c_str(), str.length())) {}++  enum StaticSlice { STATIC_SLICE };++  /// Construct a slice from a static buffer+  Slice(const void* buf, size_t len, StaticSlice)+      : slice_(grpc_slice_from_static_buffer(reinterpret_cast<const char*>(buf),+                                             len)) {}++  /// Copy constructor, adds a reference.+  Slice(const Slice& other) : slice_(grpc_slice_ref(other.slice_)) {}","I think I prefer:```Slice(const Slice&) = delete;Slice& operator=(const Slice&) = delete;Slice(Slice&& other) : slice_(other.slice_) { other.slice_ = kEmpty; }Slice& operator=(Slice&& other) {  std::swap(slice_, other.slice_);  return *this;}Slice Ref() { return Slice(grpc_slice_ref(slice_)); }Slice Copy() { return Slice(grpc_slice_copy(slice_)); }```i.e. slice itself is move only, and has explicit factory functions for reffing and copying.this clarifies intent at copy sites, and when substituting this type into existing code we don't accidentally end up with an explosion of refcounting.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27406,721532500,2021-10-04T16:35:53Z,test/core/promise/promise_fuzzer.cc,"@@ -12,10 +12,306 @@ // See the License for the specific language governing permissions and // limitations under the License. +#include ""src/core/lib/promise/activity.h""+#include ""src/core/lib/promise/join.h""+#include ""src/core/lib/promise/map.h""+#include ""src/core/lib/promise/promise.h""+#include ""src/core/lib/promise/race.h""+#include ""src/core/lib/promise/seq.h"" #include ""src/libfuzzer/libfuzzer_macro.h"" #include ""test/core/promise/promise_fuzzer.pb.h""  bool squelch = true; bool leak_check = true; -DEFINE_PROTO_FUZZER(const promise_fuzzer::Msg&) {}+namespace grpc_core {+// Return type for infallible promises.+// We choose this so that it's easy to construct, and will trigger asan failures+// if misused, and is copyable.+using IntHdl = std::shared_ptr<int>;++template <typename T>+using PromiseFactory = std::function<Promise<T>(T)>;++namespace {+class Fuzzer {+ public:+  void Run(const promise_fuzzer::Msg& msg) {+    // If there's no promise we can't construct and activity and... we're done.+    if (!msg.has_promise()) {+      return;+    }+    // Construct activity.+    activity_ = MakeActivity(+        [msg, this] {+          return Seq(MakePromise(msg.promise()),+                     [] { return absl::OkStatus(); });+        },+        Scheduler{this},+        [this](absl::Status status) {+          // Must only be called once+          GPR_ASSERT(!done_);+          // If we became certain of the eventual status, verify it.+          if (expected_status_.has_value()) {+            GPR_ASSERT(status == *expected_status_);+          }+          // Mark ourselves done.+          done_ = true;+        });+    for (int i = 0; !done_ && activity_ != nullptr && i < msg.actions_size();+         i++) {+      // Do some things+      const auto& action = msg.actions(i);+      switch (action.action_type_case()) {+        // Force a wakeup+        case promise_fuzzer::Action::kForceWakeup:+          activity_->ForceWakeup();+          break;+        // Cancel from the outside+        case promise_fuzzer::Action::kCancel:+          ExpectCancelled();+          activity_.reset();+          break;+        // Flush any pending wakeups+        case promise_fuzzer::Action::kFlushWakeup:+          if (wakeup_ != nullptr) absl::exchange(wakeup_, nullptr)();+          break;+        // Drop some wakeups (external system closed?)+        case promise_fuzzer::Action::kDropWaker: {+          int n = action.drop_waker();+          auto v = std::move(wakers_[n]);+          wakers_.erase(n);+          break;+        }+        // Wakeup some wakeups+        case promise_fuzzer::Action::kAwakeWaker: {+          int n = action.awake_waker();+          auto v = std::move(wakers_[n]);+          wakers_.erase(n);+          for (auto& w : v) {+            w.Wakeup();+          }+          break;+        }+        case promise_fuzzer::Action::ACTION_TYPE_NOT_SET:+          break;+      }+    }+    ExpectCancelled();+    activity_.reset();+    if (wakeup_ != nullptr) absl::exchange(wakeup_, nullptr)();+    GPR_ASSERT(done_);+  }++ private:+  // Schedule wakeups against the fuzzer+  struct Scheduler {+    Fuzzer* fuzzer;+    // Schedule a wakeup+    template <typename ActivityType>+    void ScheduleWakeup(ActivityType* activity) {+      GPR_ASSERT(activity == fuzzer->activity_.get());+      GPR_ASSERT(fuzzer->wakeup_ == nullptr);+      fuzzer->wakeup_ = [activity]() { activity->RunScheduledWakeup(); };+    }+  };++  // We know that if not already finished, the status when finished will be+  // cancelled.+  void ExpectCancelled() {+    if (!done_ && !expected_status_.has_value()) {+      expected_status_ = absl::CancelledError();+    }+  }++  // Construct a promise factory from a protobuf+  PromiseFactory<IntHdl> MakePromiseFactory(+      const promise_fuzzer::PromiseFactory& p) {+    switch (p.promise_factory_type_case()) {+      case promise_fuzzer::PromiseFactory::kPromise:+        return [p, this](IntHdl) { return MakePromise(p.promise()); };+      case promise_fuzzer::PromiseFactory::kLast:+        return [](IntHdl h) { return [h]() { return h; }; };+      case promise_fuzzer::PromiseFactory::PROMISE_FACTORY_TYPE_NOT_SET:+        break;+    }+    return [](IntHdl) {+      return []() -> Poll<IntHdl> { return std::make_shared<int>(42); };+    };+  }++  // Construct a promise from a protobuf+  Promise<IntHdl> MakePromise(const promise_fuzzer::Promise& p) {+    switch (p.promise_type_case()) {+      case promise_fuzzer::Promise::kSeq:+        switch (p.seq().promise_factories_size()) {","Could we eliminate a bunch of code here if the Seq type supported something like the builder pattern, or some other kind of decoupled construction from sequence definition?```std::vector<PromiseFactory> factories;for (int i = 0; i < p.seq().promise_factories_size(); ++i) factories.push_back(MakePromiseFactory(...));return Seq(MakePromise(p.seq().first)).all(factories);```",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27435,721658149,2021-10-04T19:33:19Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -482,7 +484,10 @@ class ClientChannel::SubchannelWrapper : public SubchannelInterface {       GPR_ASSERT(it != chand_->subchannel_refcount_map_.end());       --it->second;       if (it->second == 0) {-        chand_->channelz_node_->RemoveChildSubchannel(subchannel_node->uuid());+        if (chand_->channelz_node_ != nullptr) {",Similar thing here: this conditional can actually wrap all of the code from lines 481 to 493.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/27620,723125158,2021-10-06T10:55:54Z,templates/tools/dockerfile/python_stretch.include,"@@ -1,9 +1,9 @@-FROM debian:stretch+FROM debian:bullseye","this file is called ""python_stretch.include"", so you can't just change it into bullseye without renaming it.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/27537,723153826,2021-10-06T11:37:49Z,templates/tools/dockerfile/php7_deps.include,"@@ -29,17 +29,17 @@ RUN cd /var/local ${'\\'}   && tar -zxvf bison-3.4.2.tar.gz ${'\\'}   && cd /var/local/bison-3.4.2 ${'\\'}   && ./configure ${'\\'}-  && make ${'\\'}+  && make -j4 ${'\\'}","Ah, you're right. My bad.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27620,723455835,2021-10-06T16:19:45Z,tools/run_tests/run_interop_tests.py,"@@ -595,6 +595,7 @@ def unimplemented_test_cases_server(self):     def __str__(self):         return 'ruby' +_PYTHON_BINARY = 'py39_native/bin/python'",nit: should we move it as a class constant?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26428,723582776,2021-10-06T18:44:05Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -1736,7 +1668,7 @@ grpc_error_handle ClientChannel::DoPingLocked(grpc_transport_op* op) {           ABSL_EXCLUSIVE_LOCKS_REQUIRED(&ClientChannel::work_serializer_) {             SubchannelWrapper* subchannel = static_cast<SubchannelWrapper*>(                 complete_pick->subchannel.get());-            ConnectedSubchannel* connected_subchannel =+            RefCountedPtr<ConnectedSubchannel> connected_subchannel =","It's actually not a lock at all; in the client channel code, we have a (perhaps unfortunate) convention that the ""Locked"" suffix indicates running in the `WorkSerializer`.  This is documented here:https://github.com/grpc/grpc/blob/c56b0a381880e58e861647344395a2d8267f6800/src/core/ext/filters/client_channel/client_channel.h#L210So yeah, there's no issue with grabbing the data plane mutex from inside the `WorkSerializer`, since the `WorkSerializer` API is inherently non-blocking.",
133680,sampajano,https://api.github.com/repos/grpc/grpc/pulls/27563,723664831,2021-10-06T20:48:46Z,src/objective-c/Libuv-gRPC.podspec,"@@ -0,0 +1,157 @@+# This file has been automatically generated from a template file.+# Please make modifications to+# `templates/src/objective-c/libuv-gRPC.podspec.template` instead. This+# file can be regenerated from the template by running+# `tools/buildgen/generate_projects.sh`.++# Libuv CocoaPods podspec++# Copyright 2021, Google Inc.+# All rights reserved.+#+# Redistribution and use in source and binary forms, with or without+# modification, are permitted provided that the following conditions are+# met:+#+#     * Redistributions of source code must retain the above copyright+# notice, this list of conditions and the following disclaimer.+#     * Redistributions in binary form must reproduce the above+# copyright notice, this list of conditions and the following disclaimer+# in the documentation and/or other materials provided with the+# distribution.+#     * Neither the name of Google Inc. nor the names of its+# contributors may be used to endorse or promote products derived from+# this software without specific prior written permission.+#+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS+# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR+#   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT+# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,+# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT+# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,+# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY+# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT+# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.++Pod::Spec.new do |spec|++  pod_version           = ""0.0.6""+  libuv_version         = ""1.34.0""++  spec.name         = ""Libuv-gRPC""+  spec.version      = pod_version+  spec.summary      = ""Cross-platform asynchronous I/O""++  spec.description  = <<-DESC+    libuv is a multi-platform support library with a focus on asynchronous I/O.+    It was primarily developed for use by Node.js, but it's also used by Luvit,+    Julia, pyuv, and others.+  DESC++  spec.homepage     = ""https://libuv.org/""++  spec.license  = { :type => 'Mixed', :file => 'LICENSE' }+  spec.author    = ""libuv""++  # When using multiple platforms+  spec.ios.deployment_target = '9.0'+  spec.osx.deployment_target = '10.10'+  spec.tvos.deployment_target = '10.0'+  spec.watchos.deployment_target = '4.0'++  spec.source       = { :git => ""https://github.com/libuv/libuv.git"", :tag => ""v#{libuv_version}"" }++  name = 'uv'+  spec.module_name = name+  spec.header_mappings_dir = 'include'+  spec.header_dir = name++  spec.subspec 'Interface' do |ss|+    ss.header_mappings_dir = 'include'+    ss.source_files = ""include/uv.h"",+                      ""include/uv/errno.h"",+                      ""include/uv/threadpool.h"",+                      ""include/uv/version.h"",+                      ""include/uv/tree.h"",+                      ""include/uv/unix.h"",+                      ""include/uv/darwin.h""+  end++  spec.subspec 'Implementation' do |ss|+    ss.header_mappings_dir = 'src'+    ss.source_files =",Would you mind sharing some info on how this list is generated and should be maintained?Also for headers files above :),
133680,sampajano,https://api.github.com/repos/grpc/grpc/pulls/27563,723672882,2021-10-06T21:01:22Z,src/objective-c/Libuv-gRPC.podspec,"@@ -0,0 +1,157 @@+# This file has been automatically generated from a template file.+# Please make modifications to+# `templates/src/objective-c/libuv-gRPC.podspec.template` instead. This+# file can be regenerated from the template by running+# `tools/buildgen/generate_projects.sh`.++# Libuv CocoaPods podspec++# Copyright 2021, Google Inc.+# All rights reserved.+#+# Redistribution and use in source and binary forms, with or without+# modification, are permitted provided that the following conditions are+# met:+#+#     * Redistributions of source code must retain the above copyright+# notice, this list of conditions and the following disclaimer.+#     * Redistributions in binary form must reproduce the above+# copyright notice, this list of conditions and the following disclaimer+# in the documentation and/or other materials provided with the+# distribution.+#     * Neither the name of Google Inc. nor the names of its+# contributors may be used to endorse or promote products derived from+# this software without specific prior written permission.+#+# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS+# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT+# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR+#   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT+# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,+# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT+# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,+# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY+# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT+# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE+# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.++Pod::Spec.new do |spec|++  pod_version           = ""0.0.6""+  libuv_version         = ""1.34.0""++  spec.name         = ""Libuv-gRPC""+  spec.version      = pod_version+  spec.summary      = ""Cross-platform asynchronous I/O""++  spec.description  = <<-DESC+    libuv is a multi-platform support library with a focus on asynchronous I/O.+    It was primarily developed for use by Node.js, but it's also used by Luvit,+    Julia, pyuv, and others.+  DESC++  spec.homepage     = ""https://libuv.org/""++  spec.license  = { :type => 'Mixed', :file => 'LICENSE' }+  spec.author    = ""libuv""++  # When using multiple platforms+  spec.ios.deployment_target = '9.0'+  spec.osx.deployment_target = '10.10'+  spec.tvos.deployment_target = '10.0'+  spec.watchos.deployment_target = '4.0'++  spec.source       = { :git => ""https://github.com/libuv/libuv.git"", :tag => ""v#{libuv_version}"" }++  name = 'uv'+  spec.module_name = name+  spec.header_mappings_dir = 'include'+  spec.header_dir = name++  spec.subspec 'Interface' do |ss|+    ss.header_mappings_dir = 'include'+    ss.source_files = ""include/uv.h"",+                      ""include/uv/errno.h"",+                      ""include/uv/threadpool.h"",+                      ""include/uv/version.h"",+                      ""include/uv/tree.h"",+                      ""include/uv/unix.h"",+                      ""include/uv/darwin.h""+  end++  spec.subspec 'Implementation' do |ss|+    ss.header_mappings_dir = 'src'+    ss.source_files =+    ""src/fs-poll.c"",+    ""src/idna.c"",+    ""src/inet.c"",+    ""src/strscpy.c"",+    ""src/threadpool.c"",+    ""src/timer.c"",+    ""src/uv-data-getter-setters.c"",+    ""src/uv-common.c"",+    ""src/version.c"",+    ""src/unix/async.c"",+    ""src/unix/core.c"",+    ""src/unix/dl.c"",+    ""src/unix/fs.c"",+    ""src/unix/getaddrinfo.c"",+    ""src/unix/getnameinfo.c"",+    ""src/unix/loop.c"",+    ""src/unix/loop-watcher.c"",+    ""src/unix/pipe.c"",+    ""src/unix/poll.c"",+    ""src/unix/process.c"",+    ""src/unix/signal.c"",+    ""src/unix/stream.c"",+    ""src/unix/tcp.c"",+    ""src/unix/thread.c"",+    ""src/unix/tty.c"",+    ""src/unix/udp.c"",+    ""src/unix/bsd-ifaddrs.c"",+    ""src/unix/darwin.c"",+    ""src/unix/fsevents.c"",+    ""src/unix/kqueue.c"",+    ""src/unix/darwin-proctitle.c"",+    ""src/unix/proctitle.c"",+    ""src/heap-inl.h"",+    ""src/idna.h"",+    ""src/queue.h"",+    ""src/strscpy.h"",+    ""src/uv-common.h"",+    ""src/unix/atomic-ops.h"",+    ""src/unix/internal.h"",+    ""src/unix/spinlock.h""++    ss.dependency ""#{spec.name}/Interface"", pod_version+  end++  spec.requires_arc = false++  spec.pod_target_xcconfig = {+    'HEADER_SEARCH_PATHS' => '""$(inherited)"" ""$(PODS_TARGET_SRCROOT)/include""',+    'USER_HEADER_SEARCH_PATHS' => '""$(PODS_TARGET_SRCROOT)"" ""$(PODS_TARGET_SRCROOT)/src"" ""$(PODS_TARGET_SRCROOT)/include""',+    'GCC_PREPROCESSOR_DEFINITIONS' => '""$(inherited)"" ""COCOAPODS=1""',+    'CLANG_WARN_STRICT_PROTOTYPES' => 'NO',+    'CLANG_WARN_DOCUMENTATION_COMMENTS' => 'NO',+    'USE_HEADERMAP' => 'NO',+    'ALWAYS_SEARCH_USER_PATHS' => 'NO',+    'GCC_TREAT_WARNINGS_AS_ERRORS' => 'NO',+    'GCC_WARN_INHIBIT_ALL_WARNINGS' => 'YES'+  }++  spec.libraries = 'c++'++  spec.compiler_flags =","Along the same line, it would be nice to know how these flags can be kept in sync with core.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27618,723726189,2021-10-06T22:34:19Z,src/core/lib/http/httpcli.cc,"@@ -41,17 +41,59 @@ #include ""src/core/lib/iomgr/tcp_client.h"" #include ""src/core/lib/slice/slice_internal.h"" -struct internal_request {+namespace {++struct InternalRequest {",Let's turn this into a proper class and change the static functions below to be methods.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27618,723727798,2021-10-06T22:37:59Z,test/core/security/credentials_test.cc,"@@ -449,22 +449,39 @@ static void test_oauth2_token_fetcher_creds_parsing_missing_token_lifetime(   grpc_http_response_destroy(&response); } -typedef struct {+namespace {++struct ExpectedMetadata {   const char* key;   const char* value;-} expected_md;+};++struct RequestMetadataState {",Let's turn this into a proper class (make data members private and change static functions that act on it to be methods).,X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,723875401,2021-10-07T06:14:58Z,src/core/tsi/ssl/key_logging/ssl_key_logging.h,"@@ -0,0 +1,151 @@+/*+ *+ * Copyright 2021 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_TSI_SSL_KEY_LOGGING_SSL_KEY_LOGGING_H+#define GRPC_CORE_TSI_SSL_KEY_LOGGING_SSL_KEY_LOGGING_H++#include <grpc/support/port_platform.h>++#include <iostream>+#include <map>++#include <grpc/grpc_security.h>+#include <grpc/slice.h>+#include <grpc/support/sync.h>++extern ""C"" {+#include <openssl/ssl.h>+}++#include ""absl/base/thread_annotations.h""+#include ""src/core/lib/gprpp/memory.h""+#include ""src/core/lib/gprpp/ref_counted.h""+#include ""src/core/lib/gprpp/sync.h""+++namespace tsi {++// Tsi implementation of the session key log config.+// struct grpc_tls_session_key_log_config will wrap around this type.+struct TsiTlsSessionKeyLogConfig {","nit: since this struct has private members and public getter/setters, shall we just call it a class instead?",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,724425840,2021-10-07T18:20:20Z,src/core/tsi/ssl_transport_security.h,"@@ -76,6 +76,21 @@ void tsi_ssl_session_cache_ref(tsi_ssl_session_cache* cache); /* Decrement reference counter of \a cache.  */ void tsi_ssl_session_cache_unref(tsi_ssl_session_cache* cache); +/* --- tsi_ssl_key_logger object ---++   Experimental SSL Key logging functionality to enable decryption of+   packet captures.  */++typedef struct tsi_tls_session_key_logger tsi_tls_session_key_logger;","Same question as `grpc_tls_session_key_logger`: what was the reason for defining these opaque types here? Sometimes we need to do that because certain header files have to be pure C, and can't include C++ classes. In those scenarios, we have to define an opaque type, and do a type casting later on.But this header file doesn't belong to that directory. Can we just use the C++ types instead? (there are a few places following the similar patterns here...)",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/27639,724522974,2021-10-07T20:48:55Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -239,6 +240,43 @@ def assertSuccessfulRpcs(self,             0,             msg=f'Expected all RPCs to succeed: {failed} of {num_rpcs} failed') +    @staticmethod+    def diffAccumulatedStatsPerMethod(before, after):+        """"""Only diffs stats_per_method, as the other fields are deprecated.""""""+        diff = grpc_testing.LoadBalancerAccumulatedStatsResponse()+        for method, method_stats in after.stats_per_method.items():+            for status, count in method_stats.result.items():+                count -= before.stats_per_method[method].result[status]+                assert count >= 0, ""Diff of count shouldn't be negative""+                if count > 0:+                    diff.stats_per_method[method].result[status] = count+        return diff++    def assertRpcStatusCodes(self, test_client: XdsTestClient, *,+                             status_code: grpc.StatusCode, length: int,+                             method: str) -> None:+        # Sending with pre-set QPS for a period of time+        before_stats = test_client.get_load_balancer_accumulated_stats()+        logging.info(+            'Received LoadBalancerAccumulatedStatsResponse from test client %s: before:\n%s',+            test_client.ip, before_stats)+        time.sleep(length)+        after_stats = test_client.get_load_balancer_accumulated_stats()+        logging.info(+            'Received LoadBalancerAccumulatedStatsResponse from test client %s: after:\n%s',+            test_client.ip, after_stats)++        diff_stats = self.diffAccumulatedStatsPerMethod(before_stats,+                                                        after_stats)+        stats = diff_stats.stats_per_method[method]+        status = status_code.value[0]+        for found_status, count in stats.result.items():+            if found_status != status and count > 0:","Authz can't be probabilistic. If it is UNAVAILABLE, then that could be okay. But we really do need to know that the results are precise, in general. I'm not aware of any major source of nondeterminism, except from the Configure race we've spoken about before and ""the network may not work.""Really, the fact we do multiple RPCs is an artifact of the test framework. I'd be happy only doing a single RPC here. Right now we do 10-15 RPCs within each test, so 5% is actually less than one trial.",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26812,724527210,2021-10-07T20:55:45Z,src/core/lib/security/credentials/tls/grpc_tls_credentials_options.cc,"@@ -175,3 +175,65 @@ void grpc_tls_server_authorization_check_config_release(   grpc_core::ExecCtx exec_ctx;   if (config != nullptr) config->Unref(); }++grpc_tls_credentials_options::~grpc_tls_credentials_options() {+ if (tls_session_key_logger_) {+   reinterpret_cast<tsi::TlsSessionKeyLogger*>(+       tls_session_key_logger_)->Unref();","Also I've seen the calling of Unref() on RefCountedPtr objects multiple times...IIRC, RefCountedPtr is like shared_ptr and we shouldn't be increasing/decreasing the reference numbers too frequently...If we define a RefCountedPtr as a member of a class, the ref will be removed as the class is desotryed.Is it because we are doing this reinterpret_cast so that we have to do a Unref() here?",X
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/27639,724527360,2021-10-07T20:55:56Z,tools/run_tests/xds_k8s_test_driver/tests/authz_test.py,"@@ -0,0 +1,313 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++from typing import Optional++from absl import flags+from absl.testing import absltest+import grpc++from framework import xds_k8s_testcase++flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient+_SecurityMode = xds_k8s_testcase.SecurityXdsKubernetesTestCase.SecurityMode++_SAMPLE_DURATION = 0.5+++class AuthzTest(xds_k8s_testcase.SecurityXdsKubernetesTestCase):+    RPC_TYPE_CYCLE = {+        'UNARY_CALL': 'EMPTY_CALL',+        'EMPTY_CALL': 'UNARY_CALL',+    }++    def setUp(self):+        super().setUp()+        self.next_rpc_type: Optional[int] = None++    def authz_rules(self):+        return [+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-wildcard"",+                    },+                },+            },+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""header-regex-a+"",+                    },+                },+            },+            {+                ""destinations"": [{+                    ""hosts"": [f""{self.server_xds_host}:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-match1"",+                    },+                }, {+                    ""hosts"": [+                        f""a-not-it.com:{self.server_xds_port}"",+                        f""{self.server_xds_host}:{self.server_xds_port}"",+                        ""z-not-it.com:1"",+                    ],+                    ""ports"": [1, self.server_port, 65535],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-match2"",+                    },+                }],+            },+            {+                ""destinations"": {+                    ""hosts"": [+                        f""not-the-host:{self.server_xds_port}"",+                        ""not-the-host"",+                    ],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-host"",+                    },+                },+            },+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [1],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-port"",+                    },+                },+            },+            {+                ""sources"": {+                    ""principals"": [""*""],+                },+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""principal-present"",+                    },+                },+            },+            {+                ""sources"": [{+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                    ],+                }, {+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                        f""spiffe://{self.project}.svc.id.goog/ns/""+                        f""{self.client_namespace}/sa/{self.client_name}"",+                    ],+                }],+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""match-principal"",+                    },+                },+            },+            {+                ""sources"": {+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                    ],+                },+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-principal"",+                    },+                },+            },+        ]","Maybe. But I saw it as a feature to use the same config, as that way you are _guaranteed_ there isn't a typo specific to a test (here; the test itself could still have a typo). Consider ""principal-present"". If it was accidentally forgotten for plaintext then we'd think the test is passing (as the RPC fails) because the match is working correctly, when in reality the test testing the equivalent of the empty policy.",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/27639,724561614,2021-10-07T21:56:56Z,tools/run_tests/xds_k8s_test_driver/tests/authz_test.py,"@@ -0,0 +1,313 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++from typing import Optional++from absl import flags+from absl.testing import absltest+import grpc++from framework import xds_k8s_testcase++flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient+_SecurityMode = xds_k8s_testcase.SecurityXdsKubernetesTestCase.SecurityMode++_SAMPLE_DURATION = 0.5+++class AuthzTest(xds_k8s_testcase.SecurityXdsKubernetesTestCase):+    RPC_TYPE_CYCLE = {+        'UNARY_CALL': 'EMPTY_CALL',+        'EMPTY_CALL': 'UNARY_CALL',+    }++    def setUp(self):+        super().setUp()+        self.next_rpc_type: Optional[int] = None++    def authz_rules(self):+        return [+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-wildcard"",+                    },+                },+            },+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""header-regex-a+"",+                    },+                },+            },+            {+                ""destinations"": [{+                    ""hosts"": [f""{self.server_xds_host}:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-match1"",+                    },+                }, {+                    ""hosts"": [+                        f""a-not-it.com:{self.server_xds_port}"",+                        f""{self.server_xds_host}:{self.server_xds_port}"",+                        ""z-not-it.com:1"",+                    ],+                    ""ports"": [1, self.server_port, 65535],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-match2"",+                    },+                }],+            },+            {+                ""destinations"": {+                    ""hosts"": [+                        f""not-the-host:{self.server_xds_port}"",+                        ""not-the-host"",+                    ],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-host"",+                    },+                },+            },+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [1],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-port"",+                    },+                },+            },+            {+                ""sources"": {+                    ""principals"": [""*""],+                },+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""principal-present"",+                    },+                },+            },+            {+                ""sources"": [{+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                    ],+                }, {+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                        f""spiffe://{self.project}.svc.id.goog/ns/""+                        f""{self.client_namespace}/sa/{self.client_name}"",+                    ],+                }],+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""match-principal"",+                    },+                },+            },+            {+                ""sources"": {+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                    ],+                },+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-principal"",+                    },+                },+            },+        ]++    def configure_and_assert(self, test_client: _XdsTestClient,+                             test_metadata_val: Optional[str],+                             status_code: grpc.StatusCode) -> None:+        # Swap method type every sub-test to avoid mixing results","> If we want absolute isolation, we should promote subtests to testsHow do subtests provide absolute isolation? I didn't realize there was _any_ isolation between subtests, except for exception-throwing.Normal flow:1. Test client is doing Unary RPCs with some arbitrary configuration2. Reconfigure client, and swap to Empty RPCs. Since there have not been any recent empty RPCs, all empty RPCs from this point forward are using our new configuration3. Get baseline stats. Some Unary RPCs may still be happening with the old config, but we don't care4. Get more stats, Do diff. Only look at _recent_ empty RPCs5. Restart at 1 for next test, but swapping the role of Empty and Unary RPCs> Will stats.stats_per_method[t].rpcs_started become zero if we change configuration?No. We get a zero-like baseline by subtracting the stats at the beginning of the test.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27639,724568396,2021-10-07T22:10:54Z,tools/run_tests/xds_k8s_test_driver/tests/authz_test.py,"@@ -0,0 +1,313 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++from typing import Optional++from absl import flags+from absl.testing import absltest+import grpc++from framework import xds_k8s_testcase++flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient+_SecurityMode = xds_k8s_testcase.SecurityXdsKubernetesTestCase.SecurityMode++_SAMPLE_DURATION = 0.5+++class AuthzTest(xds_k8s_testcase.SecurityXdsKubernetesTestCase):+    RPC_TYPE_CYCLE = {+        'UNARY_CALL': 'EMPTY_CALL',+        'EMPTY_CALL': 'UNARY_CALL',+    }++    def setUp(self):+        super().setUp()+        self.next_rpc_type: Optional[int] = None++    def authz_rules(self):+        return [+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-wildcard"",+                    },+                },+            },+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""header-regex-a+"",+                    },+                },+            },+            {+                ""destinations"": [{+                    ""hosts"": [f""{self.server_xds_host}:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-match1"",+                    },+                }, {+                    ""hosts"": [+                        f""a-not-it.com:{self.server_xds_port}"",+                        f""{self.server_xds_host}:{self.server_xds_port}"",+                        ""z-not-it.com:1"",+                    ],+                    ""ports"": [1, self.server_port, 65535],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-match2"",+                    },+                }],+            },+            {+                ""destinations"": {+                    ""hosts"": [+                        f""not-the-host:{self.server_xds_port}"",+                        ""not-the-host"",+                    ],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-host"",+                    },+                },+            },+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [1],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-port"",+                    },+                },+            },+            {+                ""sources"": {+                    ""principals"": [""*""],+                },+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""principal-present"",+                    },+                },+            },+            {+                ""sources"": [{+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                    ],+                }, {+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                        f""spiffe://{self.project}.svc.id.goog/ns/""+                        f""{self.client_namespace}/sa/{self.client_name}"",+                    ],+                }],+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""match-principal"",+                    },+                },+            },+            {+                ""sources"": {+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                    ],+                },+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-principal"",+                    },+                },+            },+        ]++    def configure_and_assert(self, test_client: _XdsTestClient,+                             test_metadata_val: Optional[str],+                             status_code: grpc.StatusCode) -> None:+        # Swap method type every sub-test to avoid mixing results","I think the flow works. You are right, the subtests don't provide isolation of client states, test cases may.---Running the test locally, I'm getting an error:```google.protobuf.json_format.ParseError: Failed to parse metadata field: Can not find message descriptor by type_url: type.googleapis.com/google.cloud.networkservices.v1beta1.OperationMetadata..```Did you use a custom discovery file or changed the API version?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27639,724572269,2021-10-07T22:18:55Z,tools/run_tests/xds_k8s_test_driver/tests/authz_test.py,"@@ -0,0 +1,313 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++from typing import Optional++from absl import flags+from absl.testing import absltest+import grpc++from framework import xds_k8s_testcase++flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient+_SecurityMode = xds_k8s_testcase.SecurityXdsKubernetesTestCase.SecurityMode++_SAMPLE_DURATION = 0.5+++class AuthzTest(xds_k8s_testcase.SecurityXdsKubernetesTestCase):+    RPC_TYPE_CYCLE = {+        'UNARY_CALL': 'EMPTY_CALL',+        'EMPTY_CALL': 'UNARY_CALL',+    }++    def setUp(self):+        super().setUp()+        self.next_rpc_type: Optional[int] = None++    def authz_rules(self):+        return [+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-wildcard"",+                    },+                },+            },+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""header-regex-a+"",+                    },+                },+            },+            {+                ""destinations"": [{+                    ""hosts"": [f""{self.server_xds_host}:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-match1"",+                    },+                }, {+                    ""hosts"": [+                        f""a-not-it.com:{self.server_xds_port}"",+                        f""{self.server_xds_host}:{self.server_xds_port}"",+                        ""z-not-it.com:1"",+                    ],+                    ""ports"": [1, self.server_port, 65535],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-match2"",+                    },+                }],+            },+            {+                ""destinations"": {+                    ""hosts"": [+                        f""not-the-host:{self.server_xds_port}"",+                        ""not-the-host"",+                    ],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-host"",+                    },+                },+            },+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [1],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-port"",+                    },+                },+            },+            {+                ""sources"": {+                    ""principals"": [""*""],+                },+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""principal-present"",+                    },+                },+            },+            {+                ""sources"": [{+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                    ],+                }, {+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                        f""spiffe://{self.project}.svc.id.goog/ns/""+                        f""{self.client_namespace}/sa/{self.client_name}"",+                    ],+                }],+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""match-principal"",+                    },+                },+            },+            {+                ""sources"": {+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                    ],+                },+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-principal"",+                    },+                },+            },+        ]++    def configure_and_assert(self, test_client: _XdsTestClient,+                             test_metadata_val: Optional[str],+                             status_code: grpc.StatusCode) -> None:+        # Swap method type every sub-test to avoid mixing results+        rpc_type = self.next_rpc_type+        if rpc_type is None:+            stats = test_client.get_load_balancer_accumulated_stats()+            for t in self.RPC_TYPE_CYCLE:+                if not stats.stats_per_method[t].rpcs_started:+                    rpc_type = t+            assert rpc_type is not None, ""All RPC types already used""","If we specify a start `self.next_rpc_type` value, maybe we can remove line 165-170? We know what the test_client config is, and we can pin `self.next_rpc_type` to `EMPTY_CALL`, for example.Ah, this if is only true for the first ever invocation. I thought that `stats.stats_per_method[t].rpcs_started` will be checked for every call to `configure_and_assert`, my bad.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27639,724617822,2021-10-08T00:21:05Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -239,6 +240,45 @@ def assertSuccessfulRpcs(self,             0,             msg=f'Expected all RPCs to succeed: {failed} of {num_rpcs} failed') +    @staticmethod+    def diffAccumulatedStatsPerMethod(+            before: grpc_testing.LoadBalancerAccumulatedStatsResponse,+            after: grpc_testing.LoadBalancerAccumulatedStatsResponse):+        """"""Only diffs stats_per_method, as the other fields are deprecated.""""""+        diff = grpc_testing.LoadBalancerAccumulatedStatsResponse()+        for method, method_stats in after.stats_per_method.items():+            for status, count in method_stats.result.items():+                count -= before.stats_per_method[method].result[status]+                assert count >= 0, ""Diff of count shouldn't be negative""",Here and elsewhere: use [unittest](https://docs.python.org/3.6/library/unittest.html#assert-methods) or [absltest](https://abseil.io/docs/python/guides/testing#using-validation-methods) assert methods> [self.assert*] methods are used instead of the assert statement so the test runner can accumulate all test results and produce a report.— https://docs.python.org/3.6/library/unittest.html#basic-example,
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27639,724621513,2021-10-08T00:33:10Z,tools/run_tests/xds_k8s_test_driver/tests/authz_test.py,"@@ -0,0 +1,313 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++from typing import Optional++from absl import flags+from absl.testing import absltest+import grpc++from framework import xds_k8s_testcase++flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient+_SecurityMode = xds_k8s_testcase.SecurityXdsKubernetesTestCase.SecurityMode++_SAMPLE_DURATION = 0.5+++class AuthzTest(xds_k8s_testcase.SecurityXdsKubernetesTestCase):+    RPC_TYPE_CYCLE = {+        'UNARY_CALL': 'EMPTY_CALL',+        'EMPTY_CALL': 'UNARY_CALL',+    }++    def setUp(self):+        super().setUp()+        self.next_rpc_type: Optional[int] = None++    def authz_rules(self):+        return [+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-wildcard"",+                    },+                },+            },+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""header-regex-a+"",+                    },+                },+            },+            {+                ""destinations"": [{+                    ""hosts"": [f""{self.server_xds_host}:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-match1"",+                    },+                }, {+                    ""hosts"": [+                        f""a-not-it.com:{self.server_xds_port}"",+                        f""{self.server_xds_host}:{self.server_xds_port}"",+                        ""z-not-it.com:1"",+                    ],+                    ""ports"": [1, self.server_port, 65535],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""host-match2"",+                    },+                }],+            },+            {+                ""destinations"": {+                    ""hosts"": [+                        f""not-the-host:{self.server_xds_port}"",+                        ""not-the-host"",+                    ],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-host"",+                    },+                },+            },+            {+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [1],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-port"",+                    },+                },+            },+            {+                ""sources"": {+                    ""principals"": [""*""],+                },+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""principal-present"",+                    },+                },+            },+            {+                ""sources"": [{+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                    ],+                }, {+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                        f""spiffe://{self.project}.svc.id.goog/ns/""+                        f""{self.client_namespace}/sa/{self.client_name}"",+                    ],+                }],+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""match-principal"",+                    },+                },+            },+            {+                ""sources"": {+                    ""principals"": [+                        f""spiffe://{self.project}.svc.id.goog/not/the/client"",+                    ],+                },+                ""destinations"": {+                    ""hosts"": [f""*:{self.server_xds_port}""],+                    ""ports"": [self.server_port],+                    ""httpHeaderMatch"": {+                        ""headerName"": ""test"",+                        ""regexMatch"": ""never-match-principal"",+                    },+                },+            },+        ]++    def configure_and_assert(self, test_client: _XdsTestClient,+                             test_metadata_val: Optional[str],+                             status_code: grpc.StatusCode) -> None:+        # Swap method type every sub-test to avoid mixing results+        rpc_type = self.next_rpc_type+        if rpc_type is None:+            stats = test_client.get_load_balancer_accumulated_stats()+            for t in self.RPC_TYPE_CYCLE:+                if not stats.stats_per_method[t].rpcs_started:+                    rpc_type = t+            assert rpc_type is not None, ""All RPC types already used""+        self.next_rpc_type = self.RPC_TYPE_CYCLE[rpc_type]++        metadata = None+        if not test_metadata_val is None:",```suggestion        if test_metadata_val is not None:````not test_metadata_val` would inverse `test_metadata_val` as a boolean.,X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27639,724626520,2021-10-08T00:50:06Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -239,6 +240,45 @@ def assertSuccessfulRpcs(self,             0,             msg=f'Expected all RPCs to succeed: {failed} of {num_rpcs} failed') +    @staticmethod+    def diffAccumulatedStatsPerMethod(+            before: grpc_testing.LoadBalancerAccumulatedStatsResponse,+            after: grpc_testing.LoadBalancerAccumulatedStatsResponse):+        """"""Only diffs stats_per_method, as the other fields are deprecated.""""""+        diff = grpc_testing.LoadBalancerAccumulatedStatsResponse()+        for method, method_stats in after.stats_per_method.items():+            for status, count in method_stats.result.items():+                count -= before.stats_per_method[method].result[status]+                assert count >= 0, ""Diff of count shouldn't be negative""+                if count > 0:+                    diff.stats_per_method[method].result[status] = count+        return diff++    def assertRpcStatusCodes(self, test_client: XdsTestClient, *,+                             status_code: grpc.StatusCode, length: int,+                             method: str) -> None:+        # Sending with pre-set QPS for a period of time+        before_stats = test_client.get_load_balancer_accumulated_stats()+        logging.info(+            'Received LoadBalancerAccumulatedStatsResponse from test client %s: before:\n%s',+            test_client.ip, before_stats)+        time.sleep(length)","nit: instead of `length: int`, pass `delay: _timedelta` (`_timedelta` is an alias for [datetime.timedelta](https://docs.python.org/3.6/library/datetime.html#timedelta-objects)), then do```suggestion        time.sleep(delay.total_seconds())```(search for `_timedelta` usages in this file)datetime.timedelta is much more expressive, and prevents questions ""what's the unit of this time parameter?""",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27388,724627044,2021-10-08T00:51:55Z,src/core/ext/transport/chttp2/server/chttp2_server.cc,"@@ -692,15 +692,15 @@ Chttp2ServerListener::~Chttp2ServerListener() { void Chttp2ServerListener::Start(     Server* /*server*/, const std::vector<grpc_pollset*>* /* pollsets */) {   if (server_->config_fetcher() != nullptr) {","@markdroth at some point server became service, probably my typo, but here.I think this configuration monitoring logic that we're modifying here is too complex to expect every transport to implement separately, and is likely a layering violation. I'd prefer an interface where such configuration was pushed from above and the watching logic isolated in one place rather than needing to be repeated.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27639,724628521,2021-10-08T00:56:44Z,tools/run_tests/xds_k8s_test_driver/tests/authz_test.py,"@@ -0,0 +1,313 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++from typing import Optional++from absl import flags+from absl.testing import absltest+import grpc++from framework import xds_k8s_testcase++flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient+_SecurityMode = xds_k8s_testcase.SecurityXdsKubernetesTestCase.SecurityMode++_SAMPLE_DURATION = 0.5+++class AuthzTest(xds_k8s_testcase.SecurityXdsKubernetesTestCase):+    RPC_TYPE_CYCLE = {+        'UNARY_CALL': 'EMPTY_CALL',+        'EMPTY_CALL': 'UNARY_CALL',+    }","You might enjoy [`itertools.cycle()`](https://docs.python.org/3/library/itertools.html#itertools.cycle) 😉 Though I don't really get lines 165-170, this may not work.",X
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27529,724703495,2021-10-08T05:06:41Z,src/core/ext/filters/client_channel/resolver/binder/README.md,"@@ -0,0 +1,6 @@+Support for resolving the scheme used by binder transport implementation.++The URI's authority is required to be empty.++The path is used as the identifiers of endpoint binder objects. It should not be+empty and should only contain alphabets and numbers.",Decided to allow all unreserved characters https://datatracker.ietf.org/doc/html/rfc3986#section-2.3README and tests updated accordingly,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27611,725077642,2021-10-08T14:47:41Z,test/cpp/end2end/client_lb_end2end_test.cc,"@@ -1660,7 +1660,7 @@ TEST_F(ClientLbEnd2endTest, ChannelIdleness) {   // After a period time not using the channel, the channel state should switch   // to IDLE.   gpr_log(GPR_INFO, ""*** WAITING FOR CHANNEL TO GO IDLE ***"");-  gpr_sleep_until(grpc_timeout_milliseconds_to_deadline(1200));+  gpr_sleep_until(grpc_timeout_milliseconds_to_deadline(2200));","Glad you asked - it's not, but subtly so.We can, under some conditions with this new code, take two idle periods to detect no activity - since we don't cancel the timer once it's started. In this specific case, since we drop to zero and start a new timer we can - we just needed to be clearing out the activity since last timer bit, which we should have been.Fixed.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27657,725149116,2021-10-08T16:25:04Z,src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc,"@@ -475,6 +485,15 @@ class GrpcLb : public LoadBalancingPolicy {   OrphanablePtr<LoadBalancingPolicy> child_policy_;   // Child policy in state READY.   bool child_policy_ready_ = false;++  // Deleted subchannel caching.+  const grpc_millis subchannel_cache_interval_ms_;+  std::map<grpc_millis /*deletion time*/,","What if two subchannels are deleted at the same millisecond?And why are we reimplementing a timer queue here?I'm assuming what we ultimately want to do is just keep the subchannel reference around for a period of time, so ultimately we could write:```void Cache(RefCountedPtr<SubchannelInterface> p) {  event_engine->RunAt(now() + 10s, [p](){});}```Maybe we could find a way to write it similar to that with the current API?",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/27622,725177381,2021-10-08T17:11:32Z,test/cpp/qps/scenario_generator_helper.py,"@@ -0,0 +1,99 @@+#!/usr/bin/env python3++# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++from __future__ import print_function++import json+import os+import sys++import yaml++run_tests_root = os.path.abspath(+    os.path.join(os.path.dirname(sys.argv[0]), '../../../tools/run_tests'))+sys.path.append(run_tests_root)++import performance.scenario_config as scenario_config++_COPYRIGHT = """"""# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""+++def _mutate_scenario(scenario_json):","Optional: (and I _do_ mean optional) Have you considered adding types? We've been trying to add types to all new Python we add to the repo, including infrastructural things like the xDS interop test framework. It goes a long way toward aiding comprehension for people reading a script cold.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/27622,725178976,2021-10-08T17:14:09Z,test/cpp/qps/scenario_generator_helper.py,"@@ -0,0 +1,99 @@+#!/usr/bin/env python3++# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++from __future__ import print_function++import json+import os+import sys++import yaml++run_tests_root = os.path.abspath(+    os.path.join(os.path.dirname(sys.argv[0]), '../../../tools/run_tests'))+sys.path.append(run_tests_root)++import performance.scenario_config as scenario_config++_COPYRIGHT = """"""# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""+++def _mutate_scenario(scenario_json):+    # tweak parameters to get fast test times+    scenario_json = dict(scenario_json)+    scenario_json['warmup_seconds'] = 0+    scenario_json['benchmark_seconds'] = 1+    outstanding_rpcs_divisor = 1+    if scenario_json['client_config'][+            'client_type'] == 'SYNC_CLIENT' or scenario_json['server_config'][+                'server_type'] == 'SYNC_SERVER':+        outstanding_rpcs_divisor = 10+    scenario_json['client_config']['outstanding_rpcs_per_channel'] = max(+        1,+        int(scenario_json['client_config']['outstanding_rpcs_per_channel'] /","I think this field requires an int? If so, you'll want the integer division operator: `a // b`. This is one of the big differences in behavior between Python 2 and Python 3.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27657,725195114,2021-10-08T17:41:43Z,src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc,"@@ -475,6 +485,15 @@ class GrpcLb : public LoadBalancingPolicy {   OrphanablePtr<LoadBalancingPolicy> child_policy_;   // Child policy in state READY.   bool child_policy_ready_ = false;++  // Deleted subchannel caching.+  const grpc_millis subchannel_cache_interval_ms_;+  std::map<grpc_millis /*deletion time*/,","The value of the map is a vector, so if there are multiple subchannels deleted in the same millisecond, they'll go in the same map entry.  That's intentional, and in fact I expect it to happen very frequently due to the cached value of ""now"" in the `ExecCtx`.The workflow is basically this:1. The `grpclb` policy gets an update from the balancer that does not include one or more addresses that were in the previous update.2. The `grpclb` policy sends the updated address list to the `round_robin` child policy.3. The `round_robin` policy calls the helper's `CreateSubchannel()` for every address in the new list, and then as soon as it unrefs the subchannels from the previous list.  (Note that the same cached value of ""now"" in `ExecCtx` is used for all of these unrefs.)4. As each subchannel is unreffed, it gets added to `cached_subchannels_` (in the same bucket, because the same value of ""now""), and a timer is started when the first one is added.The idea here is to minimize the number of timers and therefore the amount of memory used for the cache, since we know that there can be multiple subchannels removed in the same update from the balancer, and we know that another update is likely to come in (which may remove another set of subchannels) before the timer fires for the subchannels removed in the previous update (the balancer may send updates as often as every 1s, but we cache subchannels for 10s).  This way, we basically have just one timer pending at any given time, no matter how many subchannels are cached.I could instead structure this using a separate timer for each subchannel, but that would increase the amount of memory I'd have to store for each cached subchannel: instead of just the ref to the subchannel, I'd also need to store a timer and a closure.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27657,725218067,2021-10-08T18:22:26Z,src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc,"@@ -475,6 +485,15 @@ class GrpcLb : public LoadBalancingPolicy {   OrphanablePtr<LoadBalancingPolicy> child_policy_;   // Child policy in state READY.   bool child_policy_ready_ = false;++  // Deleted subchannel caching.+  const grpc_millis subchannel_cache_interval_ms_;+  std::map<grpc_millis /*deletion time*/,","[none of this should be blocking]Yup ok... mostly trying to make the event engine conversion easier, since I expect the code I wrote above would be preferred there, but as expressed we'll probably end up keeping the infrastructure here and making more complicated code.I'm not sure that conservation of timers or memory warrants the additional long term complexity.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27657,725219942,2021-10-08T18:26:11Z,src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc,"@@ -475,6 +485,15 @@ class GrpcLb : public LoadBalancingPolicy {   OrphanablePtr<LoadBalancingPolicy> child_policy_;   // Child policy in state READY.   bool child_policy_ready_ = false;++  // Deleted subchannel caching.+  const grpc_millis subchannel_cache_interval_ms_;+  std::map<grpc_millis /*deletion time*/,","Given how expensive memory is right now, it seemed worth the optimization.  But I acknowledge that I have absolutely no data to justify it; it's just sort of a hunch.  It didn't seem that hard to do it this way, so I figured I might as well do it.  But if at some point it is causing problems, it also isn't hard to change it to work the other way.I don't think this will affect the EventEngine conversion either way.",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/27639,725240075,2021-10-08T19:02:20Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -239,6 +240,45 @@ def assertSuccessfulRpcs(self,             0,             msg=f'Expected all RPCs to succeed: {failed} of {num_rpcs} failed') +    @staticmethod+    def diffAccumulatedStatsPerMethod(+            before: grpc_testing.LoadBalancerAccumulatedStatsResponse,+            after: grpc_testing.LoadBalancerAccumulatedStatsResponse):+        """"""Only diffs stats_per_method, as the other fields are deprecated.""""""+        diff = grpc_testing.LoadBalancerAccumulatedStatsResponse()+        for method, method_stats in after.stats_per_method.items():+            for status, count in method_stats.result.items():+                count -= before.stats_per_method[method].result[status]+                assert count >= 0, ""Diff of count shouldn't be negative""","I should be preferring unittest for idiomatic reasons, but that text is strange as doesn't it use an exception itself? It seems it even uses AssertionError by default, but there is a way to change it. Strange. It is sorta neat to differentiate test failures from test errors (or breakages), and I guess they don't change the default for compatibility reasons. The reasoning provided by the documentation seems suspect though.This specific assertion didn't seem like a test failure, but more a foundational problem. The other one was more accidental as I had intended to look into what was available in unittest.",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/27639,725251396,2021-10-08T19:24:26Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -239,6 +240,45 @@ def assertSuccessfulRpcs(self,             0,             msg=f'Expected all RPCs to succeed: {failed} of {num_rpcs} failed') +    @staticmethod+    def diffAccumulatedStatsPerMethod(+            before: grpc_testing.LoadBalancerAccumulatedStatsResponse,+            after: grpc_testing.LoadBalancerAccumulatedStatsResponse):+        """"""Only diffs stats_per_method, as the other fields are deprecated.""""""+        diff = grpc_testing.LoadBalancerAccumulatedStatsResponse()+        for method, method_stats in after.stats_per_method.items():+            for status, count in method_stats.result.items():+                count -= before.stats_per_method[method].result[status]+                assert count >= 0, ""Diff of count shouldn't be negative""+                if count > 0:+                    diff.stats_per_method[method].result[status] = count+        return diff++    def assertRpcStatusCodes(self, test_client: XdsTestClient, *,+                             status_code: grpc.StatusCode, length: int,+                             method: str) -> None:+        # Sending with pre-set QPS for a period of time+        before_stats = test_client.get_load_balancer_accumulated_stats()+        logging.info(+            'Received LoadBalancerAccumulatedStatsResponse from test client %s: before:\n%s',+            test_client.ip, before_stats)+        time.sleep(length)","Ah, okay. I'm out-of-date; forever ago everyone just assumed second unit so it didn't seem out-of-place. Using ""duration"" instead of delay. It is the length of the monitoring time.FWIW, this was based on similar code, so we may want to clean that up as well later. https://github.com/grpc/grpc/blob/da2e8ddc4e6932ec1c92e36abd4f5d9e1447e8f5/tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_testcase.py#L455-L457",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/27657,725257316,2021-10-08T19:36:22Z,src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc,"@@ -234,16 +230,24 @@ class GrpcLb : public LoadBalancingPolicy {   class SubchannelWrapper : public DelegatingSubchannel {    public:     SubchannelWrapper(RefCountedPtr<SubchannelInterface> subchannel,-                      std::string lb_token,+                      RefCountedPtr<GrpcLb> lb_policy, std::string lb_token,                       RefCountedPtr<GrpcLbClientStats> client_stats)         : DelegatingSubchannel(std::move(subchannel)),+          lb_policy_(std::move(lb_policy)),           lb_token_(std::move(lb_token)),           client_stats_(std::move(client_stats)) {} +    ~SubchannelWrapper() override {+      if (!lb_policy_->shutting_down_) {+        lb_policy_->CacheDeletedSubchannel(wrapped_subchannel());","grpc shutdown should cancel all pending timers globally, though, right? Is that not sufficient to prevent this?The other thing I'm thinking about here is that one of more of these subchannels may be in the process of setting up a TCP connection, and [TCP connection setup](https://github.com/grpc/grpc/blob/8d8d07139cde6cddb20e97bf8bf25b7432825f48/src/core/lib/iomgr/tcp_client.h#L45) can't be cancelled anyways AFAIK.Is this potential leak with the cached subchannel timers different from the case where TCP connections are still in the process of establishing and grpc shutdown is called?",X
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/27639,725294828,2021-10-08T20:52:13Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -239,6 +240,45 @@ def assertSuccessfulRpcs(self,             0,             msg=f'Expected all RPCs to succeed: {failed} of {num_rpcs} failed') +    @staticmethod+    def diffAccumulatedStatsPerMethod(+            before: grpc_testing.LoadBalancerAccumulatedStatsResponse,+            after: grpc_testing.LoadBalancerAccumulatedStatsResponse):+        """"""Only diffs stats_per_method, as the other fields are deprecated.""""""+        diff = grpc_testing.LoadBalancerAccumulatedStatsResponse()+        for method, method_stats in after.stats_per_method.items():+            for status, count in method_stats.result.items():+                count -= before.stats_per_method[method].result[status]+                assert count >= 0, ""Diff of count shouldn't be negative""","For reference, this is what I had found (see also the assignment below the docstring):https://github.com/python/cpython/blob/b108db63e02797a795840152b82cab9792fd3ed2/Lib/unittest/case.py#L347So it is useful, but not for the reason given. Dunno how many people would actually notice a difference.",
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27529,725998233,2021-10-11T10:37:39Z,BUILD,"@@ -1858,6 +1858,7 @@ grpc_cc_library(         ""grpc_resolver_fake"",         ""grpc_resolver_dns_native"",         ""grpc_resolver_sockaddr"",+        ""grpc_resolver_binder"",",No it will not link any Java code into gRPC. Our Java code are runtime dependency and we have separate build targets that user need to manually depends on,
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/27529,726007698,2021-10-11T10:52:28Z,src/core/ext/filters/client_channel/resolver/binder/binder_resolver.cc,"@@ -0,0 +1,127 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include <grpc/support/alloc.h>+#include <grpc/support/string_util.h>++#include ""src/core/ext/filters/client_channel/resolver_registry.h""+#include ""src/core/ext/filters/client_channel/server_address.h""+#include ""src/core/lib/address_utils/parse_address.h""+#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/gpr/string.h""+#include ""src/core/lib/iomgr/resolve_address.h""+#include ""src/core/lib/iomgr/unix_sockets_posix.h""++namespace grpc_core {+namespace {++class BinderResolver : public Resolver {+ public:+  BinderResolver(ServerAddressList addresses, ResolverArgs args)+      : result_handler_(std::move(args.result_handler)),+        addresses_(std::move(addresses)),+        channel_args_(grpc_channel_args_copy(args.args)) {}++  ~BinderResolver() override { grpc_channel_args_destroy(channel_args_); };++  void StartLocked() override {+    Result result;+    result.addresses = std::move(addresses_);+    result.args = channel_args_;+    channel_args_ = nullptr;+    result_handler_->ReturnResult(std::move(result));+  }++  void ShutdownLocked() override {}++ private:+  std::unique_ptr<ResultHandler> result_handler_;+  ServerAddressList addresses_;+  const grpc_channel_args* channel_args_ = nullptr;+};++grpc_error_handle BinderAddrPopulate(const std::string path,+                                     grpc_resolved_address* resolved_addr) {+  if (path.empty()) {+    return GRPC_ERROR_CREATE_FROM_CPP_STRING(""path is empty"");+  }+  // See README for valid characters+  if (std::find_if(path.begin(), path.end(), [](char c) {+        return !(isalnum(c) || absl::StrContains(""-._~"", c));+      }) != path.end()) {+    return GRPC_ERROR_CREATE_FROM_CPP_STRING(path ++                                             "" contains invalid character"");+  }+  if (path.size() + 1 > sizeof(resolved_addr->addr)) {+    return GRPC_ERROR_CREATE_FROM_CPP_STRING(path ++                                             "" is too long to be handled"");+  }+  strcpy(resolved_addr->addr, path.c_str());","Yeah I saw there are some code that attempts to parse `grpc_resolved_address` to string for logging purposes (which is probably acceptable for us). However if there are code that tries to reinterpret `grpc_resolved_address` as `sockaddr` and tries to use that address to actually do something it would be pretty bad.This does not sound like an issue that can be resolved quickly. How could we proceed here? How about saving our ""binder identifier string"" in a `sockaddr` struct and make sure it have a invalid address family (e.g `AF_MAX`) so there is no security concern about accidentally connecting to some random socket?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27666,726193991,2021-10-11T14:56:37Z,src/core/ext/filters/client_channel/retry_throttle.cc,"@@ -114,79 +115,47 @@ void ServerRetryThrottleData::RecordSuccess() {       static_cast<gpr_atm>(throttle_data->max_milli_tokens_)); } -//-// avl vtable for string -> server_retry_throttle_data map-//--namespace {--void* copy_server_name(void* key, void* /*unused*/) {-  return gpr_strdup(static_cast<const char*>(key));-}--long compare_server_name(void* key1, void* key2, void* /*unused*/) {-  return strcmp(static_cast<const char*>(key1), static_cast<const char*>(key2));-}--void destroy_server_retry_throttle_data(void* value, void* /*unused*/) {-  ServerRetryThrottleData* throttle_data =-      static_cast<ServerRetryThrottleData*>(value);-  throttle_data->Unref();-}--void* copy_server_retry_throttle_data(void* value, void* /*unused*/) {-  ServerRetryThrottleData* throttle_data =-      static_cast<ServerRetryThrottleData*>(value);-  return throttle_data->Ref().release();-}--void destroy_server_name(void* key, void* /*unused*/) { gpr_free(key); }--const grpc_avl_vtable avl_vtable = {-    destroy_server_name, copy_server_name, compare_server_name,-    destroy_server_retry_throttle_data, copy_server_retry_throttle_data};--}  // namespace- // // ServerRetryThrottleMap //  static gpr_mu g_mu;-static grpc_avl g_avl;+static ManualConstructor<+    std::map<std::string, RefCountedPtr<ServerRetryThrottleData>>>",Would it make sense to use a hash map here?  I'm wondering if that would improve performance in the case where the client is talking to a large number of servers.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27666,726194959,2021-10-11T14:57:40Z,src/core/ext/filters/client_channel/retry_throttle.cc,"@@ -114,79 +115,47 @@ void ServerRetryThrottleData::RecordSuccess() {       static_cast<gpr_atm>(throttle_data->max_milli_tokens_)); } -//-// avl vtable for string -> server_retry_throttle_data map-//--namespace {--void* copy_server_name(void* key, void* /*unused*/) {-  return gpr_strdup(static_cast<const char*>(key));-}--long compare_server_name(void* key1, void* key2, void* /*unused*/) {-  return strcmp(static_cast<const char*>(key1), static_cast<const char*>(key2));-}--void destroy_server_retry_throttle_data(void* value, void* /*unused*/) {-  ServerRetryThrottleData* throttle_data =-      static_cast<ServerRetryThrottleData*>(value);-  throttle_data->Unref();-}--void* copy_server_retry_throttle_data(void* value, void* /*unused*/) {-  ServerRetryThrottleData* throttle_data =-      static_cast<ServerRetryThrottleData*>(value);-  return throttle_data->Ref().release();-}--void destroy_server_name(void* key, void* /*unused*/) { gpr_free(key); }--const grpc_avl_vtable avl_vtable = {-    destroy_server_name, copy_server_name, compare_server_name,-    destroy_server_retry_throttle_data, copy_server_retry_throttle_data};--}  // namespace- // // ServerRetryThrottleMap //  static gpr_mu g_mu;-static grpc_avl g_avl;+static ManualConstructor<",Is `ManualConstructor` better than just making this a pointer and dynamically allocating the map?  The latter seems a bit less complex.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/26287,726496722,2021-10-11T18:41:09Z,test/core/tsi/transport_security_test_lib.cc,"@@ -617,7 +617,7 @@ void tsi_test_fixture_init(tsi_test_fixture* fixture) {   fixture->notified = false; } -void tsi_test_fixture_destroy(tsi_test_fixture* fixture) {+void tsi_test_fixture_destroy(tsi_test_fixture* fixture, bool free_fixture) {",I'm not sure how this change got back in here.  I don't think the `free_fixture` parameter is needed.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27672,726885287,2021-10-12T08:30:42Z,src/core/lib/security/credentials/tls/grpc_tls_credentials_options.cc,"@@ -29,6 +29,44 @@  #include ""src/core/lib/surface/api_trace.h"" +int grpc_tls_credentials_options::cmp(+    const grpc_tls_credentials_options* other_options) const {",This parameter can be a reference instead of a pointer.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27683,726902022,2021-10-12T08:48:47Z,BUILD,"@@ -2312,6 +2332,7 @@ grpc_cc_library(         ""grpc_lb_xds_channel_args"",         ""grpc_matchers"",         ""grpc_secure"",+        ""grpc_service_config"",",I don't think this dependency is actually needed.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27529,727285448,2021-10-12T16:02:05Z,src/core/ext/filters/client_channel/resolver/binder/binder_resolver.cc,"@@ -0,0 +1,127 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include <grpc/support/alloc.h>+#include <grpc/support/string_util.h>++#include ""src/core/ext/filters/client_channel/resolver_registry.h""+#include ""src/core/ext/filters/client_channel/server_address.h""+#include ""src/core/lib/address_utils/parse_address.h""+#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/gpr/string.h""+#include ""src/core/lib/iomgr/resolve_address.h""+#include ""src/core/lib/iomgr/unix_sockets_posix.h""++namespace grpc_core {+namespace {++class BinderResolver : public Resolver {+ public:+  BinderResolver(ServerAddressList addresses, ResolverArgs args)+      : result_handler_(std::move(args.result_handler)),+        addresses_(std::move(addresses)),+        channel_args_(grpc_channel_args_copy(args.args)) {}++  ~BinderResolver() override { grpc_channel_args_destroy(channel_args_); };++  void StartLocked() override {+    Result result;+    result.addresses = std::move(addresses_);+    result.args = channel_args_;+    channel_args_ = nullptr;+    result_handler_->ReturnResult(std::move(result));+  }++  void ShutdownLocked() override {}++ private:+  std::unique_ptr<ResultHandler> result_handler_;+  ServerAddressList addresses_;+  const grpc_channel_args* channel_args_ = nullptr;+};++grpc_error_handle BinderAddrPopulate(const std::string path,+                                     grpc_resolved_address* resolved_addr) {+  if (path.empty()) {+    return GRPC_ERROR_CREATE_FROM_CPP_STRING(""path is empty"");+  }+  // See README for valid characters+  if (std::find_if(path.begin(), path.end(), [](char c) {+        return !(isalnum(c) || absl::StrContains(""-._~"", c));+      }) != path.end()) {+    return GRPC_ERROR_CREATE_FROM_CPP_STRING(path ++                                             "" contains invalid character"");+  }+  if (path.size() + 1 > sizeof(resolved_addr->addr)) {+    return GRPC_ERROR_CREATE_FROM_CPP_STRING(path ++                                             "" is too long to be handled"");+  }+  strcpy(resolved_addr->addr, path.c_str());","We don't log the address by default, but there are tracers that will log it when enabled.  For example, what happens if you merge master to pick up #27661 and then run with ""GRPC_VERBOSITY=DEBUG GRPC_TRACE=subchannel""?  That will attempt to log the subchannel address.  Does anything break?  What does the log look like?Setting the address family to an invalid value would be a good step, so at least we don't accidentally try to interpret it as some other address type.  But even with that change, I suspect we'll need to change some of the code in src/core/lib/address_utils/sockaddr_utils.cc to handle unknown address families.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27583,727301893,2021-10-12T16:21:49Z,src/core/lib/security/credentials/external/external_account_credentials.cc,"@@ -59,6 +61,26 @@ std::string UrlEncode(const absl::string_view& s) {   return result; } +// Expression to match:+// //iam.googleapis.com/locations/[^/]+/workforcePools/[^/]+/providers/.++bool MatchWorkforcePoolAudience(const absl::string_view& audience) {","The parameter can be passed in by value (i.e., not `const` and not a reference).  Then there's no need for `temp`; instead, you can just modify `audience` as you go.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27618,727541638,2021-10-12T22:13:34Z,test/core/security/credentials_test.cc,"@@ -449,101 +449,117 @@ static void test_oauth2_token_fetcher_creds_parsing_missing_token_lifetime(   grpc_http_response_destroy(&response); } -typedef struct {-  const char* key;-  const char* value;-} expected_md;+namespace { -typedef struct {-  grpc_error_handle expected_error;-  const expected_md* expected;-  size_t expected_size;-  grpc_credentials_mdelem_array md_array;-  grpc_closure on_request_metadata;-  grpc_call_credentials* creds;-  grpc_polling_entity pollent;-} request_metadata_state;--static void check_metadata(const expected_md* expected,-                           grpc_credentials_mdelem_array* md_array) {-  for (size_t i = 0; i < md_array->size; ++i) {-    size_t j;-    for (j = 0; j < md_array->size; ++j) {-      if (0 ==-          grpc_slice_str_cmp(GRPC_MDKEY(md_array->md[j]), expected[i].key)) {-        GPR_ASSERT(grpc_slice_str_cmp(GRPC_MDVALUE(md_array->md[j]),-                                      expected[i].value) == 0);-        break;-      }+struct ExpectedMetadata {+  std::string key;+  std::string value;+};++class RequestMetadataState {+ public:+  RequestMetadataState(grpc_error_handle expected_error,+                       const ExpectedMetadata* expected, size_t expected_size,+                       grpc_polling_entity pollent)+      : expected_error_(expected_error),+        expected_(expected),+        expected_size_(expected_size),+        pollent_(pollent) {+    GRPC_CLOSURE_INIT(&on_request_metadata_, OnRequestMetadata, this,+                      grpc_schedule_on_exec_ctx);+  }++  ~RequestMetadataState() {+    grpc_credentials_mdelem_array_destroy(&md_array_);+    grpc_pollset_set_destroy(grpc_polling_entity_pollset_set(&pollent_));+  }++  void RunRequestMetadataTest(grpc_call_credentials* creds,+                              grpc_auth_metadata_context auth_md_ctx) {+    grpc_error_handle error = GRPC_ERROR_NONE;+    if (creds->get_request_metadata(&pollent_, auth_md_ctx, &md_array_,+                                    &on_request_metadata_, &error)) {+      // Synchronous result.  Invoke the callback directly.+      CheckRequestMetadata(error);+      GRPC_ERROR_UNREF(error);     }-    if (j == md_array->size) {-      gpr_log(GPR_ERROR, ""key %s not found"", expected[i].key);-      GPR_ASSERT(0);+  }++ private:+  static void OnRequestMetadata(void* arg, grpc_error_handle error) {+    RequestMetadataState* state = static_cast<RequestMetadataState*>(arg);+    state->CheckRequestMetadata(error);+  }++  void CheckRequestMetadata(grpc_error_handle error) {+    gpr_log(GPR_INFO, ""expected_error: %s"",+            grpc_error_std_string(expected_error_).c_str());+    gpr_log(GPR_INFO, ""actual_error: %s"", grpc_error_std_string(error).c_str());+    if (expected_error_ == GRPC_ERROR_NONE) {+      GPR_ASSERT(error == GRPC_ERROR_NONE);+    } else {+      std::string expected_error;+      GPR_ASSERT(grpc_error_get_str(expected_error_, GRPC_ERROR_STR_DESCRIPTION,+                                    &expected_error));+      std::string actual_error;+      GPR_ASSERT(+          grpc_error_get_str(error, GRPC_ERROR_STR_DESCRIPTION, &actual_error));+      GPR_ASSERT(expected_error == actual_error);+      GRPC_ERROR_UNREF(expected_error_);     }+    gpr_log(GPR_INFO, ""expected_size=%"" PRIdPTR "" actual_size=%"" PRIdPTR,+            expected_size_, md_array_.size);+    GPR_ASSERT(md_array_.size == expected_size_);+    CheckMetadata(expected_, &md_array_);+    delete this;   }-} -static void check_request_metadata(void* arg, grpc_error_handle error) {-  request_metadata_state* state = static_cast<request_metadata_state*>(arg);-  gpr_log(GPR_INFO, ""expected_error: %s"",-          grpc_error_std_string(state->expected_error).c_str());-  gpr_log(GPR_INFO, ""actual_error: %s"", grpc_error_std_string(error).c_str());-  if (state->expected_error == GRPC_ERROR_NONE) {-    GPR_ASSERT(error == GRPC_ERROR_NONE);-  } else {-    std::string expected_error;-    GPR_ASSERT(grpc_error_get_str(state->expected_error,-                                  GRPC_ERROR_STR_DESCRIPTION, &expected_error));-    std::string actual_error;-    GPR_ASSERT(-        grpc_error_get_str(error, GRPC_ERROR_STR_DESCRIPTION, &actual_error));-    GPR_ASSERT(expected_error == actual_error);-    GRPC_ERROR_UNREF(state->expected_error);+  static void CheckMetadata(const ExpectedMetadata* expected,+                            grpc_credentials_mdelem_array* md_array) {+    for (size_t i = 0; i < md_array->size; ++i) {+      size_t j;+      for (j = 0; j < md_array->size; ++j) {+        if (0 == grpc_slice_str_cmp(GRPC_MDKEY(md_array->md[j]),+                                    expected[i].key.c_str())) {+          GPR_ASSERT(grpc_slice_str_cmp(GRPC_MDVALUE(md_array->md[j]),+                                        expected[i].value.c_str()) == 0);+          break;+        }+      }+      if (j == md_array->size) {+        gpr_log(GPR_ERROR, ""key %s not found"", expected[i].key.c_str());+        GPR_ASSERT(0);+      }+    }   }-  gpr_log(GPR_INFO, ""expected_size=%"" PRIdPTR "" actual_size=%"" PRIdPTR,-          state->expected_size, state->md_array.size);-  GPR_ASSERT(state->md_array.size == state->expected_size);-  check_metadata(state->expected, &state->md_array);-  grpc_credentials_mdelem_array_destroy(&state->md_array);-  grpc_pollset_set_destroy(grpc_polling_entity_pollset_set(&state->pollent));-  gpr_free(state);-} -static request_metadata_state* make_request_metadata_state(-    grpc_error_handle expected_error, const expected_md* expected,+ private:+  grpc_error_handle expected_error_;+  const ExpectedMetadata* expected_;+  size_t expected_size_;+  grpc_credentials_mdelem_array md_array_;+  grpc_closure on_request_metadata_;+  grpc_polling_entity pollent_;+};++}  // namespace++static RequestMetadataState* make_request_metadata_state(",This should probably be a factory method in the `RequestMetadataState` class.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27618,727542694,2021-10-12T22:15:42Z,test/core/security/credentials_test.cc,"@@ -449,101 +449,117 @@ static void test_oauth2_token_fetcher_creds_parsing_missing_token_lifetime(   grpc_http_response_destroy(&response); } -typedef struct {-  const char* key;-  const char* value;-} expected_md;+namespace { -typedef struct {-  grpc_error_handle expected_error;-  const expected_md* expected;-  size_t expected_size;-  grpc_credentials_mdelem_array md_array;-  grpc_closure on_request_metadata;-  grpc_call_credentials* creds;-  grpc_polling_entity pollent;-} request_metadata_state;--static void check_metadata(const expected_md* expected,-                           grpc_credentials_mdelem_array* md_array) {-  for (size_t i = 0; i < md_array->size; ++i) {-    size_t j;-    for (j = 0; j < md_array->size; ++j) {-      if (0 ==-          grpc_slice_str_cmp(GRPC_MDKEY(md_array->md[j]), expected[i].key)) {-        GPR_ASSERT(grpc_slice_str_cmp(GRPC_MDVALUE(md_array->md[j]),-                                      expected[i].value) == 0);-        break;-      }+struct ExpectedMetadata {","Instead of keeping this form, how about passing the expected values into `RequestMetadataState` as a `std::map<std::string, std::string>`?  Or, if there can be duplicate keys, `std::vector<std::pair<std::string, std::string>>`.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27534,727618254,2021-10-13T00:55:34Z,tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py,"@@ -0,0 +1,127 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import datetime+import logging+from typing import List, Optional++from absl import flags+from absl.testing import absltest++from framework import xds_k8s_testcase+from framework.helpers import retryers+from framework.infrastructure import k8s+from framework.test_app import server_app++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient++_TD_CONFIG_RETRY_WAIT_SEC = 2+++class ApiListenerTest(xds_k8s_testcase.RegularXdsKubernetesTestCase):+    previous_route_config_version: str++    def test_api_listener(self) -> None:+        with self.subTest('00_create_health_check'):+            self.td.create_health_check()++        with self.subTest('01_create_backend_services'):+            self.td.create_backend_service()++        with self.subTest('02_create_default_url_maps'):+            self.td.create_url_map(self.server_xds_host, self.server_xds_port)++        with self.subTest('03_create_default_target_proxies'):+            self.td.create_target_proxy()++        with self.subTest('04_create_default_forwarding_rule'):+            self.td.create_forwarding_rule(self.server_xds_port)++        with self.subTest('05_start_test_servers'):+            self.test_servers: List[_XdsTestServer] = self.startTestServers()++        with self.subTest('06_add_server_backends_to_backend_services'):+            self.setupServerBackends()++        with self.subTest('07_start_test_client'):+            self.test_client: _XdsTestClient = self.startTestClient(+                self.test_servers[0])++        with self.subTest('08_test_client_xds_config_exists'):+            self.assertXdsConfigExists(self.test_client)++        with self.subTest('09_test_server_received_rpcs'):+            self.assertSuccessfulRpcs(self.test_client)++        with self.subTest('10_create_alternate_url_maps'):+            self.td.create_alternative_url_map(self.server_xds_host,+                                               self.server_xds_port)++        with self.subTest('11_create_alternate_target_proxies'):+            self.td.create_alternative_target_proxy()++        with self.subTest('12_create_alternate_forwarding_rule'):+            self.td.create_alternative_forwarding_rule(self.server_xds_port,+                                                       ip_address='10.10.10.10')++        with self.subTest('13_test_server_received_rpcs_with_two_url_maps'):+            self.assertSuccessfulRpcs(self.test_client)+            self.previous_route_config_version = self.getRouteConfigVersion(+                self.test_client)++        with self.subTest('14_delete_one_url_map_target_proxy_forwarding_rule'):+            self.td.delete_forwarding_rule()+            self.td.delete_target_grpc_proxy()+            self.td.delete_url_map()++        with self.subTest('15_test_server_continues_to_receive_rpcs'):++            class TdPropagationRetryableError(Exception):+                pass++            def verify_route_traffic_continues():","It's better to a normal (non-nested) method of this class. Generally, nested methods are hard to read. Exception can be moved to the module top level too.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27534,727619157,2021-10-13T00:58:25Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -290,6 +290,21 @@ def assertXdsConfigExists(self, test_client: XdsTestClient):                      json_format.MessageToJson(config, indent=2))         self.assertSameElements(want, seen) +    def getRouteConfigVersion(self, test_client: XdsTestClient):","1) In these unit tests classes, it's common to start methods that do asserts with `assert`, and use lower camel case. In this case, I think it's better to replace `self.assertIsNotNone` with another exception, f.e. `TypeError`.2) It'll be useful to specify the return type.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27534,727621014,2021-10-13T01:04:22Z,tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py,"@@ -0,0 +1,127 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import datetime+import logging+from typing import List, Optional++from absl import flags+from absl.testing import absltest++from framework import xds_k8s_testcase+from framework.helpers import retryers+from framework.infrastructure import k8s+from framework.test_app import server_app++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient++_TD_CONFIG_RETRY_WAIT_SEC = 2+++class ApiListenerTest(xds_k8s_testcase.RegularXdsKubernetesTestCase):+    previous_route_config_version: str","This is only used by one method, it's better to make it a local var to avoid impression this is represents any kind of class state.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/25586,728906171,2021-10-14T11:53:50Z,build_autogenerated.yaml,"@@ -2227,6 +2767,7 @@ libs:   - absl/types:variant   - absl/utility:utility   - gpr+  - libssl","I think this addition of ""libssl"" dependency is problematic - is seem to belong to the grpc_unsecure target (which by definition shouldn't link against libssl). I'll leave a more general comment, just wanted to point out the location. Note that thanks to this change, CMake also adds the ssl dependency to the grpc_unsecure target in CMakeLists.txt",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27618,729209296,2021-10-14T17:45:30Z,test/core/security/credentials_test.cc,"@@ -449,126 +449,141 @@ static void test_oauth2_token_fetcher_creds_parsing_missing_token_lifetime(   grpc_http_response_destroy(&response); } -typedef struct {-  const char* key;-  const char* value;-} expected_md;+namespace { -typedef struct {-  grpc_error_handle expected_error;-  const expected_md* expected;-  size_t expected_size;-  grpc_credentials_mdelem_array md_array;-  grpc_closure on_request_metadata;-  grpc_call_credentials* creds;-  grpc_polling_entity pollent;-} request_metadata_state;--static void check_metadata(const expected_md* expected,-                           grpc_credentials_mdelem_array* md_array) {-  for (size_t i = 0; i < md_array->size; ++i) {-    size_t j;-    for (j = 0; j < md_array->size; ++j) {-      if (0 ==-          grpc_slice_str_cmp(GRPC_MDKEY(md_array->md[j]), expected[i].key)) {-        GPR_ASSERT(grpc_slice_str_cmp(GRPC_MDVALUE(md_array->md[j]),-                                      expected[i].value) == 0);-        break;-      }-    }-    if (j == md_array->size) {-      gpr_log(GPR_ERROR, ""key %s not found"", expected[i].key);-      GPR_ASSERT(0);+class RequestMetadataState {+ public:+  static RequestMetadataState* NewInstance(+      grpc_error_handle expected_error,+      std::map<std::string, std::string> expected) {+    RequestMetadataState* state = new RequestMetadataState(+        expected_error, std::move(expected),+        grpc_polling_entity_create_from_pollset_set(grpc_pollset_set_create()));+    return state;+  }++ private:+  RequestMetadataState(grpc_error_handle expected_error,+                       std::map<std::string, std::string> expected,+                       grpc_polling_entity pollent)+      : expected_error_(expected_error),+        expected_(expected),+        pollent_(pollent) {+    GRPC_CLOSURE_INIT(&on_request_metadata_, OnRequestMetadata, this,+                      grpc_schedule_on_exec_ctx);+  }++ public:+  ~RequestMetadataState() {+    grpc_credentials_mdelem_array_destroy(&md_array_);+    grpc_pollset_set_destroy(grpc_polling_entity_pollset_set(&pollent_));+  }++  void RunRequestMetadataTest(grpc_call_credentials* creds,+                              grpc_auth_metadata_context auth_md_ctx) {+    grpc_error_handle error = GRPC_ERROR_NONE;+    if (creds->get_request_metadata(&pollent_, auth_md_ctx, &md_array_,+                                    &on_request_metadata_, &error)) {+      // Synchronous result.  Invoke the callback directly.+      CheckRequestMetadata(error);+      GRPC_ERROR_UNREF(error);     }   }-} -static void check_request_metadata(void* arg, grpc_error_handle error) {-  request_metadata_state* state = static_cast<request_metadata_state*>(arg);-  gpr_log(GPR_INFO, ""expected_error: %s"",-          grpc_error_std_string(state->expected_error).c_str());-  gpr_log(GPR_INFO, ""actual_error: %s"", grpc_error_std_string(error).c_str());-  if (state->expected_error == GRPC_ERROR_NONE) {-    GPR_ASSERT(error == GRPC_ERROR_NONE);-  } else {-    std::string expected_error;-    GPR_ASSERT(grpc_error_get_str(state->expected_error,-                                  GRPC_ERROR_STR_DESCRIPTION, &expected_error));-    std::string actual_error;-    GPR_ASSERT(-        grpc_error_get_str(error, GRPC_ERROR_STR_DESCRIPTION, &actual_error));-    GPR_ASSERT(expected_error == actual_error);-    GRPC_ERROR_UNREF(state->expected_error);+ private:+  static void OnRequestMetadata(void* arg, grpc_error_handle error) {+    RequestMetadataState* state = static_cast<RequestMetadataState*>(arg);+    state->CheckRequestMetadata(error);   }-  gpr_log(GPR_INFO, ""expected_size=%"" PRIdPTR "" actual_size=%"" PRIdPTR,-          state->expected_size, state->md_array.size);-  GPR_ASSERT(state->md_array.size == state->expected_size);-  check_metadata(state->expected, &state->md_array);-  grpc_credentials_mdelem_array_destroy(&state->md_array);-  grpc_pollset_set_destroy(grpc_polling_entity_pollset_set(&state->pollent));-  gpr_free(state);-}--static request_metadata_state* make_request_metadata_state(-    grpc_error_handle expected_error, const expected_md* expected,-    size_t expected_size) {-  request_metadata_state* state =-      static_cast<request_metadata_state*>(gpr_zalloc(sizeof(*state)));-  state->expected_error = expected_error;-  state->expected = expected;-  state->expected_size = expected_size;-  state->pollent =-      grpc_polling_entity_create_from_pollset_set(grpc_pollset_set_create());-  GRPC_CLOSURE_INIT(&state->on_request_metadata, check_request_metadata, state,-                    grpc_schedule_on_exec_ctx);-  return state;-}--static void run_request_metadata_test(grpc_call_credentials* creds,-                                      grpc_auth_metadata_context auth_md_ctx,-                                      request_metadata_state* state) {-  grpc_error_handle error = GRPC_ERROR_NONE;-  if (creds->get_request_metadata(&state->pollent, auth_md_ctx,-                                  &state->md_array, &state->on_request_metadata,-                                  &error)) {-    // Synchronous result.  Invoke the callback directly.-    check_request_metadata(state, error);-    GRPC_ERROR_UNREF(error);++  void CheckRequestMetadata(grpc_error_handle error) {+    gpr_log(GPR_INFO, ""expected_error: %s"",+            grpc_error_std_string(expected_error_).c_str());+    gpr_log(GPR_INFO, ""actual_error: %s"", grpc_error_std_string(error).c_str());+    if (expected_error_ == GRPC_ERROR_NONE) {+      GPR_ASSERT(error == GRPC_ERROR_NONE);+    } else {+      std::string expected_error;+      GPR_ASSERT(grpc_error_get_str(expected_error_, GRPC_ERROR_STR_DESCRIPTION,+                                    &expected_error));+      std::string actual_error;+      GPR_ASSERT(+          grpc_error_get_str(error, GRPC_ERROR_STR_DESCRIPTION, &actual_error));+      GPR_ASSERT(expected_error == actual_error);+      GRPC_ERROR_UNREF(expected_error_);+    }+    gpr_log(GPR_INFO, ""expected_size=%"" PRIdPTR "" actual_size=%"" PRIdPTR,+            expected_.size(), md_array_.size);+    GPR_ASSERT(md_array_.size == expected_.size());+    CheckMetadata(expected_, &md_array_);+    delete this;   }-}++  static void CheckMetadata(const std::map<std::string, std::string>& expected,+                            grpc_credentials_mdelem_array* md_array) {+    for (auto const& i : expected) {+      size_t j;+      for (j = 0; j < md_array->size; ++j) {+        absl::string_view actual_key =+            grpc_core::StringViewFromSlice(GRPC_MDKEY(md_array->md[j]));+        if (actual_key == i.first) {+          absl::string_view actual_value =+              grpc_core::StringViewFromSlice(GRPC_MDVALUE(md_array->md[j]));+          GPR_ASSERT(actual_value == i.second);+          break;+        }+      }+      if (j == md_array->size) {+        gpr_log(GPR_ERROR, ""key %s not found"", i.first.c_str());+        GPR_ASSERT(0);+      }+    }+  }++ private:+  grpc_error_handle expected_error_;+  std::map<std::string, std::string> expected_;+  grpc_credentials_mdelem_array md_array_;+  grpc_closure on_request_metadata_;+  grpc_polling_entity pollent_;+};++}  // namespace  static void test_google_iam_creds(void) {   grpc_core::ExecCtx exec_ctx;-  expected_md emd[] = {{GRPC_IAM_AUTHORIZATION_TOKEN_METADATA_KEY,-                        test_google_iam_authorization_token},-                       {GRPC_IAM_AUTHORITY_SELECTOR_METADATA_KEY,-                        test_google_iam_authority_selector}};-  request_metadata_state* state =-      make_request_metadata_state(GRPC_ERROR_NONE, emd, GPR_ARRAY_SIZE(emd));+  std::map<std::string, std::string> emd = {+      {GRPC_IAM_AUTHORIZATION_TOKEN_METADATA_KEY,+       test_google_iam_authorization_token},+      {GRPC_IAM_AUTHORITY_SELECTOR_METADATA_KEY,+       test_google_iam_authority_selector}};+  RequestMetadataState* state =+      RequestMetadataState::NewInstance(GRPC_ERROR_NONE, emd);","`std::move()` to pass `emd` here.  Same for all tests.Alternatively, change `RequestMetadataState` to just take a pointer to the map.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27666,729248692,2021-10-14T18:42:16Z,src/core/ext/filters/client_channel/retry_throttle.cc,"@@ -114,79 +115,47 @@ void ServerRetryThrottleData::RecordSuccess() {       static_cast<gpr_atm>(throttle_data->max_milli_tokens_)); } -//-// avl vtable for string -> server_retry_throttle_data map-//--namespace {--void* copy_server_name(void* key, void* /*unused*/) {-  return gpr_strdup(static_cast<const char*>(key));-}--long compare_server_name(void* key1, void* key2, void* /*unused*/) {-  return strcmp(static_cast<const char*>(key1), static_cast<const char*>(key2));-}--void destroy_server_retry_throttle_data(void* value, void* /*unused*/) {-  ServerRetryThrottleData* throttle_data =-      static_cast<ServerRetryThrottleData*>(value);-  throttle_data->Unref();-}--void* copy_server_retry_throttle_data(void* value, void* /*unused*/) {-  ServerRetryThrottleData* throttle_data =-      static_cast<ServerRetryThrottleData*>(value);-  return throttle_data->Ref().release();-}--void destroy_server_name(void* key, void* /*unused*/) { gpr_free(key); }--const grpc_avl_vtable avl_vtable = {-    destroy_server_name, copy_server_name, compare_server_name,-    destroy_server_retry_throttle_data, copy_server_retry_throttle_data};--}  // namespace- // // ServerRetryThrottleMap //  static gpr_mu g_mu;-static grpc_avl g_avl;+static ManualConstructor<+    std::map<std::string, RefCountedPtr<ServerRetryThrottleData>>>",it might?it might also make it worse if the strings cluster badly...my thought here was to be as conservative as possible on this pass,
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27643,729250088,2021-10-14T18:44:21Z,src/core/ext/transport/chttp2/server/insecure/server_chttp2_posix.cc,"@@ -45,20 +46,15 @@ void grpc_server_add_insecure_channel_from_fd(grpc_server* server,    const grpc_channel_args* server_args = core_server->channel_args();   std::string name = absl::StrCat(""fd:"", fd);-  grpc_resource_quota* resource_quota =-      grpc_resource_quota_create(name.c_str());+  auto memory_quota =+      grpc_core::ResourceQuotaFromChannelArgs(server_args)->memory_quota();   grpc_endpoint* server_endpoint = grpc_tcp_create(-      grpc_fd_create(fd, name.c_str(), true), server_args, name.c_str(),-      grpc_slice_allocator_create(resource_quota, name, server_args));+      grpc_fd_create(fd, name.c_str(), true), server_args, name.c_str());   grpc_transport* transport = grpc_create_chttp2_transport(-      server_args, server_endpoint, false /* is_client */,-      grpc_resource_user_create(resource_quota,-                                absl::StrCat(name, "":transport"")));-  grpc_error_handle error = core_server->SetupTransport(-      transport, nullptr, server_args, nullptr,-      grpc_resource_user_create(resource_quota,-                                absl::StrCat(name, "":channel"")));-  grpc_resource_quota_unref_internal(resource_quota);+      server_args, server_endpoint, false /* is_client */","Again, I'm concerned about the loss of RU naming. Naming could be handled at a lower level than this, but I do feel it's valuable to retain.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27666,729250340,2021-10-14T18:44:46Z,src/core/ext/filters/client_channel/retry_throttle.cc,"@@ -114,79 +115,47 @@ void ServerRetryThrottleData::RecordSuccess() {       static_cast<gpr_atm>(throttle_data->max_milli_tokens_)); } -//-// avl vtable for string -> server_retry_throttle_data map-//--namespace {--void* copy_server_name(void* key, void* /*unused*/) {-  return gpr_strdup(static_cast<const char*>(key));-}--long compare_server_name(void* key1, void* key2, void* /*unused*/) {-  return strcmp(static_cast<const char*>(key1), static_cast<const char*>(key2));-}--void destroy_server_retry_throttle_data(void* value, void* /*unused*/) {-  ServerRetryThrottleData* throttle_data =-      static_cast<ServerRetryThrottleData*>(value);-  throttle_data->Unref();-}--void* copy_server_retry_throttle_data(void* value, void* /*unused*/) {-  ServerRetryThrottleData* throttle_data =-      static_cast<ServerRetryThrottleData*>(value);-  return throttle_data->Ref().release();-}--void destroy_server_name(void* key, void* /*unused*/) { gpr_free(key); }--const grpc_avl_vtable avl_vtable = {-    destroy_server_name, copy_server_name, compare_server_name,-    destroy_server_retry_throttle_data, copy_server_retry_throttle_data};--}  // namespace- // // ServerRetryThrottleMap //  static gpr_mu g_mu;-static grpc_avl g_avl;+static ManualConstructor<","Changed to the latter... I don't think it made it less complex, but it's done.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27643,729266406,2021-10-14T19:08:46Z,src/core/ext/transport/chttp2/transport/chttp2_transport.h,"@@ -24,6 +24,7 @@ #include ""src/core/lib/channel/channelz.h"" #include ""src/core/lib/debug/trace.h"" #include ""src/core/lib/iomgr/endpoint.h""+#include ""src/core/lib/resource_quota/memory_quota.h""",appears to be unused,X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27643,729428556,2021-10-15T00:01:50Z,src/core/lib/iomgr/tcp_server_posix.cc,"@@ -223,6 +220,12 @@ static void on_read(void* arg, grpc_error_handle err) {       }     } +    if (sp->server->memory_quota->IsMemoryPressureHigh()) {","I don't think there's any harm in doing this, and it does allow us to reduce work in bad situations, so I'd like to try it.",X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/27074,729472922,2021-10-15T02:04:36Z,src/core/lib/security/authorization/rbac_translator.cc,"@@ -104,6 +104,12 @@ absl::StatusOr<Rbac::Principal> ParsePeer(const Json& json) {     if (it->second.type() != Json::Type::ARRAY) {       return absl::InvalidArgumentError(""\""principals\"" is not an array."");     }+    if (it->second.array_value().empty()) {+      return Rbac::Principal(+          Rbac::Principal::RuleType::kPrincipalName,+          StringMatcher::Create(StringMatcher::Type::kExact, /*matcher=*/"""")","This path is triggered if the principals array is empty i.e. `principals: []` in json policy. It is different from `principals:[""foo""]`, this would create a StringMatcher [here](https://github.com/grpc/grpc/blob/master/src/core/lib/security/authorization/rbac_translator.cc#L86)And principal ""foo"" will match against the peer certificate ([code](https://github.com/grpc/grpc/blob/master/src/core/lib/security/authorization/matchers.cc#L178))I did update the code a bit to handle empty strings better i.e. `principals: [""""]`So previously, it would have matched [here](https://github.com/grpc/grpc/blob/master/src/core/lib/security/authorization/matchers.cc#L183), without checking for peer certificate. Should have matched only if there was no client certificate.In this PR, with the changes I made in last commits, it would return failed match. I have a [PR](https://github.com/grpc/grpc/pull/27615) pending review to add subject field to matcher implementation, which would return successful match here, behavior similar to envoy (mentioned in xds RBAC proposal [here](https://screenshot.googleplex.com/54Q4JKwCkMQJprs.png)(step4))Also, I think eventually we would have needed the changes to rbac_policy, to represent unset Authenticated proto for xDShttps://source.corp.google.com/piper///depot/google3/third_party/envoy/src/api/envoy/config/rbac/v3/rbac.proto;rcl=401765878;l=254https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/rbac/v3/rbac.proto#config-rbac-v3-principal-authenticated",X
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/27740,730108289,2021-10-15T20:48:17Z,src/objective-c/tests/CoreTests/build_and_run_tests.sh,"@@ -0,0 +1,40 @@+#!/bin/bash","Can we consider parameterize these scripts so we don't duplicate it per test, and share them with multiple test targets ? This can be in a follow up change, thanks ! ",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27714,731210294,2021-10-18T18:35:17Z,src/core/lib/transport/metadata_batch.h,"@@ -96,9 +97,115 @@ struct GrpcTimeoutMetadata {     }     return grpc_core::ExecCtx::Get()->Now() + timeout;   }+  static grpc_slice Encode(ValueType x) {+    char timeout[GRPC_HTTP2_TIMEOUT_ENCODE_MIN_BUFSIZE];+    grpc_http2_encode_timeout(x, timeout);+    return grpc_slice_from_copied_string(timeout);+  }   static MementoType DisplayValue(MementoType x) { return x; } }; +// TE metadata trait.","This trait seems very HTTP/2-specific.  I'm a little concerned that having this here is chipping away at the wall between transport-specific details and the generic metadata representation.  Is there any way to avoid this?If we define server-side trailing metadata as `MetadataMap<GrpcTimeoutMetadata, TeMetadata>`, won't we be wasting some memory for a transport that doesn't use the TE header?  Or does the underlying `Table<>` template avoid that somehow?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27714,731225192,2021-10-18T18:57:11Z,src/core/lib/transport/metadata_batch.h,"@@ -915,8 +1052,7 @@ inline void grpc_metadata_batch_assert_ok(grpc_metadata_batch* batch) { /// mdelem that will hold its own refs to the key and value slices. /// /// Currently used only in the retry code.-void grpc_metadata_batch_copy(grpc_metadata_batch* src,-                              grpc_metadata_batch* dst,-                              grpc_linked_mdelem* storage);+void grpc_metadata_batch_copy(const grpc_metadata_batch* src,",The comment for this no longer needs to reference the `storage` parameter.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27714,731230725,2021-10-18T19:02:28Z,src/core/lib/transport/metadata_batch.cc,"@@ -41,34 +41,47 @@ void grpc_metadata_batch_set_value(grpc_linked_mdelem* storage,   GRPC_MDELEM_UNREF(old_mdelem); } -void grpc_metadata_batch_copy(grpc_metadata_batch* src,-                              grpc_metadata_batch* dst,-                              grpc_linked_mdelem* storage) {-  dst->Clear();-  // TODO(ctiller): this should be templated and automatically derived.-  if (auto* p = src->get_pointer(grpc_core::GrpcTimeoutMetadata())) {-    dst->Set(grpc_core::GrpcTimeoutMetadata(), *p);-  }-  size_t i = 0;-  src->ForEach([&](grpc_mdelem md) {+namespace {++class CopySink {+ public:+  explicit CopySink(grpc_metadata_batch* dst) : dst_(dst) {}++  void Encode(grpc_mdelem md) {     // If the mdelem is not external, take a ref.     // Otherwise, create a new copy, holding its own refs to the     // underlying slices.     if (GRPC_MDELEM_STORAGE(md) != GRPC_MDELEM_STORAGE_EXTERNAL) {       md = GRPC_MDELEM_REF(md);     } else {-      md = grpc_mdelem_from_slices(grpc_slice_ref_internal(GRPC_MDKEY(md)),-                                   grpc_slice_ref_internal(GRPC_MDVALUE(md)));+      md = grpc_mdelem_from_slices(grpc_slice_copy(GRPC_MDKEY(md)),+                                   grpc_slice_copy(GRPC_MDVALUE(md)));     }     // Error unused in non-debug builds.-    grpc_error_handle GRPC_UNUSED error =-        grpc_metadata_batch_add_tail(dst, &storage[i++], md);+    grpc_error_handle GRPC_UNUSED error = dst_->Append(md);     // The only way that grpc_metadata_batch_add_tail() can fail is if",s/grpc_metadata_batch_add_tail/Append/,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27714,731256298,2021-10-18T19:37:44Z,src/core/ext/filters/http/client/http_client_filter.cc,"@@ -441,9 +440,6 @@ static void http_client_start_transport_stream_op_batch(     remove_if_present(         batch->payload->send_initial_metadata.send_initial_metadata,         GRPC_BATCH_SCHEME);-    remove_if_present(-        batch->payload->send_initial_metadata.send_initial_metadata,-        GRPC_BATCH_TE);",Just to confirm: The reason we're no longer removing this here is that the `Set()` call below will overwrite any pre-existing value that may have been set by the application?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27714,731258474,2021-10-18T19:41:09Z,src/core/ext/transport/cronet/transport/cronet_transport.cc,"@@ -408,19 +408,16 @@ static void convert_cronet_array_to_metadata(   for (size_t i = 0; i < header_array->count; i++) {     CRONET_LOG(GPR_DEBUG, ""header key=%s, value=%s"",                header_array->headers[i].key, header_array->headers[i].value);-    grpc_slice key = grpc_slice_intern(-        grpc_slice_from_static_string(header_array->headers[i].key));     grpc_slice value;-    if (grpc_is_refcounted_slice_binary_header(key)) {+    if (absl::EndsWith(header_array->headers[i].key, ""-bin"")) {       value = grpc_slice_from_static_string(header_array->headers[i].value);       value = grpc_slice_intern(grpc_chttp2_base64_decode_with_length(           value, grpc_chttp2_base64_infer_length_after_decode(value)));     } else {       value = grpc_slice_intern(           grpc_slice_from_static_string(header_array->headers[i].value));     }-    GRPC_LOG_IF_ERROR(""convert_cronet_array_to_metadata"",",Why remove the `GRPC_LOG_IF_ERROR` here?  Is there a reason this can no longer fail?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27714,731262715,2021-10-18T19:46:17Z,src/core/ext/transport/cronet/transport/cronet_transport.cc,"@@ -737,40 +738,57 @@ static void convert_metadata_to_cronet_headers(     }     if (grpc_slice_eq_static_interned(GRPC_MDKEY(mdelem), GRPC_MDSTR_METHOD)) {       if (grpc_slice_eq_static_interned(GRPC_MDVALUE(mdelem), GRPC_MDSTR_PUT)) {-        *method = ""PUT"";+        *method_ = ""PUT"";       } else {         /* POST method in default*/-        *method = ""POST"";+        *method_ = ""POST"";       }       gpr_free(key);       gpr_free(value);       return;     }     if (grpc_slice_eq_static_interned(GRPC_MDKEY(mdelem), GRPC_MDSTR_PATH)) {       /* Create URL by appending :path value to the hostname */-      *pp_url = absl::StrCat(""https://"", host, value);+      *url_ = absl::StrCat(""https://"", host_, value);       gpr_free(key);       gpr_free(value);       return;     }     CRONET_LOG(GPR_DEBUG, ""header %s = %s"", key, value);-    headers[num_headers].key = key;-    headers[num_headers].value = value;-    num_headers++;-  });-  if (metadata->deadline() != GRPC_MILLIS_INF_FUTURE) {-    char* key = grpc_slice_to_c_string(GRPC_MDSTR_GRPC_TIMEOUT);-    char* value =-        static_cast<char*>(gpr_malloc(GRPC_HTTP2_TIMEOUT_ENCODE_MIN_BUFSIZE));-    grpc_http2_encode_timeout(-        metadata->deadline() - grpc_core::ExecCtx::Get()->Now(), value);-    headers[num_headers].key = key;-    headers[num_headers].value = value;--    num_headers++;+    GPR_ASSERT(count_ < capacity_);+    headers_[count_].key = key;+    headers_[count_].value = value;+    count_++;   } -  *p_num_headers = num_headers;+  void Take(bidirectional_stream_header** headers, size_t* count) {","Why have this method instead of just passing in the `pp_headers` and `p_num_headers` parameters to the ctor, just like we're doing with the other params?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27714,731268759,2021-10-18T19:51:56Z,src/core/lib/surface/call.cc,"@@ -1755,7 +1755,7 @@ static grpc_call_error call_start_batch(grpc_call* call, const grpc_op* ops,         if (op->data.send_status_from_server.status_details != nullptr) {           call->send_extra_metadata[1].md = grpc_mdelem_from_slices(               GRPC_MDSTR_GRPC_MESSAGE,-              grpc_slice_ref_internal(+              grpc_slice_copy(",Why does this need to copy unconditionally?  Isn't it safe to take a ref based on the change I made in #27205?  Or are there other edge cases that weren't covered by that PR?I'm wondering if maybe we should change `grpc_slice_ref_internal()` to automatically make a copy if the slice is not of a ref-counted type.  It seems like there may be a whole class of bugs here where taking a ref becomes a no-op and the calling code has no way to know it isn't holding onto what it thinks it is.,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27714,731303822,2021-10-18T20:46:09Z,src/core/lib/transport/metadata_batch.h,"@@ -96,9 +97,115 @@ struct GrpcTimeoutMetadata {     }     return grpc_core::ExecCtx::Get()->Now() + timeout;   }+  static grpc_slice Encode(ValueType x) {+    char timeout[GRPC_HTTP2_TIMEOUT_ENCODE_MIN_BUFSIZE];+    grpc_http2_encode_timeout(x, timeout);+    return grpc_slice_from_copied_string(timeout);+  }   static MementoType DisplayValue(MementoType x) { return x; } }; +// TE metadata trait.","Agree that it'd be nice not to have H2 specific metadata here.However, this is an overall improvement from where we are:- right now TE is a callout, which means that it always costs 3 * 8-byte pointers even if not used (and it needs to be for performance)- with the Table<> implementation, this costs 1 bit + 1 byte (1 bit in the signalling bits that get packed together as an aggregate across the table, 1 byte for the value)Going forward we may want to rearchitect slightly and provide some utilities to HTTP2 based transports to play the roles that http_client_filter, http_server_filter are currently playing - that is eliminate these pieces from the filter stack, and locate them somewhere else. A good time for that may be during the promise conversion - or possibly just after.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27714,731309520,2021-10-18T20:55:12Z,src/core/lib/transport/metadata_batch.cc,"@@ -41,34 +41,47 @@ void grpc_metadata_batch_set_value(grpc_linked_mdelem* storage,   GRPC_MDELEM_UNREF(old_mdelem); } -void grpc_metadata_batch_copy(grpc_metadata_batch* src,-                              grpc_metadata_batch* dst,-                              grpc_linked_mdelem* storage) {-  dst->Clear();-  // TODO(ctiller): this should be templated and automatically derived.-  if (auto* p = src->get_pointer(grpc_core::GrpcTimeoutMetadata())) {-    dst->Set(grpc_core::GrpcTimeoutMetadata(), *p);-  }-  size_t i = 0;-  src->ForEach([&](grpc_mdelem md) {+namespace {++class CopySink {+ public:+  explicit CopySink(grpc_metadata_batch* dst) : dst_(dst) {}++  void Encode(grpc_mdelem md) {     // If the mdelem is not external, take a ref.     // Otherwise, create a new copy, holding its own refs to the     // underlying slices.     if (GRPC_MDELEM_STORAGE(md) != GRPC_MDELEM_STORAGE_EXTERNAL) {       md = GRPC_MDELEM_REF(md);     } else {-      md = grpc_mdelem_from_slices(grpc_slice_ref_internal(GRPC_MDKEY(md)),-                                   grpc_slice_ref_internal(GRPC_MDVALUE(md)));+      md = grpc_mdelem_from_slices(grpc_slice_copy(GRPC_MDKEY(md)),","So, ugh, we're currently broken.Whilst the mdelem doesn't disappear with your changes, the underlying slice might for the C++ code.We create a slice that references a std::string and pass it down as the key or value part of the grpc_metadata from the surface, meaning the slice refs here are no-ops and those strings can still disappear at the same time.Which means if we capture the mdelem we might end up with a valid object pointing to some stranded pointers.The positive thing is that mdelem's are absolutely going to go away in about a month and so this will be a quaint memory.",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/27756,731349468,2021-10-18T22:05:44Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -1324,18 +1324,47 @@ class XdsEnd2endTest : public ::testing::TestWithParam<TestType> {     });   } -  static Listener BuildListener(const RouteConfiguration& route_config) {+  AdsServiceImpl::ResponseState RouteConfigurationResponseState(int idx) const {+    AdsServiceImpl* ads_service = balancers_[idx]->ads_service();+    if (GetParam().enable_rds_testing()) {+      return ads_service->rds_response_state();+    }+    return ads_service->lds_response_state();+  }++  void SetListenerAndRouteConfiguration(+      int idx, Listener listener, const RouteConfiguration& route_config) {","If we want to reuse this for the server side, we would also want a boolean to let this know what kind of listener is being used. (For a server-side listener, we would want to modify the HttpConnectionManager within the default filter chain)",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27643,731353432,2021-10-18T22:13:23Z,src/core/ext/filters/client_channel/resolver/google_c2p/google_c2p_resolver.cc,"@@ -127,9 +127,8 @@ GoogleCloud2ProdResolver::MetadataQuery::MetadataQuery(   request.http.path = const_cast<char*>(path);   request.http.hdr_count = 1;   request.http.hdrs = &header;-  grpc_resource_quota* resource_quota =-      grpc_resource_quota_create(""c2p_resolver"");-  grpc_httpcli_get(&context_, pollent, resource_quota, &request,+  // TODO(ctiller): share the quota from whomever instantiates this!","I think we should be able to do this fairly easily.  The resolver gets channel args from the channel, and the RQ is be encoded in channel args, so I think we can just have the `GoogleCloud2ProdResolver` ctor grab the RQ out of channel args and store it in a data member, and then can use that here.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27643,731360504,2021-10-18T22:29:04Z,src/core/lib/resource_quota/api.h,"@@ -0,0 +1,52 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_RESOURCE_QUOTA_API_H+#define GRPC_CORE_LIB_RESOURCE_QUOTA_API_H++#include <grpc/support/port_platform.h>++#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/resource_quota/resource_quota.h""++typedef struct grpc_resource_quota grpc_resource_quota;++namespace grpc_core {++// TODO(ctiller): This is a hack. We need to do real accounting instead of",What's the long-term plan for figuring out how much space to carve out for channels and calls?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27643,731361264,2021-10-18T22:30:51Z,src/core/lib/resource_quota/resource_quota.h,"@@ -24,21 +24,29 @@ namespace grpc_core {  class ResourceQuota : public RefCounted<ResourceQuota> {  public:-  ResourceQuota();+  explicit ResourceQuota(std::string name);   ~ResourceQuota() override;    ResourceQuota(const ResourceQuota&) = delete;   ResourceQuota& operator=(const ResourceQuota&) = delete; -  std::shared_ptr<MemoryQuota> memory_quota() { return memory_quota_; }+  MemoryQuotaPtr memory_quota() { return memory_quota_; }    const RefCountedPtr<ThreadQuota>& thread_quota() { return thread_quota_; }   private:-  std::shared_ptr<MemoryQuota> memory_quota_;+  MemoryQuotaPtr memory_quota_;   RefCountedPtr<ThreadQuota> thread_quota_; }; +using ResourceQuotaPtr = RefCountedPtr<ResourceQuota>;","IMHO, this kind of alias really hurts readability, because it hides the fact that this is ref-counted from the callers.  The Envoy code-base uses this kind of alias all over the place, and it makes the code really hard to understand.Same thing for `MemoryQuotaPtr` and `ResourceQuotaPtr`.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27643,731370353,2021-10-18T22:52:12Z,src/core/lib/resource_quota/api.cc,"@@ -0,0 +1,105 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include ""src/core/lib/resource_quota/api.h""++#include <grpc/grpc.h>++#include ""src/core/lib/gpr/useful.h""++namespace grpc_core {++ResourceQuotaPtr ResourceQuotaFromChannelArgs(const grpc_channel_args* args) {+  return grpc_channel_args_find_pointer<ResourceQuota>(args,+                                                       GRPC_ARG_RESOURCE_QUOTA)+      ->Ref();+}++namespace {+grpc_arg MakeArg(ResourceQuota* quota) {+  grpc_arg arg;+  arg.type = GRPC_ARG_POINTER;+  arg.key = const_cast<char*>(GRPC_ARG_RESOURCE_QUOTA);+  arg.value.pointer.p = quota;+  arg.value.pointer.vtable = grpc_resource_quota_arg_vtable();+  return arg;+}+}  // namespace++grpc_channel_args* EnsureResourceQuotaInChannelArgs(+    const grpc_channel_args* args) {+  if (grpc_channel_args_find(args, GRPC_ARG_RESOURCE_QUOTA) != nullptr) {+    return grpc_channel_args_copy(args);+  }+  // If there's no existing quota, add it to the default one - shared between+  // all channel args declared thusly. This prevents us from accidentally not+  // sharing subchannels due to their channel args not specifying a quota.+  auto new_arg = MakeArg(DefaultResourceQuota().release());+  return grpc_channel_args_copy_and_add(args, &new_arg, 1);+}++grpc_channel_args* ChannelArgsWrappingResourceQuota(+    ResourceQuotaPtr resource_quota) {+  auto new_arg = MakeArg(resource_quota.release());+  return grpc_channel_args_copy_and_add(nullptr, &new_arg, 1);+}++}  // namespace grpc_core++extern ""C"" const grpc_arg_pointer_vtable* grpc_resource_quota_arg_vtable() {+  static const grpc_arg_pointer_vtable vtable = {+      // copy+      [](void* p) -> void* {+        return static_cast<grpc_core::ResourceQuota*>(p)->Ref().release();+      },+      // destroy+      [](void* p) { static_cast<grpc_core::ResourceQuota*>(p)->Unref(); },+      // compare+      [](void* p, void* q) { return grpc_core::QsortCompare(p, q); }};+  return &vtable;+}++extern ""C"" grpc_resource_quota* grpc_resource_quota_create(const char* name) {+  static std::atomic<uintptr_t> anonymous_counter{0};+  std::string quota_name =+      name == nullptr+          ? absl::StrCat(""anonymous-quota-"", anonymous_counter.fetch_add(1))+          : name;+  return reinterpret_cast<grpc_resource_quota*>(","I'm not really comfortable with these `reinterpret_cast<>`s, because it eliminates the type safety provided by the compiler.The pattern we use for this for `grpc_server` and `grpc_core::Server` is that [`grpc_server` is a struct containing a smart pointer to the real type inside of C-core](https://github.com/grpc/grpc/blob/a2bd7b844082413e1b46bde4c0323bb653736320/src/core/lib/surface/server.h#L448), and [`grpc_server_create()` allocates both the wrapper struct and its data member](https://github.com/grpc/grpc/blob/a2bd7b844082413e1b46bde4c0323bb653736320/src/core/lib/surface/server.cc#L1474).  That does mean two allocations instead of one, but that shouldn't happen in any performance-sensitive code path, so it seems like a reasonable trade-off for the additional type safety.As another example, most of the security code uses the pattern (which I know you dislike) of declaring the C-core API type as a C++ class (e.g., [`grpc_channel_credentials`](https://github.com/grpc/grpc/blob/b9265ab91d4d85ee88c2236e064614848577bdd0/src/core/lib/security/credentials/credentials.h#L101)).I think we need to pick a strategy for dealing with this and work toward using that same strategy everywhere.  And I'd really prefer that strategy to be something with enforced type safety.  Since I know you dislike the pattern we use in the security code, can we converge on the approach that we use in the gRPC server code?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27643,731957697,2021-10-19T14:55:04Z,src/core/lib/resource_quota/memory_quota.h,"@@ -316,6 +351,27 @@ class MemoryOwner {   // Rebind to a different quota.   void Rebind(MemoryQuota* quota); +  // Instantaneous memory pressure in the underlying quota.+  double InstantaneousPressure() const {+    return static_cast<const GrpcMemoryAllocatorImpl*>(+               allocator_.get_internal_impl_ptr())+        ->InstantaneousPressure();+  }++  // TODO(ctiller): if this continues to live here, we should rename this class.","Can you say more about what you mean by this TODO?  It's not clear to me why the presence of this method would imply a rename.As we discussed in the previous PR, I'd prefer not to subclass `MemoryAllocator`.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27643,731963025,2021-10-19T15:00:04Z,src/core/lib/resource_quota/memory_quota.cc,"@@ -327,10 +329,22 @@ void BasicMemoryQuota::Start() {         // Race biases to the first thing that completes... so this will         // choose the highest priority/least destructive thing to do that's         // available.-        return Race(self->reclaimers_[0].Next(), self->reclaimers_[1].Next(),-                    self->reclaimers_[2].Next(), self->reclaimers_[3].Next());+        auto annotate = [](const char* name) {+          return [name](ReclamationFunction f) {+            return std::make_tuple(name, std::move(f));+          };+        };+        return Race(Map(self->reclaimers_[0].Next(), annotate(""compact"")),+                    Map(self->reclaimers_[1].Next(), annotate(""benign"")),+                    Map(self->reclaimers_[2].Next(), annotate(""idle"")),+                    Map(self->reclaimers_[3].Next(), annotate(""destructive"")));       },-      [self](ReclamationFunction reclaimer) {+      [self](std::tuple<const char*, ReclamationFunction> arg) {+        auto reclaimer = std::move(std::get<1>(arg));+        if (GRPC_TRACE_FLAG_ENABLED(grpc_resource_quota_trace)) {+          gpr_log(GPR_DEBUG, ""RQ: %s perform %s reclamation"",","Does `GPR_DEBUG` actually work internally these days?  For a long time, it didn't, and we got into the habit of always using `GPR_INFO` for trace messages.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27643,732027070,2021-10-19T16:06:11Z,src/core/lib/security/credentials/oauth2/oauth2_credentials.cc,"@@ -392,12 +392,11 @@ class grpc_compute_engine_token_fetcher_credentials         const_cast<char*>(GRPC_COMPUTE_ENGINE_METADATA_TOKEN_PATH);     request.http.hdr_count = 1;     request.http.hdrs = &header;-    /* TODO(ctiller): Carry the resource_quota in ctx and share it with the host+    /* TODO(ctiller): Carry the memory quota in ctx and share it with the host","(This comment isn't blocking this PR; it's just a question about strategy for subsequent changes.)One possible way of doing this would be to have the channel pass its RQ to the [`grpc_call_credentials::get_request_metadata()` method](https://github.com/grpc/grpc/blob/790ce5c97ff3a3922d50a1fd3a4bab91e2bcd435/src/core/lib/security/credentials/credentials.h#L186), so that each attempt to get call creds can be charged to the channel that requests it.However, the oauth case demonstrates that this might not be ideal, since the same call creds object can be used by multiple channels, and each channel may have its own RQ.  That would mean that the first channel that happens to trigger fetching the oauth token would be charged for the memory usage, and calls from another channel that start while the oauth fetch is pending will just block waiting for the fetch to complete without being charged for it.Another option might be to add the ability to pass an RQ to use directly into the call credential when it is instantiated, so that the application can directly control which RQ is used by the call cred, regardless of what channels the call cred is used in.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27643,732029945,2021-10-19T16:09:35Z,src/core/lib/resource_quota/api.h,"@@ -0,0 +1,52 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_RESOURCE_QUOTA_API_H+#define GRPC_CORE_LIB_RESOURCE_QUOTA_API_H++#include <grpc/support/port_platform.h>++#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/resource_quota/resource_quota.h""++typedef struct grpc_resource_quota grpc_resource_quota;++namespace grpc_core {++// TODO(juanlishen): This is a hack. We need to do real accounting instead of+// hard coding.+constexpr size_t kResourceQuotaCallSize = 15 * 1024;+constexpr size_t kResourceQuotaChannelSize = 50 * 1024;++// Retrieve the resource quota from the channel args.+// UB if not set.+ResourceQuotaPtr ResourceQuotaFromChannelArgs(const grpc_channel_args* args);++// Take some channel args:+// If there is a resource quota set, copy args and return that.+// If there is no resource quota set, set a default, and return new+// channel args. Call grpc_channel_args_destroy on the input args.","I agree with AJ's suggestion of having `EnsureResourceQuotaInChannelArgs()` take ownership of the passed-in args.  We use this pattern in a lot of other places, and I think it generally works better.  It's true that there are some cases where you wind up needing to make an extra copy, but I think it winds up being more ergonomic the majority of the time.I do also certainly agree that channel args needs a new C++ API.  But I'd prefer to try to keep the existing API as ergonomic as possible in the interim.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27643,732048636,2021-10-19T16:31:38Z,src/core/lib/resource_quota/api.h,"@@ -0,0 +1,52 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_RESOURCE_QUOTA_API_H+#define GRPC_CORE_LIB_RESOURCE_QUOTA_API_H++#include <grpc/support/port_platform.h>++#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/resource_quota/resource_quota.h""++typedef struct grpc_resource_quota grpc_resource_quota;++namespace grpc_core {++// TODO(ctiller): This is a hack. We need to do real accounting instead of","Use the allocation apis.Once this lands, my next PR will be to make the call arena's quota backed, at which point calls will be ~done. @ananda1066 will then triage and lead the initial conversion to use these API's to get the bulk of the allocations *by byte count* tracked.Once we reach a happy threshold of allocations tracked, we'll start talking about how to finish the job (the long tail of which I expect to be a ways out)",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27643,732053926,2021-10-19T16:38:25Z,src/core/lib/resource_quota/memory_quota.h,"@@ -316,6 +351,27 @@ class MemoryOwner {   // Rebind to a different quota.   void Rebind(MemoryQuota* quota); +  // Instantaneous memory pressure in the underlying quota.+  double InstantaneousPressure() const {+    return static_cast<const GrpcMemoryAllocatorImpl*>(+               allocator_.get_internal_impl_ptr())+        ->InstantaneousPressure();+  }++  // TODO(ctiller): if this continues to live here, we should rename this class.","I'd forgotten about this. Yeah we need to chat about it - prior to this PR MemoryOwner is an allocator interface + a reclamation interface, and that separation feels ok.Post this PR as it stands, we have an allocator interface for some kinds of pointers, and then an interface that contains both allocation and reclamation, and I've no longer got a coherent story to tell about what goes where (and that lack of a coherent story is concerning as future editors will be confused as to what needs to go where).This comes up because ergonomically we want simple allocation API's for our smart pointers, but we don't want to publish our smart pointer interfaces to the world.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/27778,732569135,2021-10-20T09:10:46Z,bazel/grpc_deps.bzl,"@@ -247,13 +247,16 @@ def grpc_deps():         )      if ""com_github_google_benchmark"" not in native.existing_rules():+        # Release 2021-09-07+        BM_COMMIT=0baacde3618ca617da95375e0af13ce1baadea47","let's not introduce the variables for BM_COMMIT and BM_SHA256 since it's inconsistent with the layout of other dependencies in this file.Btw, we do have a sanity test that looks at the grpc_deps.bzl file and checks whether the submodule versions match the bazel dependency version. I think in this particular case, it might still work, but for the sake of consistency, let's keep the old format (no variables) anyway",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27529,732794647,2021-10-20T13:45:56Z,src/core/ext/filters/client_channel/resolver/binder/binder_resolver.cc,"@@ -95,10 +89,10 @@ class BinderResolverFactory : public ResolverFactory {     static_assert(sizeof(un->sun_path) >= 101,                   ""unix socket path size is unexpectedly short"");     if (path.size() + 1 > sizeof(un->sun_path)) {-      return GRPC_ERROR_CREATE_FROM_CPP_STRING(path +-                                               "" is too long to be handled"");+      return GRPC_ERROR_CREATE_FROM_CPP_STRING(+          absl::StrCat(path, "" is too long to be handled""));     }-    strcpy(un->sun_path, path.c_str());+    strcpy(un->sun_path, std::string(path).c_str());","There's no need to create a `std::string` here; that creates an unnecessary temporary copy.  Instead, just do:```memcpy(un->sun_path, path.data(), path.size());un->sun_path[path.size()] = '\0';```",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27643,732976766,2021-10-20T17:00:11Z,src/core/lib/resource_quota/api.h,"@@ -0,0 +1,52 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_RESOURCE_QUOTA_API_H+#define GRPC_CORE_LIB_RESOURCE_QUOTA_API_H++#include <grpc/support/port_platform.h>++#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/resource_quota/resource_quota.h""++typedef struct grpc_resource_quota grpc_resource_quota;++namespace grpc_core {++// TODO(juanlishen): This is a hack. We need to do real accounting instead of+// hard coding.+constexpr size_t kResourceQuotaCallSize = 15 * 1024;+constexpr size_t kResourceQuotaChannelSize = 50 * 1024;++// Retrieve the resource quota from the channel args.+// UB if not set.+ResourceQuotaPtr ResourceQuotaFromChannelArgs(const grpc_channel_args* args);++// Take some channel args:+// If there is a resource quota set, copy args and return that.+// If there is no resource quota set, set a default, and return new+// channel args. Call grpc_channel_args_destroy on the input args.","I expect I'm going to do the full C++-ification before EOQ, and would propose that we start with putting in the conditioning parts of the design proposal there - it'll need an API change when we do the full thing, but would also significantly clean up many call sites in the interim.The idea would be that we register api-entry conditioning functions of `grpc_channel_args*->grpc_channel_args*` and provide a global config api that api entry points can use to take their grpc_channel_args and get them ready for usage by the rest of the system.I think I can turn that part around in a day, and given that this works right now, would propose just doing that immediately post landing this.",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/27787,733842360,2021-10-21T16:19:29Z,src/core/lib/security/credentials/ssl/ssl_credentials.cc,"@@ -74,7 +74,8 @@ grpc_ssl_credentials::create_security_connector(       overridden_target_name = arg->value.string;     }     if (strcmp(arg->key, GRPC_SSL_SESSION_CACHE_ARG) == 0 &&-        arg->type == GRPC_ARG_POINTER) {+        arg->type == GRPC_ARG_POINTER &&+        arg->value.pointer.vtable == grpc_ssl_session_cache_arg_vtable()) {","I noticed [this](https://github.com/grpc/grpc/blob/master/include/grpc/impl/codegen/grpc_types.h#L278-L281) comment, but could not find `grpc_ssl_session_cache_arg_vtable()` defined anywhere. Do you know how it is defined?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27794,733925229,2021-10-21T18:00:21Z,src/core/ext/xds/xds_api.h,"@@ -595,12 +595,8 @@ class XdsApi {   };   using ResourceMetadataMap =       std::map<absl::string_view /*resource_name*/, const ResourceMetadata*>;-  struct ResourceTypeMetadata {-    absl::string_view version;-    ResourceMetadataMap resource_metadata_map;-  };   using ResourceTypeMetadataMap =-      std::map<absl::string_view /*type_url*/, ResourceTypeMetadata>;+      std::map<absl::string_view /*type_url*/, ResourceMetadataMap>;","Without the per-resource version field, I think we might not need a map to communicate between xds_client and xds_api. What do you think about using a list of resource metadata instead? In this way, the data structure will be more similar to the target generic_xds_config. And we might need to add a ""type_url"" field in resource metadata.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27794,733935178,2021-10-21T18:14:47Z,src/core/ext/xds/xds_api.h,"@@ -595,12 +595,8 @@ class XdsApi {   };   using ResourceMetadataMap =       std::map<absl::string_view /*resource_name*/, const ResourceMetadata*>;-  struct ResourceTypeMetadata {-    absl::string_view version;-    ResourceMetadataMap resource_metadata_map;-  };   using ResourceTypeMetadataMap =-      std::map<absl::string_view /*type_url*/, ResourceTypeMetadata>;+      std::map<absl::string_view /*type_url*/, ResourceMetadataMap>;","That seems like a less memory-efficient data structure, since it would unnecessarily repeat the type URL for every entry.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27796,733994170,2021-10-21T19:42:28Z,src/python/grpcio_tests/tests/csds/test_csds.py,"@@ -85,7 +85,10 @@ def get_xds_config_dump(self):     def test_has_node(self):         resp = self.get_xds_config_dump()         self.assertEqual(1, len(resp.config))-        self.assertEqual(4, len(resp.config[0].xds_config))+        self.assertEqual(","Removing this check, as this test case is meant for validating the Node information.The CSDS implementation change happens in #27794.There is no mutual exclusive guarantee, a server may duplicate same resource in both fields.",
1326409,mbeards,https://api.github.com/repos/grpc/grpc/pulls/27275,734013020,2021-10-21T20:13:02Z,bazel/python_rules.bzl,"@@ -22,41 +22,75 @@ load(     ""get_plugin_args"",     ""get_proto_arguments"",     ""includes_from_deps"",+    ""is_in_virtual_imports"",+    ""is_well_known"",     ""protos_from_context"", )  _GENERATED_PROTO_FORMAT = ""{}_pb2.py"" _GENERATED_GRPC_PROTO_FORMAT = ""{}_pb2_grpc.py"" -def _generate_py_impl(context):-    protos = protos_from_context(context)-    includes = includes_from_deps(context.attr.deps)+PyProtoInfo = provider(+    ""The Python outputs from the Protobuf compiler."",+    fields = {+        ""py_info"": ""A PyInfo provider for the generated code."",+        ""generated_py_srcs"": ""The direct (not transitive) generated Python source files."",+    },+)++def _get_staged_proto_file(target, context, source_file):",Turns out it wasn't as bad as I feared it might be - PTAL.,X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27534,734030846,2021-10-21T20:41:18Z,tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py,"@@ -0,0 +1,128 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import datetime+import logging+from typing import List, Optional++from absl import flags+from absl.testing import absltest++from framework import xds_k8s_testcase+from framework.helpers import retryers+from framework.infrastructure import k8s+from framework.test_app import server_app++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient++_TD_CONFIG_RETRY_WAIT_SEC = 2+++class ApiListenerTest(xds_k8s_testcase.RegularXdsKubernetesTestCase):+    previous_route_config_version: str++    def test_api_listener(self) -> None:+        with self.subTest('00_create_health_check'):+            self.td.create_health_check()++        with self.subTest('01_create_backend_services'):+            self.td.create_backend_service()++        with self.subTest('02_create_default_url_maps'):+            self.td.create_url_map(self.server_xds_host, self.server_xds_port)++        with self.subTest('03_create_default_target_proxies'):+            self.td.create_target_proxy()++        with self.subTest('04_create_default_forwarding_rule'):+            self.td.create_forwarding_rule(self.server_xds_port)++        with self.subTest('05_start_test_servers'):+            self.test_servers: List[_XdsTestServer] = self.startTestServers()++        with self.subTest('06_add_server_backends_to_backend_services'):+            self.setupServerBackends()++        with self.subTest('07_start_test_client'):+            self.test_client: _XdsTestClient = self.startTestClient(+                self.test_servers[0])++        with self.subTest('08_test_client_xds_config_exists'):+            self.assertXdsConfigExists(self.test_client)++        with self.subTest('09_test_server_received_rpcs'):+            self.assertSuccessfulRpcs(self.test_client)++        with self.subTest('10_create_alternate_url_maps'):+            self.td.create_alternative_url_map(self.server_xds_host,+                                               self.server_xds_port)++        with self.subTest('11_create_alternate_target_proxies'):+            self.td.create_alternative_target_proxy()++        with self.subTest('12_create_alternate_forwarding_rule'):+            self.td.create_alternative_forwarding_rule(self.server_xds_port,+                                                       ip_address='10.10.10.10')++        with self.subTest('13_test_server_received_rpcs_with_two_url_maps'):+            self.assertSuccessfulRpcs(self.test_client)+            self.previous_route_config_version = self.getRouteConfigVersion(+                self.test_client)++        with self.subTest('14_delete_one_url_map_target_proxy_forwarding_rule'):+            self.td.delete_forwarding_rule()+            self.td.delete_target_grpc_proxy()+            self.td.delete_url_map()++        with self.subTest('15_test_server_continues_to_receive_rpcs'):++            retryer = retryers.constant_retryer(+                wait_fixed=datetime.timedelta(+                    seconds=_TD_CONFIG_RETRY_WAIT_SEC),+                timeout=datetime.timedelta(+                    seconds=xds_k8s_testcase._TD_CONFIG_MAX_WAIT_SEC),+                retry_on_exceptions=(TdPropagationRetryableError,),+                logger=logging,+                log_level=logging.INFO)+            try:+                retryer(self.verify_route_traffic_continues)+            except retryers.RetryError as retry_error:+                logging.info(+                    'Retry exhausted. TD routing config propagation failed after timeout %ds.',+                    xds_k8s_testcase._TD_CONFIG_MAX_WAIT_SEC)+                raise retry_error++    def verify_route_traffic_continues(self):+        self.assertSuccessfulRpcs(self.test_client)",I'd recommend against over-relying on class/object state. Potentially there can be multiple test clients. It's better to pass it as an argument to this method. Then we could even move it to the parent class so it can be reused by other tests (if it makes sense).,
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27534,734072651,2021-10-21T21:55:55Z,tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py,"@@ -0,0 +1,127 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import datetime+import logging+from typing import List, Optional++from absl import flags+from absl.testing import absltest++from framework import xds_k8s_testcase+from framework.helpers import retryers+from framework.infrastructure import k8s+from framework.test_app import server_app++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient++_TD_CONFIG_RETRY_WAIT_SEC = 2+++class ApiListenerTest(xds_k8s_testcase.RegularXdsKubernetesTestCase):+    previous_route_config_version: str++    def test_api_listener(self) -> None:+        with self.subTest('00_create_health_check'):+            self.td.create_health_check()++        with self.subTest('01_create_backend_services'):+            self.td.create_backend_service()++        with self.subTest('02_create_default_url_maps'):+            self.td.create_url_map(self.server_xds_host, self.server_xds_port)++        with self.subTest('03_create_default_target_proxies'):+            self.td.create_target_proxy()++        with self.subTest('04_create_default_forwarding_rule'):+            self.td.create_forwarding_rule(self.server_xds_port)++        with self.subTest('05_start_test_servers'):+            self.test_servers: List[_XdsTestServer] = self.startTestServers()++        with self.subTest('06_add_server_backends_to_backend_services'):+            self.setupServerBackends()++        with self.subTest('07_start_test_client'):+            self.test_client: _XdsTestClient = self.startTestClient(+                self.test_servers[0])++        with self.subTest('08_test_client_xds_config_exists'):+            self.assertXdsConfigExists(self.test_client)++        with self.subTest('09_test_server_received_rpcs'):+            self.assertSuccessfulRpcs(self.test_client)++        with self.subTest('10_create_alternate_url_maps'):+            self.td.create_alternative_url_map(self.server_xds_host,+                                               self.server_xds_port)++        with self.subTest('11_create_alternate_target_proxies'):+            self.td.create_alternative_target_proxy()++        with self.subTest('12_create_alternate_forwarding_rule'):+            self.td.create_alternative_forwarding_rule(self.server_xds_port,+                                                       ip_address='10.10.10.10')","Ok, I wasn't sure why it's the case that the port is fixed, but I ran the test and dug into xDS req/resp. As far as I understand, this test tests how one listener replaces another, right? That's why both URL maps need the same port.What confused me, is `13_test_server_received_rpcs_with_two_url_maps` - I thought two separate url maps should point to the same backend at the same time, and it'll result in two separate listeners. What I didn't realize is that since they use the same port, one immediately replaces another.That's what's happening, right? If yes, we should capture some explanation in this file.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27534,734077582,2021-10-21T22:06:03Z,tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py,"@@ -0,0 +1,128 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import datetime+import logging+from typing import List, Optional++from absl import flags+from absl.testing import absltest++from framework import xds_k8s_testcase+from framework.helpers import retryers+from framework.infrastructure import k8s",Looks like there's a few unused imports - please double check.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/27794,736044136,2021-10-25T23:50:12Z,src/core/ext/xds/xds_api.h,"@@ -595,12 +595,8 @@ class XdsApi {   };   using ResourceMetadataMap =       std::map<absl::string_view /*resource_name*/, const ResourceMetadata*>;-  struct ResourceTypeMetadata {-    absl::string_view version;-    ResourceMetadataMap resource_metadata_map;-  };   using ResourceTypeMetadataMap =-      std::map<absl::string_view /*type_url*/, ResourceTypeMetadata>;+      std::map<absl::string_view /*type_url*/, ResourceMetadataMap>;",We are going to pay the cost when we constructing the generic_xds_config proto messages. This would be a trade-off between complexity and more transient memory usage.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27643,737649225,2021-10-27T16:30:39Z,src/core/lib/resource_quota/memory_quota.h,"@@ -316,6 +351,27 @@ class MemoryOwner {   // Rebind to a different quota.   void Rebind(MemoryQuota* quota); +  // Instantaneous memory pressure in the underlying quota.+  double InstantaneousPressure() const {+    return static_cast<const GrpcMemoryAllocatorImpl*>(+               allocator_.get_internal_impl_ptr())+        ->InstantaneousPressure();+  }++  // TODO(ctiller): if this continues to live here, we should rename this class.","Okay, thanks, I understand the problem.  Yeah, let's brainstorm about this offline.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27534,737729833,2021-10-27T18:14:52Z,tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py,"@@ -0,0 +1,130 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import datetime+import logging+from typing import List, Optional++from absl import flags+from absl.testing import absltest++from framework import xds_k8s_testcase+from framework.helpers import retryers++logger = logging.getLogger(__name__)+flags.adopt_module_key_flags(xds_k8s_testcase)++# Type aliases+_XdsTestServer = xds_k8s_testcase.XdsTestServer+_XdsTestClient = xds_k8s_testcase.XdsTestClient++_TD_CONFIG_RETRY_WAIT_SEC = 2+++class ApiListenerTest(xds_k8s_testcase.RegularXdsKubernetesTestCase):++    def test_api_listener(self) -> None:+        with self.subTest('00_create_health_check'):+            self.td.create_health_check()++        with self.subTest('01_create_backend_services'):+            self.td.create_backend_service()++        with self.subTest('02_create_default_url_map'):+            self.td.create_url_map(self.server_xds_host, self.server_xds_port)++        with self.subTest('03_create_default_target_proxy'):+            self.td.create_target_proxy()++        with self.subTest('04_create_default_forwarding_rule'):+            self.td.create_forwarding_rule(self.server_xds_port)++        with self.subTest('05_start_test_servers'):+            test_servers: List[_XdsTestServer] = self.startTestServers()++        with self.subTest('06_add_server_backends_to_backend_services'):+            self.setupServerBackends()++        with self.subTest('07_start_test_client'):+            test_client: _XdsTestClient = self.startTestClient(test_servers[0])++        with self.subTest('08_test_client_xds_config_exists'):+            self.assertXdsConfigExists(test_client)++        with self.subTest('09_test_server_received_rpcs'):+            self.assertSuccessfulRpcs(test_client)++        with self.subTest('10_create_alternate_url_map'):+            self.td.create_alternative_url_map(self.server_xds_host,+                                               self.server_xds_port)++        with self.subTest('11_create_alternate_target_proxy'):+            self.td.create_alternative_target_proxy()++        # create a second suite of map+tp+fr with the same host name in host rule.+        # We set fr ip_address to be different from `0.0.0.0` and then set+        # validate_for_proxyless=false because ip:port needs to be unique.+        with self.subTest('12_create_alternate_forwarding_rule'):+            self.td.create_alternative_forwarding_rule(self.server_xds_port,+                                                       ip_address='10.10.10.10')++        with self.subTest('13_test_server_received_rpcs_with_two_url_maps'):+            self.assertSuccessfulRpcs(test_client)+            previous_route_config_version = self.getRouteConfigVersion(+                test_client)++        with self.subTest('14_delete_one_url_map_target_proxy_forwarding_rule'):+            self.td.delete_forwarding_rule()+            self.td.delete_target_grpc_proxy()+            self.td.delete_url_map()++        with self.subTest('15_test_server_continues_to_receive_rpcs'):++            retryer = retryers.constant_retryer(","Now that `verify_route_traffic_continues` test has no hard dependencies on this class, it can be moved to pulled up to `XdsKubernetesTestCase`, together with the retrying logic, and `TdPropagationRetryableError`.You can create a method: `assertRouteConfigUpdateTrafficHandoff(test_client , current_route_config_version, retry_wait, timeout)` with the retryer, and move `verify_route_traffic_continues` into a code block / context manager within it. See https://tenacity.readthedocs.io/en/latest/#retrying-code-block for example.Or you can copy `verify_route_traffic_continues` into `assertRouteConfigUpdateTrafficHandoff` as a nested function, and I'll help you with converting it to the context manager format.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/27780,738275626,2021-10-28T10:58:09Z,tools/codegen/core/gen_upb_api.sh,"@@ -120,20 +120,8 @@ proto_files=( \   ""google/api/expr/v1alpha1/checked.proto"" \   ""google/api/expr/v1alpha1/syntax.proto"" \   ""google/api/http.proto"" \-  ""google/protobuf/any.proto"" \",potential improvement (to be done as a followup): can we avoid needing to manually list all .proto files in `gen_upb_api.sh`? Sounds like we could automatically extract the list of proto files from the bazel build?,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/27780,738278149,2021-10-28T11:01:46Z,tools/codegen/core/gen_upb_api_for_bazel.py,"@@ -0,0 +1,159 @@+#!/usr/bin/env python3","the `_for_bazel.py`  name of this script is quite confusing. it's using info FROM the bazel build to generate files ubp protos the are going to be used for non-bazel builds (while the current name suggest the opposite).Since you're also using the `bazel query --output xml` command as the source of metadata, let's use `_from_bazel_xml.py` suffix (consistent with `extract_metadata_from_bazel_xml.py`)",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/27780,738630155,2021-10-28T18:01:50Z,tools/codegen/core/gen_upb_api.sh,"@@ -120,20 +120,8 @@ proto_files=( \   ""google/api/expr/v1alpha1/checked.proto"" \   ""google/api/expr/v1alpha1/syntax.proto"" \   ""google/api/http.proto"" \-  ""google/protobuf/any.proto"" \",The goal of this journey includes removing this list entirely.,X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27643,738921226,2021-10-29T04:15:42Z,src/core/lib/resource_quota/api.cc,"@@ -0,0 +1,105 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include ""src/core/lib/resource_quota/api.h""++#include <grpc/grpc.h>++#include ""src/core/lib/gpr/useful.h""++namespace grpc_core {++ResourceQuotaPtr ResourceQuotaFromChannelArgs(const grpc_channel_args* args) {+  return grpc_channel_args_find_pointer<ResourceQuota>(args,+                                                       GRPC_ARG_RESOURCE_QUOTA)+      ->Ref();+}++namespace {+grpc_arg MakeArg(ResourceQuota* quota) {+  grpc_arg arg;+  arg.type = GRPC_ARG_POINTER;+  arg.key = const_cast<char*>(GRPC_ARG_RESOURCE_QUOTA);+  arg.value.pointer.p = quota;+  arg.value.pointer.vtable = grpc_resource_quota_arg_vtable();+  return arg;+}+}  // namespace++grpc_channel_args* EnsureResourceQuotaInChannelArgs(+    const grpc_channel_args* args) {+  if (grpc_channel_args_find(args, GRPC_ARG_RESOURCE_QUOTA) != nullptr) {+    return grpc_channel_args_copy(args);+  }+  // If there's no existing quota, add it to the default one - shared between+  // all channel args declared thusly. This prevents us from accidentally not+  // sharing subchannels due to their channel args not specifying a quota.+  auto new_arg = MakeArg(DefaultResourceQuota().release());+  return grpc_channel_args_copy_and_add(args, &new_arg, 1);+}++grpc_channel_args* ChannelArgsWrappingResourceQuota(+    ResourceQuotaPtr resource_quota) {+  auto new_arg = MakeArg(resource_quota.release());+  return grpc_channel_args_copy_and_add(nullptr, &new_arg, 1);+}++}  // namespace grpc_core++extern ""C"" const grpc_arg_pointer_vtable* grpc_resource_quota_arg_vtable() {+  static const grpc_arg_pointer_vtable vtable = {+      // copy+      [](void* p) -> void* {+        return static_cast<grpc_core::ResourceQuota*>(p)->Ref().release();+      },+      // destroy+      [](void* p) { static_cast<grpc_core::ResourceQuota*>(p)->Unref(); },+      // compare+      [](void* p, void* q) { return grpc_core::QsortCompare(p, q); }};+  return &vtable;+}++extern ""C"" grpc_resource_quota* grpc_resource_quota_create(const char* name) {+  static std::atomic<uintptr_t> anonymous_counter{0};+  std::string quota_name =+      name == nullptr+          ? absl::StrCat(""anonymous-quota-"", anonymous_counter.fetch_add(1))+          : name;+  return reinterpret_cast<grpc_resource_quota*>(","I don't think the grpc_server approach is lightweight enough. It doesn't show up in profiles yet, but we have bigger problems, and eventually I have ambitions of making it the problem - let's see.What do you think about an approach like this one: https://godbolt.org/z/dxPT7d4hc where we define a helper type that explicitly labels that this is the C++ implementation of a C type, and can the casts inside of there?",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27643,738925177,2021-10-29T04:30:37Z,src/core/lib/resource_quota/resource_quota.h,"@@ -24,21 +24,29 @@ namespace grpc_core {  class ResourceQuota : public RefCounted<ResourceQuota> {  public:-  ResourceQuota();+  explicit ResourceQuota(std::string name);   ~ResourceQuota() override;    ResourceQuota(const ResourceQuota&) = delete;   ResourceQuota& operator=(const ResourceQuota&) = delete; -  std::shared_ptr<MemoryQuota> memory_quota() { return memory_quota_; }+  MemoryQuotaPtr memory_quota() { return memory_quota_; }    const RefCountedPtr<ThreadQuota>& thread_quota() { return thread_quota_; }   private:-  std::shared_ptr<MemoryQuota> memory_quota_;+  MemoryQuotaPtr memory_quota_;   RefCountedPtr<ThreadQuota> thread_quota_; }; +using ResourceQuotaPtr = RefCountedPtr<ResourceQuota>;","I think we should optimize for maintainability: that includes readability, but it also includes being able to make changes when we need to.Ref counting (for both RefCountedPtr and shared_ptr) often include both a type in the pointed-to derivation (enable_shared_from_this or one of the RefCounted types) and a type of pointer wrapping it, so I don't find it unreasonable to define both types in one place -- if I need to change later, or tweak somehow, at least I have a fighting chance of doing so -- especially for types whos usage straddles repository boundaries, like these do (I'm trying to get this change finished so I can start the substantial internal changes that need to go along with this).By defining a type alias I'm not repeating part of my type declaration all over the codebase making it easier to work with in the future.For the most part in decades of deploying this idiom I've had tooling that can expand a typedef by hitting some keystroke or hovering my mouse pointer... I've attached a screengrab from vscode with what I see:![image](https://user-images.githubusercontent.com/10120821/139375617-d7d139da-0a4f-44ea-89e8-a3a659038771.png)I seem to recall you're a vim user - would neovim + clangd help mitigate some of the problems you've been seeing (and maybe help with any Envoy work you've been doing?)Alternatively, would coming up with a naming scheme help a little too, to mitigate the ref counted vs unique problems you raise?",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27835,739388101,2021-10-29T16:40:19Z,test/core/end2end/fuzzers/api_fuzzer_corpus/crashy-crash-pants,"@@ -0,0 +1,19 @@+actions {","I object to naming this `crashy-crash-pants`. I was going to use that name later ;-)Maybe name it something more descriptive, though? Like `crash-on-unknown-compression-algorithm`",
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/27615,739454816,2021-10-29T18:33:44Z,src/core/tsi/ssl_transport_security.cc,"@@ -323,6 +323,30 @@ static tsi_result peer_property_from_x509_common_name(   return result; } +/* Gets the subject of an X509 cert as a tsi_peer_property. */+static tsi_result peer_property_from_x509_subject(X509* cert,+                                                  tsi_peer_property* property) {+  X509_NAME* subject_name = X509_get_subject_name(cert);+  if (subject_name == nullptr) {+    gpr_log(GPR_INFO, ""Could not get subject name from certificate."");+    return TSI_NOT_FOUND;+  }+  BIO* bio = BIO_new(BIO_s_mem());+  X509_NAME_print_ex(bio, subject_name, 0, XN_FLAG_RFC2253);+  char* contents;+  long len = BIO_get_mem_data(bio, &contents);","Here contents points to the start of the memory bio.https://www.openssl.org/docs/manmaster/man3/BIO_get_mem_data.htmlSo we can't invoke free, until we construct peer property.",
303201,JamesNK,https://api.github.com/repos/grpc/grpc/pulls/27887,740722259,2021-11-02T05:03:15Z,src/csharp/Grpc.Core.Api/AsyncCallState.cs,"@@ -64,21 +64,21 @@ internal struct AsyncCallState         internal Task<Metadata> ResponseHeadersAsync()         {             var withState = responseHeadersAsync as Func<object, Task<Metadata>>;-            return withState != null ? withState(callbackState)+            return withState != null ? withState(callbackState!)",If withState is not null then callbackState has to also be not null. I chose not to use a Debug.Assert because this library targets netstandard2.0 and Debug.Assert isn't annotated there to correctly suppress the nullable warning.,X
303201,JamesNK,https://api.github.com/repos/grpc/grpc/pulls/27887,740727137,2021-11-02T05:20:07Z,src/csharp/Grpc.Core.Api/Interceptors/InterceptingCallInvoker.cs,"@@ -43,7 +43,7 @@ public InterceptingCallInvoker(CallInvoker invoker, Interceptor interceptor)         /// <summary>         /// Intercepts a simple blocking call with the registered interceptor.         /// </summary>-        public override TResponse BlockingUnaryCall<TRequest, TResponse>(Method<TRequest, TResponse> method, string host, CallOptions options, TRequest request)+        public override TResponse BlockingUnaryCall<TRequest, TResponse>(Method<TRequest, TResponse> method, string? host, CallOptions options, TRequest request)","Generated code passes in null - https://github.com/grpc/grpc/blob/6d287405735da0282f92fb07ab7c56aeb80abc0c/src/csharp/Grpc.Examples/MathGrpc.cs#L224There is also some Grpc.Core.Api code that passes down this code path, but I don't recall exactly where it is to link to it.",X
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/27869,740949592,2021-11-02T11:05:09Z,tools/run_tests/performance/README.md,"@@ -299,11 +299,11 @@ For more information, see the in [grpc/test-infra].  For usage examples, see the continuous integration setup defined in-[grpc_e2e_performance_gke.sh] and [grpc_e2e_performance_v2.sh].+[grpc_e2e_performance_gke.sh] and [grpc_e2e_performance_gke_experiment.sh].  [grpc/test-infra]: https://github.com/grpc/test-infra [grpc_e2e_performance_gke.sh]: ../../internal_ci/linux/grpc_e2e_performance_gke.sh-[grpc_e2e_performance_v2.sh]: ../../internal_ci/linux/grpc_e2e_performance_v2.sh+[grpc_e2e_performance_gke_experiment.sh]: ../../internal_ci/linux/grpc_e2e_performance_gke_experiment.sh.sh",Remove duplicate `.sh`.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27902,741200767,2021-11-02T15:30:12Z,src/core/lib/iomgr/work_serializer.cc,"@@ -93,50 +82,75 @@ void WorkSerializer::WorkSerializerImpl::Orphan() { }  // The thread that calls this loans itself to the work serializer so as to-// execute all the scheduled callback. This is called from within-// WorkSerializer::Run() after executing a callback immediately, and hence size_-// is at least 1.+// execute all the scheduled callbacks. void WorkSerializer::WorkSerializerImpl::DrainQueue() {+  if (GRPC_TRACE_FLAG_ENABLED(grpc_work_serializer_trace)) {+    gpr_log(GPR_INFO, ""WorkSerializer::DrainQueue() %p"", this);+  }   while (true) {-    if (GRPC_TRACE_FLAG_ENABLED(grpc_work_serializer_trace)) {-      gpr_log(GPR_INFO, ""WorkSerializer::DrainQueue() %p"", this);-    }-    size_t prev_size = size_.fetch_sub(1);-    GPR_DEBUG_ASSERT(prev_size >= 1);-    // It is possible that while draining the queue, one of the callbacks ended-    // up orphaning the work serializer. In that case, delete the object.-    if (prev_size == 1) {-      if (GRPC_TRACE_FLAG_ENABLED(grpc_work_serializer_trace)) {-        gpr_log(GPR_INFO, ""  Queue Drained. Destroying"");+    // Mark the work serializer as draining as long as there is atleast one+    // callback.+    while (true) {","Hmm... Can we maybe do something similar to what we do in `DualRefCounted<>`, where we encode both pieces of information in a single atomic (see [here](https://github.com/grpc/grpc/blob/a923eff71fc79563d40a55ced17b5130e7805cca/src/core/lib/gprpp/dual_ref_counted.h#L242))?  I'm thinking something like this:```// First 32 bits indicate ownership of the WorkSerializer, next 32 bits are queue size (i.e., refs).// (Can adjust which bits go where -- probably don't need 32 bits for ownership.)static uint64_t MakeRefPair(uint32_t owners, uint32_t size) {  return (static_cast<uint64_t>(owners) << 32) + static_cast<int64_t>(size);}static uint32_t GetOwners(uint64_t ref_pair) {  return static_cast<uint32_t>(ref_pair >> 32);}static uint32_t GetSize(uint64_t ref_pair) {  return static_cast<uint32_t>(ref_pair & 0xffffffffu);}```In `Run()`, we can do something like this:```// Increment size and attempt to take ownership of the WorkSerializer.const uint64_t prev_ref_pair =    refs_.fetch_add(MakeRefPair(1, 1));if (GetOwners(prev_ref_pair) > 1) {  // Another thread is holding the WorkSerializer, so decrement the ownership count we  // just added and queue the callback.  refs_.fetch_add(MakeRefPair(-1, 0));  // ...add to queue...  return;}// We took ownership of the WorkSerializer.// Invoke callback and drain queue.callback();DrainQueueOwned();```In `DrainQueue()`, we can do something like this:```// Attempt to take ownership of the WorkSerializer.const uint64_t prev_ref_pair =    refs_.fetch_add(MakeRefPair(1, 0));if (GetOwners(prev_ref_pair) > 1) {  // Another thread is holding the WorkSerializer, so decrement the ownership count we  // just added and let the other thread do the work.  refs_.fetch_add(MakeRefPair(-1, 0));  return;}// We took ownership, so drain the queue.DrainQueueOwned();```And `DrainQueueOwned()` can do something like this:```while (true) {  prev_ref_pair = refs_.fetch_add(MakeRefPair(0, -1));  // It is possible that while draining the queue, one of the callbacks ended  // up orphaning the work serializer. In that case, delete the object.  if (GetSize(prev_ref_pair) == 1) {    delete this;    return;  }  if (GetSize(prev_ref_pair) == 2) {    // Queue drained.  Given up ownership, but only if queue remains empty.    uint64_t expected = MakeRefPair(1, 1);    if (refs_.compare_exchange_strong(expected, MakeRefPair(0, 1),                                      /* TODO: not sure what memory order to use here */)) {      return;    }    // ...get next element from queue and execute it...  }}```",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27770,741473130,2021-11-02T21:25:27Z,src/core/lib/slice/slice.h,"@@ -0,0 +1,308 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_SLICE_SLICE_H+#define GRPC_CORE_LIB_SLICE_SLICE_H++#include <grpc/support/port_platform.h>++#include ""absl/strings/string_view.h""++#include <grpc/slice.h>++#include ""src/core/lib/slice/slice_internal.h""++// Herein lies grpc_core::Slice and its team of thin wrappers around grpc_slice.+// They aim to keep you safe by providing strong guarantees around lifetime and+// mutability.+//+// The team:+//   Slice        - provides a wrapper around an unknown type of slice.+//                  Immutable (since we don't know who else might be referencing+//                  it), and potentially ref counted.+//   StaticSlice  - provides a wrapper around a static slice. Not refcounted,+//                  fast to copy.+//   MutableSlice - provides a guarantee of unique ownership, meaning the+//                  underlying data can be mutated safely.++namespace grpc_core {++// Forward declarations+class Slice;+class StaticSlice;+class MutableSlice;++namespace slice_detail {++// Returns an empty slice.+static constexpr grpc_slice EmptySlice() { return {nullptr, {}}; }++// BaseSlice holds the grpc_slice object, but does not apply refcounting policy.+// It does export immutable access into the slice, such that this can be shared+// by all storage policies.+class BaseSlice {+ public:+  BaseSlice(const BaseSlice&) = delete;+  BaseSlice& operator=(const BaseSlice&) = delete;+  BaseSlice(BaseSlice&& other) = delete;+  BaseSlice& operator=(BaseSlice&& other) = delete;++  // Iterator access to the underlying bytes+  const uint8_t* begin() const { return GRPC_SLICE_START_PTR(this->slice_); }+  const uint8_t* end() const { return GRPC_SLICE_END_PTR(this->slice_); }+  const uint8_t* cbegin() const { return GRPC_SLICE_START_PTR(this->slice_); }+  const uint8_t* cend() const { return GRPC_SLICE_END_PTR(this->slice_); }++  // Retrieve a borrowed reference to the underlying grpc_slice.+  const grpc_slice& c_slice() const { return this->slice_; }++  // Retrieve the underlying grpc_slice, and replace the one in this object with+  // EmptySlice().+  grpc_slice TakeCSlice() {+    grpc_slice out = this->slice_;+    this->slice_ = EmptySlice();+    return out;+  }++  // As other things... borrowed references.+  absl::string_view as_string_view() const {+    return absl::string_view(reinterpret_cast<const char*>(data()), size());+  }++  // Array access+  uint8_t operator[](size_t i) const {+    return GRPC_SLICE_START_PTR(this->slice_)[i];+  }++  // Access underlying data+  const uint8_t* data() const { return GRPC_SLICE_START_PTR(this->slice_); }++  // Size of the slice+  size_t size() const { return GRPC_SLICE_LENGTH(this->slice_); }+  size_t length() const { return size(); }+  bool empty() const { return size() == 0; }++  // For inlined slices - are these two slices equal?+  // For non-inlined slices - do these two slices refer to the same block of+  // memory?+  bool is_equivalent(const BaseSlice& other) const {+    return grpc_slice_is_equivalent(slice_, other.slice_);+  }++ protected:+  BaseSlice() : slice_(EmptySlice()) {}+  explicit BaseSlice(const grpc_slice& slice) : slice_(slice) {}+  ~BaseSlice() = default;+  grpc_slice slice_;","Data members should be private, not protected.https://google.github.io/styleguide/cppguide.html#Access_ControlIt seems like it shouldn't be too hard to have the subclasses use methods like `c_slice()` and `TakeCSlice()` instead.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27770,741481573,2021-11-02T21:40:41Z,src/core/lib/slice/slice_refcount_base.h,"@@ -145,6 +145,12 @@ struct grpc_slice_refcount {     }   } +  // Only for type REGULAR, is this the only instance?+  bool IsRegularUnique() {","As per my comment in ref_counted.h, this seems like an unsafe pattern.  Just because the ref-count is 1 at a moment in time doesn't mean it won't get an additional ref by the time we make a decision based on that information.Are there enough cases where the refcount is 1 to be worth optimizing this case?  Is there some other way we can deal with this instead?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27770,741492596,2021-11-02T22:01:57Z,src/core/ext/filters/http/client/http_client_filter.cc,"@@ -577,17 +570,14 @@ static grpc_error_handle http_client_init_channel_elem(   chand->static_scheme = scheme_from_args(args->channel_args);   chand->max_payload_size_for_get =       max_payload_size_from_args(args->channel_args);-  chand->user_agent = grpc_mdelem_from_slices(-      GRPC_MDSTR_USER_AGENT,-      user_agent_from_args(args->channel_args,-                           args->optional_transport->vtable->name));+  new (&chand->user_agent) grpc_core::Slice(user_agent_from_args(","Instead of instantiating just this one field, we should probably call the ctor for the entire `channel_data` struct, since we're calling its dtor in `http_client_destroy_channel_elem()`.  We can move the field initialization into the ctor.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27770,741502280,2021-11-02T22:21:28Z,test/core/slice/slice_test.cc,"@@ -221,73 +227,217 @@ static void test_slice_split_tail_works(size_t length) {   grpc_slice_unref_internal(slice); } -static void test_slice_from_copied_string_works(void) {+INSTANTIATE_TEST_SUITE_P(GrpcSliceSizedTest, GrpcSliceSizedTest,+                         ::testing::ValuesIn([] {+                           std::vector<size_t> out;+                           for (size_t i = 0; i < 128; i++) {+                             out.push_back(i);+                           }+                           return out;+                         }()),+                         [](const testing::TestParamInfo<size_t>& info) {+                           return std::to_string(info.param);+                         });++TEST(GrpcSliceTest, SliceFromCopiedString) {   static const char* text = ""HELLO WORLD!"";   grpc_slice slice; -  LOG_TEST_NAME(""test_slice_from_copied_string_works"");-   slice = grpc_slice_from_copied_string(text);-  GPR_ASSERT(strlen(text) == GRPC_SLICE_LENGTH(slice));-  GPR_ASSERT(-      0 == memcmp(text, GRPC_SLICE_START_PTR(slice), GRPC_SLICE_LENGTH(slice)));+  EXPECT_EQ(strlen(text), GRPC_SLICE_LENGTH(slice));+  EXPECT_EQ(+      0, memcmp(text, GRPC_SLICE_START_PTR(slice), GRPC_SLICE_LENGTH(slice)));   grpc_slice_unref_internal(slice); } -static void test_moved_string_slice(void) {-  LOG_TEST_NAME(""test_moved_string_slice"");-+TEST(GrpcSliceTest, MovedStringSlice) {   // Small string should be inlined.   constexpr char kSmallStr[] = ""hello12345"";   char* small_ptr = strdup(kSmallStr);   grpc_slice small =       grpc_slice_from_moved_string(grpc_core::UniquePtr<char>(small_ptr));-  GPR_ASSERT(GRPC_SLICE_LENGTH(small) == strlen(kSmallStr));-  GPR_ASSERT(GRPC_SLICE_START_PTR(small) !=-             reinterpret_cast<uint8_t*>(small_ptr));+  EXPECT_EQ(GRPC_SLICE_LENGTH(small), strlen(kSmallStr));+  EXPECT_NE(GRPC_SLICE_START_PTR(small), reinterpret_cast<uint8_t*>(small_ptr));   grpc_slice_unref_internal(small);    // Large string should be move the reference.   constexpr char kSLargeStr[] = ""hello123456789123456789123456789"";   char* large_ptr = strdup(kSLargeStr);   grpc_slice large =       grpc_slice_from_moved_string(grpc_core::UniquePtr<char>(large_ptr));-  GPR_ASSERT(GRPC_SLICE_LENGTH(large) == strlen(kSLargeStr));-  GPR_ASSERT(GRPC_SLICE_START_PTR(large) ==-             reinterpret_cast<uint8_t*>(large_ptr));+  EXPECT_EQ(GRPC_SLICE_LENGTH(large), strlen(kSLargeStr));+  EXPECT_EQ(GRPC_SLICE_START_PTR(large), reinterpret_cast<uint8_t*>(large_ptr));   grpc_slice_unref_internal(large);    // Moved buffer must respect the provided length not the actual length of the   // string.   large_ptr = strdup(kSLargeStr);   small = grpc_slice_from_moved_buffer(grpc_core::UniquePtr<char>(large_ptr),                                        strlen(kSmallStr));-  GPR_ASSERT(GRPC_SLICE_LENGTH(small) == strlen(kSmallStr));-  GPR_ASSERT(GRPC_SLICE_START_PTR(small) !=-             reinterpret_cast<uint8_t*>(large_ptr));+  EXPECT_EQ(GRPC_SLICE_LENGTH(small), strlen(kSmallStr));+  EXPECT_NE(GRPC_SLICE_START_PTR(small), reinterpret_cast<uint8_t*>(large_ptr));   grpc_slice_unref_internal(small); } -void test_string_view_from_slice() {+TEST(GrpcSliceTest, StringViewFromSlice) {   constexpr char kStr[] = ""foo"";   absl::string_view sv(       grpc_core::StringViewFromSlice(grpc_slice_from_static_string(kStr)));-  GPR_ASSERT(std::string(sv) == kStr);+  EXPECT_EQ(std::string(sv), kStr);+}++namespace grpc_core {+namespace {++TEST(SliceTest, FromSmallCopiedString) {+  Slice slice = Slice::FromCopiedString(""hello"");+  EXPECT_EQ(slice[0], 'h');+  EXPECT_EQ(slice[1], 'e');+  EXPECT_EQ(slice[2], 'l');+  EXPECT_EQ(slice[3], 'l');+  EXPECT_EQ(slice[4], 'o');+  EXPECT_EQ(slice.size(), 5);+  EXPECT_EQ(slice.length(), 5);+  EXPECT_EQ(slice.as_string_view(), ""hello"");+  EXPECT_EQ(0, memcmp(slice.data(), ""hello"", 5));+}++class SliceSizedTest : public ::testing::TestWithParam<size_t> {};++std::string RandomString(size_t length) {+  std::string str;+  std::random_device r;+  for (size_t i = 0; i < length; ++i) {+    str.push_back(char(r()));+  }+  return str; } -int main(int, char**) {-  unsigned length;-  test_slice_malloc_returns_something_sensible();-  test_slice_new_returns_something_sensible();-  test_slice_new_with_user_data();-  test_slice_new_with_len_returns_something_sensible();-  for (length = 0; length < 128; length++) {-    test_slice_sub_works(length);-    test_slice_split_head_works(length);-    test_slice_split_tail_works(length);+TEST_P(SliceSizedTest, FromCopiedString) {+  const std::string str = RandomString(GetParam());+  Slice slice = Slice::FromCopiedString(str);++  EXPECT_EQ(slice.size(), str.size());+  EXPECT_EQ(slice.length(), str.size());+  EXPECT_EQ(slice.as_string_view(), str);+  EXPECT_EQ(0, memcmp(slice.data(), str.data(), str.size()));+  for (size_t i = 0; i < str.size(); ++i) {+    EXPECT_EQ(slice[i], uint8_t(str[i]));+  }++  EXPECT_TRUE(slice.is_equivalent(slice.Ref()));+  EXPECT_TRUE(slice.is_equivalent(slice.AsOwned()));+  EXPECT_TRUE(slice.is_equivalent(slice.Ref().IntoOwned()));+}++INSTANTIATE_TEST_SUITE_P(SliceSizedTest, SliceSizedTest,+                         ::testing::ValuesIn([] {+                           std::vector<size_t> out;+                           size_t i = 1;+                           size_t j = 1;+                           while (i < 1024 * 1024) {+                             out.push_back(j);+                             size_t n = i + j;+                             i = j;+                             j = n;+                           }+                           return out;+                         }()),+                         [](const testing::TestParamInfo<size_t>& info) {+                           return std::to_string(info.param);+                         });++size_t SumSlice(const Slice& slice) {+  size_t x = 0;+  for (size_t i = 0; i < slice.size(); ++i) {+    x += slice[i];   }-  test_slice_from_copied_string_works();-  test_moved_string_slice();-  test_string_view_from_slice();-  return 0;+  return x;+}++TEST(SliceTest, ExternalAsOwned) {+  std::unique_ptr<std::string> external_string(+      new std::string(RandomString(1024)));+  Slice slice(ExternallyManagedSlice(external_string->data(),+                                     external_string->length()));+  const auto initial_sum = SumSlice(slice);+  Slice owned = slice.AsOwned();+  EXPECT_EQ(initial_sum, SumSlice(owned));+  external_string.reset();+  // In ASAN (where we can be sure that it'll crash), go ahead and read the+  // bytes we just deleted.+#if defined(__has_feature)","Maybe use `BuiltUnderAsan()` here, since it covers a few more cases?https://github.com/grpc/grpc/blob/79d684529d9db60aa2c5e82deb43e297a6818cfb/test/core/util/test_config.h#L44",
133680,sampajano,https://api.github.com/repos/grpc/grpc/pulls/27908,741502311,2021-11-02T22:21:32Z,src/objective-c/GRPCClient/GRPCTypes.h,"@@ -23,7 +23,7 @@  * Note that a few of these are never produced by the gRPC libraries, but are of  * general utility for server applications to produce.  */-typedef NS_ENUM(NSUInteger, GRPCErrorCode) {+typedef NS_ENUM(int32_t, GRPCErrorCode) {","Ok thanks :)(In general, for the sake of API consistency i'd prefer our APIs to use similar types if possible.. But it's fine with me since you're going to do a bunch of follow-ups soon :))",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/27902,741505013,2021-11-02T22:27:04Z,src/core/lib/iomgr/work_serializer.cc,"@@ -55,17 +56,20 @@ void WorkSerializer::WorkSerializerImpl::Run(     gpr_log(GPR_INFO, ""WorkSerializer::Run() %p Scheduling callback [%s:%d]"",             this, location.file(), location.line());   }-  const size_t prev_size = size_.fetch_add(1);+  const size_t prev_size = size_.fetch_add(1, std::memory_order_acq_rel);   // The work serializer should not have been orphaned.   GPR_DEBUG_ASSERT(prev_size > 0);-  if (prev_size == 1) {-    // There is no other closure executing right now on this work serializer.-    // Execute this closure immediately.+  // If there is no other callback executing right now on this work serializer,+  // try to grab the lock and execute the callback immediately.+  bool expected = false;+  if (prev_size == 1 && draining_.compare_exchange_strong(expected, true)) {",That is similar to Mark's comment. Done!,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27846,741511470,2021-11-02T22:41:35Z,src/core/lib/address_utils/parse_address.h,"@@ -61,6 +61,11 @@ bool grpc_parse_ipv6_hostport(absl::string_view hostport, /* Converts named or numeric port to a uint16 suitable for use in a sockaddr. */ uint16_t grpc_strhtons(const char* port); +// Newer form of grpc_string_to_sockaddr which returns an error instead of+// crashing if \a addr is not IPv6/IPv6+grpc_error_handle grpc_string_to_sockaddr(grpc_resolved_address* out,","Out of curiosity, was there a dependency reason why you moved this from sockaddr_utils.h to parse_address.h?I agree that it really belongs here, since this is where we have all of the other functions that convert from other forms to `grpc_resolved_address`.  I'm just curious if there was a reason other than organization to change it.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27846,741514248,2021-11-02T22:48:21Z,src/core/lib/address_utils/sockaddr_utils.cc,"@@ -29,16 +29,20 @@ #include ""absl/strings/str_cat.h"" #include ""absl/strings/str_format.h"" -#include <grpc/event_engine/event_engine.h> #include <grpc/support/alloc.h> #include <grpc/support/log.h>  #include ""src/core/lib/gpr/string.h"" #include ""src/core/lib/gprpp/host_port.h""-#include ""src/core/lib/iomgr/event_engine/resolved_address_internal.h"" #include ""src/core/lib/iomgr/sockaddr.h"" #include ""src/core/lib/iomgr/socket_utils.h""-#include ""src/core/lib/iomgr/unix_sockets_posix.h""++#ifdef GRPC_HAVE_UNIX_SOCKET+#include <sys/un.h>+#endif++std::string grpc_sockaddr_to_uri_unix_if_possible(","Seems like this declaration should be in sockaddr_utils.h instead of staying in src/core/lib/iomgr/unix_sockets_posix.h, since the implementation moved here.Also, similar question to above: Just for my understanding, was there a reason other than organization to move this here?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27911,741526840,2021-11-02T23:19:32Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -1318,29 +1318,21 @@ class XdsEnd2endTest : public ::testing::TestWithParam<TestType> {   }    bool WaitForNack(-      std::function<AdsServiceImpl::ResponseState::State()> get_state,-      StatusCode expected_status = StatusCode::UNAVAILABLE) {","Yes, tests should explicitly check that.  And allowing them to specify what RPC response code to expect does that.  I don't see what benefit we gain by requiring them to send a separate RPC to check this when we can just check the status code of the RPCs that we're already sending.Moreover, even if we agreed that that was the right thing to do, removing this parameter means that all existing tests that are calling `WaitForNack()`, either directly or indirectly, are no longer verifying what they were before, so this is basically breaking existing tests.  To avoid that, you would need to look at all tests that are currently calling `WaitForNack()` and change them to verify the RPC status code separately.Stepping back, I think the real problem here is that we're starting the server before we set the new resources on the xDS server, which means that the server may see the default resources when it starts instead of the ones that we actually want to start the test with.  If you look at the `UseOriginalDstNotSupported` test, the test itself *should* cause the RPCs to be failed with UNAVAILABLE.  The fact that that's not happening is an artifact of the strange way that we set up the test framework, so I think that's the actual problem here.I think the right way to solve this problem is to address this TODO:https://github.com/grpc/grpc/blob/826b888e75038d00d7d13a5679613a7de66dcb95/test/cpp/end2end/xds/xds_end2end_test.cc#L671Each test should explicitly start up the set of backends it wants to run, which means that each test should be able to avoid starting the backend until after it has set the appropriate resources on the xDS server.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27902,741532728,2021-11-02T23:35:52Z,src/core/lib/iomgr/work_serializer.cc,"@@ -93,39 +128,60 @@ void WorkSerializer::WorkSerializerImpl::Orphan() { }  // The thread that calls this loans itself to the work serializer so as to-// execute all the scheduled callback. This is called from within-// WorkSerializer::Run() after executing a callback immediately, and hence size_-// is at least 1.+// execute all the scheduled callbacks. void WorkSerializer::WorkSerializerImpl::DrainQueue() {+  if (GRPC_TRACE_FLAG_ENABLED(grpc_work_serializer_trace)) {+    gpr_log(GPR_INFO, ""WorkSerializer::DrainQueue() %p"", this);+  }+  // Attempt to take ownership of the WorkSerializer.+  const uint64_t prev_ref_pair =+      refs_.fetch_add(MakeRefPair(1, 1), std::memory_order_acq_rel);+  if (GetOwners(prev_ref_pair) == 0) {+    // We took ownership of the WorkSerializer. Drain the queue.+    DrainQueueOwned();+  } else {+    // Another thread is holding the WorkSerializer, so decrement the ownership+    // count we just added and queue a no-op callback.+    refs_.fetch_sub(MakeRefPair(1, 0), std::memory_order_acq_rel);+    CallbackWrapper* cb_wrapper = new CallbackWrapper([]() {}, DEBUG_LOCATION);","Can we avoid an allocation here by using a sentinel value?  For example, we could add an `MultiProducerSingleConsumerQueue::Node sentinel_node` data member to `WorkSerializerImpl` and push that onto the queue, and then have `DrainQueueOwned()` check for that value and ignore it.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/27874,741556794,2021-11-03T00:54:38Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -87,6 +87,7 @@ def setUpClass(cls):         # GCP         cls.project: str = xds_flags.PROJECT.value         cls.network: str = xds_flags.NETWORK.value+        cls.config_scope: str = xds_flags.ROUTER_SCOPE.value","Since this is defined on the top level, let's add it to `initKubernetesClientRunner ` of all child classes, including security. Just to spare someone a debugging in the future :)Especially that `client-secure.deployment.yaml` has this field. ",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27846,741598920,2021-11-03T03:36:07Z,src/core/lib/address_utils/sockaddr_utils.cc,"@@ -29,16 +29,20 @@ #include ""absl/strings/str_cat.h"" #include ""absl/strings/str_format.h"" -#include <grpc/event_engine/event_engine.h> #include <grpc/support/alloc.h> #include <grpc/support/log.h>  #include ""src/core/lib/gpr/string.h"" #include ""src/core/lib/gprpp/host_port.h""-#include ""src/core/lib/iomgr/event_engine/resolved_address_internal.h"" #include ""src/core/lib/iomgr/sockaddr.h"" #include ""src/core/lib/iomgr/socket_utils.h""-#include ""src/core/lib/iomgr/unix_sockets_posix.h""++#ifdef GRPC_HAVE_UNIX_SOCKET+#include <sys/un.h>+#endif++std::string grpc_sockaddr_to_uri_unix_if_possible(","Again layering/dependency management: the rest of the unix_sockets code deals with file descriptors and event management, and so pulls in event engine or most of iomgr... The address handling code doesn't need that, so it gives a firm direction arrow for the dependencies.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27770,741971559,2021-11-03T14:03:44Z,src/core/lib/gprpp/ref_counted.h,"@@ -192,6 +192,8 @@ class RefCount {     return prior == 1;   } +  bool IsOne() const { return get() == 1; }","My point is that depending on having a unique reference seems inherently unsafe in the first place.  What guarantee do we have that another thread won't add a ref between when we check and when we act on that information?What I'm saying is that (a) I don't think this is safe in the general case and therefore should not be part of this API and (b) even in the specific case of slices, it seems like a bad idea even if we currently don't have any cases where it will cause a problem, because I don't think we have any structural guarantee that that will remain true.In what case do we wind up with a single ref and we know that no other thread will be reffing it?  How common is that case, and how significant is the performance penalty for copying it?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27643,742048072,2021-11-03T15:16:41Z,src/core/lib/resource_quota/memory_quota.h,"@@ -333,53 +333,44 @@ class GrpcMemoryAllocatorImpl final : public EventEngineMemoryAllocatorImpl { // MemoryOwner's lifetime, and are not queryable, so passing a MemoryOwner to a // new owning module means that module cannot reason about which reclaimers are // active, nor what they might do.-class MemoryOwner final {+class MemoryOwner final : public MemoryAllocator {  public:   MemoryOwner() = default;    explicit MemoryOwner(std::shared_ptr<GrpcMemoryAllocatorImpl> allocator)-      : allocator_(std::move(allocator)) {}--  MemoryAllocator* allocator() { return &allocator_; }--  /// Drop the underlying allocator and make this an empty object.-  /// The object will not be usable after this call unless it's a valid-  /// allocator is moved into it.-  /// Also resets the MemoryAllocator instance.-  void Reset() { allocator_.Reset(); }+      : MemoryAllocator(std::move(allocator)) {}    // Post a reclaimer for some reclamation pass.   void PostReclaimer(ReclamationPass pass, ReclamationFunction fn) {-    static_cast<GrpcMemoryAllocatorImpl*>(allocator_.get_internal_impl_ptr())-        ->PostReclaimer(pass, std::move(fn));+    impl()->PostReclaimer(pass, std::move(fn));   }    // Rebind to a different quota.   void Rebind(MemoryQuota* quota);    // Instantaneous memory pressure in the underlying quota.   double InstantaneousPressure() const {-    return static_cast<const GrpcMemoryAllocatorImpl*>(-               allocator_.get_internal_impl_ptr())-        ->InstantaneousPressure();+    return impl()->InstantaneousPressure();   }    // TODO(ctiller): if this continues to live here, we should rename this class.   // Otherwise we should subclass MemoryAllocator.   template <typename T, typename... Args>   OrphanablePtr<T> MakeOrphanable(Args&&... args) {-    return OrphanablePtr<T>(allocator_.New<T>(std::forward<Args>(args)...));+    return OrphanablePtr<T>(New<T>(std::forward<Args>(args)...));   }    // Name of this object-  absl::string_view name() const {-    return static_cast<const GrpcMemoryAllocatorImpl*>(-               allocator_.get_internal_impl_ptr())-        ->name();+  absl::string_view name() const { return impl()->name(); }++ protected:",Do we expect `MemoryOwner` to be subclassed?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27643,742056724,2021-11-03T15:24:33Z,src/core/lib/resource_quota/api.h,"@@ -0,0 +1,52 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_RESOURCE_QUOTA_API_H+#define GRPC_CORE_LIB_RESOURCE_QUOTA_API_H++#include <grpc/support/port_platform.h>++#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/resource_quota/resource_quota.h""++typedef struct grpc_resource_quota grpc_resource_quota;++namespace grpc_core {++// TODO(juanlishen): This is a hack. We need to do real accounting instead of+// hard coding.+constexpr size_t kResourceQuotaCallSize = 15 * 1024;+constexpr size_t kResourceQuotaChannelSize = 50 * 1024;++// Retrieve the resource quota from the channel args.+// UB if not set.+ResourceQuotaPtr ResourceQuotaFromChannelArgs(const grpc_channel_args* args);++// Take some channel args:+// If there is a resource quota set, copy args and return that.+// If there is no resource quota set, set a default, and return new+// channel args. Call grpc_channel_args_destroy on the input args.","I definitely like your ideas for the C++ channel args API.  But I also don't see any real down-side of changing this to work in a more ergonomic way, even if it won't live very long.  But I won't insist.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27643,742070612,2021-11-03T15:38:00Z,src/core/lib/resource_quota/api.cc,"@@ -47,13 +44,13 @@ grpc_channel_args* EnsureResourceQuotaInChannelArgs(   // If there's no existing quota, add it to the default one - shared between   // all channel args declared thusly. This prevents us from accidentally not   // sharing subchannels due to their channel args not specifying a quota.-  auto new_arg = MakeArg(DefaultResourceQuota().release());+  auto new_arg = MakeArg(ResourceQuota::Default().get());",This is only used in tests and will likely go away entirely once the channel args conditioning code comes in (which is gonna be shortly after this PR lands),
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27643,742072894,2021-11-03T15:40:16Z,src/core/lib/resource_quota/api.h,"@@ -0,0 +1,52 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_RESOURCE_QUOTA_API_H+#define GRPC_CORE_LIB_RESOURCE_QUOTA_API_H++#include <grpc/support/port_platform.h>++#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/resource_quota/resource_quota.h""++typedef struct grpc_resource_quota grpc_resource_quota;++namespace grpc_core {++// TODO(juanlishen): This is a hack. We need to do real accounting instead of+// hard coding.+constexpr size_t kResourceQuotaCallSize = 15 * 1024;+constexpr size_t kResourceQuotaChannelSize = 50 * 1024;++// Retrieve the resource quota from the channel args.+// UB if not set.+ResourceQuotaPtr ResourceQuotaFromChannelArgs(const grpc_channel_args* args);++// Take some channel args:+// If there is a resource quota set, copy args and return that.+// If there is no resource quota set, set a default, and return new+// channel args. Call grpc_channel_args_destroy on the input args.","It's just iteration cost, and it's the exact same code as I'm going to want to edit in a week to get the args conditioning code in, so I'm going to wait.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27770,742089183,2021-11-03T15:56:08Z,src/core/lib/gprpp/ref_counted.h,"@@ -192,6 +192,8 @@ class RefCount {     return prior == 1;   } +  bool IsOne() const { return get() == 1; }","So I think this check ends up being necessary to have an efficient and safe mutation API.What makes this code safe right now:- `Slice` does not offer a thread-safe mutation API - if you change a `Slice` instance, it's on you to guarantee that no other threads can access that instance (the normal value type semantics)- we only perform this check from refcounted slices inside mutating functions from `Slice` instances that own the refThat we're calling a mutating function makes it illegal to take a ref of this particular `Slice` instance from another thread, and since we prove that there's *only* one ref, there cannot be another instance that could ref this from another thread.It seems the prudent thing to do is fork `RefCounted` and have a special variant for `Slice` given your API objections.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/27770,742092456,2021-11-03T15:59:16Z,src/core/lib/slice/slice.h,"@@ -0,0 +1,308 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_SLICE_SLICE_H+#define GRPC_CORE_LIB_SLICE_SLICE_H++#include <grpc/support/port_platform.h>++#include ""absl/strings/string_view.h""++#include <grpc/slice.h>++#include ""src/core/lib/slice/slice_internal.h""++// Herein lies grpc_core::Slice and its team of thin wrappers around grpc_slice.+// They aim to keep you safe by providing strong guarantees around lifetime and+// mutability.+//+// The team:+//   Slice        - provides a wrapper around an unknown type of slice.+//                  Immutable (since we don't know who else might be referencing+//                  it), and potentially ref counted.+//   StaticSlice  - provides a wrapper around a static slice. Not refcounted,+//                  fast to copy.+//   MutableSlice - provides a guarantee of unique ownership, meaning the+//                  underlying data can be mutated safely.++namespace grpc_core {++// Forward declarations+class Slice;+class StaticSlice;+class MutableSlice;++namespace slice_detail {++// Returns an empty slice.+static constexpr grpc_slice EmptySlice() { return {nullptr, {}}; }++// BaseSlice holds the grpc_slice object, but does not apply refcounting policy.+// It does export immutable access into the slice, such that this can be shared+// by all storage policies.+class BaseSlice {+ public:+  BaseSlice(const BaseSlice&) = delete;+  BaseSlice& operator=(const BaseSlice&) = delete;+  BaseSlice(BaseSlice&& other) = delete;+  BaseSlice& operator=(BaseSlice&& other) = delete;++  // Iterator access to the underlying bytes+  const uint8_t* begin() const { return GRPC_SLICE_START_PTR(this->slice_); }+  const uint8_t* end() const { return GRPC_SLICE_END_PTR(this->slice_); }+  const uint8_t* cbegin() const { return GRPC_SLICE_START_PTR(this->slice_); }+  const uint8_t* cend() const { return GRPC_SLICE_END_PTR(this->slice_); }++  // Retrieve a borrowed reference to the underlying grpc_slice.+  const grpc_slice& c_slice() const { return this->slice_; }++  // Retrieve the underlying grpc_slice, and replace the one in this object with+  // EmptySlice().+  grpc_slice TakeCSlice() {+    grpc_slice out = this->slice_;+    this->slice_ = EmptySlice();+    return out;+  }++  // As other things... borrowed references.+  absl::string_view as_string_view() const {+    return absl::string_view(reinterpret_cast<const char*>(data()), size());+  }++  // Array access+  uint8_t operator[](size_t i) const {+    return GRPC_SLICE_START_PTR(this->slice_)[i];+  }++  // Access underlying data+  const uint8_t* data() const { return GRPC_SLICE_START_PTR(this->slice_); }++  // Size of the slice+  size_t size() const { return GRPC_SLICE_LENGTH(this->slice_); }+  size_t length() const { return size(); }+  bool empty() const { return size() == 0; }++  // For inlined slices - are these two slices equal?+  // For non-inlined slices - do these two slices refer to the same block of+  // memory?+  bool is_equivalent(const BaseSlice& other) const {+    return grpc_slice_is_equivalent(slice_, other.slice_);+  }++ protected:+  BaseSlice() : slice_(EmptySlice()) {}+  explicit BaseSlice(const grpc_slice& slice) : slice_(slice) {}+  ~BaseSlice() = default;+  grpc_slice slice_;+};++inline bool operator==(const BaseSlice& a, const BaseSlice& b) {+  return grpc_slice_eq(a.c_slice(), b.c_slice()) != 0;+}++inline bool operator!=(const BaseSlice& a, const BaseSlice& b) {+  return grpc_slice_eq(a.c_slice(), b.c_slice()) == 0;+}++inline bool operator==(const BaseSlice& a, absl::string_view b) {+  return a.as_string_view() == b;+}++inline bool operator!=(const BaseSlice& a, absl::string_view b) {+  return a.as_string_view() != b;+}++inline bool operator==(absl::string_view a, const BaseSlice& b) {+  return a == b.as_string_view();+}++inline bool operator!=(absl::string_view a, const BaseSlice& b) {+  return a != b.as_string_view();+}++inline bool operator==(const BaseSlice& a, const grpc_slice& b) {+  return grpc_slice_eq(a.c_slice(), b) != 0;+}++inline bool operator!=(const BaseSlice& a, const grpc_slice& b) {+  return grpc_slice_eq(a.c_slice(), b) == 0;+}++inline bool operator==(const grpc_slice& a, const BaseSlice& b) {+  return grpc_slice_eq(a, b.c_slice()) != 0;+}++inline bool operator!=(const grpc_slice& a, const BaseSlice& b) {+  return grpc_slice_eq(a, b.c_slice()) == 0;+}++template <typename Out>+struct CopyConstructors {+  static Out FromCopiedString(const char* s) {+    return Out(grpc_slice_from_copied_string(s));+  }+  static Out FromCopiedString(std::string s) {+    return Out(grpc_slice_from_cpp_string(std::move(s)));+  }+};++}  // namespace slice_detail++class StaticSlice : public slice_detail::BaseSlice {+ public:+  StaticSlice() = default;+  explicit StaticSlice(const grpc_slice& slice)+      : slice_detail::BaseSlice(slice) {+    GPR_DEBUG_ASSERT(+        slice.refcount->GetType() == grpc_slice_refcount::Type::STATIC ||+        slice.refcount->GetType() == grpc_slice_refcount::Type::NOP);+  }+  explicit StaticSlice(const StaticMetadataSlice& slice)+      : slice_detail::BaseSlice(slice) {}++  static StaticSlice FromStaticString(const char* s) {+    return StaticSlice(grpc_slice_from_static_string(s));+  }++  StaticSlice(const StaticSlice& other)+      : slice_detail::BaseSlice(other.slice_) {}+  StaticSlice& operator=(const StaticSlice& other) {+    slice_ = other.slice_;+    return *this;+  }+  StaticSlice(StaticSlice&& other) noexcept+      : slice_detail::BaseSlice(other.TakeCSlice()) {}+  StaticSlice& operator=(StaticSlice&& other) noexcept {+    std::swap(slice_, other.slice_);+    return *this;+  }+};++class MutableSlice : public slice_detail::BaseSlice,+                     public slice_detail::CopyConstructors<MutableSlice> {+ public:+  MutableSlice() = default;+  explicit MutableSlice(const grpc_slice& slice)+      : slice_detail::BaseSlice(slice) {+    GPR_DEBUG_ASSERT(slice.refcount == nullptr ||+                     slice.refcount->IsRegularUnique());+  }+  ~MutableSlice() { grpc_slice_unref_internal(slice_); }++  MutableSlice(const MutableSlice&) = delete;+  MutableSlice& operator=(const MutableSlice&) = delete;+  MutableSlice(MutableSlice&& other) noexcept+      : slice_detail::BaseSlice(other.TakeCSlice()) {}+  MutableSlice& operator=(MutableSlice&& other) noexcept {+    std::swap(slice_, other.slice_);+    return *this;+  }++  // Iterator access to the underlying bytes+  uint8_t* begin() { return GRPC_SLICE_START_PTR(this->slice_); }+  uint8_t* end() { return GRPC_SLICE_END_PTR(this->slice_); }++  // Array access+  uint8_t& operator[](size_t i) {+    return GRPC_SLICE_START_PTR(this->slice_)[i];+  }+};++class Slice : public slice_detail::BaseSlice,+              public slice_detail::CopyConstructors<Slice> {+ public:+  Slice() = default;+  ~Slice() { grpc_slice_unref_internal(slice_); }+  explicit Slice(const grpc_slice& slice) : slice_detail::BaseSlice(slice) {}+  template <class SliceType>+  explicit Slice(absl::enable_if_t<+                 std::is_base_of<slice_detail::BaseSlice, SliceType>::value,+                 SliceType>&& other)+      : slice_detail::BaseSlice(other.TakeCSlice()) {}++  Slice(const Slice&) = delete;+  Slice& operator=(const Slice&) = delete;+  Slice(Slice&& other) noexcept : slice_detail::BaseSlice(other.TakeCSlice()) {}+  Slice& operator=(Slice&& other) noexcept {+    std::swap(slice_, other.slice_);+    return *this;+  }++  // A slice might refer to some memory that we keep a refcount to (this is+  // owned), or some memory that's inlined into the slice (also owned), or some+  // other block of memory that we know will be available for the lifetime of+  // some operation in the common case (not owned). In the *less common* case+  // that we need to keep that slice text for longer than our API's guarantee us+  // access, we need to take a copy and turn this into something that we do own.++  // IntoOwned returns an owned slice regardless of current ownership, and+  // leaves the current slice empty - in doing so it can avoid adding a ref to+  // the underlying slice.+  Slice IntoOwned() {","I'm ok with that.(side note: the naming comes from other ecosystems, with Into implying some level of transmogrification to produce the new thing... but I think I agree the name Take is more C++-ish)",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27902,742207871,2021-11-03T18:07:34Z,src/core/lib/iomgr/work_serializer.cc,"@@ -24,6 +24,8 @@ namespace grpc_core {  DebugOnlyTraceFlag grpc_work_serializer_trace(false, ""work_serializer""); +namespace {+ struct CallbackWrapper {","I think it's better to encapsulate implementation details inside of the class that they're used in.  If we ever add another class to this file, that class will not need to see this code.I generally use an anonymous namespace if the class declaration is in a .h file, since I prefer not to have to publish implementation details in the .h file if I can avoid it.  But if the class declaration is already in the .cc file, private methods seem better.",
133680,sampajano,https://api.github.com/repos/grpc/grpc/pulls/27882,742214578,2021-11-03T18:16:35Z,src/objective-c/GRPCClient/GRPCCallOptions.h,"@@ -279,7 +279,7 @@ NS_ASSUME_NONNULL_BEGIN  * Dictionary key is of type NSString, value should be either NSString or NSData containing binary",Consider moving 2 similar comments to `GRPCMetadataDictionary`?,X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/27902,742236229,2021-11-03T18:45:57Z,src/core/lib/iomgr/work_serializer.cc,"@@ -24,6 +24,8 @@ namespace grpc_core {  DebugOnlyTraceFlag grpc_work_serializer_trace(false, ""work_serializer""); +namespace {+ struct CallbackWrapper {","https://abseil.io/tips/186 (go/totw/186) prefers functions in an anonymous namespace for the following reasons - ```Benefits over private methods include:Inputs and outputs are clear since they are (much more likely to be) specified via arguments or the return value. Note that a method may read any member variable and a non-const method may modify any non-const member. Conversely, a non-member function may only read or modify according to its interface (except for globals).The class API is simpler and shorter, and hence easier to read—unnecessary private methods may make it difficult to find inheritance-related private declarations or declarations after the class. ```Edit: I thought this was in reference to the `MakeRefPair()` and other functions. `CallbackWrapper` might deserve going inside the WorkSerializerImpl class since it makes it easier to read the class.Second Edit:```Sometimes a non-member local function does not make sense. For example:When the function is useful in multiple source files. Declaring it in a header file allows re-use.When the function has complex interactions with an object or class. For example, a function that reads from multiple fields and needs to modify state in a way that can’t be handled naturally via a return value is likely better written as a method. In particular, logic involving a mutex usually belongs in a member function.When the function belongs as part of an class’s API.```I guess it might make sense to put inside the class after all.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/27874,742247762,2021-11-03T19:02:12Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/network_services.py,"@@ -136,34 +138,37 @@ def from_response(cls, d: Dict[str, Any]) -> 'Destination':      @dataclasses.dataclass(frozen=True)     class RouteAction:-        destination: Optional['Destination']+        destinations: Optional[List['Destination']]","While this is not an optional _protobuf_ field, [Oneplatform translation does not guarantee that an empty repeated field will be present as an empty JSON list after translation](https://yaqs.corp.google.com/eng/q/4562698658381824). This needs to stay.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27902,742248499,2021-11-03T19:03:14Z,src/core/lib/iomgr/work_serializer.cc,"@@ -60,16 +34,41 @@ class WorkSerializer::WorkSerializerImpl : public Orphanable {   void Orphan() override;   private:+  struct CallbackWrapper {+    CallbackWrapper(std::function<void()> cb,+                    const grpc_core::DebugLocation& loc)+        : callback(std::move(cb)), location(loc) {}++    MultiProducerSingleConsumerQueue::Node mpscq_node;+    const std::function<void()> callback;+    const DebugLocation location;+  };+   // Callers of DrainQueueOwned should make sure to grab the lock on the   // workserializer with-  //   prev_ref_pair = refs_.fetch_add(MakeRefPair(1, 1),-  //   std::memory_order_acq_rel);+  //+  //   prev_ref_pair =+  //     refs_.fetch_add(MakeRefPair(1, 1), std::memory_order_acq_rel);+  //   // and only invoke DrainQueueOwned() if there was previously no owner. Note   // that the queue size is also incremented as part of the fetch_add to allow   // the callers to add a callback to the queue if another thread already holds   // the lock to the work serializer.   void DrainQueueOwned(); +  // First 16 bits indicate ownership of the WorkSerializer, next 48 bits are+  // queue size (i.e., refs).+  uint64_t MakeRefPair(uint16_t owners, uint64_t size) {","These methods can be `static`, since they don't need to access any data members.",
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/27882,742300630,2021-11-03T20:20:49Z,src/objective-c/GRPCClient/GRPCCallOptions.h,"@@ -279,7 +279,7 @@ NS_ASSUME_NONNULL_BEGIN  * Dictionary key is of type NSString, value should be either NSString or NSData containing binary",thanks. this part is specific to initialMetadata (either NSString // NSData for the value type). GRPCMetadataDictionary are the more generic definition which also need to apply to e.g. additionalChannelArgs.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27911,742388679,2021-11-03T22:24:17Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -1951,7 +1945,7 @@ class XdsEnd2endTest : public ::testing::TestWithParam<TestType> {  class BasicTest : public XdsEnd2endTest {  public:-  BasicTest() : XdsEnd2endTest(4, 1) {}+  BasicTest() : XdsEnd2endTest(4, 1) { StartAllBackends(); }",Looks like you missed one other spot where this change is needed:https://github.com/grpc/grpc/blob/b1e6b1f8ccff2dabdaaee6cff1d6a2c14cb09cf1/test/cpp/end2end/xds/xds_end2end_test.cc#L2584Here's an alternative that might be less invasive:- Rename `XdsEnd2endTest` to `XdsEnd2endTestBase`.- Create a subclass called `XdsEnd2endTest` whose ctor calls `StartAllBackends()`.- Change `XdsEnabledServerTest` and `XdsServerSecurityTest` to inherit from `XdsEnd2endTestBase` instead of `XdsEnd2endTest`.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27911,742391280,2021-11-03T22:30:05Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -9820,10 +9873,7 @@ TEST_P(EdsTest, EdsServiceNameDefaultsToClusterName) {  class TimeoutTest : public BasicTest {  protected:-  void SetUp() override {-    xds_resource_does_not_exist_timeout_ms_ = 500;-    BasicTest::SetUp();-  }+  TimeoutTest() { xds_resource_does_not_exist_timeout_ms_ = 500; }","This isn't going to work.  The test requires that `xds_resource_does_not_exist_timeout_ms_` be set before the parent class' `SetUp()` method is called, since this value is used to create the client stub.I see a few options here:- Go back to using `SetUp()` and `TearDown()` instead of the ctor/dtor.- Add a call to `ResetStub()` here after setting `xds_resource_does_not_exist_timeout_ms_`.  This is a little wasteful, since it will initialize the client channel twice.- Add an optional parameter to the `XdsEnd2endTest` ctor to set this value, and pass it here via the initialization list.  This would have to be plumbed through `BasicTest`, which is a little invasive.I think that for now, going back to using `SetUp()` and `TearDown()` is probably the best option.  We can probably improve this later, because in order to split this up into multiple files, we're going to have to provide a cleaner API for subclasses that does not involve reaching directly into the parent class's internals.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/27807,742410535,2021-11-03T23:03:14Z,tools/distrib/buildifier_format_code_strict.sh,"@@ -17,4 +17,4 @@ dir=$(dirname ""${0}"") buildifier_format_script=""${dir}/buildifier_format_code.sh"" -${buildifier_format_script} -lint warn+${buildifier_format_script} -lint fix",I've found it useful to have a script that proxies flags to buildifier verbatim. The problem with adding `-lint fix` unconditionally is that there are certain other flags that cannot be used with it.,
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/27911,742424253,2021-11-03T23:34:29Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -9820,10 +9873,7 @@ TEST_P(EdsTest, EdsServiceNameDefaultsToClusterName) {  class TimeoutTest : public BasicTest {  protected:-  void SetUp() override {-    xds_resource_does_not_exist_timeout_ms_ = 500;-    BasicTest::SetUp();-  }+  TimeoutTest() { xds_resource_does_not_exist_timeout_ms_ = 500; }",I simply changed `TimeoutTest` to inherit from `XdsEnd2endTest` instead and added `xds_resource_does_not_exist_timeout_ms` as an optional parameter to the `XdsEnd2endTest` ctor which seems like the right thing to do here even considering future changes.,X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/26287,743182403,2021-11-04T20:32:19Z,test/core/tsi/crl_ssl_transport_security_test.cc,"@@ -0,0 +1,280 @@+// Copyright 2021 gRPC authors.++// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at++// http://www.apache.org/licenses/LICENSE-2.0+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <stdbool.h>+#include <stdio.h>+#include <string.h>++#include <gmock/gmock.h>+#include <gtest/gtest.h>++#include <grpc/grpc.h>+#include <grpc/support/alloc.h>+#include <grpc/support/log.h>+#include <grpc/support/string_util.h>++#include ""src/core/lib/iomgr/load_file.h""+#include ""src/core/lib/security/security_connector/security_connector.h""+#include ""src/core/tsi/ssl_transport_security.h""+#include ""src/core/tsi/transport_security.h""+#include ""src/core/tsi/transport_security_interface.h""+#include ""test/core/tsi/transport_security_test_lib.h""+#include ""test/core/util/test_config.h""++extern ""C"" {+#include <openssl/crypto.h>+#include <openssl/pem.h>+}++namespace {++const int kSslTsiTestRevokedKeyCertPairsNum = 1;+const int kSslTsiTestValidKeyCertPairsNum = 1;+const char* kSslTsiTestCrlSupportedCredentialsDir = ""test/core/tsi/test_creds/"";++class CrlSslTransportSecurityTest+    : public testing::TestWithParam<tsi_tls_version> {+ protected:+  // A tsi_test_fixture implementation.+  class SslTsiTestFixture {+   public:+    static SslTsiTestFixture* Create(bool use_revoked_server_cert,+                                     bool use_revoked_client_cert) {+      auto* fixture = static_cast<SslTsiTestFixture*>(+          gpr_malloc(sizeof(SslTsiTestFixture)));+      new (fixture)+          SslTsiTestFixture(use_revoked_server_cert, use_revoked_client_cert);","I pulled the changes yesterday and did some investigations myself. I also had some discussions with @krestofur . Here is what we've found:1. the error @markdroth saw was actually because **tsi_test_fixture** was originally created using **grpc_core::Zalloc**, and now we changed it to be the first member of a class, so we can't use **grpc_core::Zalloc** anymore, since the class should be trivial[1] 2. I tried two ways, but neither of them gave me a clean result. I tried to use **gpr_malloc**, but it seems **gpr_malloc** won't initialize the class, but leave the member values undefined[2]. I tried to use **new** as well, but there were some memory leak as well(error logging: [3]).To summarize, we want to use **grpc_core::Zalloc** but we couldn't, and we didn't find a suitable substitute, after changing the struct semantics to C++ semantics. To fundamentally solve this problem, I think the best way is to eliminate all underlying C semantics structs as well, but that will be a larger effort and is not very relevant to this PR.I think I've had a better understanding about the the problem, just not sure what's our next step. Maybe we can have a meeting to discuss the possible solutions @krestofur @markdroth . Thanks![1] https://github.com/grpc/grpc/blob/master/src/core/lib/gprpp/memory.h#L51[2] https://stackoverflow.com/questions/1538420/difference-between-malloc-and-calloc[3] https://gist.github.com/ZhenLian/3bd2de8039504f43ba9fceadf2ecab58",X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/27857,743837909,2021-11-05T17:06:55Z,tools/dockerfile/grpc_iwyu/Dockerfile,"@@ -0,0 +1,31 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++FROM debian:bullseye++RUN apt-get update && apt-get install -y clang-11 llvm-11-dev libclang-11-dev clang-format-11 jq git cmake python+","Maybe we can define CLANG_FORMAT= like https://github.com/grpc/grpc/blob/86e0634fa415dd5d59dda03a847b4a7896c543e3/templates/tools/dockerfile/grpc_clang_format/Dockerfile.template#L21 so that iwyu,sh script doesn't need to specify `clang-format-11`",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27954,744018550,2021-11-05T23:04:04Z,src/core/ext/transport/chttp2/server/chttp2_server.cc,"@@ -868,10 +868,10 @@ grpc_error_handle Chttp2ServerAddPort(Server* server, const char* addr,     grpc_error_handle error = GRPC_ERROR_NONE;     if (absl::StartsWith(parsed_addr, kUnixUriPrefix)) {       error = grpc_resolve_unix_domain_address(-          addr + sizeof(kUnixUriPrefix) - 1, &resolved);+          &parsed_addr[sizeof(kUnixUriPrefix) - 1], &resolved);",I suggest writing this as `parsed_addr.data() + sizeof(kUnixUriPrefix) - 1`.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27957,744154686,2021-11-06T18:30:48Z,src/proto/grpc/lookup/v1/rls_config.proto,"@@ -0,0 +1,225 @@+// Copyright 2020 The gRPC Authors","No, it's needed in grpc/grpc-proto.  It's used from there in systems other than gRPC.  But gRPC itself doesn't use it right now, because we take in this config in JSON form.It's not clear to me why we need a copy of a proto file that we're not actually using.  However, in the case of this particular file, it's probably not a big deal, because we probably will start using it in the not-too-distant future as part of configuring RLS via xDS.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27960,744970461,2021-11-08T18:04:12Z,src/core/lib/iomgr/error.cc,"@@ -982,4 +982,17 @@ bool grpc_log_error(const char* what, grpc_error_handle error, const char* file,   return false; } +namespace grpc_core {+void AddFieldError(absl::string_view field_name, absl::string_view error,+                   std::vector<grpc_error_handle>* error_list) {+  AddStringToErrorList(absl::StrCat(""field:"", field_name, "" error:"", error),+                       error_list);+}++void AddStringToErrorList(absl::string_view error,+                          std::vector<grpc_error_handle>* error_list) {+  error_list->push_back(GRPC_ERROR_CREATE_FROM_CPP_STRING(std::string(error)));","Thinking about this further, I suspect that we may actually be better off not using `grpc_error` for the individual errors within JSON parsing.In general, when parsing a JSON object, the goal is to report *all* errors in all fields to the user, because it would be much harder to use if we just stopped at the first error and required the user to iterate after fixing each individual error.  Including all errors means that the error message needs to be somewhat structured, so that the user can see which error is in what part of the object hierarchy.  At the time we first started doing this, we weren't able to use STL or absl, so it was very hard to construct and combine strings in an effective way to provide that kind of structured error message.  So we wound up using the ability to store a tree of `grpc_error` objects as a way of providing that structure.  But the result of this actually has so much noise that it's not really that friendly to users: they really don't care what file or line number in our code produced the error (or the timestamp or any other `grpc_error` metadata); all they really care about where in JSON tree the error occurred.I think now that we can use STL and absl, we have the tools we need to provide only what the user actually wants here.  Instead of using vectors of `grpc_error`, we could use vectors of strings, and we could easily combine them into parent error messages with `absl::StrJoin()`.",X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/27957,745822519,2021-11-09T16:58:55Z,src/proto/grpc/reflection/v1/reflection.proto,"@@ -0,0 +1,147 @@+// Copyright 2016 The gRPC Authors","Yeah, that's a long-term goal for this. There are a couple hairy issues to be resolved to get that level though but this way (keeping both similar as possible) is more easy to maintain two proto repos without having an automatic sync. Internally we need to moving toward using the `grpc-proto` repo only so they should become identical enough.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/27957,745823284,2021-11-09T16:59:48Z,src/proto/grpc/reflection/v1/reflection.proto,"@@ -0,0 +1,147 @@+// Copyright 2016 The gRPC Authors","And the next plain for this particular proto, we're going to deprecate v1alpha internally and remove v1alpha proto from both repos.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27976,747849386,2021-11-11T22:10:50Z,src/core/lib/json/json_object_loader.h,"@@ -0,0 +1,350 @@+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_JSON_JSON_OBJECT_LOADER_H+#define GRPC_CORE_LIB_JSON_JSON_OBJECT_LOADER_H++#include <grpc/support/port_platform.h>++#include <sys/types.h>++#include <cstddef>+#include <cstdint>+#include <cstring>+#include <string>+#include <vector>++#include ""absl/strings/string_view.h""++#include ""src/core/lib/json/json.h""++namespace grpc_core {++class ErrorList {+ public:+  void PushField(absl::string_view ext) GPR_ATTRIBUTE_NOINLINE;+  void PopField() GPR_ATTRIBUTE_NOINLINE;+  void AddError(absl::string_view error) GPR_ATTRIBUTE_NOINLINE;++  const std::vector<std::string>& errors() const { return errors_; }++ private:+  std::vector<std::string> errors_;+  std::vector<std::string> fields_;+};++class ScopedField {+ public:+  ScopedField(ErrorList* error_list, absl::string_view field_name)+      : error_list_(error_list) {+    error_list_->PushField(field_name);+  }+  ~ScopedField() { error_list_->PopField(); }++ private:+  ErrorList* error_list_;+};++namespace json_detail {++struct Element {+  enum Type : uint8_t {+    kInt32,+    kUint32,+    kString,+    // Vector should be the first thing after scalar types.+    kVector,+    kMap,+  };+  Element() = default;+  template <typename A, typename B>+  Element(B A::*p, Type type, bool optional, uint8_t type_data,+          const char* name)+      : member_offset(static_cast<uint16_t>(+            reinterpret_cast<uintptr_t>(&(static_cast<A*>(nullptr)->*p)))),+        type(type),+        optional(optional),+        type_data(type_data),+        name{} {+    strcpy(this->name, name);+  }+  uint16_t member_offset;+  Type type;+  bool optional;+  uint8_t type_data;+  char name[11];+};++struct TypeVtable {+  void* (*create)();+  void (*destroy)(void*);+  void (*push_to_vec)(void* ptr, void* vec);+  void (*insert_to_map)(std::string key, void* ptr, void* map);+};++template <typename T>+struct TypeVtableImpl {+  static const TypeVtable vtable;+};++template <typename T>+const TypeVtable TypeVtableImpl<T>::vtable{+    // create+    []() -> void* { return new T(); },+    // destroy+    [](void* ptr) { delete static_cast<T*>(ptr); },+    // push_to_vec+    [](void* ptr, void* vec) {+      auto* vec_ptr = static_cast<std::vector<T>*>(vec);+      auto* src_ptr = static_cast<T*>(ptr);+      vec_ptr->emplace_back(std::move(*src_ptr));+    },+    // insert_to_map+    [](std::string key, void* ptr, void* map) {+      auto* map_ptr = static_cast<std::map<std::string, T>*>(map);+      auto* src_ptr = static_cast<T*>(ptr);+      map_ptr->emplace(std::move(key), std::move(*src_ptr));+    },+};++class TypeRefProvider;++struct TypeRef {+  const TypeVtable* vtable;+  const Element* elements;+  size_t count;+  const TypeRefProvider* const* type_ref_providers;++  void Load(const Json::Object& json, void* dest,+            ErrorList* errors) const GPR_ATTRIBUTE_NOINLINE;+  using LoadFn = absl::FunctionRef<void(const Json& json, void* dest_ptr)>;+  void WithLoaderForTypeData(+      uint8_t tag, ErrorList* errors,+      absl::FunctionRef<void(const TypeVtable* vtable, LoadFn load)>) const+      GPR_ATTRIBUTE_NOINLINE;+  void LoadScalar(const Json& json, Element::Type type, void* dest,+                  ErrorList* errors) const GPR_ATTRIBUTE_NOINLINE;+  void LoadVector(const Json& json, uint8_t type_data, void* dest,+                  ErrorList* errors) const GPR_ATTRIBUTE_NOINLINE;+  void LoadMap(const Json& json, uint8_t type_data, void* dest,+               ErrorList* errors) const GPR_ATTRIBUTE_NOINLINE;+};++class TypeRefProvider {+ public:+  virtual void WithTypeRef(absl::FunctionRef<void(const TypeRef&)>) const = 0;++ protected:+  ~TypeRefProvider() = default;+};++template <typename T>+struct ElementTypeOf;+template <>+struct ElementTypeOf<int32_t> {+  static Element::Type type() { return Element::kInt32; }+};+template <>+struct ElementTypeOf<uint32_t> {+  static Element::Type type() { return Element::kUint32; }+};+template <>+struct ElementTypeOf<std::string> {+  static Element::Type type() { return Element::kString; }+};++// Vec<T, kSize> provides a constant array type that can be appended to by+// copying. It's setup so that most compilers can optimize away all of its+// operations.+template <typename T, size_t kSize>+class Vec {","Should this maybe go in its own library in gprpp?  Seems like we've used this same pattern in other places, haven't we?",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/28034,749582576,2021-11-15T18:31:47Z,tools/internal_ci/linux/psm-security.cfg,"@@ -15,7 +15,7 @@ # Config file for the internal CI (in protobuf text format)  # Location of the continuous shell script in repository.-build_file: ""grpc/tools/internal_ci/linux/grpc_xds_k8s.sh""+build_file: ""grpc/tools/internal_ci/linux/psm-security.sh""","@yashykt @lidizheng We're renaming ""xDS K8S"" job to ""PSM Security"", to reflect its actual purpose. Would you be ok with changing job name to `psm-security` so we're consistent across the repos (ref https://github.com/grpc/grpc-java/pull/8695), or would you insist on snake_case here? Something like `grpc_psm_security`?",X
7281574,nicolasnoble,https://api.github.com/repos/grpc/grpc/pulls/27513,749823303,2021-11-16T01:10:55Z,src/core/lib/event_engine/uv/libuv_event_engine.cc,"@@ -0,0 +1,427 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include ""src/core/lib/event_engine/uv/libuv_event_engine.h""++#include <cmath>+#include <functional>+#include <unordered_map>++#include ""absl/strings/str_format.h""+#include ""uv.h""++#include <grpc/event_engine/event_engine.h>+#include <grpc/support/thd_id.h>++#include ""src/core/lib/gprpp/mpscq.h""+#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/gprpp/thd.h""+#include ""src/core/lib/iomgr/event_engine/promise.h""+#include ""src/core/lib/iomgr/exec_ctx.h""++extern grpc_core::TraceFlag grpc_tcp_trace;++namespace grpc_event_engine {+namespace experimental {++namespace {++struct SchedulingRequest : grpc_core::MultiProducerSingleConsumerQueue::Node {+  typedef std::function<void(LibuvEventEngine*)> functor;+  explicit SchedulingRequest(functor&& f) : f(std::move(f)) {}+  functor f;+};++}  // namespace++////////////////////////////////////////////////////////////////////////////////+/// The LibuvTask class is used for Run and RunAt from LibuvEventEngine, and is+/// allocated internally for the returned TaskHandle.+///+/// Its API is used solely by the Run and RunAt functions, while in the libuv+/// loop thread.+////////////////////////////////////////////////////////////////////////////////+class LibuvTask {+ public:+  LibuvTask(LibuvEventEngine* engine, std::function<void()>&& fn);+  /// Executes the held \a fn_ and removes itself from EventEngine's accounting.+  /// Must be called from within the libuv thread.+  void Start(LibuvEventEngine* engine, uint64_t timeout);+  /// Cancel this task.+  /// The promise meanings are the same as in \a EventEngine::Cancel.+  /// Must be called from within the libuv thread.+  /// Precondition: the EventEngine must be tracking this task.+  void Cancel(Promise<bool>& will_be_cancelled);+  /// A callback passed to uv_close to erase the timer from the EventEngine+  static void Erase(uv_handle_t* handle);+  /// A callback passed to uv_close to coordinate running the task then erasing+  /// the timer from the EventEngine. This helps avoid race conditions where the+  /// timer handle is open after the function is run and the EventEngine is+  /// being destroyed.+  static void RunAndErase(uv_handle_t* handle);+  intptr_t Key() { return key_; }++ private:+  std::function<void()> fn_;+  bool ran_ = false;+  uv_timer_t timer_;+  const intptr_t key_;+};++// TODO(hork): keys should be recycled.+LibuvTask::LibuvTask(LibuvEventEngine* engine, std::function<void()>&& fn)+    : fn_(std::move(fn)), key_(engine->task_key_.fetch_add(1)) {+  if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+    gpr_log(GPR_DEBUG, ""LibuvTask@%p, created task: key = %"" PRIiPTR, this,+            key_);+  }+  timer_.data = this;+}++void LibuvTask::Start(LibuvEventEngine* engine, uint64_t timeout) {+  uv_update_time(&engine->loop_);+  uv_timer_init(&engine->loop_, &timer_);+  uv_timer_start(+      &timer_,+      [](uv_timer_t* timer) {+        uv_timer_stop(timer);+        LibuvTask* task = reinterpret_cast<LibuvTask*>(timer->data);+        if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+          gpr_log(GPR_DEBUG, ""LibuvTask@%p, triggered: key = %"" PRIiPTR, task,+                  task->Key());+        }+        task->ran_ = true;+        // TODO(hork): Timer callbacks will be delayed by one iteration of the+        // uv_loop to avoid race conditions around EventEngine destruction.+        // Before the timer callback has run, the uv state for that timer is+        // destroyed. This is delay is not ideal, we should find a way to avoid+        // it.+        uv_close(reinterpret_cast<uv_handle_t*>(timer),+                 &LibuvTask::RunAndErase);+      },+      timeout, 0);+}++void LibuvTask::Cancel(Promise<bool>& will_be_cancelled) {+  if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+    gpr_log(GPR_DEBUG, ""LibuvTask@%p, cancelled: key = %"" PRIiPTR, this, key_);+  }+  if (uv_is_closing(reinterpret_cast<uv_handle_t*>(&timer_)) != 0) {+    // TODO(hork): check if this can be called on uv shutdown instead+#ifndef NDEBUG+    GPR_ASSERT(GPR_LIKELY(ran_));+#endif+    will_be_cancelled.Set(false);+    return;+  }+  will_be_cancelled.Set(true);+  uv_timer_stop(&timer_);+  uv_close(reinterpret_cast<uv_handle_t*>(&timer_), &LibuvTask::Erase);+}++void LibuvTask::Erase(uv_handle_t* handle) {+  uv_timer_t* timer = reinterpret_cast<uv_timer_t*>(handle);+  LibuvTask* task = reinterpret_cast<LibuvTask*>(timer->data);+  LibuvEventEngine* engine =+      reinterpret_cast<LibuvEventEngine*>(timer->loop->data);+  engine->EraseTask(task->key_);+}++void LibuvTask::RunAndErase(uv_handle_t* handle) {+  uv_timer_t* timer = reinterpret_cast<uv_timer_t*>(handle);+  LibuvTask* task = reinterpret_cast<LibuvTask*>(timer->data);+  std::function<void()> fn = std::move(task->fn_);+  LibuvEventEngine* engine =+      reinterpret_cast<LibuvEventEngine*>(timer->loop->data);+  engine->EraseTask(task->key_);+  fn();+}++LibuvEventEngine::LibuvEventEngine() {+  bool success = false;+  if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+    gpr_log(GPR_DEBUG, ""LibuvEventEngine:%p created"", this);+  }+  thread_ = grpc_core::Thread(+      ""uv loop"",+      [](void* arg) {+        LibuvEventEngine* engine = reinterpret_cast<LibuvEventEngine*>(arg);+        engine->RunThread();+      },+      this, &success);+  thread_.Start();+  GPR_ASSERT(GPR_LIKELY(success));+  // This promise will be set to true once the thread has fully started and is+  // operational, so let's wait on it.+  success = ready_.Get();+  GPR_ASSERT(GPR_LIKELY(success));+}++LibuvEventEngine::~LibuvEventEngine() {+  if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+    gpr_log(GPR_DEBUG, ""LibuvEventEngine@%p::~LibuvEventEngine"", this);+  }+  RunInLibuvThread([](LibuvEventEngine* engine) {+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_DEBUG,+              ""LibuvEventEngine@%p shutting down, unreferencing Kicker now"",+              engine);+    }+    GPR_ASSERT(engine->uv_shutdown_can_proceed_.Get());","This promise seems now to be limited to that destructor. It can be moved away from the class definition itself and into the stack of the destructor instead. You'd need to capture it, of course.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28106,750491775,2021-11-16T17:11:31Z,src/core/ext/xds/xds_api.cc,"@@ -882,7 +882,7 @@ absl::StatusOr<XdsApi::ResourceName> ParseResourceNameInternal(       uri->query_parameter_map().begin(), uri->query_parameter_map().end());   std::sort(query_parameters.begin(), query_parameters.end());   return XdsApi::ResourceName{-      uri->authority(),+      absl::StrCat(""xdstp:"", uri->authority()),       absl::StrCat(           ""xdstp:"", path_parts.second, (query_parameters.empty() ? ""?"" : """"),","Similarly, we no longer need the `""xdstp:""` here, since it's now in the authority.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/27513,750606437,2021-11-16T19:43:59Z,src/core/lib/event_engine/uv/libuv_event_engine.cc,"@@ -0,0 +1,427 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include ""src/core/lib/event_engine/uv/libuv_event_engine.h""++#include <cmath>+#include <functional>+#include <unordered_map>++#include ""absl/strings/str_format.h""+#include ""uv.h""++#include <grpc/event_engine/event_engine.h>+#include <grpc/support/thd_id.h>++#include ""src/core/lib/gprpp/mpscq.h""+#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/gprpp/thd.h""+#include ""src/core/lib/iomgr/event_engine/promise.h""+#include ""src/core/lib/iomgr/exec_ctx.h""++extern grpc_core::TraceFlag grpc_tcp_trace;++namespace grpc_event_engine {+namespace experimental {++namespace {++struct SchedulingRequest : grpc_core::MultiProducerSingleConsumerQueue::Node {+  typedef std::function<void(LibuvEventEngine*)> functor;+  explicit SchedulingRequest(functor&& f) : f(std::move(f)) {}+  functor f;+};++}  // namespace++////////////////////////////////////////////////////////////////////////////////+/// The LibuvTask class is used for Run and RunAt from LibuvEventEngine, and is+/// allocated internally for the returned TaskHandle.+///+/// Its API is used solely by the Run and RunAt functions, while in the libuv+/// loop thread.+////////////////////////////////////////////////////////////////////////////////+class LibuvTask {+ public:+  LibuvTask(LibuvEventEngine* engine, std::function<void()>&& fn);+  /// Executes the held \a fn_ and removes itself from EventEngine's accounting.+  /// Must be called from within the libuv thread.+  void Start(LibuvEventEngine* engine, uint64_t timeout);+  /// Cancel this task.+  /// The promise meanings are the same as in \a EventEngine::Cancel.+  /// Must be called from within the libuv thread.+  /// Precondition: the EventEngine must be tracking this task.+  void Cancel(Promise<bool>& will_be_cancelled);+  /// A callback passed to uv_close to erase the timer from the EventEngine+  static void Erase(uv_handle_t* handle);+  /// A callback passed to uv_close to coordinate running the task then erasing+  /// the timer from the EventEngine. This helps avoid race conditions where the+  /// timer handle is open after the function is run and the EventEngine is+  /// being destroyed.+  static void RunAndErase(uv_handle_t* handle);+  intptr_t Key() { return key_; }++ private:+  std::function<void()> fn_;+  bool ran_ = false;+  uv_timer_t timer_;+  const intptr_t key_;+};++// TODO(hork): keys should be recycled.+LibuvTask::LibuvTask(LibuvEventEngine* engine, std::function<void()>&& fn)+    : fn_(std::move(fn)), key_(engine->task_key_.fetch_add(1)) {+  if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+    gpr_log(GPR_DEBUG, ""LibuvTask@%p, created task: key = %"" PRIiPTR, this,+            key_);+  }+  timer_.data = this;+}++void LibuvTask::Start(LibuvEventEngine* engine, uint64_t timeout) {+  uv_update_time(&engine->loop_);+  uv_timer_init(&engine->loop_, &timer_);+  uv_timer_start(+      &timer_,+      [](uv_timer_t* timer) {+        uv_timer_stop(timer);+        LibuvTask* task = reinterpret_cast<LibuvTask*>(timer->data);+        if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+          gpr_log(GPR_DEBUG, ""LibuvTask@%p, triggered: key = %"" PRIiPTR, task,+                  task->Key());+        }+        task->ran_ = true;+        // TODO(hork): Timer callbacks will be delayed by one iteration of the+        // uv_loop to avoid race conditions around EventEngine destruction.+        // Before the timer callback has run, the uv state for that timer is+        // destroyed. This is delay is not ideal, we should find a way to avoid+        // it.+        uv_close(reinterpret_cast<uv_handle_t*>(timer),+                 &LibuvTask::RunAndErase);+      },+      timeout, 0);+}++void LibuvTask::Cancel(Promise<bool>& will_be_cancelled) {+  if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+    gpr_log(GPR_DEBUG, ""LibuvTask@%p, cancelled: key = %"" PRIiPTR, this, key_);+  }+  if (uv_is_closing(reinterpret_cast<uv_handle_t*>(&timer_)) != 0) {+    // TODO(hork): check if this can be called on uv shutdown instead+#ifndef NDEBUG+    GPR_ASSERT(GPR_LIKELY(ran_));+#endif+    will_be_cancelled.Set(false);+    return;+  }+  will_be_cancelled.Set(true);+  uv_timer_stop(&timer_);+  uv_close(reinterpret_cast<uv_handle_t*>(&timer_), &LibuvTask::Erase);+}++void LibuvTask::Erase(uv_handle_t* handle) {","Agreed, I think I'll leave the minor code duplication as is for simplicity.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28118,751569547,2021-11-17T19:27:37Z,test/core/end2end/invalid_call_argument_test.cc,"@@ -607,6 +607,31 @@ static void test_invalid_initial_metadata_reserved_key() {   cleanup_test(); } +static void test_multiple_recv_initial_metadata_in_a_single_batch() {","Looks like this actually tests duplicates of all ops, not just recv_initial_metadata.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28109,751661427,2021-11-17T21:45:34Z,src/core/lib/transport/parsed_metadata.h,"@@ -147,7 +147,7 @@ class ParsedMetadata {     grpc_error_handle (*const set)(const Buffer& value,                                    MetadataContainer* container);     ParsedMetadata (*const with_new_value)(const Buffer& value,","Maybe add a comment here explaining that the `new_value` param needs to be a pointer instead of being passed by value to work around the gcc 4.9 bug?  That way, we will know we can change it once we no longer support that compiler.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28122,751717431,2021-11-17T23:14:34Z,BUILD,"@@ -3341,6 +3341,266 @@ grpc_cc_library(     ], ) +grpc_cc_library(+    name = ""tsi_ssl_types"",+    hdrs = [+        ""src/core/tsi/ssl_types.h"",+    ],+    external_deps = [+        ""libssl"",+    ],+    language = ""c++"",+)++grpc_cc_library(+    name = ""tsi_base"",+    srcs = [+        ""src/core/tsi/transport_security.cc"",","Is it going to cause problems that we have the same files listed in multiple targets here?Could we avoid this by removing these files from their current targets and having those targets depend on these new targets?  For example, in the case of this `tsi_base` target, we could do the following:- Remove `src/core/tsi/transport_security.cc`, `src/core/tsi/transport_security.h`, and `src/core/tsi/transport_security_interface.h` from the existing `tsi_interface` target, and have `tsi_interface` depend on `tsi_base`.- Remove `src/core/tsi/transport_security_grpc.cc` and `src/core/tsi/transport_security_grpc.h` from the `tsi` target, and have `tsi` depend on `tsi_base`.Similar things can be done for the other targets.",X
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/28122,752446792,2021-11-18T17:01:59Z,BUILD,"@@ -3341,6 +3341,266 @@ grpc_cc_library(     ], ) +grpc_cc_library(+    name = ""tsi_ssl_types"",+    hdrs = [+        ""src/core/tsi/ssl_types.h"",+    ],+    external_deps = [+        ""libssl"",+    ],+    language = ""c++"",+)++grpc_cc_library(+    name = ""tsi_base"",+    srcs = [+        ""src/core/tsi/transport_security.cc"",","Do you mean causing problems internally or in OSS? Internally (cl/407803147), we will replace the dependency of `grpc_secure` with `grpc_security_base`  and `tsi_interface` with `tsi_base`, respectively which means the internal targets will not depend on two targets that include the same files. WDYT? ",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/28145,752794478,2021-11-19T01:19:48Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -87,7 +87,8 @@ def setUpClass(cls):         # GCP         cls.project: str = xds_flags.PROJECT.value         cls.network: str = xds_flags.NETWORK.value-        cls.config_scope: str = xds_flags.ROUTER_SCOPE.value+        cls.config_scope: str = xds_flags.CONFIG_SCOPE.value + framework.helpers.rand.random_resource_suffix(+        )","As long as individual tests in the suite will not run concurrently (which I assume they will not), I think it's fine. Regardless, adopting your preference as reviewer.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/28145,752834136,2021-11-19T03:16:46Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -137,6 +138,9 @@ def setUp(self):         logger.info('Test run resource prefix: %s, suffix: %s',                     self.resource_prefix, self.resource_suffix) +        self.config_scope = self.td.make_resource_name(+            xds_flags.CONFIG_SCOPE.value)+","Scope is currently only used by the one `Mesh`, but it will be shared by multiple resources in the future. I think it should probably stay as a member of the class.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28148,753370078,2021-11-19T17:02:03Z,src/core/ext/filters/client_channel/subchannel.cc,"@@ -74,6 +76,10 @@  namespace grpc_core { +namespace {","No need for an anonymous namespace around a `using` statement.  If you're not defining any symbols, then the anonymous namespace doesn't actually hide anything.",
16571023,ananda1066,https://api.github.com/repos/grpc/grpc/pulls/28023,753406692,2021-11-19T17:56:30Z,src/core/lib/promise/arena_promise.h,"@@ -0,0 +1,143 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_PROMISE_ARENA_PROMISE_H+#define GRPC_CORE_LIB_PROMISE_ARENA_PROMISE_H++#include <grpc/support/port_platform.h>++#include <grpc/support/log.h>++#include ""src/core/lib/gprpp/arena.h""+#include ""src/core/lib/promise/poll.h""++namespace grpc_core {++namespace arena_promise_detail {++template <typename T>+class ImplInterface {+ public:+  virtual Poll<T> PollOnce() = 0;+  virtual void Destroy() = 0;++ protected:+  ~ImplInterface() = default;+};++template <typename T>+class NullImpl final : public ImplInterface<T> {",It might be useful to comment on the use case of this class.,
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/28132,754691729,2021-11-22T22:48:04Z,src/core/lib/channel/channel_args_preconditioning.h,"@@ -0,0 +1,62 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_CHANNEL_CHANNEL_ARGS_PRECONDITIONING_H+#define GRPC_CORE_LIB_CHANNEL_CHANNEL_ARGS_PRECONDITIONING_H++#include <grpc/support/port_platform.h>++#include <functional>+#include <vector>++#include ""src/core/lib/channel/channel_args.h""++namespace grpc_core {++// Registry of mutators for channel args.+// Surface APIs should call into this with channel args received from outside+// of gRPC, in order to prepare those channel args for the expections of the+// gRPC internals.+class ChannelArgsPreconditioning {+ public:+  // Take channel args and mutate them.+  // Does not take ownership of the channel args passed in.+  // Returns a new channel args object that is owned by the caller.+  using Stage =",is the multi-stage process gonna be bad for performance? (since we are gonna be copying the channel args at each stage). Can the answer to this question also be included as a comment here so that future readers know that it was considered?,X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28132,754707830,2021-11-22T23:24:19Z,test/core/client_channel/resolvers/binder_resolver_test.cc,"@@ -23,6 +23,7 @@  #include <cstring> +#include <grpc/grpc.h>","grpc_init gets called - it used to be accidentally included from src/core/lib/channel/channel_args.h, but since that got cleaned up other things need to also.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28107,754822299,2021-11-23T06:03:24Z,src/core/ext/filters/http/server/http_server_filter.cc,"@@ -306,19 +298,11 @@ static grpc_error_handle hs_filter_incoming_metadata(grpc_call_element* elem,     }   } -  if (b->legacy_index()->named.host != nullptr &&-      b->legacy_index()->named.authority == nullptr) {-    grpc_linked_mdelem* el = b->legacy_index()->named.host;-    grpc_mdelem md = GRPC_MDELEM_REF(el->md);-    b->Remove(el);-    hs_add_error(-        error_name, &error,-        grpc_metadata_batch_add_head(-            b, el,-            grpc_mdelem_from_slices(GRPC_MDSTR_AUTHORITY,-                                    grpc_slice_ref_internal(GRPC_MDVALUE(md))),-            GRPC_BATCH_AUTHORITY));-    GRPC_MDELEM_UNREF(md);+  if (b->legacy_index()->named.authority == nullptr) {+    absl::optional<grpc_core::Slice> host = b->Take(grpc_core::HostMetadata());+    if (host.has_value()) {+      b->Append("":authority"", std::move(*host));","You're right, and `:` prefixed metadata should absolutely go to head per the http2 spec.That said - this is the server filter, and so this metadata never goes out on the wire (http2 considerations are moot) and we loose the ability to reconstruct this specific order by the time this metadata *could* hit a wire.Additionally, we don't have a `Prepend` function just yet that would work.Now we could `Append` and stick some comment here, and that might be good - but within the next two changes in this series this code gets removed and we do `b->Set(AuthorityMetadata(), std::move(*host));`.Similarly we could add `Prepend` and modify this code to use that and be completely the same behavior as before - but I'm not convinced we need `Prepend` once these changes are done and we do the aforementioned `b->Set(AuthorityMetadata(), std::move(*host));`",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/27887,758323408,2021-11-29T12:39:31Z,src/csharp/Grpc.Core.Api/Interceptors/InterceptingCallInvoker.cs,"@@ -43,7 +43,7 @@ public InterceptingCallInvoker(CallInvoker invoker, Interceptor interceptor)         /// <summary>         /// Intercepts a simple blocking call with the registered interceptor.         /// </summary>-        public override TResponse BlockingUnaryCall<TRequest, TResponse>(Method<TRequest, TResponse> method, string host, CallOptions options, TRequest request)+        public override TResponse BlockingUnaryCall<TRequest, TResponse>(Method<TRequest, TResponse> method, string? host, CallOptions options, TRequest request)","`host == null` means ""use the default host / authority"". You can set ""host"" explicitly if you want to use multiple virtual hosts on a single host.See ""Authority"" in https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.mdSo setting `host=null` is actually the most common use case.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28202,758463694,2021-11-29T15:19:49Z,templates/CMakeLists.txt.template,"@@ -164,7 +164,7 @@   set(gRPC_INSTALL_INCLUDEDIR ""include"" CACHE STRING ""Installation directory for headers"")   set(gRPC_INSTALL_CMAKEDIR ""lib/cmake/<%text>${PACKAGE_NAME}</%text>"" CACHE STRING ""Installation directory for cmake config files"")   set(gRPC_INSTALL_SHAREDIR ""share/grpc"" CACHE STRING ""Installation directory for root certificates"")-  set(gRPC_BUILD_MSVC_MP_COUNT 0 CACHE INTERNAL ""The maximum number of processes for MSVC /MP option"")+  set(gRPC_BUILD_MSVC_MP_COUNT 0 CACHE STRING ""The maximum number of processes for MSVC /MP option"")","note that the semantics of this option is a bit cryptic.(0 = don't set, -1 ""all processors available"" and positive number = number of processes).Also, have we investigated both `/MP<number_of_core>` and `/maxcpucount:<number_of_core>`?(see https://github.com/scikit-build/scikit-build/issues/61)",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28228,759993954,2021-12-01T09:21:01Z,tools/internal_ci/linux/grpc_build_artifacts.sh,"@@ -26,5 +26,4 @@ set -e  # rvm commands are very verbose rvm --default use ruby-2.4.1 set -ex -# [DO-NOT-SUBMIT]-tools/run_tests/task_runner.py -f artifact linux presubmit -j 6+tools/run_tests/task_runner.py -f artifact linux -j 6","I just updated the parallelism to -j 12 in my recent PR, so you might wanna rebase.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28251,760543597,2021-12-01T20:24:11Z,src/core/lib/json/json_reader.cc,"@@ -157,9 +157,10 @@ bool JsonReader::StringAddChar(uint32_t c) { }  bool JsonReader::StringAddUtf32(uint32_t c) {-  if (c <= 0x7f) {-    return StringAddChar(c);-  } else if (c <= 0x7ff) {+  // (b/208085307): This argument of this function is an Utf32 encoded+  // character. So we need to treat it as such and not as a simple ASCII+  // character even if it is <= 0x7f.+  if (c <= 0x7ff) {",I'm not sure this is right... wouldn't this change imply that it's impossible to encode a 1-byte long utf-8 symbol with utf-32?,X
5564114,Vignesh2208,https://api.github.com/repos/grpc/grpc/pulls/28251,760568372,2021-12-01T21:03:05Z,src/core/lib/json/json_reader.cc,"@@ -157,9 +157,10 @@ bool JsonReader::StringAddChar(uint32_t c) { }  bool JsonReader::StringAddUtf32(uint32_t c) {-  if (c <= 0x7f) {-    return StringAddChar(c);-  } else if (c <= 0x7ff) {+  // (b/208085307): This argument of this function is an Utf32 encoded+  // character. So we need to treat it as such and not as a simple ASCII+  // character even if it is <= 0x7f.+  if (c <= 0x7ff) {","Yes I think you are right. But there might be another issue.The json writer simply outputs all chars >= 32 and <= 126 without any modifications: https://github.com/grpc/grpc/blob/5961aeb2a02c280a67efa9247b2b77304af865c8/src/core/lib/json/json_writer.cc#L145But for chars < 32 or chars >= 127, it applies utf-16 encoding and creates substrings of the form ""\u{encoded bytes}"".  From my understanding of the json reader, the function in question in the PR (i.e StringAddUtf32), is called only when parsing substrings of the form ""\u{encoded bytes}"" .I think there  is some ambiguity during parsing of strings of the form ""\u{encoded bytes}"". Sometimes the ""{encoded bytes}"" are less <= 127 while the original char is < 32 while at other times the ""{encoded bytes}"" are <= 127 and the original char is >= 127 (the failing testcase is an example of this). So when {encoded bytes} is <= 127, it is not clear how the original char should be interpreted.I think this can be corrected by not doing Utf16 encoding here (for chars < 32 or char == 127) and coupling it with the current change already in the PR: https://github.com/grpc/grpc/blob/5961aeb2a02c280a67efa9247b2b77304af865c8/src/core/lib/json/json_writer.cc#L166WDYT ? or is it mandatory to do Utf16 encoding for chars < 32 ?",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/28091,760593295,2021-12-01T21:43:32Z,src/core/ext/filters/client_channel/resolver/dns/c_ares/dns_resolver_ares.cc,"@@ -353,12 +366,14 @@ void AresDnsResolver::OnResolvedLocked(grpc_error_handle error) {   } else {     GRPC_CARES_TRACE_LOG(""resolver:%p dns resolution failed: %s"", this,                          grpc_error_std_string(error).c_str());-    std::string error_message =-        absl::StrCat(""DNS resolution failed for service: "", name_to_resolve_);-    result_handler_->ReturnError(grpc_error_set_int(-        GRPC_ERROR_CREATE_REFERENCING_FROM_COPIED_STRING(error_message.c_str(),-                                                         &error, 1),-        GRPC_ERROR_INT_GRPC_STATUS, GRPC_STATUS_UNAVAILABLE));+    std::string error_message;+    grpc_error_get_str(error, GRPC_ERROR_STR_DESCRIPTION, &error_message);","Since we're no longer seeing `GRPC_ERROR_INT_GRPC_STATUS` here, I want to make sure we're still filling this into RPC error message details.Can we patch in the test for this in https://github.com/grpc/grpc/pull/28252? (or we can merge that separately). I've tested that this works on current master branch.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28091,760677663,2021-12-02T00:38:07Z,src/core/ext/filters/client_channel/resolver/dns/c_ares/dns_resolver_ares.cc,"@@ -320,21 +319,35 @@ void AresDnsResolver::OnResolvedLocked(grpc_error_handle error) {     GRPC_ERROR_UNREF(error);     return;   }+  // TODO(roth): Change logic to be able to report failures for addresses","Yeah, that's part of this TODO.I agree that the current behavior is sub-optimal, but I think it's the same behavior it's had all along, so this PR doesn't really change anything with respect to that.  There's also a related TODO [here](https://github.com/grpc/grpc/blob/9722651156ddf97e8b90d991b379a4692c5a344e/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_wrapper.cc#L593) about being able to return a service config with an empty address list; I think that's something else we should fix at the same time.  Basically, the idea is that the address lookup and the TXT lookup should be completely independent of each other.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28198,761421857,2021-12-02T19:56:55Z,src/core/lib/surface/call.cc,"@@ -983,17 +960,34 @@ class PublishToAppEncoder {    void Encode(grpc_mdelem md) { Append(GRPC_MDKEY(md), GRPC_MDVALUE(md)); } -  void Encode(grpc_core::GrpcTimeoutMetadata, grpc_millis) {}-  void Encode(grpc_core::TeMetadata, grpc_core::TeMetadata::ValueType) {}-   template <typename Which>","Maybe add a comment here saying that this is catch-all no-op for any built-in metadata that we don't want to return to the application?It's worth noting that the approach here means that any time we add new metadata that we want to be visible to the application, we'll need to remember to add it here.  I think that's a good thing, but I predict that there will be times when we'll forget.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28198,761430358,2021-12-02T20:10:02Z,src/core/lib/transport/metadata_batch.h,"@@ -184,6 +187,49 @@ struct GrpcTagsBinMetadata : public SimpleSliceBasedMetadata {   static absl::string_view key() { return ""grpc-tags-bin""; } }; +template <typename Int>+struct SimpleIntBasedMetadataBase {","Is there a reason that this is separate from `SimpleIntBasedMetadata`?  I don't see anywhere else that it's used, but maybe you have something in mind for a subsequent PR?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28008,761449081,2021-12-02T20:40:11Z,src/core/ext/filters/client_channel/health/health_check_client.cc,"@@ -249,11 +255,12 @@ bool DecodeResponse(grpc_slice_buffer* slice_buffer, grpc_error_handle* error) {  HealthCheckClient::CallState::CallState(     RefCountedPtr<HealthCheckClient> health_check_client,-    grpc_pollset_set* interested_parties)+    grpc_pollset_set* interested_parties, MemoryAllocator* allocator)","I don't think this parameter is necessary, since you can instead access it via `health_check_client_->call_allocator_`.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28198,761451478,2021-12-02T20:44:08Z,src/core/lib/surface/call.cc,"@@ -983,17 +960,34 @@ class PublishToAppEncoder {    void Encode(grpc_mdelem md) { Append(GRPC_MDKEY(md), GRPC_MDVALUE(md)); } -  void Encode(grpc_core::GrpcTimeoutMetadata, grpc_millis) {}-  void Encode(grpc_core::TeMetadata, grpc_core::TeMetadata::ValueType) {}-   template <typename Which>-  void Encode(Which, const grpc_core::Slice& value) {-    const auto key = Which::key();-    Append(grpc_core::ExternallyManagedSlice(key.data(), key.length()),-           value.c_slice());+  void Encode(Which, const typename Which::ValueType&) {}++  void Encode(grpc_core::UserAgentMetadata, const grpc_core::Slice& slice) {+    Append(grpc_core::UserAgentMetadata::key(), slice);+  }++  void Encode(grpc_core::GrpcPreviousRpcAttemptsMetadata, uint32_t count) {+    Append(grpc_core::GrpcPreviousRpcAttemptsMetadata::key(), count);+  }++  void Encode(grpc_core::GrpcRetryPushbackMsMetadata, grpc_millis count) {+    Append(grpc_core::GrpcRetryPushbackMsMetadata::key(), count);   }   private:+  void Append(absl::string_view key, int64_t value) {+    char buffer[GPR_LTOA_MIN_BUFSIZE];+    gpr_ltoa(value, buffer);",Note that such a pattern would generate about 4x the compiled code - due to needing to create the intermediate `std::string`. Added a Slice::FromInt64 instead to make the right pattern more accessible.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28008,761454265,2021-12-02T20:48:35Z,src/core/lib/surface/channel.h,"@@ -110,8 +110,9 @@ struct grpc_channel {   grpc_core::ManualConstructor<grpc_core::CallRegistrationTable>       registration_table;   grpc_core::RefCountedPtr<grpc_core::channelz::ChannelNode> channelz_node;+  grpc_core::ManualConstructor<grpc_core::MemoryAllocator> allocator;","Instead of adding more `ManualConstructor` data members, how about we go ahead and change `grpc_channel` to be allocated via `new` instead of `gpr_malloc()`, as per the TODO above?",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28008,761624144,2021-12-03T03:31:08Z,src/core/lib/surface/channel.h,"@@ -110,8 +110,9 @@ struct grpc_channel {   grpc_core::ManualConstructor<grpc_core::CallRegistrationTable>       registration_table;   grpc_core::RefCountedPtr<grpc_core::channelz::ChannelNode> channelz_node;+  grpc_core::ManualConstructor<grpc_core::MemoryAllocator> allocator;",That will be a bigger change: grpc_channel right now is allocated inline with the filter stack.Let's punt on that until we're doing the filter stack changes.,
10496191,sifmelcara,https://api.github.com/repos/grpc/grpc/pulls/28258,762007542,2021-12-03T14:58:41Z,test/core/transport/binder/end2end/fuzzers/BUILD,"@@ -0,0 +1,100 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++load(""//bazel:grpc_build_system.bzl"", ""grpc_cc_library"", ""grpc_package"")+load(""//test/core/util:grpc_fuzzer.bzl"", ""grpc_proto_fuzzer"")++grpc_package(+    name = ""test/core/transport/binder/end2end/fuzzers"",+    features = [+        ""layering_check"",+    ],+)++licenses([""notice""])++# Defines protobuf messages for generating inputs. We manually define proto+# library rules here so the same proto file can be shared between several+# grpc_proto_fuzzer targets+proto_library(+    name = ""binder_transport_fuzzer_proto"",+    srcs = [""proto/binder_transport_fuzzer.proto""],+)++cc_proto_library(+    name = ""binder_transport_fuzzer_cc_proto"",+    deps = [""binder_transport_fuzzer_proto""],+)","Self note: This fails copybara import ""name 'cc_proto_library' is not defined""Probably need to use `grpc_proto_library` here",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/28091,762171522,2021-12-03T18:53:45Z,src/core/ext/filters/client_channel/lb_policy.h,"@@ -325,8 +326,20 @@ class LoadBalancingPolicy : public InternallyRefCounted<LoadBalancingPolicy> {   /// Data passed to the UpdateLocked() method when new addresses and   /// config are available.   struct UpdateArgs {-    ServerAddressList addresses;+    /// A list of addresses, or an error indicating a failure to obtain the+    /// list of addresses.+    absl::StatusOr<ServerAddressList> addresses;+    /// The LB policy config.     RefCountedPtr<Config> config;+    /// A human-readable note providing context about the name resolution that+    /// provided this update.  LB policies may wish to include this message+    /// in RPC failure status messages.  For example, if the update has an+    /// empty list of addresses, this message might say ""no DNS entries+    /// found for <name>"".+    std::string resolution_note;","I'm a little worried that this `resolution_note` field will make it confusing as to which error should be propagated (between the statuses of addresses and config, and this field).It looks like there are no resolvers actually setting this field, and also `ClientChannel::CreateOrUpdateLbPolicyLocked` is not forwarding this field over to LB policy `UpdateArgs`Meanwhile, LB policies like pick_first are referencing this field in their error message for ""empty address list"".For such error messages, wouldn't all relevent debug info just be in the `address` field's absl status?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28091,762226612,2021-12-03T20:34:56Z,src/core/ext/filters/client_channel/lb_policy.h,"@@ -325,8 +326,20 @@ class LoadBalancingPolicy : public InternallyRefCounted<LoadBalancingPolicy> {   /// Data passed to the UpdateLocked() method when new addresses and   /// config are available.   struct UpdateArgs {-    ServerAddressList addresses;+    /// A list of addresses, or an error indicating a failure to obtain the+    /// list of addresses.+    absl::StatusOr<ServerAddressList> addresses;+    /// The LB policy config.     RefCountedPtr<Config> config;+    /// A human-readable note providing context about the name resolution that+    /// provided this update.  LB policies may wish to include this message+    /// in RPC failure status messages.  For example, if the update has an+    /// empty list of addresses, this message might say ""no DNS entries+    /// found for <name>"".+    std::string resolution_note;","The `resolution_note` is intended to be used specifically in the case where the resolver returns an empty list of addresses instead of an error.The idea here is that there are cases where a resolver may intentionally want to return an empty address list without indicating an error, because it's going to use an LB policy that gets the backend addresses via some out-of-band control plane.  However, not all LB policies will work without being given addresses; in particular, leaf policies like pick_first or round_robin require addresses.  So if someone accidentally configures a resolver to return an empty list of address with an LB policy that requires addresses, the LB policy needs some additional information in order to provide a useful status message for RPC failures.We don't currently have any resolvers that need to provide this information.  The only one we have that will return an empty list of addresses is the xds resolver, but it always configures the LB policy tree to get the backend addresses via EDS, so its result will never wind up being sent to a policy that requires a non-empty address list.However, there are cases that I think we should ultimately support that we're just not handling correctly today.  For example, I think in the c-ares resolver, when an A/AAAA query fails, we should be checking the error code from that failure: if we get NXDOMAIN, that just means that there were no matching A/AAAA entries, so the resolver should return an empty address list instead of returning an error.  (In contrast, if the query fails with something like SRVFAIL, then we should return an error.)  This would allow, for example, using grpclb without any fallback addresses, by populating SRV records but no A records.  And in the future, when we make the resolver and LB policy APIs public, people will be able to write their own policies that do this sort of thing.Internally, see go/grpc-client-channel-principles-revamp for more details.You're right that we should be propagating this in `ClientChannel::CreateOrUpdateLbPolicyLocked()` -- thanks for catching that!  I've fixed that, and I've added a TODO about a similar propagation that needs to happen in the xds_cluster_resolver LB policy.",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/28091,762314709,2021-12-03T22:07:21Z,src/core/ext/filters/client_channel/lb_policy.h,"@@ -325,8 +326,20 @@ class LoadBalancingPolicy : public InternallyRefCounted<LoadBalancingPolicy> {   /// Data passed to the UpdateLocked() method when new addresses and   /// config are available.   struct UpdateArgs {-    ServerAddressList addresses;+    /// A list of addresses, or an error indicating a failure to obtain the+    /// list of addresses.+    absl::StatusOr<ServerAddressList> addresses;+    /// The LB policy config.     RefCountedPtr<Config> config;+    /// A human-readable note providing context about the name resolution that+    /// provided this update.  LB policies may wish to include this message+    /// in RPC failure status messages.  For example, if the update has an+    /// empty list of addresses, this message might say ""no DNS entries+    /// found for <name>"".+    std::string resolution_note;","If we expect `resolution_note` to be used if and only if `addresses` is an empty address list, what do you think about changing the type of the `addresses` field to something like `absl::variant<absl::StatusOr<ServerAddressList>, std::string>` ?The second entry could be set by resolvers when they want to return an empty list with no error, but fill in a message. That way, it's easier to be sure that error messages aren't accidentally getting dropped.> However, there are cases that I think we should ultimately support that we're just not handling correctly today. For example, I think in the c-ares resolver, when an A/AAAA query fails, we should be checking the error code from that failure: if we get NXDOMAIN, that just means that there were no matching A/AAAA entries, so the resolver should return an empty address list instead of returning an error. (In contrast, if the query fails with something like SRVFAIL, then we should return an error.) This would allow, for example, using grpclb without any fallback addresses, by populating SRV records but no A records. And in the future, when we make the resolver and LB policy APIs public, people will be able to write their own policies that do this sort of thing.Interesting. For this specific case, I personally think that NXDOMAIN should still actually be considered an error, because depending on the specific DNS serving infrastructure in use, NXDOMAIN can sometimes be a transient error (e.g. b/151958298). I can see the usefulness of the ability do things like that in general though.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28091,762350204,2021-12-03T23:57:32Z,src/core/ext/filters/client_channel/lb_policy.h,"@@ -325,8 +326,20 @@ class LoadBalancingPolicy : public InternallyRefCounted<LoadBalancingPolicy> {   /// Data passed to the UpdateLocked() method when new addresses and   /// config are available.   struct UpdateArgs {-    ServerAddressList addresses;+    /// A list of addresses, or an error indicating a failure to obtain the+    /// list of addresses.+    absl::StatusOr<ServerAddressList> addresses;+    /// The LB policy config.     RefCountedPtr<Config> config;+    /// A human-readable note providing context about the name resolution that+    /// provided this update.  LB policies may wish to include this message+    /// in RPC failure status messages.  For example, if the update has an+    /// empty list of addresses, this message might say ""no DNS entries+    /// found for <name>"".+    std::string resolution_note;","I think using `absl::variant<>` in this case would be far more cumbersome.  In the majority of cases, we just want to pass along the address list as-is, without caring if it's empty or not.  But using `absl::variant<>` would force every caller to check the value, which IMHO is fairly ugly.Ultimately, I think `resolution_note` is optional, and it's not the end of the world if it's not populated.  It's useful for improving error messages, but it's not actually required for things to work right, so I'd rather not make the core functionality here more cumbersome just to avoid a case where it may be desirable but isn't present.With regard to NXDOMAIN, I do understand that there are probably existing cases in the wild where that error code will be caused by transient failures, but I would argue that any such case (including the bug you referenced) is a misbehavior on the part of the DNS serving infrastructure.  I'd probably be willing to have a channel arg to control whether or not the c-ares resolver interprets NXDOMAIN as an empty address list or as a transient failure, and we can argue about what the right default for that channel arg would be.  But in principle, I assert that NXDOMAIN should not be considered a transient failure.But in any case, we can debate that particular issue more later. :)",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28204,763234514,2021-12-06T17:47:55Z,src/core/lib/transport/metadata_batch.h,"@@ -229,6 +388,12 @@ struct GrpcRetryPushbackMsMetadata   static absl::string_view key() { return ""grpc-retry-pushback-ms""; } }; +// :status metadata trait.+// TODO(ctiller): consider moving to uint16_t+struct StatusMetadata : public SimpleIntBasedMetadata<uint32_t, 0> {","Maybe call this `HttpStatusMetadata`, to make it clear that this is not the gRPC status?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28204,763287832,2021-12-06T18:53:54Z,src/core/ext/filters/client_channel/health/health_check_client.cc,"@@ -326,12 +326,9 @@ void HealthCheckClient::CallState::StartCall() {   batch_.on_complete = GRPC_CLOSURE_INIT(&on_complete_, OnComplete, this,                                          grpc_schedule_on_exec_ctx);   // Add send_initial_metadata op.-  error = grpc_metadata_batch_add_head(-      &send_initial_metadata_, &path_metadata_storage_,",I think this means that we can remove the `path_metadata_storage_` data member.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28204,763305402,2021-12-06T19:13:34Z,src/core/ext/filters/http/server/http_server_filter.cc,"@@ -69,7 +66,6 @@ struct call_data {    // Outgoing headers to add to send_initial_metadata.   grpc_linked_mdelem status;",Looks like this field may no longer be needed.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28204,763362306,2021-12-06T20:23:46Z,src/core/tsi/alts/handshaker/alts_handshaker_client.cc,"@@ -721,7 +721,6 @@ alts_handshaker_client* alts_grpc_handshaker_client_create(                     grpc_schedule_on_exec_ctx);   GRPC_CLOSURE_INIT(&client->on_status_received, on_status_received, client,                     grpc_schedule_on_exec_ctx);-  grpc_slice_unref_internal(slice);",Why is this no longer needed?,X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28204,763484652,2021-12-06T23:18:34Z,src/core/lib/transport/metadata_batch.h,"@@ -229,6 +388,12 @@ struct GrpcRetryPushbackMsMetadata   static absl::string_view key() { return ""grpc-retry-pushback-ms""; } }; +// :status metadata trait.+// TODO(ctiller): consider moving to uint16_t+struct StatusMetadata : public SimpleIntBasedMetadata<uint32_t, 0> {",If we do that I want to do it for all : metadata I think (and I'm ok with doing so) - wdyt?,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28204,763487132,2021-12-06T23:22:19Z,src/core/ext/xds/xds_server_config_fetcher.cc,"@@ -1092,19 +1092,19 @@ ServerConfigSelector::CallConfig XdsServerConfigFetcher::ListenerWatcher::     FilterChainMatchManager::XdsServerConfigSelector::GetCallConfig(         grpc_metadata_batch* metadata) {   CallConfig call_config;-  if (metadata->legacy_index()->named.path == nullptr) {+  if (metadata->get_pointer(PathMetadata()) == nullptr) {     call_config.error = GRPC_ERROR_CREATE_FROM_STATIC_STRING(""No path found"");     return call_config;   }-  absl::string_view path = StringViewFromSlice(-      GRPC_MDVALUE(metadata->legacy_index()->named.path->md));-  if (metadata->legacy_index()->named.authority == nullptr) {+  absl::string_view path =+      metadata->get_pointer(PathMetadata())->as_string_view();","Minimal to none... get_pointer boils down to:`if ((present_fields_ & BIT_MASK_FOR_PATH) != 0) return &this->path_` (similarly for non-path things)So there's no overhead if the compiler can prove present_fields_ is unchanged, and a test-and-branch otherwise.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28204,763488177,2021-12-06T23:24:03Z,src/core/lib/security/transport/client_auth_filter.cc,"@@ -89,16 +91,16 @@ struct call_data {   void destroy() {     grpc_credentials_mdelem_array_destroy(&md_array);     creds.reset();-    grpc_slice_unref_internal(host);-    grpc_slice_unref_internal(method);     grpc_auth_metadata_context_reset(&auth_md_context);+    host.Destroy();+    method.Destroy();   }    grpc_call_stack* owning_call;   grpc_core::CallCombiner* call_combiner;   grpc_core::RefCountedPtr<grpc_call_credentials> creds;-  grpc_slice host = grpc_empty_slice();-  grpc_slice method = grpc_empty_slice();+  grpc_core::ManualConstructor<grpc_core::Slice> host;","The following comment made me believe otherwise (on line 84 in the original):```  // This method is technically the dtor of this class. However, since  // `get_request_metadata_cancel_closure` can run in parallel to  // `destroy_call_elem`, we cannot call the dtor in them. Otherwise,  // fields will be accessed after calling dtor, and msan correctly complains  // that the memory is not initialized.  void destroy() {```",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28304,764447122,2021-12-07T23:51:05Z,bazel/grpc_deps.bzl,"@@ -393,11 +393,11 @@ def grpc_deps():         http_archive(             name = ""libuv"",             build_file = ""@com_github_grpc_grpc//third_party:libuv.BUILD"",-            sha256 = ""5ca4e9091f3231d8ad8801862dc4e851c23af89c69141d27723157776f7291e7"",-            strip_prefix = ""libuv-02a9e1be252b623ee032a3137c0b0c94afbe6809"",+            sha256 = ""43f0180b9406eb10519183af462c37ec56dff28ba54c1bd9bdd4261187b0f95f"",+            strip_prefix = ""libuv-48e04275332f5753427d21a52f17ec6206451f2c"",             urls = [-                ""https://storage.googleapis.com/grpc-bazel-mirror/github.com/libuv/libuv/archive/02a9e1be252b623ee032a3137c0b0c94afbe6809.tar.gz"",-                ""https://github.com/libuv/libuv/archive/02a9e1be252b623ee032a3137c0b0c94afbe6809.tar.gz"",+                ""https://storage.googleapis.com/grpc-bazel-mirror/github.com/drfloob/libuv/archive/48e04275332f5753427d21a52f17ec6206451f2c.tar.gz"",+                ""https://github.com/drfloob/libuv/archive/48e04275332f5753427d21a52f17ec6206451f2c.tar.gz"",","This was a temporary thing in my PR, we shouldn't ship gRPC with a dependency on my personal branch. We may need to land a few more patches to libuv, but afterwards we'll likely ask them to cut a release so we can depend on released versions of libuv.Can this PR land without updating the libuv dependency? Which of the libuv changes are needed?",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28304,764466756,2021-12-08T00:43:05Z,src/objective-c/tests/ThirdPartyTests/Libuv/BUILD,"@@ -0,0 +1,24 @@+load(""@build_bazel_rules_apple//apple:ios.bzl"", ""ios_application"", ""ios_unit_test"")",`ios_application` is unused,
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/28315,765254755,2021-12-08T21:21:54Z,tools/gcp/utils/cleanup_xds_resources.py,"@@ -0,0 +1,174 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import logging+import json+import sys+import re+import os+import functools+import subprocess+import datetime+from dataclasses import dataclass+from typing import List, Any++# Parses commandline arguments+parser = argparse.ArgumentParser()+parser.add_argument('--dry_run', action='store_true')+args = parser.parse_args()++# Type alias+Json = Any++# Configures this script+KEEP_PERIOD = datetime.timedelta(days=14)+GCLOUD = os.environ.get('GCLOUD', 'gcloud')+GCLOUD_CMD_TIMEOUT_S = datetime.timedelta(seconds=5).total_seconds()+ZONE = 'us-central1-a'+SECONDARY_ZONE = 'us-west1-b'+PROJECT = 'grpc-testing'+++@functools.lru_cache()+def get_expire_timestamp():+    return (datetime.datetime.now() - KEEP_PERIOD).isoformat()+++def exec_gcloud(*cmds: List[str]) -> Json:+    cmds = [GCLOUD, '--project', PROJECT, '--quiet'] + list(cmds)+    if 'list' in cmds:+        # Add arguments to shape the list output+        cmds.extend([+            '--format', 'json', '--filter',+            f'creationTimestamp <= {get_expire_timestamp()}'+        ])+    if args.dry_run and 'delete' in cmds:+        # Skip deletion for dry-runs+        logging.debug('> Skipped: %s', "" "".join(cmds))+        return None+    # Executing the gcloud command+    logging.debug('Executing: %s', "" "".join(cmds))+    proc = subprocess.Popen(cmds,+                            stdout=subprocess.PIPE,+                            stderr=subprocess.PIPE,+                            text=True)+    returncode = proc.wait(timeout=GCLOUD_CMD_TIMEOUT_S)+    if returncode:+        logging.error('> Failed to execute cmd [%s], returned %d, stderr: %s',+                      "" "".join(cmds), returncode, proc.stderr.read())+        return None+    stdout = proc.stdout.read()+    if stdout:+        return json.loads(stdout)+    return None+++def remove_relative_resources_run_xds_tests(suffix: str):+    """"""Removing GCP resources created by run_xds_tests.py.""""""+    logging.info('Removing run_xds_tests.py resources with suffix [%s]', suffix)+    exec_gcloud('compute', 'forwarding-rules', 'delete',+                f'test-forwarding-rule{suffix}', '--global')+    exec_gcloud('compute', 'target-http-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud('alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud('compute', 'url-maps', 'delete', f'test-map{suffix}')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-alternate{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-extra{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-more-extra{suffix}', '--global')+    exec_gcloud('compute', 'firewall-rules', 'delete', f'test-fw-rule{suffix}')+    exec_gcloud('compute', 'health-checks', 'delete', f'test-hc{suffix}')+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig{suffix}', '--zone', ZONE)+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-same-zone{suffix}', '--zone', ZONE)+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-secondary-zone{suffix}', '--zone', SECONDARY_ZONE)+    exec_gcloud('compute', 'instance-templates', 'delete',+                f'test-template{suffix}')+++def remove_relative_resources_psm_sec(prefix: str):+    """"""Removing GCP resources created by PSM Sec framework.""""""+    logging.info('Removing PSM Security resources with prefix [%s]', prefix)+    exec_gcloud('compute', 'forwarding-rules', 'delete',+                f'{prefix}-forwarding-rule', '--global')+    exec_gcloud('alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'{prefix}-target-proxy')+    exec_gcloud('compute', 'url-maps', 'delete', f'{prefix}-url-map')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'{prefix}-backend-service', '--global')+    exec_gcloud('compute', 'health-checks', 'delete', f'{prefix}-health-check')+    exec_gcloud('compute', 'firewall-rules', 'delete',+                f'{prefix}-allow-health-checks')+    exec_gcloud('alpha', 'network-security', 'server-tls-policies', 'delete',+                f'{prefix}-server-tls-policy')+    exec_gcloud('alpha', 'network-security', 'client-tls-policies', 'delete',+                f'{prefix}-client-tls-policy')+++def check_one_type_of_gcp_resources(list_cmd: List[str],+                                    suffix_search: str = '',+                                    prefix_search: str = ''):+    logging.info('Checking GCP resources with %s or %s', suffix_search,+                 prefix_search)+    for resource in exec_gcloud(*list_cmd):+        if resource['name'].startswith('interop-psm-url-map'):+            logging.info('Skipping url-map test resource: %s', resource['name'])+            continue+        if suffix_search:+            result = re.search(suffix_search, resource['name'])+            if result is not None:+                remove_relative_resources_run_xds_tests(result.group(1))+                continue++        if prefix_search:+            result = re.search(prefix_search, resource['name'])+            if result is not None:+                remove_relative_resources_psm_sec(result.group(1))","Should we potentially wait for @menghanl's new script? It should support more interop tests than security, and also take care of k8s. Or we could hide this one behind a flag.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28313,765417674,2021-12-09T04:05:36Z,src/core/lib/promise/arena_promise.h,"@@ -149,10 +149,10 @@ class ArenaPromise {   ArenaPromise() = default;    // Construct an ArenaPromise that will call the given callable when polled.-  template <typename Callable>-  ArenaPromise(Arena* arena, Callable&& callable)+  template <typename Callable, typename Ignored = absl::enable_if_t<!std::is_same<Callable, ArenaPromise>::value>>","C++ nonsense... by default this template would override the `ArenaPromise&&` constructor and move would break.This makes it an error to call this particular constructor with a type that is ArenaPromise, which then uses SFINAE to choose the *other* constructor and get move semantics back.",X
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/28315,766066327,2021-12-09T18:52:29Z,tools/gcp/utils/cleanup_xds_resources.py,"@@ -0,0 +1,174 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import logging+import json+import sys+import re+import os+import functools+import subprocess+import datetime+from dataclasses import dataclass+from typing import List, Any++# Parses commandline arguments+parser = argparse.ArgumentParser()+parser.add_argument('--dry_run', action='store_true')+args = parser.parse_args()++# Type alias+Json = Any++# Configures this script+KEEP_PERIOD = datetime.timedelta(days=14)+GCLOUD = os.environ.get('GCLOUD', 'gcloud')+GCLOUD_CMD_TIMEOUT_S = datetime.timedelta(seconds=5).total_seconds()+ZONE = 'us-central1-a'+SECONDARY_ZONE = 'us-west1-b'+PROJECT = 'grpc-testing'+++@functools.lru_cache()+def get_expire_timestamp():+    return (datetime.datetime.now() - KEEP_PERIOD).isoformat()+++def exec_gcloud(*cmds: List[str]) -> Json:+    cmds = [GCLOUD, '--project', PROJECT, '--quiet'] + list(cmds)+    if 'list' in cmds:+        # Add arguments to shape the list output+        cmds.extend([+            '--format', 'json', '--filter',+            f'creationTimestamp <= {get_expire_timestamp()}'+        ])+    if args.dry_run and 'delete' in cmds:+        # Skip deletion for dry-runs+        logging.debug('> Skipped: %s', "" "".join(cmds))+        return None+    # Executing the gcloud command+    logging.debug('Executing: %s', "" "".join(cmds))+    proc = subprocess.Popen(cmds,+                            stdout=subprocess.PIPE,+                            stderr=subprocess.PIPE,+                            text=True)+    returncode = proc.wait(timeout=GCLOUD_CMD_TIMEOUT_S)+    if returncode:+        logging.error('> Failed to execute cmd [%s], returned %d, stderr: %s',+                      "" "".join(cmds), returncode, proc.stderr.read())+        return None+    stdout = proc.stdout.read()+    if stdout:+        return json.loads(stdout)+    return None+++def remove_relative_resources_run_xds_tests(suffix: str):+    """"""Removing GCP resources created by run_xds_tests.py.""""""+    logging.info('Removing run_xds_tests.py resources with suffix [%s]', suffix)+    exec_gcloud('compute', 'forwarding-rules', 'delete',+                f'test-forwarding-rule{suffix}', '--global')+    exec_gcloud('compute', 'target-http-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud('alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud('compute', 'url-maps', 'delete', f'test-map{suffix}')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-alternate{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-extra{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-more-extra{suffix}', '--global')+    exec_gcloud('compute', 'firewall-rules', 'delete', f'test-fw-rule{suffix}')+    exec_gcloud('compute', 'health-checks', 'delete', f'test-hc{suffix}')+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig{suffix}', '--zone', ZONE)+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-same-zone{suffix}', '--zone', ZONE)+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-secondary-zone{suffix}', '--zone', SECONDARY_ZONE)+    exec_gcloud('compute', 'instance-templates', 'delete',+                f'test-template{suffix}')+++def remove_relative_resources_psm_sec(prefix: str):+    """"""Removing GCP resources created by PSM Sec framework.""""""+    logging.info('Removing PSM Security resources with prefix [%s]', prefix)+    exec_gcloud('compute', 'forwarding-rules', 'delete',+                f'{prefix}-forwarding-rule', '--global')+    exec_gcloud('alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'{prefix}-target-proxy')+    exec_gcloud('compute', 'url-maps', 'delete', f'{prefix}-url-map')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'{prefix}-backend-service', '--global')+    exec_gcloud('compute', 'health-checks', 'delete', f'{prefix}-health-check')+    exec_gcloud('compute', 'firewall-rules', 'delete',+                f'{prefix}-allow-health-checks')+    exec_gcloud('alpha', 'network-security', 'server-tls-policies', 'delete',+                f'{prefix}-server-tls-policy')+    exec_gcloud('alpha', 'network-security', 'client-tls-policies', 'delete',+                f'{prefix}-client-tls-policy')+++def check_one_type_of_gcp_resources(list_cmd: List[str],+                                    suffix_search: str = '',+                                    prefix_search: str = ''):+    logging.info('Checking GCP resources with %s or %s', suffix_search,+                 prefix_search)+    for resource in exec_gcloud(*list_cmd):+        if resource['name'].startswith('interop-psm-url-map'):+            logging.info('Skipping url-map test resource: %s', resource['name'])+            continue+        if suffix_search:+            result = re.search(suffix_search, resource['name'])+            if result is not None:+                remove_relative_resources_run_xds_tests(result.group(1))+                continue++        if prefix_search:+            result = re.search(prefix_search, resource['name'])+            if result is not None:+                remove_relative_resources_psm_sec(result.group(1))+                continue+++def check_costly_gcp_resources() -> None:+    check_one_type_of_gcp_resources(['compute', 'forwarding-rules', 'list'],","E.g. `forwarding-rule` depends on the `target-proxy`. So- leaked `forwarding-rule` --> there's a leaked `target-proxy` (because this target proxy cannot deleted unless the forwarding rule is deleted)So the leaked `target-proxy` is guaranteed to be a super set of leaked `forwarding-rule`.If we follow the dependency chain, the leaf is `health-check` and `instance-template`",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28315,766066628,2021-12-09T18:52:54Z,tools/gcp/utils/cleanup_xds_resources.py,"@@ -0,0 +1,174 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import logging+import json+import sys+import re+import os+import functools+import subprocess+import datetime+from dataclasses import dataclass+from typing import List, Any++# Parses commandline arguments+parser = argparse.ArgumentParser()+parser.add_argument('--dry_run', action='store_true')+args = parser.parse_args()++# Type alias+Json = Any++# Configures this script+KEEP_PERIOD = datetime.timedelta(days=14)+GCLOUD = os.environ.get('GCLOUD', 'gcloud')+GCLOUD_CMD_TIMEOUT_S = datetime.timedelta(seconds=5).total_seconds()+ZONE = 'us-central1-a'+SECONDARY_ZONE = 'us-west1-b'+PROJECT = 'grpc-testing'+++@functools.lru_cache()+def get_expire_timestamp():+    return (datetime.datetime.now() - KEEP_PERIOD).isoformat()+++def exec_gcloud(*cmds: List[str]) -> Json:+    cmds = [GCLOUD, '--project', PROJECT, '--quiet'] + list(cmds)+    if 'list' in cmds:+        # Add arguments to shape the list output+        cmds.extend([+            '--format', 'json', '--filter',+            f'creationTimestamp <= {get_expire_timestamp()}'+        ])+    if args.dry_run and 'delete' in cmds:+        # Skip deletion for dry-runs+        logging.debug('> Skipped: %s', "" "".join(cmds))+        return None+    # Executing the gcloud command+    logging.debug('Executing: %s', "" "".join(cmds))+    proc = subprocess.Popen(cmds,+                            stdout=subprocess.PIPE,+                            stderr=subprocess.PIPE,+                            text=True)+    returncode = proc.wait(timeout=GCLOUD_CMD_TIMEOUT_S)+    if returncode:+        logging.error('> Failed to execute cmd [%s], returned %d, stderr: %s',+                      "" "".join(cmds), returncode, proc.stderr.read())+        return None+    stdout = proc.stdout.read()+    if stdout:+        return json.loads(stdout)+    return None+++def remove_relative_resources_run_xds_tests(suffix: str):+    """"""Removing GCP resources created by run_xds_tests.py.""""""+    logging.info('Removing run_xds_tests.py resources with suffix [%s]', suffix)+    exec_gcloud('compute', 'forwarding-rules', 'delete',+                f'test-forwarding-rule{suffix}', '--global')+    exec_gcloud('compute', 'target-http-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud('alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud('compute', 'url-maps', 'delete', f'test-map{suffix}')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-alternate{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-extra{suffix}', '--global')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'test-backend-service-more-extra{suffix}', '--global')+    exec_gcloud('compute', 'firewall-rules', 'delete', f'test-fw-rule{suffix}')+    exec_gcloud('compute', 'health-checks', 'delete', f'test-hc{suffix}')+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig{suffix}', '--zone', ZONE)+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-same-zone{suffix}', '--zone', ZONE)+    exec_gcloud('compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-secondary-zone{suffix}', '--zone', SECONDARY_ZONE)+    exec_gcloud('compute', 'instance-templates', 'delete',+                f'test-template{suffix}')+++def remove_relative_resources_psm_sec(prefix: str):+    """"""Removing GCP resources created by PSM Sec framework.""""""+    logging.info('Removing PSM Security resources with prefix [%s]', prefix)+    exec_gcloud('compute', 'forwarding-rules', 'delete',+                f'{prefix}-forwarding-rule', '--global')+    exec_gcloud('alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'{prefix}-target-proxy')+    exec_gcloud('compute', 'url-maps', 'delete', f'{prefix}-url-map')+    exec_gcloud('compute', 'backend-services', 'delete',+                f'{prefix}-backend-service', '--global')+    exec_gcloud('compute', 'health-checks', 'delete', f'{prefix}-health-check')+    exec_gcloud('compute', 'firewall-rules', 'delete',+                f'{prefix}-allow-health-checks')+    exec_gcloud('alpha', 'network-security', 'server-tls-policies', 'delete',+                f'{prefix}-server-tls-policy')+    exec_gcloud('alpha', 'network-security', 'client-tls-policies', 'delete',+                f'{prefix}-client-tls-policy')+++def check_one_type_of_gcp_resources(list_cmd: List[str],+                                    suffix_search: str = '',+                                    prefix_search: str = ''):+    logging.info('Checking GCP resources with %s or %s', suffix_search,+                 prefix_search)+    for resource in exec_gcloud(*list_cmd):+        if resource['name'].startswith('interop-psm-url-map'):+            logging.info('Skipping url-map test resource: %s', resource['name'])+            continue+        if suffix_search:+            result = re.search(suffix_search, resource['name'])+            if result is not None:+                remove_relative_resources_run_xds_tests(result.group(1))+                continue++        if prefix_search:+            result = re.search(prefix_search, resource['name'])+            if result is not None:+                remove_relative_resources_psm_sec(result.group(1))+                continue+++def check_costly_gcp_resources() -> None:+    check_one_type_of_gcp_resources(['compute', 'forwarding-rules', 'list'],","The quota for them is quite large, so I didn't mark them as expensive:- Instance-templates: 2500- Health-check: 1250I hope to check all expensive resources because people might delete certain parts of them and left some behind.",X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/27754,766892330,2021-12-10T18:25:49Z,src/core/lib/matchers/matchers.cc,"@@ -291,7 +291,7 @@ bool HeaderMatcher::Match(     match = value.has_value() == present_match_;   } else if (!value.has_value()) {     // All other types fail to match if field is not present.-    match = false;+    return false;","This block would be triggered for types that are `not Present match` and value is absent.Envoy also returns failed match in that case, irrespective of the invert value.https://github.com/envoyproxy/envoy/blob/4c76813087c07171996d2cd8a9c591604b9124d8/source/common/http/header_utility.cc#L142@markdroth @ejona86 Please correct me if I am wrong. Thank you.",
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/28304,767022973,2021-12-10T22:29:53Z,bazel/grpc_deps.bzl,"@@ -181,7 +181,7 @@ def grpc_deps():     )      native.bind(-        name = ""libuv"",+        name = ""uv"",","This change is grafted from the timer PR patch, it looks like the original ""libuv"" naming cannot be found by upstream bazel targets. ",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28265,767805909,2021-12-13T14:27:36Z,tools/run_tests/performance/dashboard_config/cloudbuild.yaml,"@@ -0,0 +1,36 @@+steps:",I think the tools/run_tests/performance is not a good place to put this. It is where our legacy performance benchmarks live and we don't want to add more stuff there. I'm actually not convinced that the configuration belongs to the grpc/grpc repository at all,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28265,767809591,2021-12-13T14:31:33Z,tools/run_tests/performance/dashboard_config/configure.sh,"@@ -0,0 +1,60 @@+#!/bin/bash+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++# check_env checks that all required environment variables are set+check_env () {+  env_vars=(""$@"")+  missing_var=0+  for var in ""${env_vars[@]}""; do+    if [ -z ""${!var}"" ]; then+      echo ""${var} not set""+      missing_var=1+    fi+  done+  return $missing_var+}++# substitute_env_in_files injects environment variables to files passed as+# arguments+substitute_env_in_files () {+  files=(""$@"")+  for file in ""${files[@]}""; do+    tmp=$(mktemp)+    echo ""Making substitutions in ${file}""+    cp --attributes-only --preserve ""${file}"" ""${tmp}""+    envsubst < ""${file}"" > ""$tmp"" && mv ""$tmp"" ""$file""+  done+}++# Check required environment variables+check_env GCP_PROJECT_ID GCP_GRAFANA_SERVICE GCP_DATA_TRANSFER_SERVICE BQ_PROJECT_ID PG_USER PG_PASS PG_DATABASE GRAFANA_ADMIN_PASS CLOUD_SQL_INSTANCE || exit 1++# Configure postgres replicator+git clone --depth 1 https://github.com/grpc/test-infra",I think the configuration being in this repository and the code in grpc/test-infra only makes things more complex.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28223,769031505,2021-12-14T20:46:00Z,src/core/ext/filters/message_size/message_size_filter.cc,"@@ -201,6 +201,7 @@ static void recv_message_ready(void* user_data, grpc_error_handle error) {   if (*calld->recv_message != nullptr && calld->limits.max_recv_size >= 0 &&       (*calld->recv_message)->length() >           static_cast<size_t>(calld->limits.max_recv_size)) {+    GPR_ASSERT((*calld->recv_message)->length() != 1024);","Why is this here?  This seems like a fairly arbitrary restriction.  Was this maybe added for testing something and accidentally left in?  If not, it definitely deserves a comment explaining it.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28223,769160880,2021-12-15T00:52:57Z,src/core/lib/compression/compression.cc,"@@ -23,89 +23,48 @@  #include <grpc/compression.h> -#include ""src/core/lib/compression/algorithm_metadata.h"" #include ""src/core/lib/compression/compression_internal.h"" #include ""src/core/lib/gpr/useful.h"" #include ""src/core/lib/slice/slice_utils.h"" #include ""src/core/lib/surface/api_trace.h"" #include ""src/core/lib/transport/static_metadata.h"" -int grpc_compression_algorithm_is_message(-    grpc_compression_algorithm algorithm) {-  return (algorithm >= GRPC_COMPRESS_DEFLATE && algorithm <= GRPC_COMPRESS_GZIP)-             ? 1-             : 0;+int grpc_compression_algorithm_is_message(grpc_compression_algorithm) {",Do we have a policy for (or prior examples of) how to mark a public API function as deprecated?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28370,770102789,2021-12-15T23:00:30Z,tools/run_tests/xds_k8s_test_driver/bin/cleanup/cleanup.py,"@@ -0,0 +1,253 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Clean up resources created by the tests.++This is intended as a tool to delete leaked resources from old tests.++Typical usage examples:++    # Usually called by a script searching for prefix and suffix of leaked+    # resources.+    python -m bin.cleanup --project=grpc-testing --network=default-vpc --resource_prefix=""abc""+""""""+import datetime+import functools+import logging+import json+import os+import re+import subprocess+from typing import Any, List++from absl import app+from absl import flags++from framework import xds_flags+from framework.infrastructure import gcp+from framework.infrastructure import traffic_director++logger = logging.getLogger(__name__)+Json = Any++KEEP_PERIOD = datetime.timedelta(days=14)+GCLOUD = os.environ.get('GCLOUD', 'gcloud')+GCLOUD_CMD_TIMEOUT_S = datetime.timedelta(seconds=5).total_seconds()+ZONE = 'us-central1-a'+SECONDARY_ZONE = 'us-west1-b'++PSM_SECURITY_PREFIX = 'xds-k8s-security'  # Prefix for gke resources to delete.++DRY_RUN = flags.DEFINE_bool(+    ""dry_run"",+    default=False,+    help=""dry run, print resources but do not perform deletion"")+++def load_keep_config() -> None:+    global KEEP_CONFIG+    json_path = os.path.realpath(+        os.path.join(os.path.dirname(os.path.abspath(__file__)),+                     'keep_xds_interop_resources.json'))+    with open(json_path, 'r') as f:+        KEEP_CONFIG = json.load(f)+        logging.debug('Resource keep config loaded: %s',+                      json.dumps(KEEP_CONFIG, indent=2))+++def is_marked_as_keep_gce(suffix: str) -> bool:+    return suffix in KEEP_CONFIG[""gce_framework""][""suffix""]+++def is_marked_as_keep_gke(suffix: str) -> bool:+    return suffix in KEEP_CONFIG[""gke_framework""][""suffix""]+++@functools.lru_cache()+def get_expire_timestamp() -> str:+    return (datetime.datetime.now() - KEEP_PERIOD).isoformat()+++def exec_gcloud(project: str, *cmds: List[str]) -> Json:+    cmds = [GCLOUD, '--project', project, '--quiet'] + list(cmds)+    if 'list' in cmds:+        # Add arguments to shape the list output+        cmds.extend([+            '--format', 'json', '--filter',+            f'creationTimestamp <= {get_expire_timestamp()}'+        ])+    # Executing the gcloud command+    logging.debug('Executing: %s', "" "".join(cmds))+    proc = subprocess.Popen(cmds,+                            stdout=subprocess.PIPE,+                            stderr=subprocess.PIPE)+    # NOTE(lidiz) the gcloud subprocess won't return unless its output is read+    stdout = proc.stdout.read()+    stderr = proc.stderr.read()+    try:+        returncode = proc.wait(timeout=GCLOUD_CMD_TIMEOUT_S)+    except subprocess.TimeoutExpired:+        logging.error('> Timeout executing cmd [%s]', "" "".join(cmds))+        return None+    if returncode:+        logging.error('> Failed to execute cmd [%s], returned %d, stderr: %s',+                      "" "".join(cmds), returncode, stderr)+        return None+    if stdout:+        return json.loads(stdout)+    return None+++def remove_relative_resources_run_xds_tests(project: str, suffix: str):+    """"""Removing GCP resources created by run_xds_tests.py.""""""+    logging.info('----- Removing run_xds_tests.py resources with suffix [%s]', suffix)+    exec_gcloud(project, 'compute', 'forwarding-rules', 'delete',+                f'test-forwarding-rule{suffix}', '--global')+    exec_gcloud(project, 'compute', 'target-http-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud(project, 'alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud(project, 'compute', 'url-maps', 'delete', f'test-map{suffix}')+    exec_gcloud(project, 'compute', 'backend-services', 'delete',+                f'test-backend-service{suffix}', '--global')+    exec_gcloud(project, 'compute', 'backend-services', 'delete',+                f'test-backend-service-alternate{suffix}', '--global')+    exec_gcloud(project, 'compute', 'backend-services', 'delete',+                f'test-backend-service-extra{suffix}', '--global')+    exec_gcloud(project, 'compute', 'backend-services', 'delete',+                f'test-backend-service-more-extra{suffix}', '--global')+    exec_gcloud(project, 'compute', 'firewall-rules', 'delete', f'test-fw-rule{suffix}')+    exec_gcloud(project, 'compute', 'health-checks', 'delete', f'test-hc{suffix}')+    exec_gcloud(project, 'compute', 'instance-groups', 'managed', 'delete',+                f'test-ig{suffix}', '--zone', ZONE)+    exec_gcloud(project, 'compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-same-zone{suffix}', '--zone', ZONE)+    exec_gcloud(project, 'compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-secondary-zone{suffix}', '--zone', SECONDARY_ZONE)+    exec_gcloud(project, 'compute', 'instance-templates', 'delete',+                f'test-template{suffix}')+++# cleanup_td creates TrafficDirectorManager (and its varients for security and+# AppNet), and then calls the cleanup() methods.+#+# Note that the varients are all based on the basic TrafficDirectorManager, so+# their `cleanup()` might do duplicate work. But deleting an non-exist resource+# returns 404, and is OK.+def cleanup_td_for_gke(project, network, resource_prefix, resource_suffix):+    gcp_api_manager = gcp.api.GcpApiManager()+    plain_td = traffic_director.TrafficDirectorManager(+        gcp_api_manager,+        project=project,+        network=network,+        resource_prefix=resource_prefix,+        resource_suffix=resource_suffix)+    security_td = traffic_director.TrafficDirectorSecureManager(+        gcp_api_manager,+        project=project,+        network=network,+        resource_prefix=resource_prefix,+        resource_suffix=resource_suffix)+    # TODO: cleanup appnet resources.+    # appnet_td = traffic_director.TrafficDirectorAppNetManager(+    #     gcp_api_manager,+    #     project=project,+    #     network=network,+    #     resource_prefix=resource_prefix,+    #     resource_suffix=resource_suffix)++    logger.info('----- Removing traffic director for gke, prefix %s, suffix %s', resource_prefix, resource_suffix)+    security_td.cleanup(force=True)+    # appnet_td.cleanup(force=True)+    plain_td.cleanup(force=True)","I will recommend to consider also cleaning GKE resources. When Kokoro job times out, or we made a big mistake in framework, the K8s Namespace might be left behind. If that accumulates terribly, the cluster resource could deplete.",
960845,menghanl,https://api.github.com/repos/grpc/grpc/pulls/28370,770103998,2021-12-15T23:02:51Z,tools/run_tests/xds_k8s_test_driver/bin/cleanup/cleanup.py,"@@ -0,0 +1,253 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Clean up resources created by the tests.++This is intended as a tool to delete leaked resources from old tests.++Typical usage examples:++    # Usually called by a script searching for prefix and suffix of leaked+    # resources.+    python -m bin.cleanup --project=grpc-testing --network=default-vpc --resource_prefix=""abc""+""""""+import datetime+import functools+import logging+import json+import os+import re+import subprocess+from typing import Any, List++from absl import app+from absl import flags++from framework import xds_flags+from framework.infrastructure import gcp+from framework.infrastructure import traffic_director++logger = logging.getLogger(__name__)+Json = Any++KEEP_PERIOD = datetime.timedelta(days=14)+GCLOUD = os.environ.get('GCLOUD', 'gcloud')+GCLOUD_CMD_TIMEOUT_S = datetime.timedelta(seconds=5).total_seconds()+ZONE = 'us-central1-a'+SECONDARY_ZONE = 'us-west1-b'++PSM_SECURITY_PREFIX = 'xds-k8s-security'  # Prefix for gke resources to delete.++DRY_RUN = flags.DEFINE_bool(+    ""dry_run"",+    default=False,+    help=""dry run, print resources but do not perform deletion"")+++def load_keep_config() -> None:+    global KEEP_CONFIG+    json_path = os.path.realpath(+        os.path.join(os.path.dirname(os.path.abspath(__file__)),+                     'keep_xds_interop_resources.json'))+    with open(json_path, 'r') as f:+        KEEP_CONFIG = json.load(f)+        logging.debug('Resource keep config loaded: %s',+                      json.dumps(KEEP_CONFIG, indent=2))+++def is_marked_as_keep_gce(suffix: str) -> bool:+    return suffix in KEEP_CONFIG[""gce_framework""][""suffix""]+++def is_marked_as_keep_gke(suffix: str) -> bool:+    return suffix in KEEP_CONFIG[""gke_framework""][""suffix""]+++@functools.lru_cache()+def get_expire_timestamp() -> str:+    return (datetime.datetime.now() - KEEP_PERIOD).isoformat()+++def exec_gcloud(project: str, *cmds: List[str]) -> Json:+    cmds = [GCLOUD, '--project', project, '--quiet'] + list(cmds)+    if 'list' in cmds:+        # Add arguments to shape the list output+        cmds.extend([+            '--format', 'json', '--filter',+            f'creationTimestamp <= {get_expire_timestamp()}'+        ])+    # Executing the gcloud command+    logging.debug('Executing: %s', "" "".join(cmds))+    proc = subprocess.Popen(cmds,+                            stdout=subprocess.PIPE,+                            stderr=subprocess.PIPE)+    # NOTE(lidiz) the gcloud subprocess won't return unless its output is read+    stdout = proc.stdout.read()+    stderr = proc.stderr.read()+    try:+        returncode = proc.wait(timeout=GCLOUD_CMD_TIMEOUT_S)+    except subprocess.TimeoutExpired:+        logging.error('> Timeout executing cmd [%s]', "" "".join(cmds))+        return None+    if returncode:+        logging.error('> Failed to execute cmd [%s], returned %d, stderr: %s',+                      "" "".join(cmds), returncode, stderr)+        return None+    if stdout:+        return json.loads(stdout)+    return None+++def remove_relative_resources_run_xds_tests(project: str, suffix: str):+    """"""Removing GCP resources created by run_xds_tests.py.""""""+    logging.info('----- Removing run_xds_tests.py resources with suffix [%s]', suffix)+    exec_gcloud(project, 'compute', 'forwarding-rules', 'delete',+                f'test-forwarding-rule{suffix}', '--global')+    exec_gcloud(project, 'compute', 'target-http-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud(project, 'alpha', 'compute', 'target-grpc-proxies', 'delete',+                f'test-target-proxy{suffix}')+    exec_gcloud(project, 'compute', 'url-maps', 'delete', f'test-map{suffix}')+    exec_gcloud(project, 'compute', 'backend-services', 'delete',+                f'test-backend-service{suffix}', '--global')+    exec_gcloud(project, 'compute', 'backend-services', 'delete',+                f'test-backend-service-alternate{suffix}', '--global')+    exec_gcloud(project, 'compute', 'backend-services', 'delete',+                f'test-backend-service-extra{suffix}', '--global')+    exec_gcloud(project, 'compute', 'backend-services', 'delete',+                f'test-backend-service-more-extra{suffix}', '--global')+    exec_gcloud(project, 'compute', 'firewall-rules', 'delete', f'test-fw-rule{suffix}')+    exec_gcloud(project, 'compute', 'health-checks', 'delete', f'test-hc{suffix}')+    exec_gcloud(project, 'compute', 'instance-groups', 'managed', 'delete',+                f'test-ig{suffix}', '--zone', ZONE)+    exec_gcloud(project, 'compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-same-zone{suffix}', '--zone', ZONE)+    exec_gcloud(project, 'compute', 'instance-groups', 'managed', 'delete',+                f'test-ig-secondary-zone{suffix}', '--zone', SECONDARY_ZONE)+    exec_gcloud(project, 'compute', 'instance-templates', 'delete',+                f'test-template{suffix}')+++# cleanup_td creates TrafficDirectorManager (and its varients for security and+# AppNet), and then calls the cleanup() methods.+#+# Note that the varients are all based on the basic TrafficDirectorManager, so+# their `cleanup()` might do duplicate work. But deleting an non-exist resource+# returns 404, and is OK.+def cleanup_td_for_gke(project, network, resource_prefix, resource_suffix):+    gcp_api_manager = gcp.api.GcpApiManager()+    plain_td = traffic_director.TrafficDirectorManager(+        gcp_api_manager,+        project=project,+        network=network,+        resource_prefix=resource_prefix,+        resource_suffix=resource_suffix)+    security_td = traffic_director.TrafficDirectorSecureManager(+        gcp_api_manager,+        project=project,+        network=network,+        resource_prefix=resource_prefix,+        resource_suffix=resource_suffix)+    # TODO: cleanup appnet resources.+    # appnet_td = traffic_director.TrafficDirectorAppNetManager(+    #     gcp_api_manager,+    #     project=project,+    #     network=network,+    #     resource_prefix=resource_prefix,+    #     resource_suffix=resource_suffix)++    logger.info('----- Removing traffic director for gke, prefix %s, suffix %s', resource_prefix, resource_suffix)+    security_td.cleanup(force=True)+    # appnet_td.cleanup(force=True)+    plain_td.cleanup(force=True)+++def main(argv):+    if len(argv) > 1:+        raise app.UsageError('Too many command-line arguments.')+    load_keep_config()++    project: str = xds_flags.PROJECT.value+    network: str = xds_flags.NETWORK.value+    dry_run: bool = DRY_RUN.value++    # List resources older than KEEP_PERIOD. We only list health-checks and+    # instance templates because these are leaves in the resource dependency tree.+    #+    # E.g. forwarding-rule depends on the target-proxy. So leaked+    # forwarding-rule indicates there's a leaked target-proxy (because this+    # target proxy cannot deleted unless the forwarding rule is deleted). The+    # leaked target-proxy is guaranteed to be a super set of leaked+    # forwarding-rule.+    leakedHealthChecks = exec_gcloud(project, 'compute', 'health-checks', 'list')+    for resource in leakedHealthChecks:+        logger.info('-----')+        logger.info('----- Cleaning up health check %s', resource['name'])+        if dry_run:+            # Skip deletion for dry-runs+            logging.info('----- Skipped [Dry Run]: %s', resource['name'])+            continue++        # Cleanup resources from the gce framewok.+        result = re.search(r'test-hc(.*)', resource['name'])+        if result is not None:+            if is_marked_as_keep_gce(result.group(1)):+                logging.info('Skipped [keep]: GCE resource suffix [%s] is marked as keep',+                             result.group(1))+                continue+            remove_relative_resources_run_xds_tests(project, result.group(1))+            continue++        # Cleanup resources from the gke framework.+        result = re.search(f'{PSM_SECURITY_PREFIX}-health-check-(.*)',+                           resource['name'])+        if result is not None:+            if is_marked_as_keep_gke(result.group(1)):+                logging.info('Skipped [keep]: GKE resource suffix [%s] is marked as keep',+                             result.group(1))+                continue++            cleanup_td_for_gke(project, network, PSM_SECURITY_PREFIX,+                               result.group(1))+            # TODO: cleanup gke clients and servers, but those need k8s context.","I saw the testing scripts just use `--kube_context=""${KUBE_CONTEXT}""`: https://github.com/grpc/grpc-go/blob/master/test/kokoro/psm-security.sh#L103Is it available for all kokoro jobs?",X
55257063,ashithasantosh,https://api.github.com/repos/grpc/grpc/pulls/28371,770807277,2021-12-16T18:31:01Z,src/core/lib/security/authorization/matchers.cc,"@@ -130,6 +130,23 @@ bool NotAuthorizationMatcher::Matches(const EvaluateArgs& args) const { }  bool HeaderAuthorizationMatcher::Matches(const EvaluateArgs& args) const {+  if (matcher_.name() == HttpMethodMetadata().key()) {","Thank you @yashykt for making the updates. I thought GetHeaderValue would be able to fetch :method, :authority and :path too. But recently, when I was working on the PR related to A41 proposal, I realized there is some difference. I was just curious, why the change in the handling of certain metadata keys?Also I think instead of adding these changes here, we could update EvaluateArgs::GetHeaderValuehttps://github.com/grpc/grpc/blob/master/src/core/lib/security/authorization/evaluate_args.cc#L117```if (key == HttpMethodMetadata().key()) {  return GetMethod();}// similarly for :path and :authority```",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/28371,770970008,2021-12-16T22:41:27Z,src/core/lib/security/authorization/matchers.cc,"@@ -130,6 +130,23 @@ bool NotAuthorizationMatcher::Matches(const EvaluateArgs& args) const { }  bool HeaderAuthorizationMatcher::Matches(const EvaluateArgs& args) const {+  if (matcher_.name() == HttpMethodMetadata().key()) {","Done. This seems to be a temporary requirement, so I've added a TODO to remove the special case after #28267 is merged. The tests seem like a nice addition anyway.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28092,777490853,2022-01-03T13:56:00Z,tools/run_tests/artifacts/artifact_targets.py,"@@ -403,6 +419,16 @@ def targets():         PythonArtifact('linux_extra', 'armv7', 'cp38-cp38'),         PythonArtifact('linux_extra', 'armv7', 'cp39-cp39'),         PythonArtifact('linux_extra', 'armv7', 'cp310-cp310', presubmit=True),+        PythonArtifact('musllinux_1_1', 'x64', 'cp310-cp310', presubmit=True),","FTR, I think the grpc_build_artifacts slowdown potentially added by this PR is partially masked by the fact that the all the ruby artifacts are built serially (in a single 'task') and the long tail of the ruby artifact build makes to job finish much later than it otherwise could. I'm planning to remove that limitation in https://github.com/grpc/grpc/pull/28243, and once that PR is in, the negative effect of adding the musllinux artifacts will become more significant. I don't think we want that.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28267,778408126,2022-01-04T21:44:32Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -2405,16 +2403,24 @@ void ClientChannel::CallData::CreateDynamicCall(grpc_call_element* elem) { class ClientChannel::LoadBalancedCall::Metadata     : public LoadBalancingPolicy::MetadataInterface {  public:-  Metadata(LoadBalancedCall* lb_call, grpc_metadata_batch* batch)-      : lb_call_(lb_call), batch_(batch) {}+  explicit Metadata(grpc_metadata_batch* batch) : batch_(batch) {}    void Add(absl::string_view key, absl::string_view value) override {-    grpc_linked_mdelem* linked_mdelem = static_cast<grpc_linked_mdelem*>(-        lb_call_->arena_->Alloc(sizeof(grpc_linked_mdelem)));-    linked_mdelem->md = grpc_mdelem_from_slices(-        ExternallyManagedSlice(key.data(), key.size()),-        ExternallyManagedSlice(value.data(), value.size()));-    GPR_ASSERT(batch_->LinkTail(linked_mdelem) == GRPC_ERROR_NONE);+    // Gross, egregious hack to support legacy grpclb behavior.","I'd really prefer to avoid spreading this hack from grpclb into the non-grpclb-specific client channel code.  Can we come up with a cleaner way to do this?The way we handle this has changed a couple of times, and I've never been super happy with it.  Originally, this was passed down via call context, but I changed it back in #18094 to be attached to the LB metadata token as user data, since there were problems with providing a separate call context for each subchannel call.  Then later, when I introduced the metadata interface to the LB policy API in #19405, I didn't want to make user data part of that API, so I changed it to use the current hack that is now spreading elsewhere in the code.Just thinking out loud here, but one possible approach here would be to add a layer of indirection:- Instead of passing the `GrpcLbClientStats` object directly in the metadata value, we could encode the address of the object as text and put that text in the metadata value.- Have the grpclb policy pass a `GrpcLbClientStatsProvider` object via a channel arg, to be used by the client load reporting filter to obtain the `GrpcLbClientStats` object based on the metadata address value.- The grpclb policy will update the `GrpcLbClientStats` object held by the `GrpcLbClientStatsProvider` whenever it establishes a new connection with the balancer.This approach isn't ideal, because (a) it requires a mutex to be accessed on the data plane and (b) passing the `GrpcLbClientStatsProvider` via channel args will prevent subchannel sharing between channels.  But it does at least keep the hack contained in grpclb-specific code rather than leaking it into the client channel code.I'd welcome other ideas here.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28267,779000874,2022-01-05T17:22:22Z,src/core/ext/filters/load_reporting/server_load_reporting_filter.cc,"@@ -115,12 +115,24 @@ void ServerLoadReportingCallData::StartTransportStreamOpBatch(     original_recv_initial_metadata_ready_ = op->recv_initial_metadata_ready();     // Substitute the original closure for the wrapper closure.     op->set_recv_initial_metadata_ready(&recv_initial_metadata_ready_);-  } else if (op->send_trailing_metadata() != nullptr) {","Nit: I think the ""else"" here was actually slightly more optimal, because on the server side, we know that recv_initial_metadata will always be in its own batch, so there's no need to evaluate the following condition in that case.  But I doubt it really makes any significant performance difference.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28267,779131629,2022-01-05T20:57:13Z,src/core/lib/security/transport/client_auth_filter.cc,"@@ -165,17 +155,20 @@ static void on_credentials_metadata(void* arg, grpc_error_handle input_error) {   grpc_auth_metadata_context_reset(&calld->auth_md_context);   grpc_error_handle error = GRPC_ERROR_REF(input_error);   if (error == GRPC_ERROR_NONE) {-    GPR_ASSERT(calld->md_array.size <= MAX_CREDENTIALS_METADATA_COUNT);+    GPR_ASSERT(calld->md_array->size() <= MAX_CREDENTIALS_METADATA_COUNT);     GPR_ASSERT(batch->send_initial_metadata);     grpc_metadata_batch* mdb =         batch->payload->send_initial_metadata.send_initial_metadata;-    for (size_t i = 0; i < calld->md_array.size; ++i) {-      add_error(&error, grpc_metadata_batch_add_tail(-                            mdb, &calld->md_links[i],-                            GRPC_MDELEM_REF(calld->md_array.md[i])));+    for (const auto& md : *calld->md_array) {+      mdb->Append(md.first.as_string_view(), md.second.Ref(),+                  [&](absl::string_view error, const grpc_core::Slice& value) {","I think accidental... it's a different error that could be generated, but it should still generate.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28119,779135118,2022-01-05T21:03:26Z,src/core/lib/gprpp/time.h,"@@ -0,0 +1,235 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_GPRPP_TIME_H+#define GRPC_CORE_LIB_GPRPP_TIME_H++#include <grpc/support/port_platform.h>++#include <stdint.h>++#include <cstdint>+#include <limits>+#include <string>++#include <grpc/support/time.h>++#include ""src/core/lib/gpr/time_precise.h""+#include ""src/core/lib/gpr/useful.h""++namespace grpc_core {++class Duration;++// Timestamp represents a discrete point in time.+class Timestamp {+ public:+  constexpr Timestamp() = default;+  // Constructs a Timestamp from a gpr_timespec.+  explicit Timestamp(gpr_timespec t);++  // Construct a Timestamp from a gpr_cycle_counter.+  static Timestamp FromCycleCounterRoundUp(gpr_cycle_counter c);++  static constexpr Timestamp FromMillisecondsAfterProcessEpoch(int64_t millis) {+    return Timestamp(millis);+  }++  static constexpr Timestamp ProcessEpoch() { return Timestamp(0); }++  static constexpr Timestamp InfFuture() {+    return Timestamp(std::numeric_limits<int64_t>::max());+  }++  static constexpr Timestamp InfPast() {+    return Timestamp(std::numeric_limits<int64_t>::min());+  }++  constexpr bool operator==(Timestamp other) const {+    return millis_ == other.millis_;+  }+  constexpr bool operator!=(Timestamp other) const {+    return millis_ != other.millis_;+  }+  constexpr bool operator<(Timestamp other) const {+    return millis_ < other.millis_;+  }+  constexpr bool operator<=(Timestamp other) const {+    return millis_ <= other.millis_;+  }+  constexpr bool operator>(Timestamp other) const {+    return millis_ > other.millis_;+  }+  constexpr bool operator>=(Timestamp other) const {+    return millis_ >= other.millis_;+  }+  Timestamp& operator+=(Duration duration);++  bool is_zero() const { return millis_ == 0; }++  uint64_t milliseconds_after_process_epoch() const { return millis_; }++  gpr_timespec as_timespec(gpr_clock_type type) const;++ private:+  explicit constexpr Timestamp(int64_t millis) : millis_(millis) {}++  int64_t millis_ = 0;+};++// Duration represents a span of time.+class Duration {+ public:+  constexpr Duration() : millis_(0) {}++  static Duration FromTimespec(gpr_timespec t);+  static Duration FromSecondsAndNanoseconds(int64_t seconds, int32_t nanos);+  static Duration FromSecondsAsDouble(double seconds);++  static constexpr Duration Zero() { return Duration(0); }++  // Smallest representatable positive duration.+  static constexpr Duration Epsilon() { return Duration(1); }++  static constexpr Duration NegativeInfinity() {+    return Duration(std::numeric_limits<int64_t>::min());+  }++  static constexpr Duration Infinity() {+    return Duration(std::numeric_limits<int64_t>::max());+  }++  static constexpr Duration Hours(int64_t hours) { return Minutes(hours * 60); }++  static constexpr Duration Minutes(int64_t minutes) {+    return Seconds(minutes * 60);+  }++  static constexpr Duration Seconds(int64_t seconds) {+    return Milliseconds(seconds * GPR_MS_PER_SEC);+  }++  static constexpr Duration Milliseconds(int64_t millis) {+    return Duration(millis);+  }++  static constexpr Duration MicrosecondsRoundDown(int64_t micros) {+    return Duration(micros / GPR_US_PER_MS);+  }++  static constexpr Duration NanosecondsRoundDown(int64_t nanos) {+    return Duration(nanos / GPR_NS_PER_MS);+  }++  static constexpr Duration MicrosecondsRoundUp(int64_t micros) {+    return Duration(micros / GPR_US_PER_MS + (micros % GPR_US_PER_MS != 0));+  }++  static constexpr Duration NanosecondsRoundUp(int64_t nanos) {+    return Duration(nanos / GPR_NS_PER_MS + (nanos % GPR_NS_PER_MS != 0));+  }++  constexpr bool operator==(Duration other) const {+    return millis_ == other.millis_;+  }+  constexpr bool operator!=(Duration other) const {+    return millis_ != other.millis_;+  }+  constexpr bool operator<(Duration other) const {+    return millis_ < other.millis_;+  }+  constexpr bool operator<=(Duration other) const {+    return millis_ <= other.millis_;+  }+  constexpr bool operator>(Duration other) const {+    return millis_ > other.millis_;+  }+  constexpr bool operator>=(Duration other) const {+    return millis_ >= other.millis_;+  }+  Duration& operator/=(int64_t divisor) {+    millis_ /= divisor;+    return *this;+  }+  Duration& operator+=(Duration other) {+    millis_ += other.millis_;+    return *this;+  }++  constexpr int64_t millis() const { return millis_; }+  double seconds() const { return static_cast<double>(millis_) / 1000.0; }++  gpr_timespec as_timespec() const;++  std::string ToString() const;++ private:+  explicit constexpr Duration(int64_t millis) : millis_(millis) {}++  int64_t millis_;+};++static_assert(std::is_trivially_copyable<Duration>::value,+              ""Duration is not trivially copyable"");++inline Duration operator+(Duration lhs, Duration rhs) {+  return Duration::Milliseconds(SaturatingAdd(lhs.millis(), rhs.millis()));+}++inline Duration operator-(Duration lhs, Duration rhs) {+  return Duration::Milliseconds(SaturatingAdd(lhs.millis(), -rhs.millis()));+}++inline Timestamp operator+(Timestamp lhs, Duration rhs) {+  return Timestamp::FromMillisecondsAfterProcessEpoch(+      SaturatingAdd(lhs.milliseconds_after_process_epoch(), rhs.millis()));+}++inline Timestamp operator-(Timestamp lhs, Duration rhs) {+  return Timestamp::FromMillisecondsAfterProcessEpoch(+      SaturatingAdd(lhs.milliseconds_after_process_epoch(), -rhs.millis()));+}++inline Timestamp operator+(Duration lhs, Timestamp rhs) { return rhs + lhs; }","I'm not sure if it actually makes sense to support the operands in this order.  I realize that addition is commutative, but semantically it seems odd to say ""3 hours plus 5pm"".",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28119,779140119,2022-01-05T21:12:52Z,src/core/lib/gprpp/time.cc,"@@ -0,0 +1,129 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include ""src/core/lib/gprpp/time.h""++#include <atomic>+#include <cstdint>+#include <limits>++#include <grpc/impl/codegen/gpr_types.h>+#include <grpc/support/log.h>++namespace grpc_core {++namespace {++std::atomic<int64_t> g_process_epoch_seconds;+std::atomic<gpr_cycle_counter> g_process_epoch_cycles;++GPR_ATTRIBUTE_NOINLINE std::pair<int64_t, gpr_cycle_counter> InitTime() {+  const gpr_cycle_counter cycles_start = gpr_get_cycle_counter();+  int64_t process_epoch_seconds = gpr_now(GPR_CLOCK_MONOTONIC).tv_sec;+  const gpr_cycle_counter cycles_end = gpr_get_cycle_counter();+  GPR_ASSERT(process_epoch_seconds != 0);+  int64_t expected = 0;+  gpr_cycle_counter process_epoch_cycles = (cycles_start + cycles_end) / 2;+  GPR_ASSERT(process_epoch_cycles != 0);+  if (!g_process_epoch_seconds.compare_exchange_strong(+          expected, process_epoch_seconds, std::memory_order_relaxed,+          std::memory_order_relaxed)) {+    process_epoch_seconds = expected;+    do {+      process_epoch_cycles =+          g_process_epoch_cycles.load(std::memory_order_relaxed);+    } while (process_epoch_cycles == 0);+  } else {+    g_process_epoch_cycles.store(process_epoch_cycles,+                                 std::memory_order_relaxed);+  }+  return std::make_pair(process_epoch_seconds, process_epoch_cycles);+}++gpr_timespec StartTime() {+  int64_t sec = g_process_epoch_seconds.load(std::memory_order_relaxed);+  if (GPR_UNLIKELY(sec == 0)) sec = InitTime().first;+  return {sec, 0, GPR_CLOCK_MONOTONIC};+}++gpr_cycle_counter StartCycleCounter() {+  gpr_cycle_counter cycles =+      g_process_epoch_cycles.load(std::memory_order_relaxed);+  if (GPR_UNLIKELY(cycles == 0)) cycles = InitTime().second;+  return cycles;+}++gpr_timespec MillisecondsAsTimespec(int64_t millis, gpr_clock_type clock_type) {+  // special-case infinities as Timestamp can be 32bit on some+  // platforms while gpr_time_from_millis always takes an int64_t.+  if (millis == std::numeric_limits<int64_t>::max()) {+    return gpr_inf_future(clock_type);+  }+  if (millis == std::numeric_limits<int64_t>::min()) {+    return gpr_inf_past(clock_type);+  }++  if (clock_type == GPR_TIMESPAN) {+    return gpr_time_from_millis(millis, GPR_TIMESPAN);+  }+  return gpr_time_add(gpr_convert_clock_type(StartTime(), clock_type),+                      gpr_time_from_millis(millis, GPR_TIMESPAN));+}++int64_t TimespanToMillisRoundUp(gpr_timespec ts) {+  double x = GPR_MS_PER_SEC * static_cast<double>(ts.tv_sec) ++             static_cast<double>(ts.tv_nsec) / GPR_NS_PER_MS ++             static_cast<double>(GPR_NS_PER_SEC - 1) /+                 static_cast<double>(GPR_NS_PER_SEC);+  if (x < 0) return 0;+  if (x >= static_cast<double>(std::numeric_limits<int64_t>::max())) {+    return std::numeric_limits<int64_t>::max();+  }+  return static_cast<int64_t>(x);+}++}  // namespace++Timestamp::Timestamp(gpr_timespec ts)+    : millis_(TimespanToMillisRoundUp(gpr_time_sub(+          gpr_convert_clock_type(ts, GPR_CLOCK_MONOTONIC), StartTime()))) {}++Timestamp Timestamp::FromCycleCounterRoundUp(gpr_cycle_counter c) {+  return Timestamp(gpr_cycle_counter_sub(c, StartCycleCounter()));+}++gpr_timespec Timestamp::as_timespec(gpr_clock_type clock_type) const {+  return MillisecondsAsTimespec(millis_, clock_type);+}++gpr_timespec Duration::as_timespec() const {+  return MillisecondsAsTimespec(millis_, GPR_TIMESPAN);+}++Duration Duration::FromTimespec(gpr_timespec t) {","Should this assert that the clock type is `GPR_TIMESPAN`, to prevent someone from accidentally creating a `Duration` instead of a `Timestamp`?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28119,779140966,2022-01-05T21:14:17Z,src/core/lib/gprpp/time.cc,"@@ -0,0 +1,129 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include ""src/core/lib/gprpp/time.h""++#include <atomic>+#include <cstdint>+#include <limits>++#include <grpc/impl/codegen/gpr_types.h>+#include <grpc/support/log.h>++namespace grpc_core {++namespace {++std::atomic<int64_t> g_process_epoch_seconds;+std::atomic<gpr_cycle_counter> g_process_epoch_cycles;++GPR_ATTRIBUTE_NOINLINE std::pair<int64_t, gpr_cycle_counter> InitTime() {+  const gpr_cycle_counter cycles_start = gpr_get_cycle_counter();+  int64_t process_epoch_seconds = gpr_now(GPR_CLOCK_MONOTONIC).tv_sec;+  const gpr_cycle_counter cycles_end = gpr_get_cycle_counter();+  GPR_ASSERT(process_epoch_seconds != 0);+  int64_t expected = 0;+  gpr_cycle_counter process_epoch_cycles = (cycles_start + cycles_end) / 2;+  GPR_ASSERT(process_epoch_cycles != 0);+  if (!g_process_epoch_seconds.compare_exchange_strong(+          expected, process_epoch_seconds, std::memory_order_relaxed,+          std::memory_order_relaxed)) {+    process_epoch_seconds = expected;+    do {+      process_epoch_cycles =+          g_process_epoch_cycles.load(std::memory_order_relaxed);+    } while (process_epoch_cycles == 0);+  } else {+    g_process_epoch_cycles.store(process_epoch_cycles,+                                 std::memory_order_relaxed);+  }+  return std::make_pair(process_epoch_seconds, process_epoch_cycles);+}++gpr_timespec StartTime() {+  int64_t sec = g_process_epoch_seconds.load(std::memory_order_relaxed);+  if (GPR_UNLIKELY(sec == 0)) sec = InitTime().first;+  return {sec, 0, GPR_CLOCK_MONOTONIC};+}++gpr_cycle_counter StartCycleCounter() {+  gpr_cycle_counter cycles =+      g_process_epoch_cycles.load(std::memory_order_relaxed);+  if (GPR_UNLIKELY(cycles == 0)) cycles = InitTime().second;+  return cycles;+}++gpr_timespec MillisecondsAsTimespec(int64_t millis, gpr_clock_type clock_type) {+  // special-case infinities as Timestamp can be 32bit on some+  // platforms while gpr_time_from_millis always takes an int64_t.+  if (millis == std::numeric_limits<int64_t>::max()) {+    return gpr_inf_future(clock_type);+  }+  if (millis == std::numeric_limits<int64_t>::min()) {+    return gpr_inf_past(clock_type);+  }++  if (clock_type == GPR_TIMESPAN) {+    return gpr_time_from_millis(millis, GPR_TIMESPAN);+  }+  return gpr_time_add(gpr_convert_clock_type(StartTime(), clock_type),+                      gpr_time_from_millis(millis, GPR_TIMESPAN));+}++int64_t TimespanToMillisRoundUp(gpr_timespec ts) {+  double x = GPR_MS_PER_SEC * static_cast<double>(ts.tv_sec) ++             static_cast<double>(ts.tv_nsec) / GPR_NS_PER_MS ++             static_cast<double>(GPR_NS_PER_SEC - 1) /+                 static_cast<double>(GPR_NS_PER_SEC);+  if (x < 0) return 0;+  if (x >= static_cast<double>(std::numeric_limits<int64_t>::max())) {+    return std::numeric_limits<int64_t>::max();+  }+  return static_cast<int64_t>(x);+}++}  // namespace++Timestamp::Timestamp(gpr_timespec ts)","Should this assert that the clock type is not `GPR_TIMESPAN`, to prevent someone from accidentally creating a `Timestamp` instead of a `Duration`?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28119,779145009,2022-01-05T21:21:50Z,src/core/ext/filters/client_channel/channel_connectivity.cc,"@@ -95,7 +95,7 @@ class StateWatcher : public DualRefCounted<StateWatcher> {       // watch, but we are hiding that fact from the application.       if (IsLameChannel(channel)) {         // Ref from object creation is held by timer callback.-        StartTimer(grpc_timespec_to_millis_round_up(deadline));+        StartTimer(Timestamp(deadline));","Suggest converting from `gpr_timespec` to `Timestamp` in `grpc_channel_watch_connectivity_state()`, before instantiating `StateWatcher`.  That way, we only need to do it once, and we convert from the GPR type to our internal type immediately upon entering the C-core API.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28119,779202984,2022-01-05T23:24:52Z,src/core/ext/xds/xds_route_config.cc,"@@ -595,29 +595,22 @@ grpc_error_handle RetryPolicyParse(           ""RouteAction RetryPolicy RetryBackoff missing base interval.""));     } else {       retry_to_return.retry_back_off.base_interval =-          Duration::Parse(base_interval);+          ParseDuration(base_interval);     }     const google_protobuf_Duration* max_interval =         envoy_config_route_v3_RetryPolicy_RetryBackOff_max_interval(backoff);     Duration max;     if (max_interval != nullptr) {-      max = Duration::Parse(max_interval);+      max = ParseDuration(max_interval);     } else {       // if max interval is not set, it is 10x the base, if the value in nanos","The second part of this comment (starting with ""if the value in nanos can yield another second"") is no longer relevant.",X
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/28485,780581085,2022-01-07T23:19:06Z,src/core/lib/uri/uri_parser.h,"@@ -40,15 +38,21 @@ class URI {     bool operator==(const QueryParam& other) const {       return key == other.key && value == other.value;     }+    bool operator<(const QueryParam& other) const {","Why is this added? Arbitrary query parameters can't be reordered. If you just sorted by the key and left values in their existing order, that'd probably be mostly fine, but sorting within a key is definitely asking to break something. But I don't actually see any usage of this. (So I can't check if duplicates are wrongly removed.)",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28485,781379189,2022-01-10T17:02:16Z,src/core/lib/uri/uri_parser.h,"@@ -40,15 +38,21 @@ class URI {     bool operator==(const QueryParam& other) const {       return key == other.key && value == other.value;     }+    bool operator<(const QueryParam& other) const {","The ordering isn't used by the URI parser code itself, but there are cases where the caller will want to use it.  I'll be using that in a follow-up PR to order the parameters for `xdstp` URIs, since that URI scheme dictates that [query param ordering is irrelevant for normalization](https://github.com/cncf/xds/blob/main/proposals/TP1-xds-transport-next.md#normalization).",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28500,781551531,2022-01-10T21:12:49Z,tools/run_tests/run_tests.py,"@@ -178,22 +178,18 @@ def _is_use_docker_child():     'venv_relative_python',     'toolchain',     'runner',-    'test_name',-    'iomgr_platform', ])   def _python_config_generator(name, major, minor, bits, config_vars):-    name += '_' + config_vars.iomgr_platform-    return PythonConfig(-        name, config_vars.shell + config_vars.builder +-        config_vars.builder_prefix_arguments +-        [_python_pattern_function(major=major, minor=minor, bits=bits)] +-        [name] + config_vars.venv_relative_python + config_vars.toolchain,-        config_vars.shell + config_vars.runner + [-            os.path.join(name, config_vars.venv_relative_python[0]),-            config_vars.test_name","`config_vars.test_name` was not a good name, it actually represents ""test command"" we wrote in `commands.py`. Python is special even in run_tests, we have several intermediate data class, like ""PythonConfig"", which used as a holder for the build command, and part of the run-single-test-case command. This PR moves the concatenation of the last argument to `def test_specs` (line 649).",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28480,781941410,2022-01-11T09:22:38Z,tools/run_tests/task_runner.py,"@@ -73,6 +73,18 @@ def _create_build_map():                   default='report_taskrunner_sponge_log.xml',                   type=str,                   help='Filename for the JUnit-compatible XML report')+argp.add_argument('--dry_run',+                  default=False,+                  action='store_const',+                  const=True,+                  help='Only print what would be run.')+argp.add_argument(+    '--inner_jobs',+    default=None,+    type=int,+    help=+    'Number of parallel jobs to use by each target. Passed as build_jobspec(inner_jobs=N) to each target.'","I added comments into the build_jobspec() functions, I think having a list of what's supported in the arg helptext would be hard to maintain. Also, as a followup I plan to provide inner_jobs support for all the tasks where it makes sense.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28521,782376649,2022-01-11T17:30:26Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -10291,6 +10291,35 @@ TEST_P(XdsRbacTestWithRouteOverrideAlwaysPresent,   SendRpc([this]() { return CreateInsecureChannel(); }, {}, {}); } +using XdsRbacTestWithRouteOverrideAndRdsTestingEnabled = XdsRbacTest;","This test doesn't need to use RBAC; it can be a very simple test similar to the `GlobalXdsClientTest.MultipleChannelsShareXdsClient` test.  Just point the client at one backend, create two channels, check that both channels work, then destroy one of the channels, and then send an EDS update that points to a different backend, and check that the remaining channel switches to the new backend.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28486,782480760,2022-01-11T19:54:59Z,src/core/ext/xds/xds_bootstrap.h,"@@ -39,12 +39,25 @@ namespace grpc_core {  class XdsClient; +class XdsChannelCredsFactory {+ public:","Please list the methods here in the following order:1. dtor (should be first as per https://google.github.io/styleguide/cppguide.html#Declaration_Order)2. `creds_type()` (this basically defines the identity of the creds factory, so it should go first)3. The other two methods, in the same order as they are present in `XdsChannelCredsRegistry`.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28486,782483036,2022-01-11T19:58:28Z,src/core/ext/xds/xds_bootstrap.cc,"@@ -98,39 +100,118 @@ grpc_error_handle ParseChannelCredsArray(const Json::Array& json,                                        &error_list); } +class XdsChannelCredsRegistryState {","I don't think this class is needed.  Instead, you can just do something like this:```using FactoryMap = std::map<absl::string_view, std::unique_ptr<XdsChannelCredsFactory>>;FactoryMap* g_factory_map = nullptr;```",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28486,782484532,2022-01-11T20:00:47Z,src/core/ext/xds/xds_bootstrap.cc,"@@ -98,39 +100,118 @@ grpc_error_handle ParseChannelCredsArray(const Json::Array& json,                                        &error_list); } +class XdsChannelCredsRegistryState {+ public:+  XdsChannelCredsRegistryState();+  void RegisterXdsChannelCredsFactory(+      std::unique_ptr<XdsChannelCredsFactory> factory) {+    GPR_ASSERT(LookupXdsChannelCredsFactory(factory->creds_type()) == nullptr);+    factories_.push_back(std::move(factory));+  }++  XdsChannelCredsFactory* LookupXdsChannelCredsFactory(+      absl::string_view creds_type) const {+    for (size_t i = 0; i < factories_.size(); ++i) {+      if (creds_type == factories_[i]->creds_type()) {+        return factories_[i].get();+      }+    }+    return nullptr;+  }++ private:+  // Currently we expect a small number of creds factories to be registered.+  // We may raise the default capacity or turn this into a map if we need to+  // support more.+  absl::InlinedVector<std::unique_ptr<XdsChannelCredsFactory>, 10> factories_;+};++XdsChannelCredsRegistryState* g_state = nullptr;+ }  // namespace +//+// XdsChannelCredsFactory implementations for default-supported cred types.+//++class GoogleDefaultXdsChannelCredsFactory : public XdsChannelCredsFactory {+ public:+  RefCountedPtr<grpc_channel_credentials> CreateXdsChannelCreds(+      const Json& /*config*/) const override {+    return RefCountedPtr<grpc_channel_credentials>(+        grpc_google_default_credentials_create(nullptr));+  }+  bool IsValidConfig(const Json& /*config*/) const override { return true; }+  const char* creds_type() const override { return ""google_default""; }+};++class InsecureXdsChannelCredsFactory : public XdsChannelCredsFactory {+ public:+  RefCountedPtr<grpc_channel_credentials> CreateXdsChannelCreds(+      const Json& /*config*/) const override {+    return RefCountedPtr<grpc_channel_credentials>(+        grpc_insecure_credentials_create());+  }+  bool IsValidConfig(const Json& /*config*/) const override { return true; }+  const char* creds_type() const override { return ""insecure""; }+};++class FakeXdsChannelCredsFactory : public XdsChannelCredsFactory {+ public:+  RefCountedPtr<grpc_channel_credentials> CreateXdsChannelCreds(+      const Json& /*config*/) const override {+    return RefCountedPtr<grpc_channel_credentials>(+        grpc_fake_transport_security_credentials_create());+  }+  bool IsValidConfig(const Json& /*config*/) const override { return true; }+  const char* creds_type() const override { return ""fake""; }+};+ // // XdsChannelCredsRegistry // +XdsChannelCredsRegistryState::XdsChannelCredsRegistryState() {+  RegisterXdsChannelCredsFactory(+      absl::make_unique<GoogleDefaultXdsChannelCredsFactory>());+  RegisterXdsChannelCredsFactory(+      absl::make_unique<InsecureXdsChannelCredsFactory>());+  RegisterXdsChannelCredsFactory(+      absl::make_unique<FakeXdsChannelCredsFactory>());+}+ bool XdsChannelCredsRegistry::IsSupported(const std::string& creds_type) {-  return creds_type == ""google_default"" || creds_type == ""insecure"" ||-         creds_type == ""fake"";+  return g_state->LookupXdsChannelCredsFactory(creds_type) != nullptr; } -bool XdsChannelCredsRegistry::IsValidConfig(const std::string& /*creds_type*/,-                                            const Json& /*config*/) {-  // Currently, none of the creds types actually take a config, but we-  // ignore whatever might be specified in the bootstrap file for-  // forward compatibility reasons.-  return true;+bool XdsChannelCredsRegistry::IsValidConfig(const std::string& creds_type,+                                            const Json& config) {+  XdsChannelCredsFactory* factory =+      g_state->LookupXdsChannelCredsFactory(creds_type);+  return factory != nullptr && factory->IsValidConfig(config); }  RefCountedPtr<grpc_channel_credentials> XdsChannelCredsRegistry::MakeChannelCreds(const std::string& creds_type,-                                          const Json& /*config*/) {-  if (creds_type == ""google_default"") {-    return RefCountedPtr<grpc_channel_credentials>(-        grpc_google_default_credentials_create(nullptr));-  } else if (creds_type == ""insecure"") {-    return RefCountedPtr<grpc_channel_credentials>(-        grpc_insecure_credentials_create());-  } else if (creds_type == ""fake"") {-    return RefCountedPtr<grpc_channel_credentials>(-        grpc_fake_transport_security_credentials_create());-  }-  return nullptr;+                                          const Json& config) {+  XdsChannelCredsFactory* factory =+      g_state->LookupXdsChannelCredsFactory(creds_type);+  if (factory == nullptr) return nullptr;+  return factory->CreateXdsChannelCreds(config);+}++void XdsChannelCredsRegistry::Init() {+  if (g_state == nullptr) g_state = new XdsChannelCredsRegistryState();","I think we should just unconditionally set it here, without a check to see that it's null.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28531,782596463,2022-01-11T23:17:58Z,BUILD,"@@ -5272,46 +5272,14 @@ grpc_cc_library(     ], ) -grpc_cc_library(+grpc_upb_proto_library(     name = ""proto_gen_validate_upb"",-    srcs = [-        ""src/core/ext/upb-generated/validate/validate.upb.c"",-    ],-    hdrs = [-        ""src/core/ext/upb-generated/validate/validate.upb.h"",-    ],-    external_deps = [-        ""upb_lib"",-        ""upb_lib_descriptor"",-        ""upb_generated_code_support__only_for_generated_code_do_not_use__i_give_permission_to_break_me"",-    ],-    language = ""c++"",-    deps = [-        ""protobuf_duration_upb"",-        ""protobuf_timestamp_upb"",-    ],+    deps = [""@com_envoyproxy_protoc_gen_validate//validate:validate_proto""], ) -grpc_cc_library(+grpc_upb_proto_reflection_library(     name = ""proto_gen_validate_upbdefs"",-    srcs = [-        ""src/core/ext/upbdefs-generated/validate/validate.upbdefs.c"",-    ],-    hdrs = [-        ""src/core/ext/upbdefs-generated/validate/validate.upbdefs.h"",-    ],-    external_deps = [-        ""upb_lib"",-        ""upb_lib_descriptor_reflection"",-        ""upb_textformat_lib"",-        ""upb_reflection"",-        ""upb_generated_code_support__only_for_generated_code_do_not_use__i_give_permission_to_break_me"",-    ],-    language = ""c++"",-    deps = [-        ""proto_gen_validate_upb"",-        ""protobuf_timestamp_upbdefs"",-    ],+    deps = [""@com_envoyproxy_protoc_gen_validate//validate:validate_proto""], )","optional: Reading the internal code, both `grpc_upb_proto_library` and `grpc_upb_proto_reflection_library` shared the same deps for OSS and google3 build. It might be possible to reduce the number of targets.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28531,783330265,2022-01-12T18:19:58Z,bazel/grpc_deps.bzl,"@@ -445,6 +445,17 @@ def grpc_deps():             ],         ) +    if ""com_github_cncf_udpa"" not in native.existing_rules():+        http_archive(+            name = ""com_github_cncf_udpa"",+            sha256 = ""5bc8365613fe2f8ce6cc33959b7667b13b7fe56cb9d16ba740c06e1a7c4242fc"",+            strip_prefix = ""xds-cb28da3451f158a947dfc45090fe92b07b243bc1"",+            urls = [+                ""https://storage.googleapis.com/grpc-bazel-mirror/github.com/cncf/xds/archive/cb28da3451f158a947dfc45090fe92b07b243bc1.tar.gz"",+                ""https://github.com/cncf/xds/archive/cb28da3451f158a947dfc45090fe92b07b243bc1.tar.gz"",+            ],+        )","It should actually be called `com_github_cncf_xds`, not `com_github_cncf_udpa`.  The latter name is deprecated.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/28531,783411266,2022-01-12T20:13:13Z,bazel/grpc_deps.bzl,"@@ -445,6 +445,17 @@ def grpc_deps():             ],         ) +    if ""com_github_cncf_udpa"" not in native.existing_rules():+        http_archive(+            name = ""com_github_cncf_udpa"",+            sha256 = ""5bc8365613fe2f8ce6cc33959b7667b13b7fe56cb9d16ba740c06e1a7c4242fc"",+            strip_prefix = ""xds-cb28da3451f158a947dfc45090fe92b07b243bc1"",+            urls = [+                ""https://storage.googleapis.com/grpc-bazel-mirror/github.com/cncf/xds/archive/cb28da3451f158a947dfc45090fe92b07b243bc1.tar.gz"",+                ""https://github.com/cncf/xds/archive/cb28da3451f158a947dfc45090fe92b07b243bc1.tar.gz"",+            ],+        )",This needs to be the same with https://github.com/envoyproxy/data-plane-api/blob/4140ad93e190cf4aa6a35b4caac1776fa640953e/bazel/repository_locations.bzl#L42 so that they're resolved to the same target.,X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28363,783604394,2022-01-13T03:27:47Z,src/core/lib/slice/slice_utils.h,"@@ -31,170 +31,13 @@  namespace grpc_core { extern uint32_t g_hash_seed;","yeah ok - we had needed the split to make static metadata layer properly in the build, but that requirement went so it is time.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28543,783942808,2022-01-13T13:11:26Z,tools/interop_matrix/testcases/python__master,"@@ -2,21 +2,21 @@ # DO NOT MODIFY # This file is generated by run_interop_tests.py/create_testcases.sh echo ""Testing ${docker_image:=grpc_interop_python:0de8fc68-43f4-4fa0-8738-1228de6ffe8c}""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=large_unary --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=empty_unary --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=ping_pong --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=empty_stream --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=client_streaming --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=server_streaming --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=cancel_after_begin --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=cancel_after_first_response --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=timeout_on_sleeping_server --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=large_unary --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=empty_unary --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=ping_pong --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=empty_stream --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=client_streaming --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=server_streaming --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=cancel_after_begin --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=cancel_after_first_response --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=timeout_on_sleeping_server --use_tls=true\""""+docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=large_unary --use_tls=true\""""","Careful with changes to `tools/interop_matrix/testcases/` - this has the potential to break the interop_matrix tests. What I think you need to do:- leave the old versions of the testcases (e.g. tools/interop_matrix/testcases/python__v1.11.1)  file without any change (they will run on old prebuilt images, so you need to leave the path as is).- rename python__master to python__v1.41.1 (that's the first release using the __master file currently https://github.com/grpc/grpc/blob/e22f07dc4ccecfd4193814a98a19351700504fe0/tools/interop_matrix/client_matrix.py#L394 and update client_matrix.py to use  python__v1.41.1 for the last few releases.-  create python__master with updated paths, but will only be used for upcoming releasesSince as you can see this is a bit tricky, you'll definitely need to trigger an adhoc run of interop_matrix before submitting.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28543,783945706,2022-01-13T13:15:09Z,tools/interop_matrix/testcases/python__master,"@@ -2,21 +2,21 @@ # DO NOT MODIFY # This file is generated by run_interop_tests.py/create_testcases.sh echo ""Testing ${docker_image:=grpc_interop_python:0de8fc68-43f4-4fa0-8738-1228de6ffe8c}""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=large_unary --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=empty_unary --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=ping_pong --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=empty_stream --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=client_streaming --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=server_streaming --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=cancel_after_begin --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=cancel_after_first_response --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=timeout_on_sleeping_server --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=large_unary --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=empty_unary --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=ping_pong --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=empty_stream --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=client_streaming --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=server_streaming --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=cancel_after_begin --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=cancel_after_first_response --use_tls=true\""""-docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39_native/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test4.sandbox.googleapis.com --server_port=443 --test_case=timeout_on_sleeping_server --use_tls=true\""""+docker run -i --rm=true -e PYTHONPATH=/var/local/git/grpc/src/python/gens -e LD_LIBRARY_PATH=/var/local/git/grpc/libs/opt -w /var/local/git/grpc --net=host $docker_image bash -c ""py39/bin/python src/python/grpcio_tests/setup.py run_interop --client --args=\""--server_host=grpc-test.sandbox.googleapis.com --server_port=443 --test_case=large_unary --use_tls=true\""""","Btw, since interop_matrix only runs on released versions, not updating the testcase files won't affect them, so technically you can update the testcase files as a followup PR.tools/run_tests/run_interop_tests.py needed fixing though, as you correctly realized.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28531,784257089,2022-01-13T19:19:24Z,bazel/grpc_deps.bzl,"@@ -445,6 +445,17 @@ def grpc_deps():             ],         ) +    if ""com_github_cncf_udpa"" not in native.existing_rules():+        http_archive(+            name = ""com_github_cncf_udpa"",+            sha256 = ""5bc8365613fe2f8ce6cc33959b7667b13b7fe56cb9d16ba740c06e1a7c4242fc"",+            strip_prefix = ""xds-cb28da3451f158a947dfc45090fe92b07b243bc1"",+            urls = [+                ""https://storage.googleapis.com/grpc-bazel-mirror/github.com/cncf/xds/archive/cb28da3451f158a947dfc45090fe92b07b243bc1.tar.gz"",+                ""https://github.com/cncf/xds/archive/cb28da3451f158a947dfc45090fe92b07b243bc1.tar.gz"",+            ],+        )","That's unfortunate.@htuch, how hard would it be to rename this target in the Envoy repo?  It seems like these dependency names are basically part of the API used by other projects, so we should probably remove the term ""udpa"" from them.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28486,784301405,2022-01-13T20:29:26Z,src/core/ext/xds/xds_channel_creds.cc,"@@ -0,0 +1,109 @@+//+// Copyright 2019 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include ""absl/container/flat_hash_map.h""++#include ""src/core/ext/xds/xds_channel_creds.h""+#include ""src/core/lib/security/credentials/fake/fake_credentials.h""++namespace grpc_core {++namespace {++using ChannelCredsMap =+    absl::flat_hash_map<absl::string_view,+                        std::unique_ptr<XdsChannelCredsImpl>>;+ChannelCredsMap* g_creds = nullptr;+++}  // namespace++//+// XdsChannelCredsImpl implementations for default-supported cred types.+//++class GoogleDefaultXdsChannelCredsImpl : public XdsChannelCredsImpl {+ public:+  RefCountedPtr<grpc_channel_credentials> CreateXdsChannelCreds(+      const Json& /*config*/) const override {+    return RefCountedPtr<grpc_channel_credentials>(+        grpc_google_default_credentials_create(nullptr));+  }+  bool IsValidConfig(const Json& /*config*/) const override { return true; }+  absl::string_view creds_type() const override { return ""google_default""; }",Please define this method first in each of these classes.,X
19192189,casperisfine,https://api.github.com/repos/grpc/grpc/pulls/28559,786794270,2022-01-18T14:09:51Z,src/ruby/ext/grpc/rb_channel_args.c,"@@ -111,6 +111,8 @@ typedef struct channel_convert_params { static VALUE grpc_rb_hash_convert_to_channel_args0(VALUE as_value) {   ID id_size = rb_intern(""size"");   VALUE grpc_rb_cChannelArgs = rb_define_class(""TmpChannelArgs"", rb_cObject);","By the way, is `TmpChannelArgs` being a top-level class being intented? It seems weird.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28587,787026523,2022-01-18T18:10:52Z,tools/internal_ci/linux/grpc_run_tests_matrix.sh,"@@ -28,6 +28,8 @@ if [ -n ""$KOKORO_GITHUB_PULL_REQUEST_NUMBER"" ] && [ -n ""$RUN_TESTS_FLAGS"" ]; the   export RUN_TESTS_FLAGS=""--filter_pr_tests --base_branch origin/$KOKORO_GITHUB_PULL_REQUEST_TARGET_BRANCH $RUN_TESTS_FLAGS"" fi +nproc",I left that in accidentallyWill remove.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28443,787046527,2022-01-18T18:38:45Z,src/python/grpcio_reflection/grpc_reflection/v1alpha/proto_reflection_descriptor_database.py,"@@ -0,0 +1,140 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Reference implementation for reflection client in gRPC Python.""""""++from google.protobuf.descriptor_database import DescriptorDatabase+from google.protobuf.descriptor_pb2 import FileDescriptorProto+import grpc+from grpc_reflection.v1alpha.reflection_pb2 import (+    ErrorResponse,+    ExtensionNumberResponse,+    ExtensionRequest,+    FileDescriptorResponse,+    ListServiceResponse,+    ServerReflectionRequest,+    ServerReflectionResponse,+    ServiceResponse,+)+from grpc_reflection.v1alpha.reflection_pb2_grpc import ServerReflectionStub+import logging+++class ProtoReflectionDescriptorDatabase(DescriptorDatabase):+    """"""+    ProtoReflectionDescriptorDatabase takes a stub of ServerReflection and provides the methods defined by+    DescriptorDatabase interfaces. It can be used to feed a DescriptorPool instance.++    Python implementation based on C++ version found here:+      https://github.com/grpc/grpc/blob/v1.39.1/test/cpp/util/proto_reflection_descriptor_database.cc+    while implementing the interface given here:+      https://googleapis.dev/python/protobuf/3.17.0/google/protobuf/descriptor_database.html++    """"""++    def __init__(self, channel: grpc.Channel):+        DescriptorDatabase.__init__(self)+        self._logger = logging.getLogger(__name__)+        self._stub = ServerReflectionStub(channel)+        self._known_files: Set[str] = set()+        self._cached_extension_numbers: Dict[str, List[int]] = dict()+        self._cached_extension_files: Dict[str, FileDescriptorProto] = dict()++    def _DoOneRequest(+        self, request: ServerReflectionRequest+    ) -> ServerReflectionResponse:","stylistic: except for proto generated methods, gRPC Python uses snake case for methods for consistency.",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/28243,787049579,2022-01-18T18:43:01Z,Rakefile,"@@ -78,12 +78,30 @@ namespace :suite do   end end -desc 'Build the Windows gRPC DLLs for Ruby'-task 'dlls' do+desc 'Build the Windows gRPC DLLs for Ruby. Optionally one can pass argument to build only dll for given platform.'+task 'dlls', [:plat] do |t, args|   grpc_config = ENV['GRPC_CONFIG'] || 'opt'   verbose = ENV['V'] || '0'   # use env variable to set artifact build paralellism   nproc_override = ENV['GRPC_RUBY_BUILD_PROCS'] || `nproc`.strip+  selected_plat = ""#{args[:plat]}""++  w64 = { cross: 'x86_64-w64-mingw32', out: 'grpc_c.64.ruby', platform: 'x64-mingw32' }","nit: I think we can clean this up by allowing optional `plat` argument to be a listWe can then remove `selected_plat` variable and do something like this to generate build_configs```[w64, w32].each do |config|  if config[:platform] in args[:plat]    build_configs.append(config)  endend```",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28565,788114348,2022-01-19T20:32:30Z,src/core/lib/channel/promise_based_filter.h,"@@ -0,0 +1,490 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_CHANNEL_PROMISE_BASED_FILTER_H+#define GRPC_CORE_LIB_CHANNEL_PROMISE_BASED_FILTER_H++// Scaffolding to allow the per-call part of a filter to be authored in a+// promise-style. Most of this will be removed once the promises conversion is+// completed.++#include <grpc/support/port_platform.h>++#include ""absl/utility/utility.h""++#include <grpc/status.h>+#include <grpc/support/log.h>++#include ""src/core/lib/channel/channel_stack.h""+#include ""src/core/lib/promise/arena_promise.h""+#include ""src/core/lib/promise/promise.h""+#include ""src/core/lib/transport/error_utils.h""++namespace grpc_core {++namespace promise_filter_detail {+class BaseCallData;+};++// Small unowned ""handle"" type to ensure one accessor at a time to metadata.+// The focus here is to get promises to use the syntax we'd like - we'll+// probably substitute some other smart pointer later.+template <typename T>+class MetadataHandle {+ public:+  MetadataHandle() = default;++  MetadataHandle(const MetadataHandle&) = delete;+  MetadataHandle& operator=(const MetadataHandle&) = delete;++  MetadataHandle(MetadataHandle&& other) noexcept : handle_(other.handle_) {+    other.handle_ = nullptr;+  }+  MetadataHandle& operator=(MetadataHandle&& other) noexcept {+    handle_ = other.handle_;+    other.handle_ = nullptr;+    return *this;+  }++  T* operator->() const { return handle_; }+  bool has_value() const { return handle_ != nullptr; }++ private:+  friend class promise_filter_detail::BaseCallData;++  explicit MetadataHandle(T* handle) : handle_(handle) {}+  T* Unwrap() {+    T* result = handle_;+    handle_ = nullptr;+    return result;+  }++  T* handle_ = nullptr;+};++// Trailing metadata type+// TODO(ctiller): This should be a bespoke instance of MetadataMap<>+using TrailingMetadata = MetadataHandle<grpc_metadata_batch>;++// Initial metadata type+// TODO(ctiller): This should be a bespoke instance of MetadataMap<>+using InitialMetadata = MetadataHandle<grpc_metadata_batch>;","Should we have different types for initial metadata sent by the client vs. sent by the server?  There are different fields required in the two cases (e.g., client needs to send path, timeout, etc).I realize that right now, we're not separating them into two different types in the underlying metadata code, so it doesn't matter.  But if we split it up in this interface, it means less code to change later when we do split up the representations in the underlying metadata code.I also realize that we're not actually using initial metadata sent by the server in this PR.  But I assume that eventually, we will convert a filter that does need to do that.  So I guess this suggestion probably just boils down to ""rename this to `ClientInitialMetadata`"".",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28565,788126907,2022-01-19T20:52:18Z,src/core/lib/channel/promise_based_filter.h,"@@ -0,0 +1,490 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_CHANNEL_PROMISE_BASED_FILTER_H+#define GRPC_CORE_LIB_CHANNEL_PROMISE_BASED_FILTER_H++// Scaffolding to allow the per-call part of a filter to be authored in a+// promise-style. Most of this will be removed once the promises conversion is+// completed.++#include <grpc/support/port_platform.h>++#include ""absl/utility/utility.h""++#include <grpc/status.h>+#include <grpc/support/log.h>++#include ""src/core/lib/channel/channel_stack.h""+#include ""src/core/lib/promise/arena_promise.h""+#include ""src/core/lib/promise/promise.h""+#include ""src/core/lib/transport/error_utils.h""++namespace grpc_core {++namespace promise_filter_detail {+class BaseCallData;+};++// Small unowned ""handle"" type to ensure one accessor at a time to metadata.+// The focus here is to get promises to use the syntax we'd like - we'll+// probably substitute some other smart pointer later.+template <typename T>+class MetadataHandle {+ public:+  MetadataHandle() = default;++  MetadataHandle(const MetadataHandle&) = delete;+  MetadataHandle& operator=(const MetadataHandle&) = delete;++  MetadataHandle(MetadataHandle&& other) noexcept : handle_(other.handle_) {+    other.handle_ = nullptr;+  }+  MetadataHandle& operator=(MetadataHandle&& other) noexcept {+    handle_ = other.handle_;+    other.handle_ = nullptr;+    return *this;+  }++  T* operator->() const { return handle_; }+  bool has_value() const { return handle_ != nullptr; }++ private:+  friend class promise_filter_detail::BaseCallData;++  explicit MetadataHandle(T* handle) : handle_(handle) {}+  T* Unwrap() {+    T* result = handle_;+    handle_ = nullptr;+    return result;+  }++  T* handle_ = nullptr;+};++// Trailing metadata type+// TODO(ctiller): This should be a bespoke instance of MetadataMap<>+using TrailingMetadata = MetadataHandle<grpc_metadata_batch>;++// Initial metadata type+// TODO(ctiller): This should be a bespoke instance of MetadataMap<>+using InitialMetadata = MetadataHandle<grpc_metadata_batch>;++using NextPromiseFactory =+    std::function<ArenaPromise<TrailingMetadata>(InitialMetadata)>;++namespace promise_filter_detail {++// Call data shared between all implementations of promise-based filters.+class BaseCallData {+ public:+  BaseCallData(grpc_call_element* elem, const grpc_call_element_args* args)+      : elem_(elem),+        arena_(args->arena),+        call_combiner_(args->call_combiner),+        owning_call_(args->call_stack),+        deadline_(args->deadline) {}++ protected:+  class ScopedContext : public promise_detail::Context<Arena> {+   public:+    explicit ScopedContext(BaseCallData* call_data)+        : promise_detail::Context<Arena>(call_data->arena_) {}+  };++  static MetadataHandle<grpc_metadata_batch> WrapMetadata(+      grpc_metadata_batch* p) {+    return MetadataHandle<grpc_metadata_batch>(p);+  }++  static grpc_metadata_batch* UnwrapMetadata(+      MetadataHandle<grpc_metadata_batch> p) {+    return p.Unwrap();+  }++  grpc_call_element* const elem_;+  Arena* const arena_;+  CallCombiner* const call_combiner_;+  grpc_call_stack* const owning_call_;+  const grpc_millis deadline_;+  ArenaPromise<TrailingMetadata> promise_;+};++// Specific call data per channel filter.+// Note that we further specialize for clients and servers since their+// implementations are very different.+template <class ChannelFilter, bool kIsClient = ChannelFilter::is_client()>+class CallData;++// Client implementation of call data.+template <class ChannelFilter>+class CallData<ChannelFilter, true> : public BaseCallData {+ public:+  CallData(grpc_call_element* elem, const grpc_call_element_args* args)+      : BaseCallData(elem, args) {+    GRPC_CLOSURE_INIT(&recv_trailing_metadata_ready_,+                      RecvTrailingMetadataReadyCallback, this,+                      grpc_schedule_on_exec_ctx);+  }++  ~CallData() {+    GPR_ASSERT(!is_polling_);+    GRPC_ERROR_UNREF(cancelled_error_);+  }++  // Handle one grpc_transport_stream_op_batch+  void Op(grpc_transport_stream_op_batch* op) {+    // Fake out the activity based context.+    ScopedContext context(this);++    // If this is a cancel stream, cancel anything we have pending and propagate+    // the cancellation.+    if (op->cancel_stream) {+      GPR_ASSERT(!op->send_initial_metadata && !op->send_trailing_metadata &&+                 !op->send_message && !op->recv_initial_metadata &&+                 !op->recv_message && !op->recv_trailing_metadata);+      Cancel(op->payload->cancel_stream.cancel_error);+      grpc_call_next_op(elem_, op);+      return;+    }++    // send_initial_metadata: seeing this triggers the start of the promise part+    // of this filter.+    if (op->send_initial_metadata) {+      // If we're already cancelled, just terminate the batch.+      if (send_initial_state_ == SendInitialState::kCancelled) {+        grpc_transport_stream_op_batch_finish_with_failure(+            op, GRPC_ERROR_REF(cancelled_error_), call_combiner_);+        return;+      }+      // Otherwise, we should not have seen a send_initial_metadata op yet.+      GPR_ASSERT(send_initial_state_ == SendInitialState::kInitial);+      // Mark ourselves as queued.+      send_initial_state_ = SendInitialState::kQueued;+      if (op->recv_trailing_metadata) {+        // If there's a recv_trailing_metadata op, we queue that too.+        GPR_ASSERT(recv_trailing_state_ == RecvTrailingState::kInitial);+        recv_trailing_state_ = RecvTrailingState::kQueued;+      }+      // This is the queuing!+      send_initial_metadata_batch_ = op;+      // And kick start the promise.+      StartPromise();+      return;+    }++    // recv_trailing_metadata *without* send_initial_metadata: hook it so we can+    // respond to it, and push it down.+    if (op->recv_trailing_metadata) {+      GPR_ASSERT(recv_trailing_state_ == RecvTrailingState::kInitial);+      recv_trailing_state_ = RecvTrailingState::kForwarded;+      HookRecvTrailingMetadata(op);+    }++    grpc_call_next_op(elem_, op);+  }++ private:+  // At what stage is our handling of send initial metadata?+  enum class SendInitialState {+    // Start state: no op seen+    kInitial,+    // We've seen the op, and started the promise in response to it, but have+    // not yet sent the op to the next filter.+    kQueued,+    // We've sent the op to the next filter.+    kForwarded,+    // We were cancelled.+    kCancelled+  };+  // At what stage is our handling of recv trailing metadata?+  enum class RecvTrailingState {+    // Start state: no op seen+    kInitial,+    // We saw the op, and since it was bundled with send initial metadata, we+    // queued it until the send initial metadata can be sent to the next filter.+    kQueued,+    // We've forwarded the op to the next filter.+    kForwarded,+    // The op has completed from below, but we haven't yet forwarded it up (the+    // promise gets to interject and mutate it).+    kComplete,+    // We've called the recv_metadata_ready callback from the original+    // recv_trailing_metadata op that was presented to us.+    kResponded,+    // We've been cancelled and handled that locally.+    // (i.e. whilst the recv_trailing_metadata op is queued in this filter).+    kCancelled+  };++  // Handle cancellation.+  void Cancel(grpc_error_handle error) {+    // Track the latest reason for cancellation.+    GRPC_ERROR_UNREF(cancelled_error_);+    cancelled_error_ = GRPC_ERROR_REF(error);+    // Stop running the promise.+    promise_ = ArenaPromise<TrailingMetadata>();+    // If we have an op queued, fail that op.+    // Record what we've done.+    if (send_initial_state_ == SendInitialState::kQueued) {+      send_initial_state_ = SendInitialState::kCancelled;+      if (recv_trailing_state_ == RecvTrailingState::kQueued) {+        recv_trailing_state_ = RecvTrailingState::kCancelled;+      }+      grpc_transport_stream_op_batch_finish_with_failure(+          absl::exchange(send_initial_metadata_batch_, nullptr),+          GRPC_ERROR_REF(cancelled_error_), call_combiner_);+    } else {+      send_initial_state_ = SendInitialState::kCancelled;+    }+  }++  // Begin running the promise - which will ultimately take some initial+  // metadata and return some trailing metadata.+  void StartPromise() {+    GPR_ASSERT(send_initial_state_ == SendInitialState::kQueued);+    ChannelFilter* filter = static_cast<ChannelFilter*>(elem_->channel_data);++    // Construct the promise.+    promise_ = filter->MakeCallPromise(+        WrapMetadata(send_initial_metadata_batch_->payload+                         ->send_initial_metadata.send_initial_metadata),+        [this](InitialMetadata initial_metadata) {+          return NextPromiseFactory(std::move(initial_metadata));+        });+    // Poll once.+    WakeInsideCombiner();+  }++  // Interject our callback into the op batch for recv trailing metadata ready.+  // Stash a pointer to the trailing metadata that will be filled in, so we can+  // manipulate it later.+  void HookRecvTrailingMetadata(grpc_transport_stream_op_batch* op) {+    recv_trailing_metadata_ =+        op->payload->recv_trailing_metadata.recv_trailing_metadata;+    original_recv_trailing_metadata_ready_ =+        op->payload->recv_trailing_metadata.recv_trailing_metadata_ready;+    op->payload->recv_trailing_metadata.recv_trailing_metadata_ready =+        &recv_trailing_metadata_ready_;+  }++  // Construct a promise that will ""call"" the next filter.+  // Effectively:+  //   - put the modified initial metadata into the batch to be sent down.+  //   - return a wrapper around PollTrailingMetadata as the promise.+  ArenaPromise<TrailingMetadata> NextPromiseFactory(+      InitialMetadata initial_metadata) {+    GPR_ASSERT(send_initial_state_ == SendInitialState::kQueued);+    send_initial_metadata_batch_->payload->send_initial_metadata+        .send_initial_metadata = UnwrapMetadata(std::move(initial_metadata));+    return ArenaPromise<TrailingMetadata>(+        [this]() { return PollTrailingMetadata(); });+  }++  // Wrapper to make it look like we're calling the next filter as a promise.+  // First poll: send the send_initial_metadata op down the stack.+  // All polls: await receiving the trailing metadata, then return it to the+  // application.+  Poll<TrailingMetadata> PollTrailingMetadata() {+    if (send_initial_state_ == SendInitialState::kQueued) {+      // First poll: pass the send_initial_metadata op down the stack.+      GPR_ASSERT(send_initial_metadata_batch_ != nullptr);+      send_initial_state_ = SendInitialState::kForwarded;+      if (recv_trailing_state_ == RecvTrailingState::kQueued) {+        // (and the recv_trailing_metadata op if it's part of the queuing)+        HookRecvTrailingMetadata(send_initial_metadata_batch_);+        recv_trailing_state_ = RecvTrailingState::kForwarded;+      }+      forward_send_initial_metadata_ = true;+    }+    switch (recv_trailing_state_) {+      case RecvTrailingState::kInitial:+      case RecvTrailingState::kQueued:+      case RecvTrailingState::kForwarded:+        // No trailing metadata yet: we are pending.+        // We return that and expect the promise to be repolled later (if it's+        // not cancelled).+        return Pending{};+      case RecvTrailingState::kComplete:+        // We've received trailing metadata: pass it to the promise and allow it+        // to adjust it.+        return WrapMetadata(recv_trailing_metadata_);+      case RecvTrailingState::kCancelled: {+        // We've been cancelled: synthesize some trailing metadata and pass it+        // to the calling promise for adjustment.+        recv_trailing_metadata_->Clear();+        SetStatusFromError(recv_trailing_metadata_, cancelled_error_);+        return WrapMetadata(recv_trailing_metadata_);+      }+      case RecvTrailingState::kResponded:+        // We've already responded to the caller: we can't do anything and we+        // should never reach here.+        abort();+    }+    GPR_UNREACHABLE_CODE(return Pending{});+  }++  static void RecvTrailingMetadataReadyCallback(void* arg,+                                                grpc_error_handle error) {+    static_cast<CallData*>(arg)->RecvTrailingMetadataReady(error);+  }++  void RecvTrailingMetadataReady(grpc_error_handle error) {+    // If there was an error, we'll put that into the trailing metadata and+    // proceed as if there was not.+    if (error != GRPC_ERROR_NONE) {+      SetStatusFromError(recv_trailing_metadata_, error);+    }+    // Record that we've got the callback.+    GPR_ASSERT(recv_trailing_state_ == RecvTrailingState::kForwarded);+    recv_trailing_state_ = RecvTrailingState::kComplete;+    // Repoll the promise.+    ScopedContext context(this);+    WakeInsideCombiner();+  }++  // Given an error, fill in TrailingMetadata to represent that error.+  void SetStatusFromError(grpc_metadata_batch* metadata,+                          grpc_error_handle error) {+    grpc_status_code status_code = GRPC_STATUS_UNKNOWN;+    std::string status_details;+    grpc_error_get_status(error, deadline_, &status_code, &status_details,+                          nullptr, nullptr);+    metadata->Set(GrpcStatusMetadata(), status_code);+    metadata->Set(GrpcMessageMetadata(),+                  Slice::FromCopiedString(status_details));+  }++  // Wakeup and poll the promise if appropriate.+  void WakeInsideCombiner() {+    GPR_ASSERT(!is_polling_);+    grpc_closure* call_closure = nullptr;+    is_polling_ = true;+    switch (send_initial_state_) {+      case SendInitialState::kQueued:+      case SendInitialState::kForwarded: {+        // Poll the promise once since we're waiting for it.+        Poll<TrailingMetadata> poll = promise_();+        if (auto* r = absl::get_if<TrailingMetadata>(&poll)) {+          GPR_ASSERT(recv_trailing_state_ == RecvTrailingState::kComplete);+          GPR_ASSERT(recv_trailing_metadata_ == UnwrapMetadata(std::move(*r)));+          recv_trailing_state_ = RecvTrailingState::kResponded;+          call_closure =+              absl::exchange(original_recv_trailing_metadata_ready_, nullptr);+        }+      } break;+      case SendInitialState::kInitial:+      case SendInitialState::kCancelled:+        // If we get a response without sending anything, we just propagate that+        // up. (note: that situation isn't possible once we finish the promise+        // transition).+        if (recv_trailing_state_ == RecvTrailingState::kComplete) {+          recv_trailing_state_ = RecvTrailingState::kResponded;+          call_closure =+              absl::exchange(original_recv_trailing_metadata_ready_, nullptr);+        }+        break;+    }+    is_polling_ = false;+    if (absl::exchange(forward_send_initial_metadata_, false)) {+      grpc_call_next_op(elem_,+                        absl::exchange(send_initial_metadata_batch_, nullptr));+    }+    if (call_closure != nullptr) {+      Closure::Run(DEBUG_LOCATION, call_closure, GRPC_ERROR_NONE);+    }+  }++  // Queued batch containing at least a send_initial_metadata op.+  grpc_transport_stream_op_batch* send_initial_metadata_batch_ = nullptr;+  // Pointer to where trailing metadata will be stored.+  grpc_metadata_batch* recv_trailing_metadata_ = nullptr;+  // Closure to call when we're done with the trailing metadata.+  grpc_closure* original_recv_trailing_metadata_ready_ = nullptr;+  // Our closure pointing to RecvTrailingMetadataReadyCallback.+  grpc_closure recv_trailing_metadata_ready_;+  // Error received during cancellation.+  grpc_error_handle cancelled_error_ = GRPC_ERROR_NONE;+  // State of the send_initial_metadata op.+  SendInitialState send_initial_state_ = SendInitialState::kInitial;+  // State of the recv_trailing_metadata op.+  RecvTrailingState recv_trailing_state_ = RecvTrailingState::kInitial;+  // Whether we're currently polling the promise.+  bool is_polling_ = false;+  // Whether we should forward send initial metadata after polling?+  bool forward_send_initial_metadata_ = false;+};++}  // namespace promise_filter_detail++// ChannelFilter contains the following:+// class SomeChannelFilter {+//  public:+//   static constexpr bool is_client();+//   static constexpr const char* name();+//   static absl::StatusOr<SomeChannelFilter> Create(+//       const grpc_channel_args* args);+//   ArenaPromise<TrailingMetadata> MakeCallPromise(+//       InitialMetadata* initial_metadata, NextPromiseFactory next_promise);+// };+// TODO(ctiller): allow implementing get_channel_info, start_transport_op in+// some way on ChannelFilter.+template <typename ChannelFilter>+grpc_channel_filter MakePromiseBasedFilter() {+  using CallData = promise_filter_detail::CallData<ChannelFilter>;++  return grpc_channel_filter{+      // start_transport_stream_op_batch+      [](grpc_call_element* elem, grpc_transport_stream_op_batch* op) {",Please call this parameter `batch` instead of `op`.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28565,788144553,2022-01-19T21:20:18Z,src/core/lib/channel/promise_based_filter.h,"@@ -0,0 +1,490 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_CHANNEL_PROMISE_BASED_FILTER_H+#define GRPC_CORE_LIB_CHANNEL_PROMISE_BASED_FILTER_H++// Scaffolding to allow the per-call part of a filter to be authored in a+// promise-style. Most of this will be removed once the promises conversion is+// completed.++#include <grpc/support/port_platform.h>++#include ""absl/utility/utility.h""++#include <grpc/status.h>+#include <grpc/support/log.h>++#include ""src/core/lib/channel/channel_stack.h""+#include ""src/core/lib/promise/arena_promise.h""+#include ""src/core/lib/promise/promise.h""+#include ""src/core/lib/transport/error_utils.h""++namespace grpc_core {++namespace promise_filter_detail {+class BaseCallData;+};++// Small unowned ""handle"" type to ensure one accessor at a time to metadata.+// The focus here is to get promises to use the syntax we'd like - we'll+// probably substitute some other smart pointer later.+template <typename T>+class MetadataHandle {+ public:+  MetadataHandle() = default;++  MetadataHandle(const MetadataHandle&) = delete;+  MetadataHandle& operator=(const MetadataHandle&) = delete;++  MetadataHandle(MetadataHandle&& other) noexcept : handle_(other.handle_) {+    other.handle_ = nullptr;+  }+  MetadataHandle& operator=(MetadataHandle&& other) noexcept {+    handle_ = other.handle_;+    other.handle_ = nullptr;+    return *this;+  }++  T* operator->() const { return handle_; }+  bool has_value() const { return handle_ != nullptr; }++ private:+  friend class promise_filter_detail::BaseCallData;++  explicit MetadataHandle(T* handle) : handle_(handle) {}+  T* Unwrap() {+    T* result = handle_;+    handle_ = nullptr;+    return result;+  }++  T* handle_ = nullptr;+};++// Trailing metadata type+// TODO(ctiller): This should be a bespoke instance of MetadataMap<>+using TrailingMetadata = MetadataHandle<grpc_metadata_batch>;++// Initial metadata type+// TODO(ctiller): This should be a bespoke instance of MetadataMap<>+using InitialMetadata = MetadataHandle<grpc_metadata_batch>;++using NextPromiseFactory =+    std::function<ArenaPromise<TrailingMetadata>(InitialMetadata)>;++namespace promise_filter_detail {++// Call data shared between all implementations of promise-based filters.+class BaseCallData {+ public:+  BaseCallData(grpc_call_element* elem, const grpc_call_element_args* args)+      : elem_(elem),+        arena_(args->arena),+        call_combiner_(args->call_combiner),+        owning_call_(args->call_stack),+        deadline_(args->deadline) {}++ protected:+  class ScopedContext : public promise_detail::Context<Arena> {+   public:+    explicit ScopedContext(BaseCallData* call_data)+        : promise_detail::Context<Arena>(call_data->arena_) {}+  };++  static MetadataHandle<grpc_metadata_batch> WrapMetadata(+      grpc_metadata_batch* p) {+    return MetadataHandle<grpc_metadata_batch>(p);+  }++  static grpc_metadata_batch* UnwrapMetadata(+      MetadataHandle<grpc_metadata_batch> p) {+    return p.Unwrap();+  }++  grpc_call_element* const elem_;+  Arena* const arena_;+  CallCombiner* const call_combiner_;+  grpc_call_stack* const owning_call_;+  const grpc_millis deadline_;+  ArenaPromise<TrailingMetadata> promise_;+};++// Specific call data per channel filter.+// Note that we further specialize for clients and servers since their+// implementations are very different.+template <class ChannelFilter, bool kIsClient = ChannelFilter::is_client()>+class CallData;++// Client implementation of call data.+template <class ChannelFilter>+class CallData<ChannelFilter, true> : public BaseCallData {+ public:+  CallData(grpc_call_element* elem, const grpc_call_element_args* args)+      : BaseCallData(elem, args) {+    GRPC_CLOSURE_INIT(&recv_trailing_metadata_ready_,+                      RecvTrailingMetadataReadyCallback, this,+                      grpc_schedule_on_exec_ctx);+  }++  ~CallData() {+    GPR_ASSERT(!is_polling_);+    GRPC_ERROR_UNREF(cancelled_error_);+  }++  // Handle one grpc_transport_stream_op_batch+  void Op(grpc_transport_stream_op_batch* op) {+    // Fake out the activity based context.+    ScopedContext context(this);++    // If this is a cancel stream, cancel anything we have pending and propagate+    // the cancellation.+    if (op->cancel_stream) {+      GPR_ASSERT(!op->send_initial_metadata && !op->send_trailing_metadata &&+                 !op->send_message && !op->recv_initial_metadata &&+                 !op->recv_message && !op->recv_trailing_metadata);+      Cancel(op->payload->cancel_stream.cancel_error);+      grpc_call_next_op(elem_, op);+      return;+    }++    // send_initial_metadata: seeing this triggers the start of the promise part+    // of this filter.+    if (op->send_initial_metadata) {+      // If we're already cancelled, just terminate the batch.+      if (send_initial_state_ == SendInitialState::kCancelled) {+        grpc_transport_stream_op_batch_finish_with_failure(+            op, GRPC_ERROR_REF(cancelled_error_), call_combiner_);+        return;+      }+      // Otherwise, we should not have seen a send_initial_metadata op yet.+      GPR_ASSERT(send_initial_state_ == SendInitialState::kInitial);+      // Mark ourselves as queued.+      send_initial_state_ = SendInitialState::kQueued;+      if (op->recv_trailing_metadata) {+        // If there's a recv_trailing_metadata op, we queue that too.+        GPR_ASSERT(recv_trailing_state_ == RecvTrailingState::kInitial);+        recv_trailing_state_ = RecvTrailingState::kQueued;+      }+      // This is the queuing!+      send_initial_metadata_batch_ = op;+      // And kick start the promise.+      StartPromise();+      return;+    }++    // recv_trailing_metadata *without* send_initial_metadata: hook it so we can+    // respond to it, and push it down.+    if (op->recv_trailing_metadata) {+      GPR_ASSERT(recv_trailing_state_ == RecvTrailingState::kInitial);+      recv_trailing_state_ = RecvTrailingState::kForwarded;+      HookRecvTrailingMetadata(op);+    }++    grpc_call_next_op(elem_, op);+  }++ private:+  // At what stage is our handling of send initial metadata?+  enum class SendInitialState {+    // Start state: no op seen+    kInitial,+    // We've seen the op, and started the promise in response to it, but have+    // not yet sent the op to the next filter.+    kQueued,+    // We've sent the op to the next filter.+    kForwarded,+    // We were cancelled.+    kCancelled+  };+  // At what stage is our handling of recv trailing metadata?+  enum class RecvTrailingState {+    // Start state: no op seen+    kInitial,+    // We saw the op, and since it was bundled with send initial metadata, we+    // queued it until the send initial metadata can be sent to the next filter.+    kQueued,+    // We've forwarded the op to the next filter.+    kForwarded,+    // The op has completed from below, but we haven't yet forwarded it up (the+    // promise gets to interject and mutate it).+    kComplete,+    // We've called the recv_metadata_ready callback from the original+    // recv_trailing_metadata op that was presented to us.+    kResponded,+    // We've been cancelled and handled that locally.+    // (i.e. whilst the recv_trailing_metadata op is queued in this filter).+    kCancelled+  };++  // Handle cancellation.+  void Cancel(grpc_error_handle error) {+    // Track the latest reason for cancellation.+    GRPC_ERROR_UNREF(cancelled_error_);+    cancelled_error_ = GRPC_ERROR_REF(error);+    // Stop running the promise.+    promise_ = ArenaPromise<TrailingMetadata>();+    // If we have an op queued, fail that op.+    // Record what we've done.+    if (send_initial_state_ == SendInitialState::kQueued) {+      send_initial_state_ = SendInitialState::kCancelled;+      if (recv_trailing_state_ == RecvTrailingState::kQueued) {+        recv_trailing_state_ = RecvTrailingState::kCancelled;+      }+      grpc_transport_stream_op_batch_finish_with_failure(+          absl::exchange(send_initial_metadata_batch_, nullptr),+          GRPC_ERROR_REF(cancelled_error_), call_combiner_);+    } else {+      send_initial_state_ = SendInitialState::kCancelled;+    }+  }++  // Begin running the promise - which will ultimately take some initial+  // metadata and return some trailing metadata.+  void StartPromise() {+    GPR_ASSERT(send_initial_state_ == SendInitialState::kQueued);+    ChannelFilter* filter = static_cast<ChannelFilter*>(elem_->channel_data);++    // Construct the promise.+    promise_ = filter->MakeCallPromise(+        WrapMetadata(send_initial_metadata_batch_->payload+                         ->send_initial_metadata.send_initial_metadata),+        [this](InitialMetadata initial_metadata) {+          return NextPromiseFactory(std::move(initial_metadata));+        });+    // Poll once.+    WakeInsideCombiner();+  }++  // Interject our callback into the op batch for recv trailing metadata ready.+  // Stash a pointer to the trailing metadata that will be filled in, so we can+  // manipulate it later.+  void HookRecvTrailingMetadata(grpc_transport_stream_op_batch* op) {",Suggest calling this something like `InjectRecvTrailingMetadataReadyCallback()`.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28563,789167613,2022-01-20T21:34:36Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -6209,6 +6209,52 @@ TEST_P(CdsTest, AggregateClusterType) {       ""GRPC_XDS_EXPERIMENTAL_ENABLE_AGGREGATE_AND_LOGICAL_DNS_CLUSTER""); } +TEST_P(CdsTest, AggregateClusterFallBackFromRingHashAtStartup) {+  gpr_setenv(""GRPC_XDS_EXPERIMENTAL_ENABLE_AGGREGATE_AND_LOGICAL_DNS_CLUSTER"",+             ""true"");+  const char* kNewCluster1Name = ""new_cluster_1"";+  const char* kNewEdsService1Name = ""new_eds_service_name_1"";+  const char* kNewCluster2Name = ""new_cluster_2"";+  const char* kNewEdsService2Name = ""new_eds_service_name_2"";+  // Populate new EDS resources.+  EdsResourceArgs args1({+      {""locality0"", {MakeNonExistantEndpoint(), MakeNonExistantEndpoint()}},+  });+  EdsResourceArgs args2({+      {""locality0"", CreateEndpointsForBackends(0, 1)},+  });+  balancer_->ads_service()->SetEdsResource(+      BuildEdsResource(args1, kNewEdsService1Name));+  balancer_->ads_service()->SetEdsResource(+      BuildEdsResource(args2, kNewEdsService2Name));+  // Populate new CDS resources.+  Cluster new_cluster1 = default_cluster_;+  new_cluster1.set_name(kNewCluster1Name);+  new_cluster1.mutable_eds_cluster_config()->set_service_name(+      kNewEdsService1Name);+  new_cluster1.set_lb_policy(Cluster::RING_HASH);+  balancer_->ads_service()->SetCdsResource(new_cluster1);+  Cluster new_cluster2 = default_cluster_;+  new_cluster2.set_name(kNewCluster2Name);+  new_cluster2.mutable_eds_cluster_config()->set_service_name(+      kNewEdsService2Name);+  balancer_->ads_service()->SetCdsResource(new_cluster2);+  // Create Aggregate Cluster+  auto cluster = default_cluster_;+  CustomClusterType* custom_cluster = cluster.mutable_cluster_type();+  custom_cluster->set_name(""envoy.clusters.aggregate"");+  ClusterConfig cluster_config;+  cluster_config.add_clusters(kNewCluster1Name);+  cluster_config.add_clusters(kNewCluster2Name);+  custom_cluster->mutable_typed_config()->PackFrom(cluster_config);+  balancer_->ads_service()->SetCdsResource(cluster);+  // Wait for traffic to go backend 0, meaning we failed over to the second+  // priority.+  WaitForBackend(0);+  gpr_unsetenv(","Yeah, probably true, but (a) this matches how we've done all of the existing tests, (b) the env vars are only temporary anyway, and (b) none of these tests actually have any assertions that would cause them to exit early.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28565,789774282,2022-01-21T15:51:29Z,src/core/lib/transport/metadata_batch.h,"@@ -889,6 +889,15 @@ class MetadataMap {   void Log(absl::FunctionRef<void(absl::string_view, absl::string_view)> log_fn)       const; +  std::string DebugString() const {+    std::string out;+    Log([&out](absl::string_view key, absl::string_view value) {+      if (!out.empty()) out.append("", "");+      absl::StrAppend(&out, key, "": "", value);","This should probably use `absl::CEscape()` on the key and value, to support binary metadata.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28565,789798446,2022-01-21T16:20:36Z,src/core/lib/channel/promise_based_filter.h,"@@ -0,0 +1,492 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_CHANNEL_PROMISE_BASED_FILTER_H+#define GRPC_CORE_LIB_CHANNEL_PROMISE_BASED_FILTER_H++// Scaffolding to allow the per-call part of a filter to be authored in a+// promise-style. Most of this will be removed once the promises conversion is+// completed.++#include <grpc/support/port_platform.h>++#include ""absl/utility/utility.h""++#include <grpc/status.h>+#include <grpc/support/log.h>++#include ""src/core/lib/channel/channel_stack.h""+#include ""src/core/lib/promise/arena_promise.h""+#include ""src/core/lib/promise/promise.h""+#include ""src/core/lib/transport/error_utils.h""++namespace grpc_core {++namespace promise_filter_detail {+class BaseCallData;+};++// Small unowned ""handle"" type to ensure one accessor at a time to metadata.+// The focus here is to get promises to use the syntax we'd like - we'll+// probably substitute some other smart pointer later.+template <typename T>+class MetadataHandle {+ public:+  MetadataHandle() = default;++  MetadataHandle(const MetadataHandle&) = delete;+  MetadataHandle& operator=(const MetadataHandle&) = delete;++  MetadataHandle(MetadataHandle&& other) noexcept : handle_(other.handle_) {+    other.handle_ = nullptr;+  }+  MetadataHandle& operator=(MetadataHandle&& other) noexcept {+    handle_ = other.handle_;+    other.handle_ = nullptr;+    return *this;+  }++  T* operator->() const { return handle_; }+  bool has_value() const { return handle_ != nullptr; }++  static MetadataHandle TestOnlyWrap(T* p) { return MetadataHandle(p); }++ private:+  friend class promise_filter_detail::BaseCallData;++  explicit MetadataHandle(T* handle) : handle_(handle) {}+  T* Unwrap() {+    T* result = handle_;+    handle_ = nullptr;+    return result;+  }++  T* handle_ = nullptr;+};++// Trailing metadata type+// TODO(ctiller): This should be a bespoke instance of MetadataMap<>+using TrailingMetadata = MetadataHandle<grpc_metadata_batch>;++// Initial metadata type+// TODO(ctiller): This should be a bespoke instance of MetadataMap<>+using InitialMetadata = MetadataHandle<grpc_metadata_batch>;++using NextPromiseFactory =+    std::function<ArenaPromise<TrailingMetadata>(InitialMetadata)>;++namespace promise_filter_detail {++// Call data shared between all implementations of promise-based filters.+class BaseCallData {+ public:+  BaseCallData(grpc_call_element* elem, const grpc_call_element_args* args)+      : elem_(elem),+        arena_(args->arena),+        call_combiner_(args->call_combiner),+        owning_call_(args->call_stack),+        deadline_(args->deadline) {}++ protected:+  class ScopedContext : public promise_detail::Context<Arena> {+   public:+    explicit ScopedContext(BaseCallData* call_data)+        : promise_detail::Context<Arena>(call_data->arena_) {}+  };++  static MetadataHandle<grpc_metadata_batch> WrapMetadata(+      grpc_metadata_batch* p) {+    return MetadataHandle<grpc_metadata_batch>(p);+  }++  static grpc_metadata_batch* UnwrapMetadata(+      MetadataHandle<grpc_metadata_batch> p) {+    return p.Unwrap();+  }++  grpc_call_element* const elem_;","Not all filters actually store all of these values.  By storing these once for every promise-based filter, are we going to increase memory usage in the short term?  I realize that in the long term, these will be stored once across all filters instead of once per filter, so it will actually decrease memory usage, but I'm wondering if the short-term increase is going to be a problem.Could we ameliorate this by providing a call context struct that we pass down through the filter stack, and then use that here?  In principle, the change to have a call context object seems orthogonal to the switch from filters to promises.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28565,789924128,2022-01-21T19:04:45Z,test/core/filters/client_authority_filter_test.cc,"@@ -0,0 +1,124 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/ext/filters/http/client_authority_filter.h""++#include <gtest/gtest.h>++#include ""src/core/lib/resource_quota/resource_quota.h""++namespace grpc_core {+namespace {++auto* g_memory_allocator = new MemoryAllocator(+    ResourceQuota::Default()->memory_quota()->CreateMemoryAllocator(""test""));++class TestChannelArgs {+ public:+  explicit TestChannelArgs(const char* default_authority) {+    arg_.key = const_cast<char*>(GRPC_ARG_DEFAULT_AUTHORITY);+    arg_.type = GRPC_ARG_STRING;+    arg_.value.string = const_cast<char*>(default_authority);+    args_.num_args = 1;+    args_.args = &arg_;+  }++  const grpc_channel_args* args() const { return &args_; }++ private:+  grpc_arg arg_;+  grpc_channel_args args_;+};++TEST(ClientAuthorityFilterTest, DefaultFails) {+  EXPECT_FALSE(ClientAuthorityFilter::Create(nullptr).ok());+}++TEST(ClientAuthorityFilterTest, WithArgSucceeds) {+  EXPECT_TRUE(ClientAuthorityFilter::Create(+                  TestChannelArgs(""foo.test.google.au"").args())+                  .ok());+}++TEST(ClientAuthorityFilterTest, NonStringArgFails) {+  grpc_arg arg;+  arg.type = GRPC_ARG_INTEGER;+  arg.key = const_cast<char*>(GRPC_ARG_DEFAULT_AUTHORITY);+  arg.value.integer = 0;+  grpc_channel_args args;+  args.num_args = 1;+  args.args = &arg;+  EXPECT_FALSE(ClientAuthorityFilter::Create(&args).ok());+}++TEST(ClientAuthorityFilterTest, PromiseCompletesImmediatelyAndSetsAuthority) {+  auto filter = *ClientAuthorityFilter::Create(+      TestChannelArgs(""foo.test.google.au"").args());+  auto arena = MakeScopedArena(1024, g_memory_allocator);+  grpc_metadata_batch initial_metadata_batch(arena.get());+  grpc_metadata_batch trailing_metadata_batch(arena.get());+  bool seen = false;+  // TODO(ctiller): use Activity here, once it's ready.",Need to make it understand that `TrailingMetadata` is a final status,X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28565,789970970,2022-01-21T20:22:42Z,src/core/lib/channel/promise_based_filter.h,"@@ -0,0 +1,492 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_CHANNEL_PROMISE_BASED_FILTER_H+#define GRPC_CORE_LIB_CHANNEL_PROMISE_BASED_FILTER_H++// Scaffolding to allow the per-call part of a filter to be authored in a+// promise-style. Most of this will be removed once the promises conversion is+// completed.++#include <grpc/support/port_platform.h>++#include ""absl/utility/utility.h""++#include <grpc/status.h>+#include <grpc/support/log.h>++#include ""src/core/lib/channel/channel_stack.h""+#include ""src/core/lib/promise/arena_promise.h""+#include ""src/core/lib/promise/promise.h""+#include ""src/core/lib/transport/error_utils.h""++namespace grpc_core {++namespace promise_filter_detail {+class BaseCallData;+};++// Small unowned ""handle"" type to ensure one accessor at a time to metadata.+// The focus here is to get promises to use the syntax we'd like - we'll+// probably substitute some other smart pointer later.+template <typename T>+class MetadataHandle {+ public:+  MetadataHandle() = default;++  MetadataHandle(const MetadataHandle&) = delete;+  MetadataHandle& operator=(const MetadataHandle&) = delete;++  MetadataHandle(MetadataHandle&& other) noexcept : handle_(other.handle_) {+    other.handle_ = nullptr;+  }+  MetadataHandle& operator=(MetadataHandle&& other) noexcept {+    handle_ = other.handle_;+    other.handle_ = nullptr;+    return *this;+  }++  T* operator->() const { return handle_; }+  bool has_value() const { return handle_ != nullptr; }++  static MetadataHandle TestOnlyWrap(T* p) { return MetadataHandle(p); }++ private:+  friend class promise_filter_detail::BaseCallData;++  explicit MetadataHandle(T* handle) : handle_(handle) {}+  T* Unwrap() {+    T* result = handle_;+    handle_ = nullptr;+    return result;+  }++  T* handle_ = nullptr;+};++// Trailing metadata type+// TODO(ctiller): This should be a bespoke instance of MetadataMap<>+using TrailingMetadata = MetadataHandle<grpc_metadata_batch>;++// Initial metadata type+// TODO(ctiller): This should be a bespoke instance of MetadataMap<>+using InitialMetadata = MetadataHandle<grpc_metadata_batch>;++using NextPromiseFactory =+    std::function<ArenaPromise<TrailingMetadata>(InitialMetadata)>;++namespace promise_filter_detail {++// Call data shared between all implementations of promise-based filters.+class BaseCallData {+ public:+  BaseCallData(grpc_call_element* elem, const grpc_call_element_args* args)+      : elem_(elem),+        arena_(args->arena),+        call_combiner_(args->call_combiner),+        owning_call_(args->call_stack),+        deadline_(args->deadline) {}++ protected:+  class ScopedContext : public promise_detail::Context<Arena> {+   public:+    explicit ScopedContext(BaseCallData* call_data)+        : promise_detail::Context<Arena>(call_data->arena_) {}+  };++  static MetadataHandle<grpc_metadata_batch> WrapMetadata(+      grpc_metadata_batch* p) {+    return MetadataHandle<grpc_metadata_batch>(p);+  }++  static grpc_metadata_batch* UnwrapMetadata(+      MetadataHandle<grpc_metadata_batch> p) {+    return p.Unwrap();+  }++  grpc_call_element* const elem_;+  Arena* const arena_;+  CallCombiner* const call_combiner_;+  grpc_call_stack* const owning_call_;+  const grpc_millis deadline_;+  ArenaPromise<TrailingMetadata> promise_;+};++// Specific call data per channel filter.+// Note that we further specialize for clients and servers since their+// implementations are very different.+template <class ChannelFilter, bool kIsClient = ChannelFilter::is_client()>+class CallData;++// Client implementation of call data.+template <class ChannelFilter>+class CallData<ChannelFilter, true> : public BaseCallData {+ public:+  CallData(grpc_call_element* elem, const grpc_call_element_args* args)+      : BaseCallData(elem, args) {+    GRPC_CLOSURE_INIT(&recv_trailing_metadata_ready_,+                      RecvTrailingMetadataReadyCallback, this,+                      grpc_schedule_on_exec_ctx);+  }++  ~CallData() {+    GPR_ASSERT(!is_polling_);+    GRPC_ERROR_UNREF(cancelled_error_);+  }++  // Handle one grpc_transport_stream_op_batch+  void Op(grpc_transport_stream_op_batch* op) {+    // Fake out the activity based context.+    ScopedContext context(this);++    // If this is a cancel stream, cancel anything we have pending and propagate+    // the cancellation.+    if (op->cancel_stream) {+      GPR_ASSERT(!op->send_initial_metadata && !op->send_trailing_metadata &&+                 !op->send_message && !op->recv_initial_metadata &&+                 !op->recv_message && !op->recv_trailing_metadata);+      Cancel(op->payload->cancel_stream.cancel_error);+      grpc_call_next_op(elem_, op);",absolutely... I predict we end up with a handful of these classes tbh.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28650,789986814,2022-01-21T20:52:36Z,test/core/transport/metadata_map_test.cc,"@@ -97,6 +104,14 @@ TEST(MetadataMapTest, TimeoutEncodeTest) {   EXPECT_EQ(encoder.output(), ""grpc-timeout: deadline=1234\n""); } +TEST(MetadataMapTest, NonEncodableTrait) {",Maybe also test that this metadata trait is not processed by `Encode()`?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28660,790968972,2022-01-24T17:01:25Z,src/core/lib/channel/channel_stack_builder.h,"@@ -26,130 +26,83 @@ #include ""src/core/lib/channel/channel_args.h"" #include ""src/core/lib/channel/channel_stack.h"" -/// grpc_channel_stack_builder offers a programmatic interface to selected-/// and order channel filters-typedef struct grpc_channel_stack_builder grpc_channel_stack_builder;-typedef struct grpc_channel_stack_builder_iterator-    grpc_channel_stack_builder_iterator;--/// Create a new channel stack builder.-/// \a name must be statically allocated.-grpc_channel_stack_builder* grpc_channel_stack_builder_create(const char* name);--/// Set the target uri-void grpc_channel_stack_builder_set_target(grpc_channel_stack_builder* b,-                                           const char* target);--std::string grpc_channel_stack_builder_get_target(-    grpc_channel_stack_builder* b);--/// Attach \a transport to the builder (does not take ownership)-void grpc_channel_stack_builder_set_transport(-    grpc_channel_stack_builder* builder, grpc_transport* transport);--/// Fetch attached transport-grpc_transport* grpc_channel_stack_builder_get_transport(-    grpc_channel_stack_builder* builder);--/// Set channel arguments: copies args-void grpc_channel_stack_builder_set_channel_arguments(-    grpc_channel_stack_builder* builder, const grpc_channel_args* args);--/// Return a borrowed pointer to the channel arguments-const grpc_channel_args* grpc_channel_stack_builder_get_channel_arguments(-    grpc_channel_stack_builder* builder);--/// Begin iterating over already defined filters in the builder at the beginning-grpc_channel_stack_builder_iterator*-grpc_channel_stack_builder_create_iterator_at_first(-    grpc_channel_stack_builder* builder);--/// Begin iterating over already defined filters in the builder at the end-grpc_channel_stack_builder_iterator*-grpc_channel_stack_builder_create_iterator_at_last(-    grpc_channel_stack_builder* builder);--/// Is an iterator at the first element?-bool grpc_channel_stack_builder_iterator_is_first(-    grpc_channel_stack_builder_iterator* iterator);--/// Is an iterator at the end?-bool grpc_channel_stack_builder_iterator_is_end(-    grpc_channel_stack_builder_iterator* iterator);--/// What is the name of the filter at this iterator position?-const char* grpc_channel_stack_builder_iterator_filter_name(-    grpc_channel_stack_builder_iterator* iterator);--/// Move an iterator to the next item-bool grpc_channel_stack_builder_move_next(-    grpc_channel_stack_builder_iterator* iterator);--/// Move an iterator to the previous item-bool grpc_channel_stack_builder_move_prev(-    grpc_channel_stack_builder_iterator* iterator);--/// Return an iterator at \a filter_name, or at the end of the list if not-/// found.-grpc_channel_stack_builder_iterator* grpc_channel_stack_builder_iterator_find(-    grpc_channel_stack_builder* builder, const char* filter_name);--typedef void (*grpc_post_filter_create_init_func)(-    grpc_channel_stack* channel_stack, grpc_channel_element* elem, void* arg);--/// Add \a filter to the stack, after \a iterator.-/// Call \a post_init_func(..., \a user_data) once the channel stack is-/// created.-bool grpc_channel_stack_builder_add_filter_after(-    grpc_channel_stack_builder_iterator* iterator,-    const grpc_channel_filter* filter,-    grpc_post_filter_create_init_func post_init_func,-    void* user_data) GRPC_MUST_USE_RESULT;--/// Add \a filter to the stack, before \a iterator.-/// Call \a post_init_func(..., \a user_data) once the channel stack is-/// created.-bool grpc_channel_stack_builder_add_filter_before(-    grpc_channel_stack_builder_iterator* iterator,-    const grpc_channel_filter* filter,-    grpc_post_filter_create_init_func post_init_func,-    void* user_data) GRPC_MUST_USE_RESULT;--/// Add \a filter to the beginning of the filter list.-/// Call \a post_init_func(..., \a user_data) once the channel stack is-/// created.-bool grpc_channel_stack_builder_prepend_filter(-    grpc_channel_stack_builder* builder, const grpc_channel_filter* filter,-    grpc_post_filter_create_init_func post_init_func,-    void* user_data) GRPC_MUST_USE_RESULT;--/// Add \a filter to the end of the filter list.-/// Call \a post_init_func(..., \a user_data) once the channel stack is-/// created.-bool grpc_channel_stack_builder_append_filter(-    grpc_channel_stack_builder* builder, const grpc_channel_filter* filter,-    grpc_post_filter_create_init_func post_init_func,-    void* user_data) GRPC_MUST_USE_RESULT;--/// Remove any filter whose name is \a filter_name from \a builder. Returns true-/// if \a filter_name was not found.-bool grpc_channel_stack_builder_remove_filter(-    grpc_channel_stack_builder* builder, const char* filter_name);--/// Terminate iteration and destroy \a iterator-void grpc_channel_stack_builder_iterator_destroy(-    grpc_channel_stack_builder_iterator* iterator);--/// Destroy the builder, return the freshly minted channel stack in \a result.-/// Allocates \a prefix_bytes bytes before the channel stack-/// Returns the base pointer of the allocated block-/// \a initial_refs, \a destroy, \a destroy_arg are as per-/// grpc_channel_stack_init-grpc_error_handle grpc_channel_stack_builder_finish(-    grpc_channel_stack_builder* builder, size_t prefix_bytes, int initial_refs,-    grpc_iomgr_cb_func destroy, void* destroy_arg, void** result);--/// Destroy the builder without creating a channel stack-void grpc_channel_stack_builder_destroy(grpc_channel_stack_builder* builder);+namespace grpc_core {++// Build a channel stack.+// Allows interested parties to add filters to the stack, and to query an+// in-progress build.+// Carries some useful context for the channel stack, such as a target string+// and a transport.+class ChannelStackBuilder {+ public:+  // A function that will be called after the channel stack is successfully+  // built.+  using PostInitFunc = std::function<void(grpc_channel_stack* channel_stack,+                                          grpc_channel_element* elem)>;++  // Initialize with a name.+  explicit ChannelStackBuilder(std::string name) : name_(std::move(name)) {}++  ~ChannelStackBuilder();++  // Set the target string.+  ChannelStackBuilder& SetTarget(const char* target);++  // Query the target.+  absl::string_view target() const { return target_; }++  // Set the transport.+  ChannelStackBuilder& SetTransport(grpc_transport* transport) {+    GPR_ASSERT(transport_ == nullptr);+    transport_ = transport;+    return *this;+  }++  // Query the transport.+  grpc_transport* transport() const { return transport_; }++  // Set channel args (takes a copy of them).+  ChannelStackBuilder& SetChannelArgs(const grpc_channel_args* args);++  // Query the channel args.+  const grpc_channel_args* channel_args() const { return args_; }++  // One filter in the currently building stack.+  struct StackEntry {+    const grpc_channel_filter* filter;+    PostInitFunc post_init;+  };++  // Mutable vector of proposed stack entries.+  std::vector<StackEntry>* mutable_stack() { return &stack_; }++  // Helper to add a filter to the front of the stack.+  void PrependFilter(const grpc_channel_filter* filter, PostInitFunc post_init);++  // Helper to add a filter to the end of the stack.+  void AppendFilter(const grpc_channel_filter* filter, PostInitFunc post_init);++  // Build the channel stack.+  // After success, *result holds the new channel stack,+  // prefix_bytes are allocated before the channel stack,+  // initial_refs, destroy, destroy_arg are as per grpc_channel_stack_init+  // On failure, *result is nullptr.+  grpc_error_handle Build(size_t prefix_bytes, int initial_refs,+                          grpc_iomgr_cb_func destroy, void* destroy_arg,+                          void** result);++ private:+  // The name of the stack+  const std::string name_;+  // The target+  std::string target_;+  // The transport+  grpc_transport* transport_ = nullptr;+  // Channel args+  const grpc_channel_args* args_ = nullptr;+  // The in-progress stack+  std::vector<StackEntry> stack_;","Would it make sense to use `std::list<>` here, to make it more efficient to add entries in the middle?",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28654,790976688,2022-01-24T17:10:30Z,tools/run_tests/xds_k8s_test_driver/bin/cleanup/cleanup.py,"@@ -51,7 +51,7 @@ KubernetesServerRunner = server_app.KubernetesServerRunner utc = pytz.UTC -KEEP_PERIOD = datetime.timedelta(days=85)","At recent rate of failures, we probably want something smaller than 2 weeks. Older resources might not help debug as control plane won't keep logs that long. After 7 days, we won't have fragment content logs; after 14 days, we won't have update logs.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28660,790977075,2022-01-24T17:10:56Z,src/core/lib/surface/channel.cc,"@@ -58,21 +58,20 @@ static void destroy_channel(void* arg, grpc_error_handle error);  grpc_channel* grpc_channel_create_with_builder(-    grpc_channel_stack_builder* builder,+    grpc_core::ChannelStackBuilder* builder,     grpc_channel_stack_type channel_stack_type, grpc_error_handle* error) {-  std::string target = grpc_channel_stack_builder_get_target(builder);-  grpc_channel_args* args = grpc_channel_args_copy(-      grpc_channel_stack_builder_get_channel_arguments(builder));+  std::string target(builder->target());","I think we no longer need to make these copies of the target or channel args.  They were previously necessary because `grpc_channel_stack_builder_finish()` would destroy the builder, but that's no longer the case, so we should be able to continue to access the copies inside of the builder where we need them below.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28660,790980115,2022-01-24T17:14:40Z,src/core/lib/surface/channel_init.h,"@@ -33,15 +33,15 @@ /// It also provides a universal entry path to run those mutators to build /// a channel stack for various subsystems. -typedef struct grpc_channel_stack_builder grpc_channel_stack_builder;- namespace grpc_core { +class ChannelStackBuilder;",Can we include channel_stack_builder.h instead of using a forward declaration here?  Or will that cause a circular dependency?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28667,791144935,2022-01-24T20:57:11Z,doc/environment_variables.md,"@@ -40,6 +40,14 @@ some configuration as environment variables that can be set.     fallback engine when nothing better exists   - legacy - the (deprecated) original polling engine for gRPC +* GRPC_EVENTENGINE_STRATEGY","We should definitely not document this, since this is not something that we plan to keep as part of our API in the long term.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28667,791146198,2022-01-24T20:59:09Z,src/core/lib/event_engine/event_engine_factory.cc,"@@ -36,12 +36,18 @@ void SetDefaultEventEngineFactory(   g_event_engine_factory = factory; } -std::unique_ptr<EventEngine> CreateEventEngine() {+void MaybeSetDefaultEventEngineFactory(","+1.  Is there a reason this is conditional?  Why not just have it always set the factory, even if one already exists?",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28667,791152282,2022-01-24T21:08:48Z,src/core/lib/event_engine/event_engine_factory.cc,"@@ -36,12 +36,18 @@ void SetDefaultEventEngineFactory(   g_event_engine_factory = factory; } -std::unique_ptr<EventEngine> CreateEventEngine() {+void MaybeSetDefaultEventEngineFactory(","I'm not sure what the caller (`grpc_init`) would do with that information. If anything, I believe that failure to initialize the EventEngine factory should abort the process early, otherwise things will go badly in less obvious ways.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28667,791171232,2022-01-24T21:38:21Z,bazel/grpc_build_system.bzl,"@@ -298,39 +310,58 @@ def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data         ""linkstatic"": linkstatic,     }     if uses_polling:-        # the vanilla version of the test should run on platforms that only-        # support a single poller-        native.cc_test(-            name = name,-            testonly = True,-            tags = (tags + [-                ""no_linux"",  # linux supports multiple pollers-            ]),-            **args-        )--        # on linux we run the same test multiple times, once for each poller-        for poller in POLLERS:-            native.sh_test(-                name = name + ""@poller="" + poller,-                data = [name] + data,-                srcs = [-                    ""//test/core/util:run_with_poller_sh"",-                ],-                size = size,-                timeout = timeout,-                args = [-                    poller,-                    ""$(location %s)"" % name,-                ] + args[""args""],-                tags = (tags + [""no_windows"", ""no_mac""]),-                exec_compatible_with = exec_compatible_with,-                exec_properties = exec_properties,-                shard_count = shard_count,-                flaky = flaky,-            )+        # When true, EventEngines will only be exercised against one poller+        engine_only = False+        for engine in EVENT_ENGINES:+            if ""no_windows"" not in engine[""tags""] and ""no_mac"" not in engine[""tags""]:+                # the vanilla version of the test should run on platforms that only","The logic flow here is fairly complex and hard to follow.  Can we simplify this somehow?For example, maybe we can generate the basic test rule as a `cc_library` instead of a `cc_test`, and then generate separate test targets for each platform that depend on that `cc_library`?  I'm thinking of something like this (not tested; there are probably some details that need to be changed):```# cc_library target for test code itselfnative.cc_library(    name = name + ""_lib"",    testonly = True,    **args)# Non-Linux platforms.if ""no_windows"" not in engine[""tags""] and ""no_mac"" not in engine[""tags""]:  # These platforms do not support multiple polling engines,  # so just create a target for each EventEngine.  for engine in EVENT_ENGINE:    native.cc_test(      name = name + ""@engine="" + engine[""name""],      deps = deps + (name + ""_lib""),      tags = tags + engine[""tags""] + [""no_linux""],      env = {""GRPC_EVENTENGINE_STRATEGY"": engine[""name""]},      **args    )# Linux platforms.if ""no_linux"" not in engine[""tags""]:  # Generate one test for each poller, all using the first EventEngine.  for poller in POLLERS:    native.cc_test(      name = name + ""@poller="" + poller + "",engine="" + EVENT_ENGINES[0][""name""],      deps = deps + (name + ""_lib""),      tags = tags + EVENT_ENGINES[0][""tags""] + [""no_windows"", ""no_mac""]),      # ...etc...    )  # Now generate one test for each subsequent EventEngine,  # all using the first poller.  for engine in EVENT_ENGINES[1:]:    native.cc_test(      name = name + ""@poller="" + POLLERS[0] + "",engine="" + engine[""name""],      deps = deps + (name + ""_lib""),      tags = tags + engine[""tags""] + [""no_windows"", ""no_mac""]),      # ...etc...    )```",X
1535036,tamird,https://api.github.com/repos/grpc/grpc/pulls/28667,791172445,2022-01-24T21:40:10Z,bazel/grpc_build_system.bzl,"@@ -35,6 +35,18 @@ load(""@build_bazel_rules_apple//apple:ios.bzl"", ""ios_unit_test"") # The set of pollers to test against if a test exercises polling POLLERS = [""epollex"", ""epoll1"", ""poll""] +# The set of known EventEngines to test+EVENT_ENGINES = [+    {+        ""name"": ""libuv"",+        ""tags"": [],+    },+    {+        ""name"": ""poll"",+        ""tags"": [""no_windows"", ""no_mac""],",I think it would be good to enumerate the costs before prematurely optimizing. In my experience testing on multiple platforms is useful in shaking out bad assumptions.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28667,791176702,2022-01-24T21:46:38Z,src/core/lib/event_engine/event_engine_factory.cc,"@@ -36,12 +36,18 @@ void SetDefaultEventEngineFactory(   g_event_engine_factory = factory; } -std::unique_ptr<EventEngine> CreateEventEngine() {+void MaybeSetDefaultEventEngineFactory(","We have not yet determined what we want our long-term story for registering a global EventEngine impl to be, so I don't think we should try to solve that problem at this point.  Right now, we only need to address first-party EventEngine impls.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28667,791185694,2022-01-24T22:00:41Z,src/core/lib/event_engine/event_engine_factory.cc,"@@ -36,12 +36,18 @@ void SetDefaultEventEngineFactory(   g_event_engine_factory = factory; } -std::unique_ptr<EventEngine> CreateEventEngine() {+void MaybeSetDefaultEventEngineFactory(","> [SetDefaultEventEngineFactory] has no other callersIt is part of the public EventEngine API. `Default` may be a bit misleading, I'm open to naming alternatives, but `default` is in contrast to those EventEngine objects that users can provide manually on channel or server creation. If none are provided, one will be created using the default factory. https://github.com/grpc/grpc/blob/e523825c75aba052a3e36362fc20cdb9abca0b8d/include/grpc/event_engine/event_engine.h#L381-L391. ",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28667,791237164,2022-01-24T23:34:12Z,bazel/grpc_build_system.bzl,"@@ -298,39 +310,58 @@ def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data         ""linkstatic"": linkstatic,     }     if uses_polling:-        # the vanilla version of the test should run on platforms that only-        # support a single poller-        native.cc_test(-            name = name,-            testonly = True,-            tags = (tags + [-                ""no_linux"",  # linux supports multiple pollers-            ]),-            **args-        )--        # on linux we run the same test multiple times, once for each poller-        for poller in POLLERS:-            native.sh_test(-                name = name + ""@poller="" + poller,-                data = [name] + data,-                srcs = [-                    ""//test/core/util:run_with_poller_sh"",-                ],-                size = size,-                timeout = timeout,-                args = [-                    poller,-                    ""$(location %s)"" % name,-                ] + args[""args""],-                tags = (tags + [""no_windows"", ""no_mac""]),-                exec_compatible_with = exec_compatible_with,-                exec_properties = exec_properties,-                shard_count = shard_count,-                flaky = flaky,-            )+        # When true, EventEngines will only be exercised against one poller+        engine_only = False+        for engine in EVENT_ENGINES:+            if ""no_windows"" not in engine[""tags""] and ""no_mac"" not in engine[""tags""]:+                # the vanilla version of the test should run on platforms that only","Sure, I've added back in some code duplication (for now) to make the logic easier to follow. The logic flows much like your pseudocode above, and generates the same results.I'm on the fence about how far to dig in here to refactor the `native.sh_test`s into `native.cc_test`s as you coded above. I'd eventually like to deduplicate this file and `test/cpp/end2end/generate_tests.bzl`, which apparently uses a mostly-copied `native.sh_test` setup from here. But I'd like to handle that in a separate cleanup, if that's alright.",
1926539,tomerv,https://api.github.com/repos/grpc/grpc/pulls/28443,791637085,2022-01-25T11:54:05Z,src/python/grpcio_reflection/grpc_reflection/v1alpha/proto_reflection_descriptor_database.py,"@@ -0,0 +1,140 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Reference implementation for reflection client in gRPC Python.""""""++from google.protobuf.descriptor_database import DescriptorDatabase+from google.protobuf.descriptor_pb2 import FileDescriptorProto+import grpc+from grpc_reflection.v1alpha.reflection_pb2 import (+    ErrorResponse,+    ExtensionNumberResponse,+    ExtensionRequest,+    FileDescriptorResponse,+    ListServiceResponse,+    ServerReflectionRequest,+    ServerReflectionResponse,+    ServiceResponse,+)+from grpc_reflection.v1alpha.reflection_pb2_grpc import ServerReflectionStub+import logging+++class ProtoReflectionDescriptorDatabase(DescriptorDatabase):+    """"""+    ProtoReflectionDescriptorDatabase takes a stub of ServerReflection and provides the methods defined by+    DescriptorDatabase interfaces. It can be used to feed a DescriptorPool instance.++    Python implementation based on C++ version found here:+      https://github.com/grpc/grpc/blob/v1.39.1/test/cpp/util/proto_reflection_descriptor_database.cc+    while implementing the interface given here:+      https://googleapis.dev/python/protobuf/3.17.0/google/protobuf/descriptor_database.html++    """"""++    def __init__(self, channel: grpc.Channel):+        DescriptorDatabase.__init__(self)+        self._logger = logging.getLogger(__name__)+        self._stub = ServerReflectionStub(channel)+        self._known_files: Set[str] = set()+        self._cached_extension_numbers: Dict[str, List[int]] = dict()+        self._cached_extension_files: Dict[str, FileDescriptorProto] = dict()++    def _DoOneRequest(+        self, request: ServerReflectionRequest+    ) -> ServerReflectionResponse:+        response = self._stub.ServerReflectionInfo(iter([request]))+        res = next(response)+        if res.WhichOneof(""message_response"") == ""error_response"":+            raise grpc.RpcError(res.error_response)","I looked further into this, and indeed this flow isn't correct (I assume that's where you were going with your question 😃).Making a call such as `desc_pool.FindFileByName(""foo"")` first calls the C++ layer: [FindFileByName](https://github.com/protocolbuffers/protobuf/blob/63f952b987bd848ac278c714c81c47250214adc7/python/google/protobuf/pyext/descriptor_pool.cc#L263) -> PyDescriptorDatabase::FindFileByName, and in case of error it checks its type. If it's not a KeyError it [prints a stack trace](https://github.com/protocolbuffers/protobuf/blob/63f952b987bd848ac278c714c81c47250214adc7/python/google/protobuf/pyext/descriptor_database.cc#L64). Then it always triggers a KeyError with the message [""Couldn't find file ...""](https://github.com/protocolbuffers/protobuf/blob/63f952b987bd848ac278c714c81c47250214adc7/python/google/protobuf/pyext/descriptor_pool.cc#L235) - which IMO is not the correct message, since usually the value of a KeyError is the key that was searched, but that's out of scope.So the conclusion is that here in Python code we should verify that the error is a NOT_FOUND error. If it is, then raise a KeyError (which sadly will be transformed to another KeyError by the C++ layer). Otherwise, it's just an AssertionError, since we shouldn't get any other error here anyway (and again, sadly this AssertionError will again be converted to a KeyError, but in addition will get logging of the original error).I'll also add this to the unit tests.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28668,791953450,2022-01-25T17:24:19Z,src/core/ext/transport/chttp2/transport/chttp2_transport.cc,"@@ -1092,21 +1127,18 @@ void grpc_chttp2_add_incoming_goaway(grpc_chttp2_transport* t,   }   // lie: use transient failure from the transport to indicate goaway has been   // received.-  connectivity_state_set(t, GRPC_CHANNEL_TRANSIENT_FAILURE, status,-                         ""got_goaway"");+  if (!grpc_core::test_only_disable_transient_failure_state_notification) {+    connectivity_state_set(t, GRPC_CHANNEL_TRANSIENT_FAILURE, status,+                           ""got_goaway"");+  } }  static void maybe_start_some_streams(grpc_chttp2_transport* t) {   grpc_chttp2_stream* s;-  // cancel out streams that haven't yet started if we have received a GOAWAY+  // maybe cancel out streams that haven't yet started if we have received a+  // GOAWAY   if (t->goaway_error != GRPC_ERROR_NONE) {-    while (grpc_chttp2_list_pop_waiting_for_concurrency(t, &s)) {-      grpc_chttp2_cancel_stream(-          t, s,-          grpc_error_set_int(-              GRPC_ERROR_CREATE_FROM_STATIC_STRING(""GOAWAY received""),-              GRPC_ERROR_INT_GRPC_STATUS, GRPC_STATUS_UNAVAILABLE));","It looks like we're no longer setting the status to UNAVAILABLE here, but I see that the tests are still checking for this, and they are presumably passing, so it must be happening somewhere else.  Where is this happening now?",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/28668,792008738,2022-01-25T18:31:27Z,src/core/ext/transport/chttp2/transport/chttp2_transport.cc,"@@ -1092,21 +1127,18 @@ void grpc_chttp2_add_incoming_goaway(grpc_chttp2_transport* t,   }   // lie: use transient failure from the transport to indicate goaway has been   // received.-  connectivity_state_set(t, GRPC_CHANNEL_TRANSIENT_FAILURE, status,-                         ""got_goaway"");+  if (!grpc_core::test_only_disable_transient_failure_state_notification) {+    connectivity_state_set(t, GRPC_CHANNEL_TRANSIENT_FAILURE, status,+                           ""got_goaway"");+  } }  static void maybe_start_some_streams(grpc_chttp2_transport* t) {   grpc_chttp2_stream* s;-  // cancel out streams that haven't yet started if we have received a GOAWAY+  // maybe cancel out streams that haven't yet started if we have received a+  // GOAWAY   if (t->goaway_error != GRPC_ERROR_NONE) {-    while (grpc_chttp2_list_pop_waiting_for_concurrency(t, &s)) {-      grpc_chttp2_cancel_stream(-          t, s,-          grpc_error_set_int(-              GRPC_ERROR_CREATE_FROM_STATIC_STRING(""GOAWAY received""),-              GRPC_ERROR_INT_GRPC_STATUS, GRPC_STATUS_UNAVAILABLE));",https://github.com/grpc/grpc/blob/cf81e41162603d229d0c8ef05f137a0afbe1f3c6/src/core/ext/transport/chttp2/transport/chttp2_transport.cc#L1054goaway_error sets the status already,X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/28694,792225993,2022-01-26T00:16:01Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/k8s.py,"@@ -329,18 +335,32 @@ def port_forward_pod(                         raise PortForwardingError(                             'Error forwarding port, kubectl return '                             f'code {return_code}, output {errors}')-                elif output != expected:-                    raise PortForwardingError(-                        f'Error forwarding port, unexpected output {output}')+                    # If there is no output, and the subprocess is not exiting,+                    # continue waiting for the log line.+                    continue++                # Validate output log+                if local_port:+                    if output != local_port_expected:+                        raise PortForwardingError(+                            f'Error forwarding port, unexpected output {output}'+                        )                 else:-                    logger.info(output)-                    break+                    groups = local_port_re.search(output)+                    if groups is None:+                        raise PortForwardingError(+                            f'Error forwarding port, unexpected output {output}'+                        )+                    local_port = int(groups[1])++                logger.info(output)+                break         except Exception:             self.port_forward_stop(pf)             raise          # TODO(sergiitk): return new PortForwarder object-        return pf+        return local_port, pf",Nit: maybe we could create a `PortForwarder` class that will wrap `subprocess.Popen` and the local/remote port pair?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28694,792241502,2022-01-26T00:59:33Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/k8s.py,"@@ -329,18 +335,32 @@ def port_forward_pod(                         raise PortForwardingError(                             'Error forwarding port, kubectl return '                             f'code {return_code}, output {errors}')-                elif output != expected:-                    raise PortForwardingError(-                        f'Error forwarding port, unexpected output {output}')+                    # If there is no output, and the subprocess is not exiting,+                    # continue waiting for the log line.+                    continue++                # Validate output log+                if local_port:+                    if output != local_port_expected:+                        raise PortForwardingError(+                            f'Error forwarding port, unexpected output {output}'+                        )                 else:-                    logger.info(output)-                    break+                    groups = local_port_re.search(output)+                    if groups is None:+                        raise PortForwardingError(+                            f'Error forwarding port, unexpected output {output}'+                        )+                    local_port = int(groups[1])++                logger.info(output)+                break         except Exception:             self.port_forward_stop(pf)             raise          # TODO(sergiitk): return new PortForwarder object-        return pf+        return local_port, pf",Done. Created a PortForwarder class that includes both and controls its lifecycle.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28694,792241750,2022-01-26T01:00:07Z,tools/run_tests/xds_k8s_test_driver/framework/test_app/client_app.py,"@@ -344,18 +345,20 @@ def run(self,         pod = self.k8s_namespace.list_deployment_pods(self.deployment)[0]         self._wait_pod_started(pod.metadata.name)         pod_ip = pod.status.pod_ip+        rpc_port = self.stats_port         rpc_host = None          # Experimental, for local debugging.         if self.debug_use_port_forwarding:             logger.info('LOCAL DEV MODE: Enabling port forwarding to %s:%s',                         pod_ip, self.stats_port)-            self.port_forwarder = self.k8s_namespace.port_forward_pod(+            self.local_forwarding_port, self.port_forwarder = self.k8s_namespace.port_forward_pod(","`local_forwarding_port` removed, for the `PortForwarder` class.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28694,792241784,2022-01-26T01:00:14Z,tools/run_tests/xds_k8s_test_driver/framework/test_app/server_app.py,"@@ -325,25 +323,22 @@ def run(self,              pod_ip = pod.status.pod_ip             rpc_host = None+            rpc_port = test_port             # Experimental, for local debugging.             local_port = maintenance_port             if self.debug_use_port_forwarding:-                with KubernetesServerRunner._lock:-                    local_port = maintenance_port + KubernetesServerRunner._server_port_forwarding_offset-                    KubernetesServerRunner._server_port_forwarding_offset += 1-                logger.info(-                    'LOCAL DEV MODE: Enabling port forwarding to %s:%s using local port %s',-                    pod_ip, maintenance_port, local_port)-                self.port_forwarders.append(-                    self.k8s_namespace.port_forward_pod(-                        pod,-                        remote_port=maintenance_port,-                        local_port=local_port))+                logger.info('LOCAL DEV MODE: Enabling port forwarding to %s:%s',+                            pod_ip, maintenance_port)+                local_port, port_forwarder = self.k8s_namespace.port_forward_pod(+                    pod, remote_port=maintenance_port)+                self.port_forwarders.append(port_forwarder)+                self.local_forwarding_ports.append(local_port)","`local_forwarding_ports` removed, for the `PortForwarder` class.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/28694,792243882,2022-01-26T01:06:13Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/k8s.py,"@@ -78,9 +79,92 @@ class PortForwardingError(Exception):     """"""Error forwarding port""""""  +class PortForwarder:+    PORT_FORWARD_LOCAL_ADDRESS: str = '127.0.0.1'++    def __init__(self,+                 context: str,+                 namespace: str,+                 destination: str,+                 remote_port: int,+                 local_port: Optional[int] = None,+                 local_address: Optional[str] = None):+        self.context = context+        self.namespace = namespace+        self.destination = destination+        self.remote_port = remote_port+        self.local_address = local_address or self.PORT_FORWARD_LOCAL_ADDRESS+        self.local_port, self.subprocess = self._establish_port_forwarding(","Normally I'd not be doing any heavy work in the constructor, and have separate `connect` (so class can be instantiated for unit tests and such), but it doesn't matter as much in this case. ",
1926539,tomerv,https://api.github.com/repos/grpc/grpc/pulls/28443,792390728,2022-01-26T08:11:35Z,src/python/grpcio_reflection/grpc_reflection/v1alpha/proto_reflection_descriptor_database.py,"@@ -0,0 +1,140 @@+# Copyright 2021 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Reference implementation for reflection client in gRPC Python.""""""++from google.protobuf.descriptor_database import DescriptorDatabase+from google.protobuf.descriptor_pb2 import FileDescriptorProto+import grpc+from grpc_reflection.v1alpha.reflection_pb2 import (+    ErrorResponse,+    ExtensionNumberResponse,+    ExtensionRequest,+    FileDescriptorResponse,+    ListServiceResponse,+    ServerReflectionRequest,+    ServerReflectionResponse,+    ServiceResponse,+)+from grpc_reflection.v1alpha.reflection_pb2_grpc import ServerReflectionStub+import logging+++class ProtoReflectionDescriptorDatabase(DescriptorDatabase):+    """"""+    ProtoReflectionDescriptorDatabase takes a stub of ServerReflection and provides the methods defined by+    DescriptorDatabase interfaces. It can be used to feed a DescriptorPool instance.++    Python implementation based on C++ version found here:+      https://github.com/grpc/grpc/blob/v1.39.1/test/cpp/util/proto_reflection_descriptor_database.cc+    while implementing the interface given here:+      https://googleapis.dev/python/protobuf/3.17.0/google/protobuf/descriptor_database.html++    """"""++    def __init__(self, channel: grpc.Channel):+        DescriptorDatabase.__init__(self)+        self._logger = logging.getLogger(__name__)+        self._stub = ServerReflectionStub(channel)+        self._known_files: Set[str] = set()+        self._cached_extension_numbers: Dict[str, List[int]] = dict()+        self._cached_extension_files: Dict[str, FileDescriptorProto] = dict()++    def _DoOneRequest(+        self, request: ServerReflectionRequest+    ) -> ServerReflectionResponse:","Some methods must be CamelCase since they override the [parent class](https://googleapis.dev/python/protobuf/latest/google/protobuf/descriptor_database.html), and then there's always the tension between being consistent with framework A (Python, snake_case) and framework B (Protobuf, CamelCase) when they meet in a single place.I'll change all the methods that don't override.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/28720,793138761,2022-01-26T23:35:47Z,tools/run_tests/xds_k8s_test_driver/bin/cleanup/cleanup.py,"@@ -334,8 +334,12 @@ def main(argv):     # target proxy cannot deleted unless the forwarding rule is deleted). The     # leaked target-proxy is guaranteed to be a super set of leaked     # forwarding-rule.-    leakedHealthChecks = exec_gcloud(project, 'compute', 'health-checks',-                                     'list')+    compute = gcp.compute.ComputeV1(gcp.api.GcpApiManager(), project)+    r = compute.list_health_check()+    leakedHealthChecks = [+        ii for ii in r['items'] if datetime.datetime.fromisoformat(+            ii['creationTimestamp']) <= get_expire_timestamp()+    ]","nit: consider rewriting using regular `for` loop instead of list comprehension to avoid non-descriptive variable names: `r`, `ii`.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28667,793948255,2022-01-27T19:51:02Z,bazel/grpc_build_system.bzl,"@@ -305,46 +317,69 @@ def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data         ""flaky"": flaky,         ""linkstatic"": linkstatic,     }-    if uses_polling:-        # the vanilla version of the test should run on platforms that only-        # support a single poller-        native.cc_test(-            name = name,-            testonly = True,-            tags = (tags + [-                ""no_linux"",  # linux supports multiple pollers-            ]),-            **args-        )--        # on linux we run the same test multiple times, once for each poller-        for poller in POLLERS:-            native.sh_test(-                name = name + ""@poller="" + poller,-                data = [name] + data,-                srcs = [-                    ""//test/core/util:run_with_poller_sh"",-                ],-                size = size,-                timeout = timeout,-                args = [-                    poller,-                    ""$(location %s)"" % name,-                ] + args[""args""],-                tags = (tags + [""no_windows"", ""no_mac""]),-                exec_compatible_with = exec_compatible_with,-                exec_properties = exec_properties,-                shard_count = shard_count,-                flaky = flaky,-            )-    else:-        # the test behavior doesn't depend on polling, just generate the test-        native.cc_test(name = name, tags = tags + [""no_uses_polling""], **args)     ios_cc_test(         name = name,         tags = tags,         **args     )+    if not uses_polling:+        # the test behavior doesn't depend on polling, just generate the test+        # TODO(hork): identify if any non-polling tests should be exercised by+        # all known EventEngines. It is assumed that non-polling tests are+        # engine-agnostic.+        native.cc_test(name = name, tags = tags + [""no_uses_polling""], **args)+        return++    # -- Tests that exercise the polling system --++    # Non-Linux platforms+    for engine in EVENT_ENGINES:+        if ""no_windows"" not in engine[""tags""] and ""no_mac"" not in engine[""tags""]:+            # These platforms do not support multiple polling engines,+            # so just create a target for each EventEngine.+            native.cc_test(+                # TODO(hork): these names are depended upon in cpp end2end","I don't understand what dependency you're referring to here.  Where exactly does this dependency come from?  Can you point me to a specific example?Whatever the source of this problem, I think we need to address it.  Otherwise, if there are multiple EE impls that work on Windows or MacOS, we will generate multiple targets with the same name, which will yield a broken build configuration.(I realize that we have only one EE impl now, but I think the purpose of this PR is to put in place the machinery to support that.)",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28667,793957361,2022-01-27T20:04:06Z,bazel/grpc_build_system.bzl,"@@ -305,46 +317,69 @@ def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data         ""flaky"": flaky,         ""linkstatic"": linkstatic,     }-    if uses_polling:-        # the vanilla version of the test should run on platforms that only-        # support a single poller-        native.cc_test(-            name = name,-            testonly = True,-            tags = (tags + [-                ""no_linux"",  # linux supports multiple pollers-            ]),-            **args-        )--        # on linux we run the same test multiple times, once for each poller-        for poller in POLLERS:-            native.sh_test(-                name = name + ""@poller="" + poller,-                data = [name] + data,-                srcs = [-                    ""//test/core/util:run_with_poller_sh"",-                ],-                size = size,-                timeout = timeout,-                args = [-                    poller,-                    ""$(location %s)"" % name,-                ] + args[""args""],-                tags = (tags + [""no_windows"", ""no_mac""]),-                exec_compatible_with = exec_compatible_with,-                exec_properties = exec_properties,-                shard_count = shard_count,-                flaky = flaky,-            )-    else:-        # the test behavior doesn't depend on polling, just generate the test-        native.cc_test(name = name, tags = tags + [""no_uses_polling""], **args)     ios_cc_test(         name = name,         tags = tags,         **args     )+    if not uses_polling:+        # the test behavior doesn't depend on polling, just generate the test+        # TODO(hork): identify if any non-polling tests should be exercised by+        # all known EventEngines. It is assumed that non-polling tests are+        # engine-agnostic.+        native.cc_test(name = name, tags = tags + [""no_uses_polling""], **args)+        return++    # -- Tests that exercise the polling system --++    # Non-Linux platforms+    for engine in EVENT_ENGINES:+        if ""no_windows"" not in engine[""tags""] and ""no_mac"" not in engine[""tags""]:+            # These platforms do not support multiple polling engines,+            # so just create a target for each EventEngine.+            native.cc_test(+                # TODO(hork): these names are depended upon in cpp end2end+                # tests data arguments. Those tests need to be updated to pull+                # the right dependencies.+                # name = name + ""_engine_"" + engine[""name""],+                name = name,+                tags = (tags + engine[""tags""] + [+                    ""no_linux"",  # linux supports multiple pollers+                ]),+                env = {""GRPC_EVENTENGINE_STRATEGY"": engine[""name""]},+                **args+            )++    # Linux platforms+    if ""no_linux"" in EVENT_ENGINES[0][""tags""]:+        fail(""EVENT_ENGINES[0] should be the default engine, and must support linux."")++    # On linux we run the same test multiple times, once for each poller, with the default EventEngine.+    for poller in POLLERS:+        native.cc_test(","Is bazel smart enough to realize that all of these targets are the same binary and not try to compile them multiple times?If not, I think you could avoid that duplication by overriding `srcs` to be empty and then adding the Windows/MacOS target as a dependency here.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28667,793966132,2022-01-27T20:17:19Z,test/core/end2end/generate_tests.bzl,"@@ -468,33 +480,68 @@ def grpc_end2end_tests():             #print(_compatible(fopt, topt), f, t, fopt, topt)             if not _compatible(fopt, topt):                 continue-             test_short_name = str(t) if not topt.short_name else topt.short_name-            native.sh_test(-                name = ""%s_test@%s"" % (f, test_short_name),-                data = ["":%s_test"" % f],-                srcs = [""end2end_test.sh""],-                args = [-                    ""$(location %s_test)"" % f,-                    t,-                ],-                tags = [""no_linux""] + _platform_support_tags(fopt),-                flaky = t in fopt.flaky_tests,-            ) +            # Non-Linux platforms","Can't we just replace all of this with the following:```grpc_cc_test(    name = ""%s_test@%s"" % (f, test_short_name),    data = ["":%s_test"" % f],    args = [t],    tags = _platform_support_tags(fopt),    flaky = t in fopt.flaky_tests,    uses_polling = True,)```That way, the `grpc_cc_test()` macro will automatically expand the EE and polling engine parameters for us, and we don't have to duplicate the code here.Similar thing for the nosec tests below.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28667,793969055,2022-01-27T20:21:33Z,bazel/grpc_build_system.bzl,"@@ -305,46 +317,69 @@ def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data         ""flaky"": flaky,         ""linkstatic"": linkstatic,     }-    if uses_polling:-        # the vanilla version of the test should run on platforms that only-        # support a single poller-        native.cc_test(-            name = name,-            testonly = True,-            tags = (tags + [-                ""no_linux"",  # linux supports multiple pollers-            ]),-            **args-        )--        # on linux we run the same test multiple times, once for each poller-        for poller in POLLERS:-            native.sh_test(-                name = name + ""@poller="" + poller,-                data = [name] + data,-                srcs = [-                    ""//test/core/util:run_with_poller_sh"",-                ],-                size = size,-                timeout = timeout,-                args = [-                    poller,-                    ""$(location %s)"" % name,-                ] + args[""args""],-                tags = (tags + [""no_windows"", ""no_mac""]),-                exec_compatible_with = exec_compatible_with,-                exec_properties = exec_properties,-                shard_count = shard_count,-                flaky = flaky,-            )-    else:-        # the test behavior doesn't depend on polling, just generate the test-        native.cc_test(name = name, tags = tags + [""no_uses_polling""], **args)     ios_cc_test(         name = name,         tags = tags,         **args     )+    if not uses_polling:+        # the test behavior doesn't depend on polling, just generate the test+        # TODO(hork): identify if any non-polling tests should be exercised by+        # all known EventEngines. It is assumed that non-polling tests are+        # engine-agnostic.+        native.cc_test(name = name, tags = tags + [""no_uses_polling""], **args)+        return++    # -- Tests that exercise the polling system --++    # Non-Linux platforms+    for engine in EVENT_ENGINES:+        if ""no_windows"" not in engine[""tags""] and ""no_mac"" not in engine[""tags""]:+            # These platforms do not support multiple polling engines,+            # so just create a target for each EventEngine.+            native.cc_test(+                # TODO(hork): these names are depended upon in cpp end2end+                # tests data arguments. Those tests need to be updated to pull+                # the right dependencies.+                # name = name + ""_engine_"" + engine[""name""],+                name = name,+                tags = (tags + engine[""tags""] + [+                    ""no_linux"",  # linux supports multiple pollers+                ]),+                env = {""GRPC_EVENTENGINE_STRATEGY"": engine[""name""]},","I did some experimentation around that (see commit history), but it would require replacing all sh_tests with cc_tests, and I decided that it may be best to consider the repercussions of that in separate work. This code can be considered temporary, but it's not terribly harmful even if it isn't.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28667,793979583,2022-01-27T20:37:11Z,bazel/grpc_build_system.bzl,"@@ -305,46 +317,69 @@ def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data         ""flaky"": flaky,         ""linkstatic"": linkstatic,     }-    if uses_polling:-        # the vanilla version of the test should run on platforms that only-        # support a single poller-        native.cc_test(-            name = name,-            testonly = True,-            tags = (tags + [-                ""no_linux"",  # linux supports multiple pollers-            ]),-            **args-        )--        # on linux we run the same test multiple times, once for each poller-        for poller in POLLERS:-            native.sh_test(-                name = name + ""@poller="" + poller,-                data = [name] + data,-                srcs = [-                    ""//test/core/util:run_with_poller_sh"",-                ],-                size = size,-                timeout = timeout,-                args = [-                    poller,-                    ""$(location %s)"" % name,-                ] + args[""args""],-                tags = (tags + [""no_windows"", ""no_mac""]),-                exec_compatible_with = exec_compatible_with,-                exec_properties = exec_properties,-                shard_count = shard_count,-                flaky = flaky,-            )-    else:-        # the test behavior doesn't depend on polling, just generate the test-        native.cc_test(name = name, tags = tags + [""no_uses_polling""], **args)     ios_cc_test(         name = name,         tags = tags,         **args     )+    if not uses_polling:+        # the test behavior doesn't depend on polling, just generate the test+        # TODO(hork): identify if any non-polling tests should be exercised by+        # all known EventEngines. It is assumed that non-polling tests are+        # engine-agnostic.+        native.cc_test(name = name, tags = tags + [""no_uses_polling""], **args)+        return++    # -- Tests that exercise the polling system --++    # Non-Linux platforms+    for engine in EVENT_ENGINES:+        if ""no_windows"" not in engine[""tags""] and ""no_mac"" not in engine[""tags""]:+            # These platforms do not support multiple polling engines,+            # so just create a target for each EventEngine.+            native.cc_test(+                # TODO(hork): these names are depended upon in cpp end2end","You're correct. I've removed the comment and updated the names.This is a stale comment now that we're using cc_tests below, and now that we're eliminating poller-specific tests from CMake (pending your response on the extract_metadata changes). Previously, all of the following tests were `native.sh_test`s that called these same binaries with different arguments. Also, Cmake did not appreciate the `@` symbol in the name, so there were two problems.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28667,794052790,2022-01-27T22:25:31Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -104,6 +104,8 @@ def _extract_rules_from_bazel_xml(xml_tree):             ]:                 if rule_name in result:                     raise Exception('Rule %s already present' % rule_name)+                if ""@poller="" in rule_name:","These tests used to be skipped because extract_metadata_from_bazel_xml.py didn't parse sh_tests. Now that I've replaced some sh_tests with cc_tests, we have to exclude them manually. Otherwise, we'd be adding 5x tests the CMakeLists.txt.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28667,794064951,2022-01-27T22:47:36Z,test/core/end2end/generate_tests.bzl,"@@ -468,33 +480,68 @@ def grpc_end2end_tests():             #print(_compatible(fopt, topt), f, t, fopt, topt)             if not _compatible(fopt, topt):                 continue-             test_short_name = str(t) if not topt.short_name else topt.short_name-            native.sh_test(-                name = ""%s_test@%s"" % (f, test_short_name),-                data = ["":%s_test"" % f],-                srcs = [""end2end_test.sh""],-                args = [-                    ""$(location %s_test)"" % f,-                    t,-                ],-                tags = [""no_linux""] + _platform_support_tags(fopt),-                flaky = t in fopt.flaky_tests,-            ) +            # Non-Linux platforms+            for engine in EVENT_ENGINES:+                if ""no_windows"" not in engine[""tags""] and ""no_mac"" not in engine[""tags""]:+                    # These platforms do not support multiple polling engines,+                    # so just create a target for each EventEngine.+                    native.sh_test(+                        name = ""%s_test@%s@engine=%s"" % (f, test_short_name, engine[""name""]),+                        data = ["":%s_test"" % f],+                        srcs = [""end2end_test.sh""],","replacing `run_with_poller.sh` with cc_tests added roughly 4x the number of cc_tests (and removed that many sh_tests). Doing it here would be around 100x. In the long run I hope to replace sh_test with cc_test, which would allow us to get rid of the intermediate script. That's mostly orthogonal to this work, so I decided to wait on that in case it unearths problems that would block us landing this feature.While we still have sh_tests, we could replace this script with a simple genrule in the short-term, but it seems kindof trivial.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28704,794390079,2022-01-28T10:41:10Z,tools/run_tests/run_tests.py,"@@ -1583,14 +1583,15 @@ def git(cmd, cwd=cwd):         child_argv[1:])      env = os.environ.copy()-    env['RUN_TESTS_COMMAND'] = run_tests_cmd     env['DOCKERFILE_DIR'] = dockerfile_dir-    env['DOCKER_RUN_SCRIPT'] = 'tools/run_tests/dockerize/docker_run_tests.sh'+    env['DOCKER_RUN_SCRIPT'] = 'tools/run_tests/dockerize/docker_run.sh'+    env['DOCKER_RUN_SCRIPT_COMMAND'] = run_tests_cmd+    # TODO(jtattermusch): is the XML_REPORT env variable any useful?     if args.xml_report:         env['XML_REPORT'] = args.xml_report","AFAICT this is unnecessary since the `run_tests_cmd` already contains all the arguments required by the child running inside docker container (including the `-x` argument used for reports). Based on my knowledge, the XML_REPORT variable is unused.I will will remove in a followup PR to not risk breaking something.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28704,794404378,2022-01-28T11:01:14Z,tools/run_tests/dockerize/docker_run.sh,"@@ -32,13 +35,39 @@ else   cp -r ""$EXTERNAL_GIT_ROOT/$RELATIVE_COPY_PATH""/* ""/var/local/git/grpc/$RELATIVE_COPY_PATH"" fi +# ensure the ""reports"" directory exists+mkdir -p reports++# TODO(jtattermusch): this is garbage, remove it. $POST_GIT_STEP","Actually, It was used for gcc4.4 compatibility test (and that test is long gone).https://github.com/grpc/grpc/pull/5384",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28727,794691973,2022-01-28T17:03:58Z,examples/cpp/helloworld/greeter_client_ssl.cc,"@@ -0,0 +1,123 @@+/**+ * Copyright 2022 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ **/++#include <memory>+#include <string>+#include <sstream>+#include <fstream>+#include <iostream>++#include <grpcpp/grpcpp.h>++#ifdef BAZEL_BUILD+#include ""examples/protos/helloworld.grpc.pb.h""+#else+#include ""helloworld.grpc.pb.h""+#endif++using grpc::Channel;+using grpc::ClientContext;+using grpc::Status;++using helloworld::HelloRequest;+using helloworld::HelloReply;+using helloworld::Greeter;++class GreeterClient+{","The open curly brace is always on the last line of the class name declaration, never on a line by itself",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28456,794734074,2022-01-28T17:55:10Z,test/cpp/end2end/mtls_end2end_test.cc,"@@ -0,0 +1,409 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//    http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <memory>+#include <string>+#include <thread>  // NOLINT+#include <vector>++#include ""absl/strings/str_cat.h""+#include ""absl/strings/string_view.h""+#include ""gmock/gmock.h""+#include ""gtest/gtest.h""++#include <grpc++/grpc++.h>+#include <grpc/grpc.h>+#include <grpc/grpc_security.h>+#include <grpcpp/security/server_credentials.h>+#include <grpcpp/security/tls_credentials_options.h>+#include <grpcpp/support/channel_arguments.h>++#include ""src/core/lib/gpr/env.h""+#include ""src/core/lib/gpr/tmpfile.h""+#include ""src/core/lib/iomgr/load_file.h""+#include ""src/cpp/client/secure_credentials.h""+#include ""src/proto/grpc/testing/echo.grpc.pb.h""+#include ""test/core/util/test_config.h""+#include ""test/core/util/tls_utils.h""+#include ""test/cpp/util/tls_test_utils.h""++extern ""C"" {+#include <openssl/ssl.h>+}++#define CA_CERT_PATH ""src/core/tsi/test_creds/ca.pem""+#define SERVER_KEY_PATH ""src/core/tsi/test_creds/server0.key""+#define SERVER_CERT_PATH ""src/core/tsi/test_creds/server0.pem""+#define CLIENT_KEY_PATH ""src/core/tsi/test_creds/client.key""+#define CLIENT_CERT_PATH ""src/core/tsi/test_creds/client.pem""++#define NUM_REQUESTS_PER_CHANNEL 5++using ::grpc::experimental::FileWatcherCertificateProvider;+using ::grpc::experimental::TlsChannelCredentialsOptions;+using ::grpc::experimental::TlsServerCredentialsOptions;+using ::grpc_core::testing::SecurityPrimitives;++namespace grpc {+namespace testing {+namespace {++class EchoServer final : public EchoTestService::Service {+  ::grpc::Status Echo(::grpc::ServerContext* /*context*/,+                      const EchoRequest* request,+                      EchoResponse* response) override {+    if (request->param().expected_error().code() == 0) {+      response->set_message(request->message());+      return ::grpc::Status::OK;+    } else {+      return ::grpc::Status(static_cast<::grpc::StatusCode>(+                                request->param().expected_error().code()),+                            """");+    }+  }+};++class TestScenario {+ public:+  TestScenario(int num_listening_ports,+               SecurityPrimitives::ProviderType client_provider_type,+               SecurityPrimitives::ProviderType server_provider_type,+               SecurityPrimitives::VerifierType client_verifier_type,+               SecurityPrimitives::VerifierType server_verifier_type)+      : num_listening_ports_(num_listening_ports),+        client_provider_type_(client_provider_type),+        server_provider_type_(server_provider_type),+        client_verifier_type_(client_verifier_type),+        server_verifier_type_(server_verifier_type) {}+  std::string AsString() const {+    return absl::StrCat(""TestScenario__num_listening_ports_"",+                        num_listening_ports_, ""__client_provider_type_"",+                        client_provider_type_, ""__server_provider_type_"",+                        server_provider_type_, ""__client_verifier_type_"",+                        client_verifier_type_, ""__server_verifier_type_"",+                        server_verifier_type_);+  }++  int num_listening_ports() const { return num_listening_ports_; }++  SecurityPrimitives::ProviderType client_provider_type() const {+    return client_provider_type_;+  }++  SecurityPrimitives::ProviderType server_provider_type() const {+    return server_provider_type_;+  }++  SecurityPrimitives::VerifierType client_verifier_type() const {+    return client_verifier_type_;+  }++  SecurityPrimitives::VerifierType server_verifier_type() const {+    return server_verifier_type_;+  }++ private:+  int num_listening_ports_;+  SecurityPrimitives::ProviderType client_provider_type_;+  SecurityPrimitives::ProviderType server_provider_type_;+  SecurityPrimitives::VerifierType client_verifier_type_;+  SecurityPrimitives::VerifierType server_verifier_type_;+};++std::string TestScenarioName(+    const ::testing::TestParamInfo<TestScenario>& info) {+  return info.param.AsString();+}++namespace {++// This helper function reads the contents specified by the filename as+// std::string.+std::string GetContentsFromFilePath(const char* filename) {+  grpc_slice slice;+  GPR_ASSERT(+      GRPC_LOG_IF_ERROR(""load_file"", grpc_load_file(filename, 1, &slice)));+  std::string contents = std::string(grpc_core::StringViewFromSlice(slice));+  grpc_slice_unref(slice);+  return contents;+}++}  // namespace++class AdvancedTlsEnd2EndTest : public ::testing::TestWithParam<TestScenario> {+ protected:+  AdvancedTlsEnd2EndTest() = default;++  void SetUp() override {+    ::grpc::ServerBuilder builder;+    ::grpc::ChannelArguments args;+    // We will need to override the peer name on the certificate if using+    // hostname verification, as we can't connect to that name in a test+    // environment.+    if (GetParam().client_verifier_type() ==+            SecurityPrimitives::HOSTNAME_VERIFIER ||+        GetParam().client_verifier_type() ==+            SecurityPrimitives::DEFAULT_VERIFIER) {+      args.SetSslTargetNameOverride(""foo.test.google.com.au"");+    }+    // Set up server certificate provider.+    // Hostname verifier on the server side is not applicable.+    GPR_ASSERT(GetParam().server_verifier_type() !=+               SecurityPrimitives::HOSTNAME_VERIFIER);+    std::shared_ptr<experimental::CertificateProviderInterface>+        server_certificate_provider;+    switch (GetParam().server_provider_type()) {+      case SecurityPrimitives::STATIC_PROVIDER: {+        std::string root_certs = GetContentsFromFilePath(CA_CERT_PATH);+        std::string server_key = GetContentsFromFilePath(SERVER_KEY_PATH);+        std::string server_certs = GetContentsFromFilePath(SERVER_CERT_PATH);+        experimental::IdentityKeyCertPair server_pair;+        server_pair.private_key = server_key;+        server_pair.certificate_chain = server_certs;+        std::vector<experimental::IdentityKeyCertPair> server_pair_list;+        server_pair_list.emplace_back(server_pair);+        server_certificate_provider =+            std::make_shared<experimental::StaticDataCertificateProvider>(+                root_certs, server_pair_list);+        break;+      }+      case SecurityPrimitives::FILE_PROVIDER: {+        server_certificate_provider =+            std::make_shared<FileWatcherCertificateProvider>(+                SERVER_KEY_PATH, SERVER_CERT_PATH, CA_CERT_PATH, 1);+        break;+      }+    }+    // Set up server certificate verifier.+    std::shared_ptr<experimental::CertificateVerifier>+        server_certificate_verifier;+    switch (GetParam().server_verifier_type()) {+      case SecurityPrimitives::EXTERNAL_SYNC_VERIFIER: {+        server_certificate_verifier =+            experimental::ExternalCertificateVerifier::Create<+                SyncCertificateVerifier>(true);+        break;+      }+      case SecurityPrimitives::EXTERNAL_ASYNC_VERIFIER: {+        server_certificate_verifier =+            experimental::ExternalCertificateVerifier::Create<+                AsyncCertificateVerifier>(true);+        break;+      }+      case SecurityPrimitives::HOSTNAME_VERIFIER: {+        server_certificate_verifier =+            std::make_shared<experimental::HostNameCertificateVerifier>();+        break;+      }+      default: {+        break;+      }+    }+    // Build the server and add listening ports.+    if (GetParam().num_listening_ports() > 0) {+      ports_.resize(GetParam().num_listening_ports(), 0);+    }+    for (int i = 0; i < GetParam().num_listening_ports(); i++) {+      // Configure tls credential options for each port+      TlsServerCredentialsOptions server_creds_options(+          server_certificate_provider);+      server_creds_options.set_cert_request_type(+          GRPC_SSL_REQUEST_AND_REQUIRE_CLIENT_CERTIFICATE_AND_VERIFY);+      server_creds_options.watch_identity_key_cert_pairs();+      server_creds_options.watch_root_certs();+      if (GetParam().server_verifier_type() !=+          SecurityPrimitives::DEFAULT_VERIFIER) {+        server_creds_options.set_certificate_verifier(+            server_certificate_verifier);+      }+      builder.AddListeningPort(+          ""0.0.0.0:0"",+          ::grpc::experimental::TlsServerCredentials(server_creds_options),+          &ports_[i]);+    }+    builder.RegisterService(&service_);+    server_ = builder.BuildAndStart();+    ASSERT_NE(nullptr, server_);+    server_thread_ = std::thread(&AdvancedTlsEnd2EndTest::RunServerLoop, this);+    for (int i = 0; i < GetParam().num_listening_ports(); i++) {+      ASSERT_NE(0, ports_[i]);+      server_addresses_.push_back(absl::StrCat(""localhost:"", ports_[i]));+      // Set up client certificate provider.+      std::shared_ptr<experimental::CertificateProviderInterface>+          channel_certificate_provider;+      switch (GetParam().server_provider_type()) {+        case SecurityPrimitives::STATIC_PROVIDER: {+          std::string root_certs = GetContentsFromFilePath(CA_CERT_PATH);+          std::string client_key = GetContentsFromFilePath(CLIENT_KEY_PATH);+          std::string client_certs = GetContentsFromFilePath(CLIENT_CERT_PATH);+          experimental::IdentityKeyCertPair client_pair;+          client_pair.private_key = client_key;+          client_pair.certificate_chain = client_certs;+          std::vector<experimental::IdentityKeyCertPair> client_pair_list;+          client_pair_list.emplace_back(client_pair);+          channel_certificate_provider =+              std::make_shared<experimental::StaticDataCertificateProvider>(+                  root_certs, client_pair_list);+          break;+        }+        case SecurityPrimitives::FILE_PROVIDER: {+          channel_certificate_provider =+              std::make_shared<FileWatcherCertificateProvider>(+                  CLIENT_KEY_PATH, CLIENT_CERT_PATH, CA_CERT_PATH, 1);+          break;+        }+      }+      // Set up client certificate verifier.+      std::shared_ptr<experimental::CertificateVerifier>+          client_certificate_verifier;+      switch (GetParam().client_verifier_type()) {+        case SecurityPrimitives::EXTERNAL_SYNC_VERIFIER: {+          client_certificate_verifier =+              experimental::ExternalCertificateVerifier::Create<+                  SyncCertificateVerifier>(true);+          break;+        }+        case SecurityPrimitives::EXTERNAL_ASYNC_VERIFIER: {+          client_certificate_verifier =+              experimental::ExternalCertificateVerifier::Create<+                  AsyncCertificateVerifier>(true);+          break;+        }+        case SecurityPrimitives::HOSTNAME_VERIFIER: {+          client_certificate_verifier =+              std::make_shared<experimental::HostNameCertificateVerifier>();+          break;+        }+        default: {+          break;+        }+      }+      // Configure tls credential options for each stub. Each stub connects to+      // a separate port on the server.+      TlsChannelCredentialsOptions channel_creds_options;+      channel_creds_options.set_certificate_provider(+          channel_certificate_provider);+      channel_creds_options.watch_identity_key_cert_pairs();+      channel_creds_options.watch_root_certs();+      if (GetParam().client_verifier_type() !=+          SecurityPrimitives::DEFAULT_VERIFIER) {+        channel_creds_options.set_certificate_verifier(+            client_certificate_verifier);+        // When using a customized external verifier, we need to disable the+        // per-host check.+        if (GetParam().client_verifier_type() !=+            SecurityPrimitives::HOSTNAME_VERIFIER) {+          channel_creds_options.set_check_call_host(false);+        }+      }+      stubs_.push_back(EchoTestService::NewStub(::grpc::CreateCustomChannel(+          server_addresses_[i],+          ::grpc::experimental::TlsCredentials(channel_creds_options), args)));+    }+  }++  void TearDown() override {+    server_->Shutdown();+    server_thread_.join();+  }++  void RunServerLoop() { server_->Wait(); }++  const std::string client_method_name_ = ""grpc.testing.EchoTestService/Echo"";+  const std::string server_method_name_ = ""grpc.testing.EchoTestService/Echo"";++  std::vector<int> ports_;","Instead of having many different vectors here, please create a struct with all of the fields needed for each listening port, and then have a vector of those structs.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/28736,794790123,2022-01-28T19:22:28Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/api.py,"@@ -260,29 +261,68 @@ class OperationError(Error):     https://cloud.google.com/apis/design/design_patterns#long_running_operations     https://github.com/googleapis/googleapis/blob/master/google/longrunning/operations.proto     """"""+    api_name: str+    name: str+    metadata: str+    code_name: code_pb2.Code+    error: status_pb2.Status -    def __init__(self, api_name, operation_response, message=None):+    def __init__(self, api_name: str, response: dict):         self.api_name = api_name-        operation = json_format.ParseDict(-            operation_response,-            Operation(),-            ignore_unknown_fields=True,-            descriptor_pool=error_details_pb2.DESCRIPTOR.pool)++        # Operation.metadata field is Any specific to the API. It may not be+        # present in the default descriptor pool, and that's expected.+        # To avoid json_format.ParseError, handle it separately.+        self.metadata = response.pop('metadata', {})++        # Must be after removing metadata field.+        operation: Operation = self._parse_operation_response(response)         self.name = operation.name or 'unknown'         self.code_name = code_pb2.Code.Name(operation.error.code)         self.error = operation.error-        # Collect error details packed as Any without parsing concrete types.-        self.error_details = [-            text_format.MessageToString(any_error, as_one_line=True)-            for any_error in self.error.details-        ]-        if message is None:-            message = (f'{api_name} operation ""{self.name}"" failed. Error '-                       f'code: {self.error.code} ({self.code_name}), '-                       f'message: {self.error.message}, '-                       f'details: {self.error_details}')-        self.message = message-        super().__init__(message)+        super().__init__()++    @staticmethod+    def _parse_operation_response(operation_response: dict) -> Operation:+        try:+            return json_format.ParseDict(+                operation_response,+                Operation(),+                ignore_unknown_fields=True,+                descriptor_pool=error_details_pb2.DESCRIPTOR.pool)+        except (json_format.Error, TypeError) as e:+            # Swallow parsing errors if any. Building correct OperationError()+            # is more important than losing debug information. Details still+            # can be extracted from the warning.+            logger.warning(","With the change I made (not unpacking metadata `Any`), I don't expect this code branch to occur normally. In fact, if it does happen, it needs to be investigated, and, potentially, fixed.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/28736,794811619,2022-01-28T19:37:29Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/api.py,"@@ -260,29 +261,68 @@ class OperationError(Error):     https://cloud.google.com/apis/design/design_patterns#long_running_operations     https://github.com/googleapis/googleapis/blob/master/google/longrunning/operations.proto     """"""+    api_name: str+    name: str+    metadata: str+    code_name: code_pb2.Code+    error: status_pb2.Status -    def __init__(self, api_name, operation_response, message=None):+    def __init__(self, api_name: str, response: dict):         self.api_name = api_name-        operation = json_format.ParseDict(-            operation_response,-            Operation(),-            ignore_unknown_fields=True,-            descriptor_pool=error_details_pb2.DESCRIPTOR.pool)++        # Operation.metadata field is Any specific to the API. It may not be+        # present in the default descriptor pool, and that's expected.+        # To avoid json_format.ParseError, handle it separately.+        self.metadata = response.pop('metadata', {})++        # Must be after removing metadata field.+        operation: Operation = self._parse_operation_response(response)         self.name = operation.name or 'unknown'         self.code_name = code_pb2.Code.Name(operation.error.code)         self.error = operation.error-        # Collect error details packed as Any without parsing concrete types.-        self.error_details = [-            text_format.MessageToString(any_error, as_one_line=True)-            for any_error in self.error.details-        ]-        if message is None:-            message = (f'{api_name} operation ""{self.name}"" failed. Error '-                       f'code: {self.error.code} ({self.code_name}), '-                       f'message: {self.error.message}, '-                       f'details: {self.error_details}')-        self.message = message-        super().__init__(message)+        super().__init__()++    @staticmethod+    def _parse_operation_response(operation_response: dict) -> Operation:+        try:+            return json_format.ParseDict(+                operation_response,+                Operation(),+                ignore_unknown_fields=True,+                descriptor_pool=error_details_pb2.DESCRIPTOR.pool)+        except (json_format.Error, TypeError) as e:+            # Swallow parsing errors if any. Building correct OperationError()+            # is more important than losing debug information. Details still+            # can be extracted from the warning.+            logger.warning(+                (""Can't parse response while processing OperationError: '%r', ""+                 ""error %r""), operation_response, e)+            return Operation()++    def __str__(self):+        indent_l1 = ' ' * 2+        indent_l2 = indent_l1 * 2++        result = (f'{self.api_name} operation ""{self.name}"" failed.\n'+                  f'{indent_l1}code: {self.error.code} ({self.code_name})\n'+                  f'{indent_l1}message: ""{self.error.message}""')++        if self.error.details:+            result += f'\n{indent_l1}details: [\n'+            for any_error in self.error.details:+                error_str = json_format.MessageToJson(any_error)+                for line in error_str.splitlines():+                    result += indent_l2 + line + '\n'+            result += f'{indent_l1}]'","Not sure we need the new line at the end of the message. In fact when printing metadata below I intentionally strip it out: `result = result.rstrip()`. I think python prints exception messages with the new line at the end anyway, so I was removing an extra new line. But I could be wrong.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28736,794826517,2022-01-28T19:41:04Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/api.py,"@@ -260,29 +261,68 @@ class OperationError(Error):     https://cloud.google.com/apis/design/design_patterns#long_running_operations     https://github.com/googleapis/googleapis/blob/master/google/longrunning/operations.proto     """"""+    api_name: str+    name: str+    metadata: str+    code_name: code_pb2.Code+    error: status_pb2.Status -    def __init__(self, api_name, operation_response, message=None):+    def __init__(self, api_name: str, response: dict):         self.api_name = api_name-        operation = json_format.ParseDict(-            operation_response,-            Operation(),-            ignore_unknown_fields=True,-            descriptor_pool=error_details_pb2.DESCRIPTOR.pool)++        # Operation.metadata field is Any specific to the API. It may not be+        # present in the default descriptor pool, and that's expected.+        # To avoid json_format.ParseError, handle it separately.+        self.metadata = response.pop('metadata', {})++        # Must be after removing metadata field.+        operation: Operation = self._parse_operation_response(response)         self.name = operation.name or 'unknown'         self.code_name = code_pb2.Code.Name(operation.error.code)         self.error = operation.error-        # Collect error details packed as Any without parsing concrete types.-        self.error_details = [-            text_format.MessageToString(any_error, as_one_line=True)-            for any_error in self.error.details-        ]-        if message is None:-            message = (f'{api_name} operation ""{self.name}"" failed. Error '-                       f'code: {self.error.code} ({self.code_name}), '-                       f'message: {self.error.message}, '-                       f'details: {self.error_details}')-        self.message = message-        super().__init__(message)+        super().__init__()++    @staticmethod+    def _parse_operation_response(operation_response: dict) -> Operation:+        try:+            return json_format.ParseDict(+                operation_response,+                Operation(),+                ignore_unknown_fields=True,+                descriptor_pool=error_details_pb2.DESCRIPTOR.pool)+        except (json_format.Error, TypeError) as e:+            # Swallow parsing errors if any. Building correct OperationError()+            # is more important than losing debug information. Details still+            # can be extracted from the warning.+            logger.warning(+                (""Can't parse response while processing OperationError: '%r', ""+                 ""error %r""), operation_response, e)+            return Operation()++    def __str__(self):+        indent_l1 = ' ' * 2+        indent_l2 = indent_l1 * 2++        result = (f'{self.api_name} operation ""{self.name}"" failed.\n'+                  f'{indent_l1}code: {self.error.code} ({self.code_name})\n'+                  f'{indent_l1}message: ""{self.error.message}""')++        if self.error.details:+            result += f'\n{indent_l1}details: [\n'+            for any_error in self.error.details:+                error_str = json_format.MessageToJson(any_error)+                for line in error_str.splitlines():+                    result += indent_l2 + line + '\n'+            result += f'{indent_l1}]'","My bad, you are right, we should not add a trailing line break. I'm commenting about separate different errors in `self.error.details`.",
6731364,jagranados-dev,https://api.github.com/repos/grpc/grpc/pulls/28727,794878190,2022-01-28T20:46:52Z,examples/cpp/helloworld/greeter_client_ssl.cc,"@@ -0,0 +1,123 @@+/**+ * Copyright 2022 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ **/++#include <memory>+#include <string>+#include <sstream>+#include <fstream>+#include <iostream>++#include <grpcpp/grpcpp.h>++#ifdef BAZEL_BUILD+#include ""examples/protos/helloworld.grpc.pb.h""+#else+#include ""helloworld.grpc.pb.h""+#endif++using grpc::Channel;+using grpc::ClientContext;+using grpc::Status;++using helloworld::HelloRequest;+using helloworld::HelloReply;+using helloworld::Greeter;++class GreeterClient+{+public:+  GreeterClient ( const std::string& cert,+                  const std::string& key,+                  const std::string& root,+                  const std::string& server )+  {+    grpc::SslCredentialsOptions opts = {root,	key, cert};++    _stub = Greeter::NewStub ( grpc::CreateChannel ( +    server, grpc::SslCredentials ( opts ) ));+  }++  // Assembles the client's payload, sends it and presents the response back+  // from the server.+  std::string say_hello ( const std::string& user )+  {+    // Data we are sending to the server.+    HelloRequest request;+    request.set_name ( user );++    // Container for the data we expect from the server.+    HelloReply reply;++    // Context for the client. It could be used to convey extra information to+    // the server and/or tweak certain RPC behaviors.+    ClientContext context;++    // The actual RPC.+    Status status = _stub->SayHello ( &context, request, &reply );++    std::string msg { ""RPC failed"" };++    // Act upon its status.+    if ( status.ok () )+    {+      msg = reply.message ();+    }+    else+    {+      std::cout << status.error_code () << "": "" +        << status.error_message () << std::endl;+    }++    return msg;+  }+private:+  std::unique_ptr<Greeter::Stub> _stub;+};++void read ( const std::string& filename, std::string& data )",I decided not to use absl::string_view because this is an example that most people will copy and paste into their personal projects that may not include the absl dependency.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28276,794940377,2022-01-28T22:55:10Z,src/python/grpcio_tests/tests/unit/_rpc_test_helpers.py,"@@ -36,7 +36,7 @@ _STREAM_STREAM = '/test/StreamStream' _STREAM_STREAM_NON_BLOCKING = '/test/StreamStreamNonBlocking' -TIMEOUT_SHORT = datetime.timedelta(seconds=1).total_seconds()+TIMEOUT_SHORT = datetime.timedelta(seconds=4).total_seconds()",I'm fine increasing the timeout. Do you know what gevent tests require larger timeout? Gevent is supposed to be faster?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28276,794941536,2022-01-28T22:58:16Z,src/python/grpcio/grpc/_cython/_cygrpc/grpc_gevent.pyx.pxi,"@@ -14,418 +14,91 @@ # distutils: language=c++  from libc cimport string-import errno-import sys-gevent_g = None-gevent_socket = None-gevent_hub = None-gevent_event = None-g_event = None-g_pool = None--def _spawn_greenlet(*args):-  greenlet = g_pool.spawn(*args)--###############################-### socket implementation ###-###############################--cdef class SocketWrapper:-  def __cinit__(self):-    fork_handlers_and_grpc_init()-    self.sockopts = []-    self.socket = None-    self.c_socket = NULL-    self.c_buffer = NULL-    self.len = 0--  def __dealloc__(self):-    grpc_shutdown()--cdef grpc_error_handle socket_init(grpc_custom_socket* socket, int domain) with gil:-  sw = SocketWrapper()-  sw.c_socket = socket-  sw.sockopts = []-  cpython.Py_INCREF(sw)-  # Python doesn't support AF_UNSPEC sockets, so we defer creation until-  # bind/connect when we know what type of socket we need-  sw.socket = None-  sw.closed = False-  sw.accepting_socket = NULL-  socket.impl = <void*>sw-  return grpc_error_none()--cdef socket_connect_async_cython(SocketWrapper socket_wrapper, addr_tuple):-  try:-    socket_wrapper.socket.connect(addr_tuple)-    socket_wrapper.connect_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                              grpc_error_none())-  except IOError as io_error:-    socket_wrapper.connect_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                              socket_error(""connect"", str(io_error)))-  g_event.set()--def socket_connect_async(socket_wrapper, addr_tuple):-  socket_connect_async_cython(socket_wrapper, addr_tuple)--cdef void socket_connect(grpc_custom_socket* socket, const grpc_sockaddr* addr,-                         size_t addr_len,-                         grpc_custom_connect_callback cb) with gil:-  py_socket = None-  socket_wrapper = <SocketWrapper>socket.impl-  socket_wrapper.connect_cb = cb-  addr_tuple = sockaddr_to_tuple(addr, addr_len)-  if sockaddr_is_ipv4(addr, addr_len):-      py_socket = gevent_socket.socket(gevent_socket.AF_INET)-  else:-      py_socket = gevent_socket.socket(gevent_socket.AF_INET6)-  applysockopts(py_socket)-  socket_wrapper.socket = py_socket-  _spawn_greenlet(socket_connect_async, socket_wrapper, addr_tuple)--cdef void socket_destroy(grpc_custom_socket* socket) with gil:-  cpython.Py_DECREF(<SocketWrapper>socket.impl)--cdef void socket_shutdown(grpc_custom_socket* socket) with gil:-  try:-    (<SocketWrapper>socket.impl).socket.shutdown(gevent_socket.SHUT_RDWR)-  except IOError as io_error:-    if io_error.errno != errno.ENOTCONN:-      raise io_error--cdef void socket_close(grpc_custom_socket* socket,-                       grpc_custom_close_callback cb) with gil:-  socket_wrapper = (<SocketWrapper>socket.impl)-  if socket_wrapper.socket is not None:-    socket_wrapper.socket.close()-    socket_wrapper.closed = True-    socket_wrapper.close_cb = cb-    # Delay the close callback until the accept() call has picked it up-    if socket_wrapper.accepting_socket != NULL:-      return-  socket_wrapper.close_cb(socket)--def socket_sendmsg(socket, write_bytes):-  try:-    return socket.sendmsg(write_bytes)-  except AttributeError:-    # sendmsg not available on all Pythons/Platforms-    return socket.send(b''.join(write_bytes))--cdef socket_write_async_cython(SocketWrapper socket_wrapper, write_bytes):-  try:-    while write_bytes:-      sent_byte_count = socket_sendmsg(socket_wrapper.socket, write_bytes)-      while sent_byte_count > 0:-        if sent_byte_count < len(write_bytes[0]):-          write_bytes[0] = write_bytes[0][sent_byte_count:]-          sent_byte_count = 0-        else:-          sent_byte_count -= len(write_bytes[0])-          write_bytes = write_bytes[1:]-    socket_wrapper.write_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                            grpc_error_none())-  except IOError as io_error:-    socket_wrapper.write_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                            socket_error(""send"", str(io_error)))-  g_event.set()--def socket_write_async(socket_wrapper, write_bytes):-  socket_write_async_cython(socket_wrapper, write_bytes)--cdef void socket_write(grpc_custom_socket* socket, grpc_slice_buffer* buffer,-                       grpc_custom_write_callback cb) with gil:-  cdef char* start-  sw = <SocketWrapper>socket.impl-  sw.write_cb = cb-  write_bytes = []-  for i in range(buffer.count):-    start = grpc_slice_buffer_start(buffer, i)-    length = grpc_slice_buffer_length(buffer, i)-    write_bytes.append(<bytes>start[:length])-  _spawn_greenlet(socket_write_async, <SocketWrapper>socket.impl, write_bytes)--cdef socket_read_async_cython(SocketWrapper socket_wrapper):-  cdef char* buff_char_arr-  try:-    buff_str = socket_wrapper.socket.recv(socket_wrapper.len)-    buff_char_arr = buff_str-    string.memcpy(<void*>socket_wrapper.c_buffer, buff_char_arr, len(buff_str))-    socket_wrapper.read_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                           len(buff_str), grpc_error_none())-  except IOError as io_error:-    socket_wrapper.read_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                           -1, socket_error(""recv"", str(io_error)))-  g_event.set()--def socket_read_async(socket_wrapper):-  socket_read_async_cython(socket_wrapper)--cdef void socket_read(grpc_custom_socket* socket, char* buffer,-                      size_t length, grpc_custom_read_callback cb) with gil:-  sw = <SocketWrapper>socket.impl-  sw.read_cb = cb-  sw.c_buffer = buffer-  sw.len = length-  _spawn_greenlet(socket_read_async, sw)--cdef grpc_error_handle socket_getpeername(grpc_custom_socket* socket,-                                    const grpc_sockaddr* addr,-                                    int* length) with gil:-  cdef char* src_buf-  peer = (<SocketWrapper>socket.impl).socket.getpeername()--  cdef grpc_resolved_address c_addr-  hostname = str_to_bytes(peer[0])-  grpc_string_to_sockaddr(&c_addr, hostname, peer[1])-  string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)-  length[0] = c_addr.len-  return grpc_error_none()  --cdef grpc_error_handle socket_getsockname(grpc_custom_socket* socket,-                                    const grpc_sockaddr* addr,-                                    int* length) with gil:-  cdef char* src_buf-  cdef grpc_resolved_address c_addr-  if (<SocketWrapper>socket.impl).socket is None:-    peer = ('0.0.0.0', 0)-  else:-    peer = (<SocketWrapper>socket.impl).socket.getsockname()-  hostname = str_to_bytes(peer[0])-  grpc_string_to_sockaddr(&c_addr, hostname, peer[1])-  string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)-  length[0] = c_addr.len-  return grpc_error_none()--def applysockopts(s):-  s.setsockopt(gevent_socket.SOL_SOCKET, gevent_socket.SO_REUSEADDR, 1)-  s.setsockopt(gevent_socket.IPPROTO_TCP, gevent_socket.TCP_NODELAY, True)--cdef grpc_error_handle socket_bind(grpc_custom_socket* socket,-                             const grpc_sockaddr* addr,-                             size_t len, int flags) with gil:-  addr_tuple = sockaddr_to_tuple(addr, len)-  try:-    try:-      py_socket = gevent_socket.socket(gevent_socket.AF_INET)-      applysockopts(py_socket)-      py_socket.bind(addr_tuple)-    except gevent_socket.gaierror as e:-      py_socket = gevent_socket.socket(gevent_socket.AF_INET6)-      applysockopts(py_socket)-      py_socket.bind(addr_tuple)-    (<SocketWrapper>socket.impl).socket = py_socket-  except IOError as io_error:-    return socket_error(""bind"", str(io_error))-  else:-    return grpc_error_none()--cdef grpc_error_handle socket_listen(grpc_custom_socket* socket) with gil:-  (<SocketWrapper>socket.impl).socket.listen(50)-  return grpc_error_none()--cdef void accept_callback_cython(SocketWrapper s) except *:-   try:-     conn, address = s.socket.accept()-     sw = SocketWrapper()-     sw.closed = False-     sw.c_socket = s.accepting_socket-     sw.sockopts = []-     sw.socket = conn-     sw.c_socket.impl = <void*>sw-     sw.accepting_socket = NULL-     cpython.Py_INCREF(sw)-     s.accepting_socket = NULL-     s.accept_cb(<grpc_custom_socket*>s.c_socket, sw.c_socket, grpc_error_none())-   except IOError as io_error:-      #TODO actual error-      s.accepting_socket = NULL-      s.accept_cb(<grpc_custom_socket*>s.c_socket, s.accepting_socket,-                  socket_error(""accept"", str(io_error)))-      if s.closed:-        s.close_cb(<grpc_custom_socket*>s.c_socket)-   g_event.set()--def socket_accept_async(s):-  accept_callback_cython(s)+from cython.operator cimport dereference -cdef void socket_accept(grpc_custom_socket* socket, grpc_custom_socket* client,-                        grpc_custom_accept_callback cb) with gil:-  sw = <SocketWrapper>socket.impl-  sw.accepting_socket = client-  sw.accept_cb = cb-  _spawn_greenlet(socket_accept_async, sw)+from cpython cimport Py_INCREF, Py_DECREF -#####################################-######Resolver implementation #######-#####################################--cdef class ResolveWrapper:-  def __cinit__(self):-    fork_handlers_and_grpc_init()-    self.c_resolver = NULL-    self.c_host = NULL-    self.c_port = NULL--  def __dealloc__(self):-    grpc_shutdown()--cdef socket_resolve_async_cython(ResolveWrapper resolve_wrapper):-  try:-    res = gevent_socket.getaddrinfo(resolve_wrapper.c_host, resolve_wrapper.c_port)-    grpc_custom_resolve_callback(<grpc_custom_resolver*>resolve_wrapper.c_resolver,-                                 tuples_to_resolvaddr(res), grpc_error_none())-  except IOError as io_error:-    grpc_custom_resolve_callback(<grpc_custom_resolver*>resolve_wrapper.c_resolver,-                                 <grpc_resolved_addresses*>0,-                                 socket_error(""getaddrinfo"", str(io_error)))-  g_event.set()--def socket_resolve_async_python(resolve_wrapper):-  socket_resolve_async_cython(resolve_wrapper)--cdef void socket_resolve_async(grpc_custom_resolver* r, const char* host, const char* port) with gil:-  rw = ResolveWrapper()-  rw.c_resolver = r-  rw.c_host = host-  rw.c_port = port-  _spawn_greenlet(socket_resolve_async_python, rw)--cdef grpc_error_handle socket_resolve(const char* host, const char* port,-                                grpc_resolved_addresses** res) with gil:-    try:-      result = gevent_socket.getaddrinfo(host, port)-      res[0] = tuples_to_resolvaddr(result)-      return grpc_error_none()-    except IOError as io_error:-      return socket_error(""getaddrinfo"", str(io_error))--###############################-### timer implementation ######-###############################--cdef class TimerWrapper:-  def __cinit__(self, deadline):-    fork_handlers_and_grpc_init()-    self.timer = gevent_hub.get_hub().loop.timer(deadline)-    self.event = None--  def start(self):-    self.event = gevent_event.Event()-    self.timer.start(self.on_finish)--  def on_finish(self):-    grpc_custom_timer_callback(self.c_timer, grpc_error_none())-    self.timer.stop()-    g_event.set()--  def stop(self):-    self.event.set()-    self.timer.stop()--  def __dealloc__(self):-    grpc_shutdown()--cdef void timer_start(grpc_custom_timer* t) with gil:-  timer = TimerWrapper(t.timeout_ms / 1000.0)-  timer.c_timer = t-  t.timer = <void*>timer-  timer.start()--cdef void timer_stop(grpc_custom_timer* t) with gil:-  time_wrapper = <object>t.timer-  time_wrapper.stop()--###############################-### pollset implementation ###-###############################--cdef void init_loop() with gil:-  pass--cdef void destroy_loop() with gil:-  g_pool.join()--cdef void kick_loop() with gil:-  g_event.set()--def _run_loop(timeout_ms):-  timeout = timeout_ms / 1000.0-  if timeout_ms > 0:-    try:-      g_event.wait(timeout)-    finally:-      g_event.clear()--cdef grpc_error_handle run_loop(size_t timeout_ms) with gil:-  try:-    _run_loop(timeout_ms)-    return grpc_error_none()-  except BaseException:-    exc_info = sys.exc_info()-    # Avoid running any Python code after setting the exception-    cpython.PyErr_SetObject(exc_info[0], exc_info[1])-    return GRPC_ERROR_CANCELLED--###############################-### Initializer ###############-###############################+import atexit+import errno+import sys -cdef grpc_socket_vtable gevent_socket_vtable-cdef grpc_custom_resolver_vtable gevent_resolver_vtable-cdef grpc_custom_timer_vtable gevent_timer_vtable-cdef grpc_custom_poller_vtable gevent_pollset_vtable+gevent_hub = None+g_gevent_pool = None+g_gevent_threadpool = None+g_gevent_activated = False+++cdef queue[void*] g_greenlets_to_run+cdef condition_variable g_greenlets_cv+cdef mutex g_greenlets_mu+cdef bint g_shutdown_greenlets_to_run_queue = False+++cdef _submit_to_greenlet_queue(object cb, tuple args):+  cdef tuple to_call = (cb,) + args+  cdef unique_lock[mutex]* lk+  Py_INCREF(to_call)+  with nogil:+    lk = new unique_lock[mutex](g_greenlets_mu)+    g_greenlets_to_run.push(<void*>(to_call))+    del lk+    g_greenlets_cv.notify_all()+++cdef object await_next_greenlet():+  cdef unique_lock[mutex]* lk+  with nogil:+    # Cython doesn't allow us to do proper stack allocations, so we can't take+    # advantage of RAII.+    lk = new unique_lock[mutex](g_greenlets_mu)+    while not g_shutdown_greenlets_to_run_queue:+      if not g_greenlets_to_run.empty():+        break+      g_greenlets_cv.wait(dereference(lk))+  cdef object to_call = <object>g_greenlets_to_run.front()+  Py_DECREF(to_call)+  g_greenlets_to_run.pop()+  del lk+  return to_call++def spawn_greenlets():+  while True:+    to_call = g_gevent_threadpool.apply(await_next_greenlet, ())+    fn = to_call[0]+    args = to_call[1:]+    fn(*args)++def shutdown_await_next_greenlet():+  global g_shutdown_greenlets_to_run_queue+  cdef unique_lock[mutex]* lk+  with nogil:+    lk = new unique_lock[mutex](g_greenlets_mu)+    g_shutdown_greenlets_to_run_queue = True+  del lk+  g_greenlets_cv.notify_all()  def init_grpc_gevent():   # Lazily import gevent-  global gevent_socket-  global gevent_g   global gevent_hub-  global gevent_event-  global g_event-  global g_pool+  global g_gevent_threadpool+  global g_gevent_activated+  global g_interrupt_check_period_ms+  global g_gevent_pool+   import gevent-  gevent_g = gevent-  import gevent.socket-  gevent_socket = gevent.socket-  import gevent.hub-  gevent_hub = gevent.hub-  import gevent.event-  gevent_event = gevent.event   import gevent.pool -  g_event = gevent.event.Event()-  g_pool = gevent.pool.Group()--  def cb_func(cb, args):-    _spawn_greenlet(cb, *args)-  set_async_callback_func(cb_func)+  gevent_hub = gevent.hub+  g_gevent_threadpool = gevent_hub.get_hub().threadpool -  gevent_resolver_vtable.resolve = socket_resolve-  gevent_resolver_vtable.resolve_async = socket_resolve_async+  g_gevent_activated = True+  g_interrupt_check_period_ms = 2000",Why gevent needs longer check time? 2s might be less responsive?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28276,794950455,2022-01-28T23:21:53Z,src/python/grpcio/grpc/_cython/_cygrpc/grpc_gevent.pyx.pxi,"@@ -14,418 +14,91 @@ # distutils: language=c++  from libc cimport string-import errno-import sys-gevent_g = None-gevent_socket = None-gevent_hub = None-gevent_event = None-g_event = None-g_pool = None--def _spawn_greenlet(*args):-  greenlet = g_pool.spawn(*args)--###############################-### socket implementation ###-###############################--cdef class SocketWrapper:-  def __cinit__(self):-    fork_handlers_and_grpc_init()-    self.sockopts = []-    self.socket = None-    self.c_socket = NULL-    self.c_buffer = NULL-    self.len = 0--  def __dealloc__(self):-    grpc_shutdown()--cdef grpc_error_handle socket_init(grpc_custom_socket* socket, int domain) with gil:-  sw = SocketWrapper()-  sw.c_socket = socket-  sw.sockopts = []-  cpython.Py_INCREF(sw)-  # Python doesn't support AF_UNSPEC sockets, so we defer creation until-  # bind/connect when we know what type of socket we need-  sw.socket = None-  sw.closed = False-  sw.accepting_socket = NULL-  socket.impl = <void*>sw-  return grpc_error_none()--cdef socket_connect_async_cython(SocketWrapper socket_wrapper, addr_tuple):-  try:-    socket_wrapper.socket.connect(addr_tuple)-    socket_wrapper.connect_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                              grpc_error_none())-  except IOError as io_error:-    socket_wrapper.connect_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                              socket_error(""connect"", str(io_error)))-  g_event.set()--def socket_connect_async(socket_wrapper, addr_tuple):-  socket_connect_async_cython(socket_wrapper, addr_tuple)--cdef void socket_connect(grpc_custom_socket* socket, const grpc_sockaddr* addr,-                         size_t addr_len,-                         grpc_custom_connect_callback cb) with gil:-  py_socket = None-  socket_wrapper = <SocketWrapper>socket.impl-  socket_wrapper.connect_cb = cb-  addr_tuple = sockaddr_to_tuple(addr, addr_len)-  if sockaddr_is_ipv4(addr, addr_len):-      py_socket = gevent_socket.socket(gevent_socket.AF_INET)-  else:-      py_socket = gevent_socket.socket(gevent_socket.AF_INET6)-  applysockopts(py_socket)-  socket_wrapper.socket = py_socket-  _spawn_greenlet(socket_connect_async, socket_wrapper, addr_tuple)--cdef void socket_destroy(grpc_custom_socket* socket) with gil:-  cpython.Py_DECREF(<SocketWrapper>socket.impl)--cdef void socket_shutdown(grpc_custom_socket* socket) with gil:-  try:-    (<SocketWrapper>socket.impl).socket.shutdown(gevent_socket.SHUT_RDWR)-  except IOError as io_error:-    if io_error.errno != errno.ENOTCONN:-      raise io_error--cdef void socket_close(grpc_custom_socket* socket,-                       grpc_custom_close_callback cb) with gil:-  socket_wrapper = (<SocketWrapper>socket.impl)-  if socket_wrapper.socket is not None:-    socket_wrapper.socket.close()-    socket_wrapper.closed = True-    socket_wrapper.close_cb = cb-    # Delay the close callback until the accept() call has picked it up-    if socket_wrapper.accepting_socket != NULL:-      return-  socket_wrapper.close_cb(socket)--def socket_sendmsg(socket, write_bytes):-  try:-    return socket.sendmsg(write_bytes)-  except AttributeError:-    # sendmsg not available on all Pythons/Platforms-    return socket.send(b''.join(write_bytes))--cdef socket_write_async_cython(SocketWrapper socket_wrapper, write_bytes):-  try:-    while write_bytes:-      sent_byte_count = socket_sendmsg(socket_wrapper.socket, write_bytes)-      while sent_byte_count > 0:-        if sent_byte_count < len(write_bytes[0]):-          write_bytes[0] = write_bytes[0][sent_byte_count:]-          sent_byte_count = 0-        else:-          sent_byte_count -= len(write_bytes[0])-          write_bytes = write_bytes[1:]-    socket_wrapper.write_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                            grpc_error_none())-  except IOError as io_error:-    socket_wrapper.write_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                            socket_error(""send"", str(io_error)))-  g_event.set()--def socket_write_async(socket_wrapper, write_bytes):-  socket_write_async_cython(socket_wrapper, write_bytes)--cdef void socket_write(grpc_custom_socket* socket, grpc_slice_buffer* buffer,-                       grpc_custom_write_callback cb) with gil:-  cdef char* start-  sw = <SocketWrapper>socket.impl-  sw.write_cb = cb-  write_bytes = []-  for i in range(buffer.count):-    start = grpc_slice_buffer_start(buffer, i)-    length = grpc_slice_buffer_length(buffer, i)-    write_bytes.append(<bytes>start[:length])-  _spawn_greenlet(socket_write_async, <SocketWrapper>socket.impl, write_bytes)--cdef socket_read_async_cython(SocketWrapper socket_wrapper):-  cdef char* buff_char_arr-  try:-    buff_str = socket_wrapper.socket.recv(socket_wrapper.len)-    buff_char_arr = buff_str-    string.memcpy(<void*>socket_wrapper.c_buffer, buff_char_arr, len(buff_str))-    socket_wrapper.read_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                           len(buff_str), grpc_error_none())-  except IOError as io_error:-    socket_wrapper.read_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                           -1, socket_error(""recv"", str(io_error)))-  g_event.set()--def socket_read_async(socket_wrapper):-  socket_read_async_cython(socket_wrapper)--cdef void socket_read(grpc_custom_socket* socket, char* buffer,-                      size_t length, grpc_custom_read_callback cb) with gil:-  sw = <SocketWrapper>socket.impl-  sw.read_cb = cb-  sw.c_buffer = buffer-  sw.len = length-  _spawn_greenlet(socket_read_async, sw)--cdef grpc_error_handle socket_getpeername(grpc_custom_socket* socket,-                                    const grpc_sockaddr* addr,-                                    int* length) with gil:-  cdef char* src_buf-  peer = (<SocketWrapper>socket.impl).socket.getpeername()--  cdef grpc_resolved_address c_addr-  hostname = str_to_bytes(peer[0])-  grpc_string_to_sockaddr(&c_addr, hostname, peer[1])-  string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)-  length[0] = c_addr.len-  return grpc_error_none()  --cdef grpc_error_handle socket_getsockname(grpc_custom_socket* socket,-                                    const grpc_sockaddr* addr,-                                    int* length) with gil:-  cdef char* src_buf-  cdef grpc_resolved_address c_addr-  if (<SocketWrapper>socket.impl).socket is None:-    peer = ('0.0.0.0', 0)-  else:-    peer = (<SocketWrapper>socket.impl).socket.getsockname()-  hostname = str_to_bytes(peer[0])-  grpc_string_to_sockaddr(&c_addr, hostname, peer[1])-  string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)-  length[0] = c_addr.len-  return grpc_error_none()--def applysockopts(s):-  s.setsockopt(gevent_socket.SOL_SOCKET, gevent_socket.SO_REUSEADDR, 1)-  s.setsockopt(gevent_socket.IPPROTO_TCP, gevent_socket.TCP_NODELAY, True)--cdef grpc_error_handle socket_bind(grpc_custom_socket* socket,-                             const grpc_sockaddr* addr,-                             size_t len, int flags) with gil:-  addr_tuple = sockaddr_to_tuple(addr, len)-  try:-    try:-      py_socket = gevent_socket.socket(gevent_socket.AF_INET)-      applysockopts(py_socket)-      py_socket.bind(addr_tuple)-    except gevent_socket.gaierror as e:-      py_socket = gevent_socket.socket(gevent_socket.AF_INET6)-      applysockopts(py_socket)-      py_socket.bind(addr_tuple)-    (<SocketWrapper>socket.impl).socket = py_socket-  except IOError as io_error:-    return socket_error(""bind"", str(io_error))-  else:-    return grpc_error_none()--cdef grpc_error_handle socket_listen(grpc_custom_socket* socket) with gil:-  (<SocketWrapper>socket.impl).socket.listen(50)-  return grpc_error_none()--cdef void accept_callback_cython(SocketWrapper s) except *:-   try:-     conn, address = s.socket.accept()-     sw = SocketWrapper()-     sw.closed = False-     sw.c_socket = s.accepting_socket-     sw.sockopts = []-     sw.socket = conn-     sw.c_socket.impl = <void*>sw-     sw.accepting_socket = NULL-     cpython.Py_INCREF(sw)-     s.accepting_socket = NULL-     s.accept_cb(<grpc_custom_socket*>s.c_socket, sw.c_socket, grpc_error_none())-   except IOError as io_error:-      #TODO actual error-      s.accepting_socket = NULL-      s.accept_cb(<grpc_custom_socket*>s.c_socket, s.accepting_socket,-                  socket_error(""accept"", str(io_error)))-      if s.closed:-        s.close_cb(<grpc_custom_socket*>s.c_socket)-   g_event.set()--def socket_accept_async(s):-  accept_callback_cython(s)+from cython.operator cimport dereference -cdef void socket_accept(grpc_custom_socket* socket, grpc_custom_socket* client,-                        grpc_custom_accept_callback cb) with gil:-  sw = <SocketWrapper>socket.impl-  sw.accepting_socket = client-  sw.accept_cb = cb-  _spawn_greenlet(socket_accept_async, sw)+from cpython cimport Py_INCREF, Py_DECREF -#####################################-######Resolver implementation #######-#####################################--cdef class ResolveWrapper:-  def __cinit__(self):-    fork_handlers_and_grpc_init()-    self.c_resolver = NULL-    self.c_host = NULL-    self.c_port = NULL--  def __dealloc__(self):-    grpc_shutdown()--cdef socket_resolve_async_cython(ResolveWrapper resolve_wrapper):-  try:-    res = gevent_socket.getaddrinfo(resolve_wrapper.c_host, resolve_wrapper.c_port)-    grpc_custom_resolve_callback(<grpc_custom_resolver*>resolve_wrapper.c_resolver,-                                 tuples_to_resolvaddr(res), grpc_error_none())-  except IOError as io_error:-    grpc_custom_resolve_callback(<grpc_custom_resolver*>resolve_wrapper.c_resolver,-                                 <grpc_resolved_addresses*>0,-                                 socket_error(""getaddrinfo"", str(io_error)))-  g_event.set()--def socket_resolve_async_python(resolve_wrapper):-  socket_resolve_async_cython(resolve_wrapper)--cdef void socket_resolve_async(grpc_custom_resolver* r, const char* host, const char* port) with gil:-  rw = ResolveWrapper()-  rw.c_resolver = r-  rw.c_host = host-  rw.c_port = port-  _spawn_greenlet(socket_resolve_async_python, rw)--cdef grpc_error_handle socket_resolve(const char* host, const char* port,-                                grpc_resolved_addresses** res) with gil:-    try:-      result = gevent_socket.getaddrinfo(host, port)-      res[0] = tuples_to_resolvaddr(result)-      return grpc_error_none()-    except IOError as io_error:-      return socket_error(""getaddrinfo"", str(io_error))--###############################-### timer implementation ######-###############################--cdef class TimerWrapper:-  def __cinit__(self, deadline):-    fork_handlers_and_grpc_init()-    self.timer = gevent_hub.get_hub().loop.timer(deadline)-    self.event = None--  def start(self):-    self.event = gevent_event.Event()-    self.timer.start(self.on_finish)--  def on_finish(self):-    grpc_custom_timer_callback(self.c_timer, grpc_error_none())-    self.timer.stop()-    g_event.set()--  def stop(self):-    self.event.set()-    self.timer.stop()--  def __dealloc__(self):-    grpc_shutdown()--cdef void timer_start(grpc_custom_timer* t) with gil:-  timer = TimerWrapper(t.timeout_ms / 1000.0)-  timer.c_timer = t-  t.timer = <void*>timer-  timer.start()--cdef void timer_stop(grpc_custom_timer* t) with gil:-  time_wrapper = <object>t.timer-  time_wrapper.stop()--###############################-### pollset implementation ###-###############################--cdef void init_loop() with gil:-  pass--cdef void destroy_loop() with gil:-  g_pool.join()--cdef void kick_loop() with gil:-  g_event.set()--def _run_loop(timeout_ms):-  timeout = timeout_ms / 1000.0-  if timeout_ms > 0:-    try:-      g_event.wait(timeout)-    finally:-      g_event.clear()--cdef grpc_error_handle run_loop(size_t timeout_ms) with gil:-  try:-    _run_loop(timeout_ms)-    return grpc_error_none()-  except BaseException:-    exc_info = sys.exc_info()-    # Avoid running any Python code after setting the exception-    cpython.PyErr_SetObject(exc_info[0], exc_info[1])-    return GRPC_ERROR_CANCELLED--###############################-### Initializer ###############-###############################+import atexit+import errno+import sys -cdef grpc_socket_vtable gevent_socket_vtable-cdef grpc_custom_resolver_vtable gevent_resolver_vtable-cdef grpc_custom_timer_vtable gevent_timer_vtable-cdef grpc_custom_poller_vtable gevent_pollset_vtable+gevent_hub = None+g_gevent_pool = None+g_gevent_threadpool = None+g_gevent_activated = False+++cdef queue[void*] g_greenlets_to_run+cdef condition_variable g_greenlets_cv+cdef mutex g_greenlets_mu+cdef bint g_shutdown_greenlets_to_run_queue = False+++cdef _submit_to_greenlet_queue(object cb, tuple args):+  cdef tuple to_call = (cb,) + args+  cdef unique_lock[mutex]* lk","I see, it's for metadata plugin.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28276,794952063,2022-01-28T23:26:49Z,src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi,"@@ -58,11 +55,25 @@ cdef _interpret_event(grpc_event c_event):     cpython.Py_DECREF(tag)     return tag, tag.event(c_event) +cdef _internal_latent_event(_LatentEventArg latent_event_arg):+  cdef grpc_event c_event = _next(latent_event_arg.c_completion_queue, latent_event_arg.deadline)+  return _interpret_event(c_event)  cdef _latent_event(grpc_completion_queue *c_completion_queue, object deadline):-  cdef grpc_event c_event = _next(c_completion_queue, deadline)-  return _interpret_event(c_event)+    global g_gevent_activated++    latent_event_arg = _LatentEventArg()+    latent_event_arg.c_completion_queue = c_completion_queue+    latent_event_arg.deadline = deadline +    if g_gevent_activated:+      # For gevent, completion_queue_next is run in a native thread pool.+      global g_gevent_threadpool++      result = g_gevent_threadpool.apply(_internal_latent_event, (latent_event_arg,))","From the design doc, it seems like it's using actual native threads.> This approach does have one drawback - if many channels start RPCs concurrently, they may saturate the threadpool.When implementing the asyncio platform, I did faced similar issue. Maybe we can merge all completion queues into one? Then we only need 1 worker thread.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/28276,794960616,2022-01-28T23:56:37Z,bazel/_gevent_test_main.py,"@@ -38,8 +38,14 @@ def loadTestsFromNames(self, names: Sequence[str], module: str = None) -> unitte         return self.suite  if __name__ == ""__main__"":+    import gevent     from gevent import monkey +    threadpool = gevent.hub.get_hub().threadpool+    threadpool.maxsize = 1024+    threadpool.size = 32","Some of the tests create hundreds of channels concurrently (which is atypical in production). Without increasing the maximum size of the threadpool, some channels will get starved out, causing the test to take much longer than the native implementation to finish.This can be removed once the change is made for all channels to share a single completion queue and therefore a single native thread.Comment added for now.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/28276,794960651,2022-01-28T23:56:49Z,src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi,"@@ -58,11 +55,25 @@ cdef _interpret_event(grpc_event c_event):     cpython.Py_DECREF(tag)     return tag, tag.event(c_event) +cdef _internal_latent_event(_LatentEventArg latent_event_arg):+  cdef grpc_event c_event = _next(latent_event_arg.c_completion_queue, latent_event_arg.deadline)+  return _interpret_event(c_event)  cdef _latent_event(grpc_completion_queue *c_completion_queue, object deadline):-  cdef grpc_event c_event = _next(c_completion_queue, deadline)-  return _interpret_event(c_event)+    global g_gevent_activated++    latent_event_arg = _LatentEventArg()+    latent_event_arg.c_completion_queue = c_completion_queue+    latent_event_arg.deadline = deadline +    if g_gevent_activated:+      # For gevent, completion_queue_next is run in a native thread pool.+      global g_gevent_threadpool++      result = g_gevent_threadpool.apply(_internal_latent_event, (latent_event_arg,))","> I wonder this blocks the coroutine?It does not block the coroutine.> When implementing the asyncio platform, I did faced similar issue. Maybe we can merge all completion queues into one? Then we only need 1 worker thread.I considered this. I think that it's a better overall design, but it deviates further from the current model. In the interest of unblocking EventEngine however,  my suggestion would be that I file a bug for this change (internal, so stalebot doesn't close it) and then I'll implement it in a follow-up PR.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/28276,794960744,2022-01-28T23:57:09Z,src/python/grpcio/grpc/_cython/_cygrpc/grpc_gevent.pyx.pxi,"@@ -14,418 +14,91 @@ # distutils: language=c++  from libc cimport string-import errno-import sys-gevent_g = None-gevent_socket = None-gevent_hub = None-gevent_event = None-g_event = None-g_pool = None--def _spawn_greenlet(*args):-  greenlet = g_pool.spawn(*args)--###############################-### socket implementation ###-###############################--cdef class SocketWrapper:-  def __cinit__(self):-    fork_handlers_and_grpc_init()-    self.sockopts = []-    self.socket = None-    self.c_socket = NULL-    self.c_buffer = NULL-    self.len = 0--  def __dealloc__(self):-    grpc_shutdown()--cdef grpc_error_handle socket_init(grpc_custom_socket* socket, int domain) with gil:-  sw = SocketWrapper()-  sw.c_socket = socket-  sw.sockopts = []-  cpython.Py_INCREF(sw)-  # Python doesn't support AF_UNSPEC sockets, so we defer creation until-  # bind/connect when we know what type of socket we need-  sw.socket = None-  sw.closed = False-  sw.accepting_socket = NULL-  socket.impl = <void*>sw-  return grpc_error_none()--cdef socket_connect_async_cython(SocketWrapper socket_wrapper, addr_tuple):-  try:-    socket_wrapper.socket.connect(addr_tuple)-    socket_wrapper.connect_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                              grpc_error_none())-  except IOError as io_error:-    socket_wrapper.connect_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                              socket_error(""connect"", str(io_error)))-  g_event.set()--def socket_connect_async(socket_wrapper, addr_tuple):-  socket_connect_async_cython(socket_wrapper, addr_tuple)--cdef void socket_connect(grpc_custom_socket* socket, const grpc_sockaddr* addr,-                         size_t addr_len,-                         grpc_custom_connect_callback cb) with gil:-  py_socket = None-  socket_wrapper = <SocketWrapper>socket.impl-  socket_wrapper.connect_cb = cb-  addr_tuple = sockaddr_to_tuple(addr, addr_len)-  if sockaddr_is_ipv4(addr, addr_len):-      py_socket = gevent_socket.socket(gevent_socket.AF_INET)-  else:-      py_socket = gevent_socket.socket(gevent_socket.AF_INET6)-  applysockopts(py_socket)-  socket_wrapper.socket = py_socket-  _spawn_greenlet(socket_connect_async, socket_wrapper, addr_tuple)--cdef void socket_destroy(grpc_custom_socket* socket) with gil:-  cpython.Py_DECREF(<SocketWrapper>socket.impl)--cdef void socket_shutdown(grpc_custom_socket* socket) with gil:-  try:-    (<SocketWrapper>socket.impl).socket.shutdown(gevent_socket.SHUT_RDWR)-  except IOError as io_error:-    if io_error.errno != errno.ENOTCONN:-      raise io_error--cdef void socket_close(grpc_custom_socket* socket,-                       grpc_custom_close_callback cb) with gil:-  socket_wrapper = (<SocketWrapper>socket.impl)-  if socket_wrapper.socket is not None:-    socket_wrapper.socket.close()-    socket_wrapper.closed = True-    socket_wrapper.close_cb = cb-    # Delay the close callback until the accept() call has picked it up-    if socket_wrapper.accepting_socket != NULL:-      return-  socket_wrapper.close_cb(socket)--def socket_sendmsg(socket, write_bytes):-  try:-    return socket.sendmsg(write_bytes)-  except AttributeError:-    # sendmsg not available on all Pythons/Platforms-    return socket.send(b''.join(write_bytes))--cdef socket_write_async_cython(SocketWrapper socket_wrapper, write_bytes):-  try:-    while write_bytes:-      sent_byte_count = socket_sendmsg(socket_wrapper.socket, write_bytes)-      while sent_byte_count > 0:-        if sent_byte_count < len(write_bytes[0]):-          write_bytes[0] = write_bytes[0][sent_byte_count:]-          sent_byte_count = 0-        else:-          sent_byte_count -= len(write_bytes[0])-          write_bytes = write_bytes[1:]-    socket_wrapper.write_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                            grpc_error_none())-  except IOError as io_error:-    socket_wrapper.write_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                            socket_error(""send"", str(io_error)))-  g_event.set()--def socket_write_async(socket_wrapper, write_bytes):-  socket_write_async_cython(socket_wrapper, write_bytes)--cdef void socket_write(grpc_custom_socket* socket, grpc_slice_buffer* buffer,-                       grpc_custom_write_callback cb) with gil:-  cdef char* start-  sw = <SocketWrapper>socket.impl-  sw.write_cb = cb-  write_bytes = []-  for i in range(buffer.count):-    start = grpc_slice_buffer_start(buffer, i)-    length = grpc_slice_buffer_length(buffer, i)-    write_bytes.append(<bytes>start[:length])-  _spawn_greenlet(socket_write_async, <SocketWrapper>socket.impl, write_bytes)--cdef socket_read_async_cython(SocketWrapper socket_wrapper):-  cdef char* buff_char_arr-  try:-    buff_str = socket_wrapper.socket.recv(socket_wrapper.len)-    buff_char_arr = buff_str-    string.memcpy(<void*>socket_wrapper.c_buffer, buff_char_arr, len(buff_str))-    socket_wrapper.read_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                           len(buff_str), grpc_error_none())-  except IOError as io_error:-    socket_wrapper.read_cb(<grpc_custom_socket*>socket_wrapper.c_socket,-                           -1, socket_error(""recv"", str(io_error)))-  g_event.set()--def socket_read_async(socket_wrapper):-  socket_read_async_cython(socket_wrapper)--cdef void socket_read(grpc_custom_socket* socket, char* buffer,-                      size_t length, grpc_custom_read_callback cb) with gil:-  sw = <SocketWrapper>socket.impl-  sw.read_cb = cb-  sw.c_buffer = buffer-  sw.len = length-  _spawn_greenlet(socket_read_async, sw)--cdef grpc_error_handle socket_getpeername(grpc_custom_socket* socket,-                                    const grpc_sockaddr* addr,-                                    int* length) with gil:-  cdef char* src_buf-  peer = (<SocketWrapper>socket.impl).socket.getpeername()--  cdef grpc_resolved_address c_addr-  hostname = str_to_bytes(peer[0])-  grpc_string_to_sockaddr(&c_addr, hostname, peer[1])-  string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)-  length[0] = c_addr.len-  return grpc_error_none()  --cdef grpc_error_handle socket_getsockname(grpc_custom_socket* socket,-                                    const grpc_sockaddr* addr,-                                    int* length) with gil:-  cdef char* src_buf-  cdef grpc_resolved_address c_addr-  if (<SocketWrapper>socket.impl).socket is None:-    peer = ('0.0.0.0', 0)-  else:-    peer = (<SocketWrapper>socket.impl).socket.getsockname()-  hostname = str_to_bytes(peer[0])-  grpc_string_to_sockaddr(&c_addr, hostname, peer[1])-  string.memcpy(<void*>addr, <void*>c_addr.addr, c_addr.len)-  length[0] = c_addr.len-  return grpc_error_none()--def applysockopts(s):-  s.setsockopt(gevent_socket.SOL_SOCKET, gevent_socket.SO_REUSEADDR, 1)-  s.setsockopt(gevent_socket.IPPROTO_TCP, gevent_socket.TCP_NODELAY, True)--cdef grpc_error_handle socket_bind(grpc_custom_socket* socket,-                             const grpc_sockaddr* addr,-                             size_t len, int flags) with gil:-  addr_tuple = sockaddr_to_tuple(addr, len)-  try:-    try:-      py_socket = gevent_socket.socket(gevent_socket.AF_INET)-      applysockopts(py_socket)-      py_socket.bind(addr_tuple)-    except gevent_socket.gaierror as e:-      py_socket = gevent_socket.socket(gevent_socket.AF_INET6)-      applysockopts(py_socket)-      py_socket.bind(addr_tuple)-    (<SocketWrapper>socket.impl).socket = py_socket-  except IOError as io_error:-    return socket_error(""bind"", str(io_error))-  else:-    return grpc_error_none()--cdef grpc_error_handle socket_listen(grpc_custom_socket* socket) with gil:-  (<SocketWrapper>socket.impl).socket.listen(50)-  return grpc_error_none()--cdef void accept_callback_cython(SocketWrapper s) except *:-   try:-     conn, address = s.socket.accept()-     sw = SocketWrapper()-     sw.closed = False-     sw.c_socket = s.accepting_socket-     sw.sockopts = []-     sw.socket = conn-     sw.c_socket.impl = <void*>sw-     sw.accepting_socket = NULL-     cpython.Py_INCREF(sw)-     s.accepting_socket = NULL-     s.accept_cb(<grpc_custom_socket*>s.c_socket, sw.c_socket, grpc_error_none())-   except IOError as io_error:-      #TODO actual error-      s.accepting_socket = NULL-      s.accept_cb(<grpc_custom_socket*>s.c_socket, s.accepting_socket,-                  socket_error(""accept"", str(io_error)))-      if s.closed:-        s.close_cb(<grpc_custom_socket*>s.c_socket)-   g_event.set()--def socket_accept_async(s):-  accept_callback_cython(s)+from cython.operator cimport dereference -cdef void socket_accept(grpc_custom_socket* socket, grpc_custom_socket* client,-                        grpc_custom_accept_callback cb) with gil:-  sw = <SocketWrapper>socket.impl-  sw.accepting_socket = client-  sw.accept_cb = cb-  _spawn_greenlet(socket_accept_async, sw)+from cpython cimport Py_INCREF, Py_DECREF -#####################################-######Resolver implementation #######-#####################################--cdef class ResolveWrapper:-  def __cinit__(self):-    fork_handlers_and_grpc_init()-    self.c_resolver = NULL-    self.c_host = NULL-    self.c_port = NULL--  def __dealloc__(self):-    grpc_shutdown()--cdef socket_resolve_async_cython(ResolveWrapper resolve_wrapper):-  try:-    res = gevent_socket.getaddrinfo(resolve_wrapper.c_host, resolve_wrapper.c_port)-    grpc_custom_resolve_callback(<grpc_custom_resolver*>resolve_wrapper.c_resolver,-                                 tuples_to_resolvaddr(res), grpc_error_none())-  except IOError as io_error:-    grpc_custom_resolve_callback(<grpc_custom_resolver*>resolve_wrapper.c_resolver,-                                 <grpc_resolved_addresses*>0,-                                 socket_error(""getaddrinfo"", str(io_error)))-  g_event.set()--def socket_resolve_async_python(resolve_wrapper):-  socket_resolve_async_cython(resolve_wrapper)--cdef void socket_resolve_async(grpc_custom_resolver* r, const char* host, const char* port) with gil:-  rw = ResolveWrapper()-  rw.c_resolver = r-  rw.c_host = host-  rw.c_port = port-  _spawn_greenlet(socket_resolve_async_python, rw)--cdef grpc_error_handle socket_resolve(const char* host, const char* port,-                                grpc_resolved_addresses** res) with gil:-    try:-      result = gevent_socket.getaddrinfo(host, port)-      res[0] = tuples_to_resolvaddr(result)-      return grpc_error_none()-    except IOError as io_error:-      return socket_error(""getaddrinfo"", str(io_error))--###############################-### timer implementation ######-###############################--cdef class TimerWrapper:-  def __cinit__(self, deadline):-    fork_handlers_and_grpc_init()-    self.timer = gevent_hub.get_hub().loop.timer(deadline)-    self.event = None--  def start(self):-    self.event = gevent_event.Event()-    self.timer.start(self.on_finish)--  def on_finish(self):-    grpc_custom_timer_callback(self.c_timer, grpc_error_none())-    self.timer.stop()-    g_event.set()--  def stop(self):-    self.event.set()-    self.timer.stop()--  def __dealloc__(self):-    grpc_shutdown()--cdef void timer_start(grpc_custom_timer* t) with gil:-  timer = TimerWrapper(t.timeout_ms / 1000.0)-  timer.c_timer = t-  t.timer = <void*>timer-  timer.start()--cdef void timer_stop(grpc_custom_timer* t) with gil:-  time_wrapper = <object>t.timer-  time_wrapper.stop()--###############################-### pollset implementation ###-###############################--cdef void init_loop() with gil:-  pass--cdef void destroy_loop() with gil:-  g_pool.join()--cdef void kick_loop() with gil:-  g_event.set()--def _run_loop(timeout_ms):-  timeout = timeout_ms / 1000.0-  if timeout_ms > 0:-    try:-      g_event.wait(timeout)-    finally:-      g_event.clear()--cdef grpc_error_handle run_loop(size_t timeout_ms) with gil:-  try:-    _run_loop(timeout_ms)-    return grpc_error_none()-  except BaseException:-    exc_info = sys.exc_info()-    # Avoid running any Python code after setting the exception-    cpython.PyErr_SetObject(exc_info[0], exc_info[1])-    return GRPC_ERROR_CANCELLED--###############################-### Initializer ###############-###############################+import atexit+import errno+import sys -cdef grpc_socket_vtable gevent_socket_vtable-cdef grpc_custom_resolver_vtable gevent_resolver_vtable-cdef grpc_custom_timer_vtable gevent_timer_vtable-cdef grpc_custom_poller_vtable gevent_pollset_vtable+gevent_hub = None+g_gevent_pool = None+g_gevent_threadpool = None+g_gevent_activated = False+++cdef queue[void*] g_greenlets_to_run+cdef condition_variable g_greenlets_cv+cdef mutex g_greenlets_mu+cdef bint g_shutdown_greenlets_to_run_queue = False+++cdef _submit_to_greenlet_queue(object cb, tuple args):+  cdef tuple to_call = (cb,) + args+  cdef unique_lock[mutex]* lk+  Py_INCREF(to_call)+  with nogil:+    lk = new unique_lock[mutex](g_greenlets_mu)+    g_greenlets_to_run.push(<void*>(to_call))+    del lk+    g_greenlets_cv.notify_all()+++cdef object await_next_greenlet():+  cdef unique_lock[mutex]* lk+  with nogil:+    # Cython doesn't allow us to do proper stack allocations, so we can't take+    # advantage of RAII.+    lk = new unique_lock[mutex](g_greenlets_mu)+    while not g_shutdown_greenlets_to_run_queue:+      if not g_greenlets_to_run.empty():+        break+      g_greenlets_cv.wait(dereference(lk))+  cdef object to_call = <object>g_greenlets_to_run.front()+  Py_DECREF(to_call)+  g_greenlets_to_run.pop()+  del lk+  return to_call++def spawn_greenlets():+  while True:+    to_call = g_gevent_threadpool.apply(await_next_greenlet, ())+    fn = to_call[0]+    args = to_call[1:]+    fn(*args)++def shutdown_await_next_greenlet():+  global g_shutdown_greenlets_to_run_queue+  cdef unique_lock[mutex]* lk+  with nogil:+    lk = new unique_lock[mutex](g_greenlets_mu)+    g_shutdown_greenlets_to_run_queue = True+  del lk+  g_greenlets_cv.notify_all()  def init_grpc_gevent():   # Lazily import gevent-  global gevent_socket-  global gevent_g   global gevent_hub-  global gevent_event-  global g_event-  global g_pool+  global g_gevent_threadpool+  global g_gevent_activated+  global g_interrupt_check_period_ms+  global g_gevent_pool+   import gevent-  gevent_g = gevent-  import gevent.socket-  gevent_socket = gevent.socket-  import gevent.hub-  gevent_hub = gevent.hub-  import gevent.event-  gevent_event = gevent.event   import gevent.pool -  g_event = gevent.event.Event()-  g_pool = gevent.pool.Group()--  def cb_func(cb, args):-    _spawn_greenlet(cb, *args)-  set_async_callback_func(cb_func)+  gevent_hub = gevent.hub+  g_gevent_threadpool = gevent_hub.get_hub().threadpool -  gevent_resolver_vtable.resolve = socket_resolve-  gevent_resolver_vtable.resolve_async = socket_resolve_async+  g_gevent_activated = True+  g_interrupt_check_period_ms = 2000","Since completion queue polling no longer happens on the main thread, we no longer need to pause completion queue polling to check for signals. This is actually _more_ responsive.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28746,795839486,2022-01-31T16:23:33Z,src/core/lib/config/core_configuration.h,"@@ -19,6 +19,7 @@  #include <atomic> +#include ""src/core/ext/xds/xds_channel_creds.h""","We should not have code from src/core/lib depending on code in src/core/ext.  Instead, I suggest moving xds_channel_creds_registry.{cc,h} and xds_channel_default_channel_creds.cc from src/core/ext/xds to src/core/lib/security/credentials.  You can drop the ""xds_"" prefix from the filenames and the ""Xds"" prefix from the class names, since this mechanism isn't necessarily xDS-specific.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28687,795874167,2022-01-31T16:58:51Z,src/core/lib/channel/promise_based_filter.h,"@@ -380,6 +382,276 @@ class CallData<ChannelFilter, true> : public BaseCallData {   bool forward_send_initial_metadata_ = false; }; +// Server implementation of call data.+template <class ChannelFilter>+class CallData<ChannelFilter, false> : public BaseCallData {+ public:+  CallData(grpc_call_element* elem, const grpc_call_element_args* args)+      : BaseCallData(elem, args) {+    GRPC_CLOSURE_INIT(&recv_initial_metadata_ready_,+                      RecvInitialMetadataReadyCallback, this,+                      grpc_schedule_on_exec_ctx);+  }++  ~CallData() {+    GPR_ASSERT(!is_polling_);+    GRPC_ERROR_UNREF(cancelled_error_);+  }++  // Handle one grpc_transport_stream_op_batch+  void StartBatch(grpc_transport_stream_op_batch* batch) {+    // Fake out the activity based context.+    ScopedContext context(this);++    // If this is a cancel stream, cancel anything we have pending and propagate+    // the cancellation.+    if (batch->cancel_stream) {+      GPR_ASSERT(!batch->send_initial_metadata &&+                 !batch->send_trailing_metadata && !batch->send_message &&+                 !batch->recv_initial_metadata && !batch->recv_message &&+                 !batch->recv_trailing_metadata);+      Cancel(batch->payload->cancel_stream.cancel_error);+      grpc_call_next_op(elem(), batch);+      return;+    }++    // recv_initial_metadata: we hook the response of this so we can start the+    // promise at an appropriate time.+    if (batch->recv_initial_metadata) {+      GPR_ASSERT(!batch->send_initial_metadata &&+                 !batch->send_trailing_metadata && !batch->send_message &&+                 !batch->recv_message && !batch->recv_trailing_metadata);+      // Otherwise, we should not have seen a send_initial_metadata op yet.+      GPR_ASSERT(recv_initial_state_ == RecvInitialState::kInitial);+      // Hook the callback so we know when to start the promise.+      recv_initial_metadata_ =+          batch->payload->recv_initial_metadata.recv_initial_metadata;+      original_recv_initial_metadata_ready_ =+          batch->payload->recv_initial_metadata.recv_initial_metadata_ready;+      batch->payload->recv_initial_metadata.recv_initial_metadata_ready =+          &recv_initial_metadata_ready_;+      recv_initial_state_ = RecvInitialState::kForwarded;+    }++    // send_trailing_metadata+    if (batch->send_trailing_metadata) {+      switch (send_trailing_state_) {+        case SendTrailingState::kInitial:+          send_trailing_metadata_batch_ = batch;+          send_trailing_state_ = SendTrailingState::kQueued;+          WakeInsideCombiner([this](grpc_error_handle error) {+            GPR_ASSERT(send_trailing_state_ == SendTrailingState::kQueued);+            Cancel(error);+          });+          break;+        case SendTrailingState::kQueued:+        case SendTrailingState::kForwarded:+          abort();  // unreachable+          break;+        case SendTrailingState::kCancelled:+          abort();  // unimplemented+          break;+      }+      return;+    }++    grpc_call_next_op(elem(), batch);+  }++ private:+  // At what stage is our handling of recv initial metadata?+  enum class RecvInitialState {+    // Start state: no op seen+    kInitial,+    // Op seen, and forwarded to the next filter.+    // Now waiting for the callback.+    kForwarded,+    // The op has completed from below, but we haven't yet forwarded it up+    // (the promise gets to interject and mutate it).+    kComplete,+    // We've sent the response to the next filter up.+    kResponded,+  };+  // At what stage is our handling of send trailing metadata?+  enum class SendTrailingState {+    // Start state: no op seen+    kInitial,+    // We saw the op, and are waiting for the promise to complete+    // to forward it.+    kQueued,+    // We've forwarded the op to the next filter.+    kForwarded,+    // We were cancelled.+    kCancelled+  };++  // Handle cancellation.+  void Cancel(grpc_error_handle error) {+    // Track the latest reason for cancellation.+    GRPC_ERROR_UNREF(cancelled_error_);+    cancelled_error_ = GRPC_ERROR_REF(error);+    // Stop running the promise.+    promise_ = ArenaPromise<TrailingMetadata>();+    if (send_trailing_state_ == SendTrailingState::kQueued) {+      grpc_transport_stream_op_batch_finish_with_failure(+          absl::exchange(send_trailing_metadata_batch_, nullptr),+          GRPC_ERROR_REF(cancelled_error_), call_combiner());+    }+    send_trailing_state_ = SendTrailingState::kCancelled;+  }++  // Construct a promise that will ""call"" the next filter.+  // Effectively:+  //   - put the modified initial metadata into the batch being sent up.+  //   - return a wrapper around PollTrailingMetadata as the promise.+  ArenaPromise<TrailingMetadata> MakeNextPromise(+      ClientInitialMetadata initial_metadata) {+    GPR_ASSERT(recv_initial_state_ == RecvInitialState::kComplete);+    GPR_ASSERT(UnwrapMetadata(std::move(initial_metadata)) ==+               recv_initial_metadata_);+    forward_recv_initial_metadata_callback_ = true;+    return ArenaPromise<TrailingMetadata>(+        [this]() { return PollTrailingMetadata(); });+  }++  // Wrapper to make it look like we're calling the next filter as a promise.+  // All polls: await sending the trailing metadata, then foward it down the+  // stack.+  Poll<TrailingMetadata> PollTrailingMetadata() {+    switch (send_trailing_state_) {+      case SendTrailingState::kInitial:+        return Pending{};+      case SendTrailingState::kQueued:+        return WrapMetadata(+            send_trailing_metadata_batch_->payload->send_trailing_metadata+                .send_trailing_metadata);+      case SendTrailingState::kForwarded:+        abort();  // unreachable+      case SendTrailingState::kCancelled:+        // We could translate cancelled_error to metadata and return it... BUT+        // we're not gonna be running much longer and the results going to be+        // ignored.+        return Pending{};+    }+    GPR_UNREACHABLE_CODE(return Pending{});+  }++  static void RecvInitialMetadataReadyCallback(void* arg,+                                               grpc_error_handle error) {+    static_cast<CallData*>(arg)->RecvInitialMetadataReady(error);+  }++  void RecvInitialMetadataReady(grpc_error_handle error) {+    GPR_ASSERT(recv_initial_state_ == RecvInitialState::kForwarded);+    // If there was an error we just propagate that through+    if (error != GRPC_ERROR_NONE) {+      recv_initial_state_ = RecvInitialState::kResponded;+      Closure::Run(DEBUG_LOCATION, original_recv_initial_metadata_ready_,+                   GRPC_ERROR_REF(error));+      return;+    }+    // Record that we've got the callback.+    recv_initial_state_ = RecvInitialState::kComplete;++    // Start the promise.+    ScopedContext context(this);+    // Construct the promise.+    ChannelFilter* filter = static_cast<ChannelFilter*>(elem()->channel_data);+    promise_ = filter->MakeCallPromise(+        WrapMetadata(recv_initial_metadata_),+        [this](ClientInitialMetadata initial_metadata) {+          return MakeNextPromise(std::move(initial_metadata));+        });+    // Poll once.+    bool own_error = false;+    WakeInsideCombiner([&error, &own_error](grpc_error_handle new_error) {+      GPR_ASSERT(error == GRPC_ERROR_NONE);+      error = GRPC_ERROR_REF(new_error);+      own_error = true;+    });+    Closure::Run(DEBUG_LOCATION, original_recv_initial_metadata_ready_,+                 GRPC_ERROR_REF(error));+    if (own_error) GRPC_ERROR_UNREF(error);+  }++  // Wakeup and poll the promise if appropriate.+  void WakeInsideCombiner(absl::FunctionRef<void(grpc_error_handle)> cancel) {+    GPR_ASSERT(!is_polling_);+    bool forward_send_trailing_metadata = false;+    is_polling_ = true;+    if (recv_initial_state_ == RecvInitialState::kComplete) {+      Poll<TrailingMetadata> poll = promise_();+      if (auto* r = absl::get_if<TrailingMetadata>(&poll)) {+        auto* md = UnwrapMetadata(std::move(*r));+        bool destroy_md = true;+        switch (send_trailing_state_) {+          case SendTrailingState::kQueued: {+            if (send_trailing_metadata_batch_->payload->send_trailing_metadata+                    .send_trailing_metadata != md) {+              *send_trailing_metadata_batch_->payload->send_trailing_metadata+                   .send_trailing_metadata = std::move(*md);+            } else {+              destroy_md = false;+            }+            forward_send_trailing_metadata = true;+          } break;+          case SendTrailingState::kForwarded:+            abort();  // unreachable+            break;+          case SendTrailingState::kInitial: {+            GPR_ASSERT(*md->get_pointer(GrpcStatusMetadata()) !=+                       GRPC_STATUS_OK);+            grpc_error_handle error = grpc_error_set_int(+                GRPC_ERROR_CREATE_FROM_STATIC_STRING(+                    ""early return from promise based filter""),+                GRPC_ERROR_INT_GRPC_STATUS,+                *md->get_pointer(GrpcStatusMetadata()));+            if (auto* message = md->get_pointer(GrpcMessageMetadata())) {+              error = grpc_error_set_str(error, GRPC_ERROR_STR_GRPC_MESSAGE,+                                         message->as_string_view());+            }+            cancel(error);+            GRPC_ERROR_UNREF(error);+          } break;+          case SendTrailingState::kCancelled:+            // Nothing to do.+            break;+        }+        if (destroy_md) {+          md->~grpc_metadata_batch();+        }+      }+    }+    is_polling_ = false;+    if (absl::exchange(forward_send_trailing_metadata, false)) {","Why are we changing the value of `forward_send_trailing_metadata` here, since it's not going to be used again after this?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28687,795875971,2022-01-31T17:00:49Z,src/core/lib/channel/promise_based_filter.h,"@@ -380,6 +382,276 @@ class CallData<ChannelFilter, true> : public BaseCallData {   bool forward_send_initial_metadata_ = false; }; +// Server implementation of call data.+template <class ChannelFilter>+class CallData<ChannelFilter, false> : public BaseCallData {+ public:+  CallData(grpc_call_element* elem, const grpc_call_element_args* args)+      : BaseCallData(elem, args) {+    GRPC_CLOSURE_INIT(&recv_initial_metadata_ready_,+                      RecvInitialMetadataReadyCallback, this,+                      grpc_schedule_on_exec_ctx);+  }++  ~CallData() {+    GPR_ASSERT(!is_polling_);+    GRPC_ERROR_UNREF(cancelled_error_);+  }++  // Handle one grpc_transport_stream_op_batch+  void StartBatch(grpc_transport_stream_op_batch* batch) {+    // Fake out the activity based context.+    ScopedContext context(this);++    // If this is a cancel stream, cancel anything we have pending and propagate+    // the cancellation.+    if (batch->cancel_stream) {+      GPR_ASSERT(!batch->send_initial_metadata &&+                 !batch->send_trailing_metadata && !batch->send_message &&+                 !batch->recv_initial_metadata && !batch->recv_message &&+                 !batch->recv_trailing_metadata);+      Cancel(batch->payload->cancel_stream.cancel_error);+      grpc_call_next_op(elem(), batch);+      return;+    }++    // recv_initial_metadata: we hook the response of this so we can start the+    // promise at an appropriate time.+    if (batch->recv_initial_metadata) {+      GPR_ASSERT(!batch->send_initial_metadata &&+                 !batch->send_trailing_metadata && !batch->send_message &&+                 !batch->recv_message && !batch->recv_trailing_metadata);+      // Otherwise, we should not have seen a send_initial_metadata op yet.+      GPR_ASSERT(recv_initial_state_ == RecvInitialState::kInitial);+      // Hook the callback so we know when to start the promise.+      recv_initial_metadata_ =+          batch->payload->recv_initial_metadata.recv_initial_metadata;+      original_recv_initial_metadata_ready_ =+          batch->payload->recv_initial_metadata.recv_initial_metadata_ready;+      batch->payload->recv_initial_metadata.recv_initial_metadata_ready =+          &recv_initial_metadata_ready_;+      recv_initial_state_ = RecvInitialState::kForwarded;+    }++    // send_trailing_metadata+    if (batch->send_trailing_metadata) {+      switch (send_trailing_state_) {+        case SendTrailingState::kInitial:+          send_trailing_metadata_batch_ = batch;+          send_trailing_state_ = SendTrailingState::kQueued;+          WakeInsideCombiner([this](grpc_error_handle error) {+            GPR_ASSERT(send_trailing_state_ == SendTrailingState::kQueued);+            Cancel(error);+          });+          break;+        case SendTrailingState::kQueued:+        case SendTrailingState::kForwarded:+          abort();  // unreachable+          break;+        case SendTrailingState::kCancelled:+          abort();  // unimplemented+          break;+      }+      return;+    }++    grpc_call_next_op(elem(), batch);+  }++ private:+  // At what stage is our handling of recv initial metadata?+  enum class RecvInitialState {+    // Start state: no op seen+    kInitial,+    // Op seen, and forwarded to the next filter.+    // Now waiting for the callback.+    kForwarded,+    // The op has completed from below, but we haven't yet forwarded it up+    // (the promise gets to interject and mutate it).+    kComplete,+    // We've sent the response to the next filter up.+    kResponded,+  };+  // At what stage is our handling of send trailing metadata?+  enum class SendTrailingState {+    // Start state: no op seen+    kInitial,+    // We saw the op, and are waiting for the promise to complete+    // to forward it.+    kQueued,+    // We've forwarded the op to the next filter.+    kForwarded,+    // We were cancelled.+    kCancelled+  };++  // Handle cancellation.+  void Cancel(grpc_error_handle error) {+    // Track the latest reason for cancellation.+    GRPC_ERROR_UNREF(cancelled_error_);+    cancelled_error_ = GRPC_ERROR_REF(error);+    // Stop running the promise.+    promise_ = ArenaPromise<TrailingMetadata>();+    if (send_trailing_state_ == SendTrailingState::kQueued) {+      grpc_transport_stream_op_batch_finish_with_failure(+          absl::exchange(send_trailing_metadata_batch_, nullptr),+          GRPC_ERROR_REF(cancelled_error_), call_combiner());+    }+    send_trailing_state_ = SendTrailingState::kCancelled;+  }++  // Construct a promise that will ""call"" the next filter.+  // Effectively:+  //   - put the modified initial metadata into the batch being sent up.+  //   - return a wrapper around PollTrailingMetadata as the promise.+  ArenaPromise<TrailingMetadata> MakeNextPromise(+      ClientInitialMetadata initial_metadata) {+    GPR_ASSERT(recv_initial_state_ == RecvInitialState::kComplete);+    GPR_ASSERT(UnwrapMetadata(std::move(initial_metadata)) ==+               recv_initial_metadata_);+    forward_recv_initial_metadata_callback_ = true;+    return ArenaPromise<TrailingMetadata>(+        [this]() { return PollTrailingMetadata(); });+  }++  // Wrapper to make it look like we're calling the next filter as a promise.+  // All polls: await sending the trailing metadata, then foward it down the+  // stack.+  Poll<TrailingMetadata> PollTrailingMetadata() {+    switch (send_trailing_state_) {+      case SendTrailingState::kInitial:+        return Pending{};+      case SendTrailingState::kQueued:+        return WrapMetadata(+            send_trailing_metadata_batch_->payload->send_trailing_metadata+                .send_trailing_metadata);+      case SendTrailingState::kForwarded:+        abort();  // unreachable+      case SendTrailingState::kCancelled:+        // We could translate cancelled_error to metadata and return it... BUT+        // we're not gonna be running much longer and the results going to be+        // ignored.+        return Pending{};+    }+    GPR_UNREACHABLE_CODE(return Pending{});+  }++  static void RecvInitialMetadataReadyCallback(void* arg,+                                               grpc_error_handle error) {+    static_cast<CallData*>(arg)->RecvInitialMetadataReady(error);+  }++  void RecvInitialMetadataReady(grpc_error_handle error) {+    GPR_ASSERT(recv_initial_state_ == RecvInitialState::kForwarded);+    // If there was an error we just propagate that through+    if (error != GRPC_ERROR_NONE) {+      recv_initial_state_ = RecvInitialState::kResponded;+      Closure::Run(DEBUG_LOCATION, original_recv_initial_metadata_ready_,+                   GRPC_ERROR_REF(error));+      return;+    }+    // Record that we've got the callback.+    recv_initial_state_ = RecvInitialState::kComplete;++    // Start the promise.+    ScopedContext context(this);+    // Construct the promise.+    ChannelFilter* filter = static_cast<ChannelFilter*>(elem()->channel_data);+    promise_ = filter->MakeCallPromise(+        WrapMetadata(recv_initial_metadata_),+        [this](ClientInitialMetadata initial_metadata) {+          return MakeNextPromise(std::move(initial_metadata));+        });+    // Poll once.+    bool own_error = false;",This is a nice mechanism for preserving the error for the callback.  Can we use this same approach on the client side for the `recv_trailing_metadata_ready` callback?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28687,795880564,2022-01-31T17:05:54Z,src/core/lib/security/authorization/sdk_server_authz_filter.cc,"@@ -107,14 +66,14 @@ bool SdkServerAuthzFilter::CallData::IsAuthorized(SdkServerAuthzFilter* chand) {         std::string(args.GetPeerAddressString()).c_str(), args.GetPeerPort());   }   grpc_authorization_policy_provider::AuthorizationEngines engines =-      chand->provider_->engines();+      provider_->engines();   if (engines.deny_engine != nullptr) {     AuthorizationEngine::Decision decision =         engines.deny_engine->Evaluate(args);     if (decision.type == AuthorizationEngine::Decision::Type::kDeny) {       if (GRPC_TRACE_FLAG_ENABLED(grpc_sdk_authz_trace)) {-        gpr_log(GPR_INFO, ""chand=%p calld=%p: request denied by policy %s."",","Hmm.  In the promise world, is there no longer any identifier to represent the individual call?  Losing this may make logs harder to understand, because if there are multiple calls in flight, it won't be clear which log messages are associated with which calls.It's not so much a problem in this particular filter, since all of the per-call log messages are going to be logged in the same thread.  But there are other filters that do async work where this will be a problem.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28687,795897378,2022-01-31T17:24:36Z,src/core/lib/security/authorization/sdk_server_authz_filter.cc,"@@ -107,14 +66,14 @@ bool SdkServerAuthzFilter::CallData::IsAuthorized(SdkServerAuthzFilter* chand) {         std::string(args.GetPeerAddressString()).c_str(), args.GetPeerPort());   }   grpc_authorization_policy_provider::AuthorizationEngines engines =-      chand->provider_->engines();+      provider_->engines();   if (engines.deny_engine != nullptr) {     AuthorizationEngine::Decision decision =         engines.deny_engine->Evaluate(args);     if (decision.type == AuthorizationEngine::Decision::Type::kDeny) {       if (GRPC_TRACE_FLAG_ENABLED(grpc_sdk_authz_trace)) {-        gpr_log(GPR_INFO, ""chand=%p calld=%p: request denied by policy %s."",",There's absolutely an identifier: the current activity pointer would probably make a good one (though we're not emulating that just yet) - and that's probably better than the call data pointer of some arbitrary filter because it's stable across the stack.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28745,795942941,2022-01-31T18:18:32Z,tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py,"@@ -83,8 +85,12 @@ def test_api_listener(self) -> None:          with self.subTest('13_test_server_received_rpcs_with_two_url_maps'):             self.assertSuccessfulRpcs(test_client)-            previous_route_config_version = self.getRouteConfigVersion(-                test_client)+            config = test_client.csds.fetch_client_status(+                log_level=logging.INFO)+            logger.info('received client config from CSDS, dump config: %s',+                        config)",Dumping a `DumpedXdsConfig` (JSON) format is better than dumping in text proto. Text proto's list and map is confusing.,
85715732,yousukseung,https://api.github.com/repos/grpc/grpc/pulls/28746,796086077,2022-01-31T21:40:24Z,src/core/ext/xds/xds_channel_creds.h,"@@ -19,30 +19,45 @@  #include <grpc/support/port_platform.h> +#include <grpc/impl/codegen/grpc_types.h>+ #include ""src/core/lib/json/json.h""-#include ""src/core/lib/security/credentials/credentials.h""++struct grpc_channel_credentials;  namespace grpc_core { -class XdsChannelCredsImpl {+class XdsChannelCredsFactory {  public:-  virtual ~XdsChannelCredsImpl() {}+  virtual ~XdsChannelCredsFactory() {}   virtual absl::string_view creds_type() const = 0;   virtual bool IsValidConfig(const Json& config) const = 0;-  virtual RefCountedPtr<grpc_channel_credentials> CreateXdsChannelCreds(+  virtual grpc_channel_credentials* CreateXdsChannelCreds(","Thanks, I tried but could not figure out how to make it work with forward declaration (due to a circular dependency) here.Is there a tip?",X
4111145,YifeiZhuang,https://api.github.com/repos/grpc/grpc/pulls/28745,796113610,2022-01-31T22:23:00Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -350,26 +353,13 @@ def assertXdsConfigExists(self, test_client: XdsTestClient):                      json_format.MessageToJson(config, indent=2))         self.assertSameElements(want, seen) -    @staticmethod-    def getRouteConfigVersion(test_client: XdsTestClient) -> Optional[str]:-        config = test_client.csds.fetch_client_status(log_level=logging.INFO)-        route_config_version = None-        for xds_config in config.xds_config:-            if xds_config.WhichOneof('per_xds_config') == ""route_config"":-                route_config = xds_config.route_config-                logger.info('Route config found: %s',-                            json_format.MessageToJson(route_config, indent=2))-                route_config_version = route_config.dynamic_route_configs[-                    0].version_info-                logger.info('found routing config version: %s',-                            route_config_version)-                break-        return route_config_version-     def assertRouteConfigUpdateTrafficHandoff(             self, test_client: XdsTestClient,             previous_route_config_version: str, retry_wait_second: int,             timeout_second: int):+        logger.info(+            'comparing route config versions, previous_route_config_version: %s',+            previous_route_config_version)","oh it may not be completely the same, but I learned a while ago that surfacing the exception might be good as well.Plus we actually handle the situation by itself.```----------------------------------------------------------------------Traceback (most recent call last):  File ""/Users/zivy/src/grpc/tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py"", line 90, in test_api_listener    json_format.MessageToDict(raw_config))  File ""/Users/zivy/src/grpc/tools/run_tests/xds_k8s_test_driver/venv/lib/python3.6/site-packages/google/protobuf/json_format.py"", line 165, in MessageToDict    return printer._MessageToJsonObject(message)  File ""/Users/zivy/src/grpc/tools/run_tests/xds_k8s_test_driver/venv/lib/python3.6/site-packages/google/protobuf/json_format.py"", line 199, in _MessageToJsonObject    message_descriptor = message.DESCRIPTORAttributeError: 'NoneType' object has no attribute 'DESCRIPTOR'----------------------------------------------------------------------Ran 1 test in 465.016s```",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28745,796115203,2022-01-31T22:25:57Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -350,26 +353,13 @@ def assertXdsConfigExists(self, test_client: XdsTestClient):                      json_format.MessageToJson(config, indent=2))         self.assertSameElements(want, seen) -    @staticmethod-    def getRouteConfigVersion(test_client: XdsTestClient) -> Optional[str]:-        config = test_client.csds.fetch_client_status(log_level=logging.INFO)-        route_config_version = None-        for xds_config in config.xds_config:-            if xds_config.WhichOneof('per_xds_config') == ""route_config"":-                route_config = xds_config.route_config-                logger.info('Route config found: %s',-                            json_format.MessageToJson(route_config, indent=2))-                route_config_version = route_config.dynamic_route_configs[-                    0].version_info-                logger.info('found routing config version: %s',-                            route_config_version)-                break-        return route_config_version-     def assertRouteConfigUpdateTrafficHandoff(             self, test_client: XdsTestClient,             previous_route_config_version: str, retry_wait_second: int,             timeout_second: int):+        logger.info(+            'comparing route config versions, previous_route_config_version: %s',+            previous_route_config_version)","What I hope for is a clear log line when the client didn't return any xDS config. Surfacing the exception can be done via ""raise"", but we can help oncalls with an info log.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28745,796115935,2022-01-31T22:27:12Z,tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py,"@@ -83,8 +85,12 @@ def test_api_listener(self) -> None:          with self.subTest('13_test_server_received_rpcs_with_two_url_maps'):             self.assertSuccessfulRpcs(test_client)-            previous_route_config_version = self.getRouteConfigVersion(-                test_client)+            config = test_client.csds.fetch_client_status(+                log_level=logging.INFO)+            logger.info('received client config from CSDS, dump config: %s',+                        config)","The return value is a proto type ""ClientConfig"": https://github.com/grpc/grpc/blob/2d4f3c56001cd1e1f85734b2f7c5ce5f2797c38a/tools/run_tests/xds_k8s_test_driver/framework/rpc/grpc_csds.py#L50Did I miss something?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28746,796156280,2022-01-31T23:36:35Z,src/core/ext/xds/xds_channel_creds.h,"@@ -19,30 +19,45 @@  #include <grpc/support/port_platform.h> +#include <grpc/impl/codegen/grpc_types.h>+ #include ""src/core/lib/json/json.h""-#include ""src/core/lib/security/credentials/credentials.h""++struct grpc_channel_credentials;  namespace grpc_core { -class XdsChannelCredsImpl {+class XdsChannelCredsFactory {  public:-  virtual ~XdsChannelCredsImpl() {}+  virtual ~XdsChannelCredsFactory() {}   virtual absl::string_view creds_type() const = 0;   virtual bool IsValidConfig(const Json& config) const = 0;-  virtual RefCountedPtr<grpc_channel_credentials> CreateXdsChannelCreds(+  virtual grpc_channel_credentials* CreateXdsChannelCreds(","What is the circular dependency you're seeing?  I would expect that you can simply have the BUILD target for ""xds_channel_creds"" depend on ""ref_counted_ptr"".",
4111145,YifeiZhuang,https://api.github.com/repos/grpc/grpc/pulls/28745,796163508,2022-01-31T23:53:19Z,tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py,"@@ -83,8 +85,12 @@ def test_api_listener(self) -> None:          with self.subTest('13_test_server_received_rpcs_with_two_url_maps'):             self.assertSuccessfulRpcs(test_client)-            previous_route_config_version = self.getRouteConfigVersion(-                test_client)+            config = test_client.csds.fetch_client_status(+                log_level=logging.INFO)+            logger.info('received client config from CSDS, dump config: %s',+                        config)","this is the latest code. https://github.com/grpc/grpc/blob/master/tools/run_tests/xds_k8s_test_driver/tests/api_listener_test.py#L95I assume this line you commented on should be outdated, tho. I am confused.",
4111145,YifeiZhuang,https://api.github.com/repos/grpc/grpc/pulls/28745,796164701,2022-01-31T23:55:58Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -350,26 +353,13 @@ def assertXdsConfigExists(self, test_client: XdsTestClient):                      json_format.MessageToJson(config, indent=2))         self.assertSameElements(want, seen) -    @staticmethod-    def getRouteConfigVersion(test_client: XdsTestClient) -> Optional[str]:-        config = test_client.csds.fetch_client_status(log_level=logging.INFO)-        route_config_version = None-        for xds_config in config.xds_config:-            if xds_config.WhichOneof('per_xds_config') == ""route_config"":-                route_config = xds_config.route_config-                logger.info('Route config found: %s',-                            json_format.MessageToJson(route_config, indent=2))-                route_config_version = route_config.dynamic_route_configs[-                    0].version_info-                logger.info('found routing config version: %s',-                            route_config_version)-                break-        return route_config_version-     def assertRouteConfigUpdateTrafficHandoff(             self, test_client: XdsTestClient,             previous_route_config_version: str, retry_wait_second: int,             timeout_second: int):+        logger.info(+            'comparing route config versions, previous_route_config_version: %s',+            previous_route_config_version)","> What I hope for is a clear log line when the client didn't return any xDS config. Surfacing the exception can be done via ""raise"", but we can help oncalls with an info log.Thank you. I'm actually leaning towards what you are suggesting, but still, i guess it is minor.",
85715732,yousukseung,https://api.github.com/repos/grpc/grpc/pulls/28746,796168568,2022-02-01T00:05:05Z,src/core/ext/xds/xds_channel_creds.h,"@@ -19,30 +19,45 @@  #include <grpc/support/port_platform.h> +#include <grpc/impl/codegen/grpc_types.h>+ #include ""src/core/lib/json/json.h""-#include ""src/core/lib/security/credentials/credentials.h""++struct grpc_channel_credentials;  namespace grpc_core { -class XdsChannelCredsImpl {+class XdsChannelCredsFactory {  public:-  virtual ~XdsChannelCredsImpl() {}+  virtual ~XdsChannelCredsFactory() {}   virtual absl::string_view creds_type() const = 0;   virtual bool IsValidConfig(const Json& config) const = 0;-  virtual RefCountedPtr<grpc_channel_credentials> CreateXdsChannelCreds(+  virtual grpc_channel_credentials* CreateXdsChannelCreds(","That gave me the following error:./src/core/lib/gprpp/ref_counted_ptr.h:105:34: error: member access into incomplete type 'grpc_channel_credentials'    if (value_ != nullptr) value_->Unref();src/core/ext/xds/xds_channel_creds.cc:37:41: note: in instantiation of member function 'grpc_core::RefCountedPtr<grpc_channel_credentials>::~RefCountedPtr' requested here  if (iter == factories_.cend()) return nullptr;./src/core/ext/xds/xds_channel_creds.h:26:8: note: forward declaration of 'grpc_channel_credentials'struct grpc_channel_credentials;When I put RefCounted<grpc_channel_credentials> instead of grpc_channel_credentials, then it gives:./src/core/lib/gprpp/ref_counted.h:304:49: error: static_cast from 'grpc_core::RefCounted<grpc_channel_credentials, grpc_core::PolymorphicRefCount, grpc_core::kUnrefDelete> *' to 'grpc_channel_credentials *', which are not related by inheritance, is not allowed      internal::Delete<Child, UnrefBehaviorArg>(static_cast<Child*>(this));./src/core/lib/gprpp/ref_counted_ptr.h:105:36: note: in instantiation of member function 'grpc_core::RefCounted<grpc_channel_credentials, grpc_core::PolymorphicRefCount, grpc_core::kUnrefDelete>::Unref' requested here    if (value_ != nullptr) value_->Unref();// Same last two lines.",X
85715732,yousukseung,https://api.github.com/repos/grpc/grpc/pulls/28746,796175493,2022-02-01T00:22:30Z,src/core/ext/xds/xds_channel_creds.h,"@@ -19,30 +19,45 @@  #include <grpc/support/port_platform.h> +#include <grpc/impl/codegen/grpc_types.h>+ #include ""src/core/lib/json/json.h""-#include ""src/core/lib/security/credentials/credentials.h""++struct grpc_channel_credentials;  namespace grpc_core { -class XdsChannelCredsImpl {+class XdsChannelCredsFactory {  public:-  virtual ~XdsChannelCredsImpl() {}+  virtual ~XdsChannelCredsFactory() {}   virtual absl::string_view creds_type() const = 0;   virtual bool IsValidConfig(const Json& config) const = 0;-  virtual RefCountedPtr<grpc_channel_credentials> CreateXdsChannelCreds(+  virtual grpc_channel_credentials* CreateXdsChannelCreds(",BTW by the circular dependency I meant grpc_security_base for grpc_channel_credentials.config -> xds_channel_creds -> grpc_security_base -> grpc_base -> config,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28745,796188709,2022-02-01T00:56:14Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -350,6 +353,42 @@ def assertXdsConfigExists(self, test_client: XdsTestClient):                      json_format.MessageToJson(config, indent=2))         self.assertSameElements(want, seen) +    def assertRouteConfigUpdateTrafficHandoff(+            self, test_client: XdsTestClient,+            previous_route_config_version: str, retry_wait_second: int,+            timeout_second: int):+        retryer = retryers.constant_retryer(+            wait_fixed=datetime.timedelta(seconds=retry_wait_second),+            timeout=datetime.timedelta(seconds=timeout_second),+            retry_on_exceptions=(TdPropagationRetryableError,),+            logger=logger,+            log_level=logging.INFO)+        try:+            for attempt in retryer:+                with attempt:+                    self.assertSuccessfulRpcs(test_client)+                    raw_config = test_client.csds.fetch_client_status(+                        log_level=logging.INFO)+                    dumped_config = DumpedXdsConfig(",@YifeiZhuang Can we use `xds_url_map_testcase.DumpedXdsConfig` instead of `DumpedXdsConfig` here to avoid accessing the `DumpedXdsConfig` class during initialization stage? This can avoid the circular dependency issue we see in staging.,
85715732,yousukseung,https://api.github.com/repos/grpc/grpc/pulls/28746,796189728,2022-02-01T00:58:54Z,src/core/ext/xds/xds_channel_creds.h,"@@ -19,30 +19,45 @@  #include <grpc/support/port_platform.h> +#include <grpc/impl/codegen/grpc_types.h>+ #include ""src/core/lib/json/json.h""-#include ""src/core/lib/security/credentials/credentials.h""++struct grpc_channel_credentials;  namespace grpc_core { -class XdsChannelCredsImpl {+class XdsChannelCredsFactory {  public:-  virtual ~XdsChannelCredsImpl() {}+  virtual ~XdsChannelCredsFactory() {}   virtual absl::string_view creds_type() const = 0;   virtual bool IsValidConfig(const Json& config) const = 0;-  virtual RefCountedPtr<grpc_channel_credentials> CreateXdsChannelCreds(+  virtual grpc_channel_credentials* CreateXdsChannelCreds(","(Sorry, I guess I accidentally resolved this comment.)",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28722,797185898,2022-02-02T00:42:43Z,test/core/promise/sleep_test.cc,"@@ -0,0 +1,91 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""src/core/lib/promise/sleep.h""++#include <atomic>++#include <gmock/gmock.h>+#include <gtest/gtest.h>++#include ""absl/synchronization/notification.h""++#include <grpc/grpc.h>++#include ""src/core/lib/promise/race.h""+#include ""src/core/lib/promise/seq.h""+#include ""test/core/promise/test_wakeup_schedulers.h""++namespace grpc_core {+namespace {++TEST(Sleep, Zzzz) {",Consider using a fixture to share setup and teardown logic. Simple example: https://github.com/grpc/grpc/blob/63398540b65c775de36f1e7e370c1800aca6637d/test/cpp/server/server_builder_test.cc#L47-L53,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/28769,797841656,2022-02-02T17:19:18Z,src/core/lib/iomgr/work_serializer.cc,"@@ -175,8 +174,16 @@ void WorkSerializer::WorkSerializerImpl::DrainQueueOwned() {       // failure since we don't care about that value.",nit: this comment needs to be updated since we're no longer using relaxed load on failure?,X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/28769,797928103,2022-02-02T18:58:14Z,src/core/lib/iomgr/work_serializer.cc,"@@ -123,8 +123,7 @@ void WorkSerializer::WorkSerializerImpl::Orphan() {   }   uint64_t prev_ref_pair =       refs_.fetch_sub(MakeRefPair(0, 1), std::memory_order_acq_rel);-  if (GetSize(prev_ref_pair) == 1) {-    GPR_DEBUG_ASSERT(GetOwners(prev_ref_pair) == 0);+  if (GetSize(prev_ref_pair) == 1 && GetOwners(prev_ref_pair) == 0) {",nit: I think this may be more readable if the conditions are reversed to be in order of the layout of prev_ref_pairi.e. `if (GetOwners(prev_ref_pair) == 0 && GetSize(prev_ref_pair) == 1) {`,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28774,797999260,2022-02-02T20:46:55Z,src/core/lib/surface/call.cc,"@@ -1441,6 +1441,10 @@ static grpc_call_error call_start_batch(grpc_call* call, const grpc_op* ops,           error = GRPC_CALL_ERROR_INVALID_METADATA;           goto done_with_error;         }+        if (call->send_initial_metadata.get(grpc_core::TeMetadata()) ==",could we simplify this to `call->send_initial_metadata.Remove(grpc_core::TeMetadata());` - ie do it unilaterally,X
5564114,Vignesh2208,https://api.github.com/repos/grpc/grpc/pulls/28774,798003750,2022-02-02T20:53:39Z,src/core/lib/surface/call.cc,"@@ -1441,6 +1441,10 @@ static grpc_call_error call_start_batch(grpc_call* call, const grpc_op* ops,           error = GRPC_CALL_ERROR_INVALID_METADATA;           goto done_with_error;         }+        if (call->send_initial_metadata.get(grpc_core::TeMetadata()) ==",sure just for initial metadata or for both initial and trailing metadata ?,
85715732,yousukseung,https://api.github.com/repos/grpc/grpc/pulls/28746,798047944,2022-02-02T21:59:49Z,src/core/ext/xds/xds_channel_creds.h,"@@ -19,30 +19,45 @@  #include <grpc/support/port_platform.h> +#include <grpc/impl/codegen/grpc_types.h>+ #include ""src/core/lib/json/json.h""-#include ""src/core/lib/security/credentials/credentials.h""++struct grpc_channel_credentials;  namespace grpc_core { -class XdsChannelCredsImpl {+class XdsChannelCredsFactory {  public:-  virtual ~XdsChannelCredsImpl() {}+  virtual ~XdsChannelCredsFactory() {}   virtual absl::string_view creds_type() const = 0;   virtual bool IsValidConfig(const Json& config) const = 0;-  virtual RefCountedPtr<grpc_channel_credentials> CreateXdsChannelCreds(+  virtual grpc_channel_credentials* CreateXdsChannelCreds(","Updated, finally could make it work with templates.I blocked instantiating the factory/registry with other types. But new factories will have to extend XdsChannelCredsFactory<> now and I guess we cannot remove '<>' until we have CTAD in C++17. PTAL.",X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/28768,798088250,2022-02-02T23:08:13Z,tools/dockerfile/test/binder_transport_apk/Dockerfile,"@@ -15,58 +15,24 @@ # Pinned version of the base image is used to avoid regressions caused # by rebuilding of this docker image. To see available versions, you can run # ""gcloud container images list-tags gcr.io/oss-fuzz-base/base-builder""-# Image(c7f1523ebd92) is built on Jul 29, 2021-FROM gcr.io/oss-fuzz-base/base-builder@sha256:c7f1523ebd9234b9ff57e5240f8c06569143373be019c92f1e6df18a1e048f37+# Image(5eceb81f5759) is built on Jan 31, 2022+FROM gcr.io/oss-fuzz-base/base-builder@sha256:5eceb81f57599d63ca7c9a70c8968b23b128119699626ca749017019eb0b523f  # -------------------------- WARNING -------------------------------------- # If you are making changes to this file, consider changing # https://github.com/google/oss-fuzz/blob/master/projects/grpc/Dockerfile # accordingly. # ------------------------------------------------------------------------- -# Install basic packages and Bazel dependencies.-RUN apt-get update && apt-get install -y software-properties-common python-software-properties-RUN add-apt-repository ppa:webupd8team/java+# Install basic packages RUN apt-get update && apt-get -y install \   autoconf \   build-essential \   curl \-  wget \   libtool \   make \-  openjdk-8-jdk \-  vim--#====================-# Python dependencies--# Install dependencies-# TODO(jtattermusch): This installs python3.5. Is it even needed-# when we install python3.6 in the next step?-RUN apt-get update && apt-get install -y \-    python3-all-dev--#=================-# Compile CPython 3.6.9 from source--RUN apt-get update && apt-get install -y zlib1g-dev libssl-dev && apt-get clean-RUN apt-get update && apt-get install -y jq build-essential libffi-dev && apt-get clean--RUN cd /tmp && \-    wget -q https://www.python.org/ftp/python/3.6.9/Python-3.6.9.tgz && \-    tar xzvf Python-3.6.9.tgz && \-    cd Python-3.6.9 && \-    ./configure && \-    make -j4 && \-    make install--RUN cd /tmp && \-    echo ""ff7cdaef4846c89c1ec0d7b709bbd54d Python-3.6.9.tgz"" > checksum.md5 && \-    md5sum -c checksum.md5--RUN python3.6 -m ensurepip && \-    python3.6 -m pip install coverage-+  vim \+  wget ",I checked the history and python isn't actually needed. It seemed to be added accidentally from the clean-up.,
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/28544,801891085,2022-02-08T17:32:50Z,bazel/copts.bzl,"@@ -56,7 +56,28 @@ GRPC_LLVM_WARNING_FLAGS = [     ""-Wno-unused-function"", ] +# Warning suppression for MSVC on Windows.+# The list of suppressed warnings should be kept in sync with the cmake build.+GRPC_MSVC_DEFAULT_COPTS = [","Could you add comments helpful to see what these warnings are for? something like```    ""/wd4146"" # unary minus operator applied to unsigned type, result still unsigned    ""/wd4200"" # nonstandard extension used : zero-sized array in struct/union```You can see a list of msvc warnings [here](https://docs.microsoft.com/en-us/cpp/error-messages/compiler-warnings/compiler-warnings-c4000-c5999?view=msvc-170)",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28746,802050959,2022-02-08T20:56:23Z,src/core/ext/xds/xds_channel_creds.h,"@@ -19,30 +19,45 @@  #include <grpc/support/port_platform.h> +#include <grpc/impl/codegen/grpc_types.h>+ #include ""src/core/lib/json/json.h""-#include ""src/core/lib/security/credentials/credentials.h""++struct grpc_channel_credentials;  namespace grpc_core { -class XdsChannelCredsImpl {+class XdsChannelCredsFactory {  public:-  virtual ~XdsChannelCredsImpl() {}+  virtual ~XdsChannelCredsFactory() {}   virtual absl::string_view creds_type() const = 0;   virtual bool IsValidConfig(const Json& config) const = 0;-  virtual RefCountedPtr<grpc_channel_credentials> CreateXdsChannelCreds(+  virtual grpc_channel_credentials* CreateXdsChannelCreds(","Ah, okay, I see -- the problem is not depending on `RefCountedPtr<>`, but rather the fact that `RefCountedPtr<>` requires the implementation of `grpc_channel_credentials`, which is a larger dependency that we don't want to drag in here.It's probably better to just use `grpc_channel_credentials*` instead of `RefCountedPtr<grpc_channel_credentials>`.  That's probably simpler than this templating approach.",
303201,JamesNK,https://api.github.com/repos/grpc/grpc/pulls/27886,802082407,2022-02-08T21:41:17Z,src/csharp/Grpc.Core.Api/IAsyncStreamWriter.cs,"@@ -36,6 +37,18 @@ public interface IAsyncStreamWriter<in T>         /// <param name=""message"">The message to be written. Cannot be null.</param>         Task WriteAsync(T message); +#if NETSTANDARD2_1_OR_GREATER+        /// <summary>+        /// Writes a message asynchronously. Only one write can be pending at a time.+        /// </summary>+        /// <param name=""message"">The message to be written. Cannot be null.</param>+        /// <param name=""cancellationToken"">Cancellation token that can be used to cancel the operation.</param>+        Task WriteAsync(T message, CancellationToken cancellationToken)",> Grpc.CoreWould Grpc.Core cancellation be supported with this code:https://github.com/grpc/grpc/blob/84101427d0a699e65837af43f4f0ac326c7a2aca/src/csharp/Grpc.Core/Internal/AsyncCallBase.cs#L393-L399It is used by `IAsyncStreamReader<T>.MoveNext` which takes a cancellation token:https://github.com/grpc/grpc/blob/2d4f3c56001cd1e1f85734b2f7c5ce5f2797c38a/src/csharp/Grpc.Core/Internal/ServerRequestStream.cs#L50-L58It seems to me that what works for canceling while reading should also work for canceling while writing.,X
85715732,yousukseung,https://api.github.com/repos/grpc/grpc/pulls/28746,802084054,2022-02-08T21:43:42Z,src/core/lib/security/credentials/xds/xds_channel_default_creds.cc,"@@ -1,5 +1,6 @@ //-// Copyright 2019 gRPC authors.+//+// Copyright 2022 gRPC authors.","Craig asked the same question actually. This is a new file that I added last month where I accidentally put 2019, and I'm fixing here. Is that fine?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28773,802088849,2022-02-08T21:50:48Z,test/cpp/end2end/client_lb_end2end_test.cc,"@@ -798,6 +798,38 @@ TEST_F(ClientLbEnd2endTest, PickFirstUpdateSuperset) {   EXPECT_EQ(""pick_first"", channel->GetLoadBalancingPolicyName()); } +TEST_F(ClientLbEnd2endTest, PickFirstUpdateToUnconnected) {+  const int kNumServers = 4;","There's not actually any need to create servers that we never need to start.  Instead, you can just push the result of `grpc_pick_unused_port_or_die()` onto `ports` when you want a server that won't respond.In fact, it's not clear to me that we actually need more than one server here.  I think we could just start one server and have the resolver first send that server's address, then send `grpc_pick_unused_port_or_die()`, then send that same server's address again.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28773,802092391,2022-02-08T21:55:59Z,test/cpp/end2end/client_lb_end2end_test.cc,"@@ -798,6 +798,38 @@ TEST_F(ClientLbEnd2endTest, PickFirstUpdateSuperset) {   EXPECT_EQ(""pick_first"", channel->GetLoadBalancingPolicyName()); } +TEST_F(ClientLbEnd2endTest, PickFirstUpdateToUnconnected) {+  const int kNumServers = 4;+  CreateServers(kNumServers);+  auto response_generator = BuildResolverResponseGenerator();+  auto channel = BuildChannel(""pick_first"", response_generator);+  auto stub = BuildStub(channel);++  std::vector<int> ports;++  // Perform try to send rpc against a list where one server is available.+  ports.emplace_back(servers_[0]->port_);+  ports.emplace_back(servers_[1]->port_);+  response_generator.SetNextResolution(ports);+  gpr_log(GPR_INFO, ""****** SET [0, 1] *******"");+  CheckRpcSendFailure(stub);","Is it actually necessary to have an initially failing server list?  It seems like the bug happens when there is a valid server, followed by having no valid servers, followed by having a valid server again.  It's not clear to me what is added by having this failure initially.  I think we should be able to start out by just starting the server and then calling `CheckRpcSendOk()`.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28786,802146705,2022-02-08T23:31:27Z,templates/tools/dockerfile/test/cxx_gcc_5_x64/Dockerfile.template,"@@ -14,7 +14,7 @@   # See the License for the specific language governing permissions and   # limitations under the License.   -  FROM gcc:4.9+  FROM gcc:5",Shall we make this `gcc:5.1`? The `gcc:5` tag is currently pointed at version 5.5.0 https://hub.docker.com/layers/gcc/library/gcc/5/images/sha256-37139c6da088e06151617e741dc1dc62adefa3a4c9b9d28fc29ac83e629b4575?context=explore,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28746,802833176,2022-02-09T16:09:57Z,BUILD,"@@ -1467,10 +1467,10 @@ grpc_cc_library( )  grpc_cc_library(-    name = ""xds_channel_creds"",+    name = ""channel_creds"",     language = ""c++"",     public_hdrs = [-        ""src/core/lib/security/credentials/xds/xds_channel_creds.h"",+        ""src/core/lib/security/credentials/channel_creds.h"",",Suggest renaming this file to `channel_creds_registry.h`.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28746,802839636,2022-02-09T16:15:51Z,BUILD,"@@ -2942,6 +2956,21 @@ grpc_cc_library(     ], ) +grpc_cc_library(+    name = ""grpc_channel_default_creds"",",Suggest renaming this to `grpc_channel_creds_registry_init`.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28746,802839927,2022-02-09T16:16:08Z,BUILD,"@@ -2942,6 +2956,21 @@ grpc_cc_library(     ], ) +grpc_cc_library(+    name = ""grpc_channel_default_creds"",+    srcs = [+        ""src/core/lib/security/credentials/channel_default_creds.cc"",",Suggest renaming this file to `channel_creds_registry_init.cc`.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28746,802842644,2022-02-09T16:18:29Z,BUILD,"@@ -413,6 +413,7 @@ GRPC_XDS_TARGETS = [     ""grpc_lb_policy_xds_cluster_resolver"",     ""grpc_resolver_xds"",     ""grpc_resolver_c2p"",+    ""grpc_channel_default_creds"",","Please move this to the end of the list, with a comment indicating that it is not really xDS-specific but currently not used by anything except xDS, which is why it's here.  If something else needs to use it in the future, it can be moved into the grpc_channel_creds_registry library.",X
4431052,scwhittle,https://api.github.com/repos/grpc/grpc/pulls/28824,802932402,2022-02-09T17:45:16Z,src/core/ext/filters/client_idle/idle_filter_state.cc,"@@ -18,6 +18,8 @@  #include <assert.h> +#include <stdint>","Oops, I didn't mean to include this, I was having unrelated build errors.  This file was the first the build broke on but it seems to be something wrong with my bazel setup (or building internal to google) since other files also had warnings.$ bazel test //test/cpp/end2end:client_lb_end2end_test...ERROR: /usr/local/google/home/samuelw/grpc/BUILD:1688:16: Compiling src/core/lib/event_engine/sockaddr.cc failed: undeclared inclusion(s) in rule '//:grpc_sockaddr':this rule is missing dependency declarations for the following files included by 'src/core/lib/event_engine/sockaddr.cc':  '/usr/lib/clang/13.0.1/include/stdint.h'",X
85715732,yousukseung,https://api.github.com/repos/grpc/grpc/pulls/28746,802984086,2022-02-09T18:46:33Z,BUILD,"@@ -1467,10 +1467,10 @@ grpc_cc_library( )  grpc_cc_library(-    name = ""xds_channel_creds"",+    name = ""channel_creds"",","Done.> This looks pretty good! Just a few organizational comments remaining.> > For future reference, please do not rebase or force-push in the middle of a review, since that makes it very hard for the reviewer to track the changes. If you really need to merge in changes from master in the middle of a review, just merge from master into your branch and push that merge to github.> > Thanks!Sure, I had to merge #28819 due to failing tests. So I guess the merge commit created after a rebase will go away when I squash and commit later, will avoid force-pushing definitely.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28092,803137451,2022-02-09T22:20:05Z,tools/dockerfile/grpc_artifact_python_musllinux_1_1_x86/Dockerfile,"@@ -0,0 +1,23 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++FROM quay.io/pypa/musllinux_1_1_i686:2021-11-15-a808c18++###################################+# Install Python build requirements+RUN /opt/python/cp36-cp36m/bin/pip install --upgrade cython+RUN /opt/python/cp37-cp37m/bin/pip install --upgrade cython+RUN /opt/python/cp38-cp38/bin/pip install --upgrade cython+RUN /opt/python/cp39-cp39/bin/pip install --upgrade cython+RUN /opt/python/cp310-cp310/bin/pip install --upgrade cython","Done. Based on code, the ccache is enabled by `source tools/internal_ci/helper_scripts/prepare_ccache_rc` in `grpc_build_artifacts.sh`. It should technically be enabled.",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/28727,803181950,2022-02-09T23:40:54Z,examples/cpp/helloworld/greeter_server_ssl.cc,"@@ -0,0 +1,102 @@+/*+ *+ * Copyright 2022 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <iostream>+#include <memory>+#include <string>++#include <grpcpp/ext/proto_server_reflection_plugin.h>+#include <grpcpp/grpcpp.h>+#include <grpcpp/health_check_service_interface.h>++#ifdef BAZEL_BUILD+#include ""examples/protos/helloworld.grpc.pb.h""+#else+#include ""helloworld.grpc.pb.h""+#endif+#include ""greeter_utils.h""++using ::grpc::Server;+using ::grpc::ServerBuilder;+using ::grpc::ServerContext;+using ::grpc::Status;++using ::helloworld::Greeter;+using ::helloworld::HelloReply;+using ::helloworld::HelloRequest;++// Logic and data behind the server's behavior.+class GreeterServiceImpl final : public Greeter::Service {+  Status SayHello(ServerContext* context, const HelloRequest* request,+                  HelloReply* reply) override {+    std::string prefix{""Hello ""};++    reply->set_message(prefix + request->name());++    return Status::OK;+  }+};++void run_server() {+  std::string server_address{""localhost:50051""};++  grpc::EnableDefaultHealthCheckService(true);+  grpc::reflection::InitProtoReflectionServerBuilderPlugin();++  ServerBuilder builder;++  // Use the gen_certs.sh for the generation of required certificates+  // [!] Be carefull here using a server.crt with the CN != localhost [!]+  std::string key, cert, root;++  read(""server.crt"", cert);+  read(""server.key"", key);+  read(""ca.crt"", root);++  // Configure SSL options+  grpc::SslServerCredentialsOptions::PemKeyCertPair keycert = {key, cert};+  grpc::SslServerCredentialsOptions sslOps;","The `Ssl*Credentials` is the old credential type we support. It's good to have them in our examples, but for any new features we added to our security API, we'd add it to `Tls*Credentials`.Are you interested in adding `Tls*Credentials` example as well? It would be very similar to this one. As a reference for its usage, here is the e2e test for `Tls*Credentials`: https://github.com/grpc/grpc/pull/28456(not merged yet, but likely it will soon)",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28839,803884320,2022-02-10T16:50:36Z,src/core/ext/filters/http/client_authority_filter.cc,"@@ -71,7 +71,7 @@ ArenaPromise<TrailingMetadata> ClientAuthorityFilter::MakeCallPromise(  namespace { const grpc_channel_filter grpc_client_authority_filter =-    MakePromiseBasedFilter<ClientAuthorityFilter>();+    MakePromiseBasedFilter<ClientAuthorityFilter, true>(""authority"");","How about defining an enum for `kIsClient` instead of a bool, so that call sites are self-documenting w.r.t. this parameter?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28092,804009918,2022-02-10T18:53:23Z,tools/run_tests/artifacts/artifact_targets.py,"@@ -444,6 +460,16 @@ def targets():         PythonArtifact('linux_extra', 'armv7', 'cp38-cp38'),         PythonArtifact('linux_extra', 'armv7', 'cp39-cp39'),         PythonArtifact('linux_extra', 'armv7', 'cp310-cp310', presubmit=True),+        PythonArtifact('musllinux_1_1', 'x64', 'cp310-cp310', presubmit=True),+        PythonArtifact('musllinux_1_1', 'x64', 'cp36-cp36m', presubmit=True),+        PythonArtifact('musllinux_1_1', 'x64', 'cp37-cp37m'),+        PythonArtifact('musllinux_1_1', 'x64', 'cp38-cp38'),+        PythonArtifact('musllinux_1_1', 'x64', 'cp39-cp39'),+        PythonArtifact('musllinux_1_1', 'x86', 'cp310-cp310', presubmit=True),",qq: do we really need support for 32bit musl linux? I'd have hoped that 32bit is long dead.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804082902,2022-02-10T20:34:00Z,src/core/lib/channel/promise_based_filter.h,"@@ -38,19 +39,34 @@ namespace grpc_core { namespace promise_filter_detail {  // Call data shared between all implementations of promise-based filters.-class BaseCallData {+class BaseCallData : public Activity, private Wakeable {","Inheritence should be public, not private.  Can we avoid this by making it a data member instead of inheritting?",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28092,804099879,2022-02-10T20:57:47Z,tools/run_tests/artifacts/artifact_targets.py,"@@ -444,6 +460,16 @@ def targets():         PythonArtifact('linux_extra', 'armv7', 'cp38-cp38'),         PythonArtifact('linux_extra', 'armv7', 'cp39-cp39'),         PythonArtifact('linux_extra', 'armv7', 'cp310-cp310', presubmit=True),+        PythonArtifact('musllinux_1_1', 'x64', 'cp310-cp310', presubmit=True),+        PythonArtifact('musllinux_1_1', 'x64', 'cp36-cp36m', presubmit=True),+        PythonArtifact('musllinux_1_1', 'x64', 'cp37-cp37m'),+        PythonArtifact('musllinux_1_1', 'x64', 'cp38-cp38'),+        PythonArtifact('musllinux_1_1', 'x64', 'cp39-cp39'),+        PythonArtifact('musllinux_1_1', 'x86', 'cp310-cp310', presubmit=True),","Personally, I hope so. But PyPA is releasing 32bit musllinux images, if the upstream still supports it, I think we should try. If the usage data proofs 32bit musllinux is not really used, I can remove them.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804107871,2022-02-10T21:07:44Z,src/core/lib/channel/promise_based_filter.h,"@@ -196,9 +228,23 @@ class CallData<ChannelFilter, true> : public BaseCallData {       if (recv_trailing_state_ == RecvTrailingState::kQueued) {         recv_trailing_state_ = RecvTrailingState::kCancelled;","Now that we're no longer yielding the call combiner here, we can set this just once for both cases, rather than doing it separately here and below.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804111624,2022-02-10T21:13:11Z,src/core/lib/channel/promise_based_filter.h,"@@ -325,39 +374,137 @@ class CallData<ChannelFilter, true> : public BaseCallData {     GPR_ASSERT(!is_polling_);     grpc_closure* call_closure = nullptr;     is_polling_ = true;+    grpc_error_handle cancel_send_initial_metadata_error = GRPC_ERROR_NONE;+    grpc_transport_stream_op_batch* forward_batch = nullptr;     switch (send_initial_state_) {       case SendInitialState::kQueued:       case SendInitialState::kForwarded: {         // Poll the promise once since we're waiting for it.-        Poll<TrailingMetadata> poll = promise_();+        Poll<TrailingMetadata> poll;+        {+          ScopedActivity activity(this);+          poll = promise_();+        }         if (auto* r = absl::get_if<TrailingMetadata>(&poll)) {-          GPR_ASSERT(recv_trailing_state_ == RecvTrailingState::kComplete);-          GPR_ASSERT(recv_trailing_metadata_ == UnwrapMetadata(std::move(*r)));-          recv_trailing_state_ = RecvTrailingState::kResponded;-          call_closure =-              absl::exchange(original_recv_trailing_metadata_ready_, nullptr);+          promise_ = ArenaPromise<TrailingMetadata>();+          auto* md = UnwrapMetadata(std::move(*r));+          bool destroy_md = true;+          switch (recv_trailing_state_) {+            case RecvTrailingState::kComplete:+              if (recv_trailing_metadata_ != md) {+                *recv_trailing_metadata_ = std::move(*md);+              } else {+                destroy_md = false;+              }+              recv_trailing_state_ = RecvTrailingState::kResponded;+              call_closure = absl::exchange(+                  original_recv_trailing_metadata_ready_, nullptr);+              break;+            case RecvTrailingState::kQueued:+            case RecvTrailingState::kForwarded: {+              GPR_ASSERT(*md->get_pointer(GrpcStatusMetadata()) !=+                         GRPC_STATUS_OK);+              grpc_error_handle error = grpc_error_set_int(+                  GRPC_ERROR_CREATE_FROM_STATIC_STRING(+                      ""early return from promise based filter""),+                  GRPC_ERROR_INT_GRPC_STATUS,+                  *md->get_pointer(GrpcStatusMetadata()));+              if (auto* message = md->get_pointer(GrpcMessageMetadata())) {+                error = grpc_error_set_str(error, GRPC_ERROR_STR_GRPC_MESSAGE,+                                           message->as_string_view());+              }+              if (recv_trailing_state_ == RecvTrailingState::kQueued) {+                GPR_ASSERT(send_initial_state_ == SendInitialState::kQueued);+                send_initial_state_ = SendInitialState::kCancelled;+                cancel_send_initial_metadata_error = error;+              } else {+                forward_batch =","Isn't it the case that the only time a filter will return a failure is when it is actually processing some op, either being started on the way down the stack or being completed on the way back up?  If so, it seems like there should always be some completion callback that we can return with an error instead of starting our own cancellation op.If we can make that work, I think it would be a better approach, for two reasons:1. Returning an error for the batch completion is how filters do this today, so by using the expected code-path for this, we're less likely to trigger a bug.  (For example, the code in surface/call.cc knows to send only one cancellation batch for the call, but we're bypassing that here.  I don't know what bugs that might tickle down the road.)2. Generating our own cancellation is actually a bit tricky, and we already have too many places in the code-base where we do this.  At minimum, before starting the batch, we need to call `call_combiner->Cancel()`, which will notify any filters downstream of us to cancel any async processing that they may be doing while holding the call combiner, so that the cancellation can go through in a timely manner.  (Yes, I know this is awful; I did a bunch of work toward fixing it in #26172, but I'm no longer pursuing that since it's being superceded by the promise work.)",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804117018,2022-02-10T21:21:28Z,src/core/lib/channel/promise_based_filter.h,"@@ -325,39 +374,137 @@ class CallData<ChannelFilter, true> : public BaseCallData {     GPR_ASSERT(!is_polling_);     grpc_closure* call_closure = nullptr;     is_polling_ = true;+    grpc_error_handle cancel_send_initial_metadata_error = GRPC_ERROR_NONE;+    grpc_transport_stream_op_batch* forward_batch = nullptr;     switch (send_initial_state_) {       case SendInitialState::kQueued:       case SendInitialState::kForwarded: {         // Poll the promise once since we're waiting for it.-        Poll<TrailingMetadata> poll = promise_();+        Poll<TrailingMetadata> poll;+        {+          ScopedActivity activity(this);+          poll = promise_();+        }         if (auto* r = absl::get_if<TrailingMetadata>(&poll)) {-          GPR_ASSERT(recv_trailing_state_ == RecvTrailingState::kComplete);-          GPR_ASSERT(recv_trailing_metadata_ == UnwrapMetadata(std::move(*r)));-          recv_trailing_state_ = RecvTrailingState::kResponded;-          call_closure =-              absl::exchange(original_recv_trailing_metadata_ready_, nullptr);+          promise_ = ArenaPromise<TrailingMetadata>();+          auto* md = UnwrapMetadata(std::move(*r));+          bool destroy_md = true;+          switch (recv_trailing_state_) {+            case RecvTrailingState::kComplete:+              if (recv_trailing_metadata_ != md) {+                *recv_trailing_metadata_ = std::move(*md);+              } else {+                destroy_md = false;+              }+              recv_trailing_state_ = RecvTrailingState::kResponded;+              call_closure = absl::exchange(+                  original_recv_trailing_metadata_ready_, nullptr);+              break;+            case RecvTrailingState::kQueued:+            case RecvTrailingState::kForwarded: {+              GPR_ASSERT(*md->get_pointer(GrpcStatusMetadata()) !=+                         GRPC_STATUS_OK);+              grpc_error_handle error = grpc_error_set_int(+                  GRPC_ERROR_CREATE_FROM_STATIC_STRING(+                      ""early return from promise based filter""),+                  GRPC_ERROR_INT_GRPC_STATUS,+                  *md->get_pointer(GrpcStatusMetadata()));+              if (auto* message = md->get_pointer(GrpcMessageMetadata())) {+                error = grpc_error_set_str(error, GRPC_ERROR_STR_GRPC_MESSAGE,+                                           message->as_string_view());+              }+              if (recv_trailing_state_ == RecvTrailingState::kQueued) {+                GPR_ASSERT(send_initial_state_ == SendInitialState::kQueued);+                send_initial_state_ = SendInitialState::kCancelled;+                cancel_send_initial_metadata_error = error;+              } else {+                forward_batch =+                    grpc_make_transport_stream_op(GRPC_CLOSURE_CREATE(+                        [](void*, grpc_error_handle) {}, nullptr, nullptr));+                forward_batch->cancel_stream = true;+                forward_batch->payload->cancel_stream.cancel_error = error;+              }+              recv_trailing_state_ = RecvTrailingState::kCancelled;+            } break;+            case RecvTrailingState::kInitial:+              abort();  // unimplemented+            case RecvTrailingState::kResponded:+            case RecvTrailingState::kCancelled:+              abort();  // unreachable+          }+          if (destroy_md) {+            md->~grpc_metadata_batch();+          }         }       } break;       case SendInitialState::kInitial:       case SendInitialState::kCancelled:-        // If we get a response without sending anything, we just propagate that-        // up. (note: that situation isn't possible once we finish the promise-        // transition).+        // If we get a response without sending anything, we just propagate+        // that up. (note: that situation isn't possible once we finish the+        // promise transition).         if (recv_trailing_state_ == RecvTrailingState::kComplete) {           recv_trailing_state_ = RecvTrailingState::kResponded;           call_closure =               absl::exchange(original_recv_trailing_metadata_ready_, nullptr);         }         break;     }+    GRPC_CALL_STACK_REF(call_stack(), ""finish_poll"");     is_polling_ = false;+    bool in_combiner = true;+    if (forward_batch != nullptr) {+      in_combiner = false;+      forward_send_initial_metadata_ = false;+      grpc_call_next_op(elem(), forward_batch);","Any of these calls to `grpc_call_next_op()`, `grpc_transport_stream_op_batch_finish_with_failure()`, or invoking `call_closure` will result in yielding the call combiner.  That means that we can execute at most one of them inline here.  However, they are each triggered by independent `if` statements, with no `else`es to ensure that they are mutually exclusive.  Do we have some guarantee that at most one of these will be executed?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804119430,2022-02-10T21:24:56Z,src/core/lib/channel/promise_based_filter.h,"@@ -325,39 +374,137 @@ class CallData<ChannelFilter, true> : public BaseCallData {     GPR_ASSERT(!is_polling_);     grpc_closure* call_closure = nullptr;     is_polling_ = true;+    grpc_error_handle cancel_send_initial_metadata_error = GRPC_ERROR_NONE;+    grpc_transport_stream_op_batch* forward_batch = nullptr;     switch (send_initial_state_) {       case SendInitialState::kQueued:       case SendInitialState::kForwarded: {         // Poll the promise once since we're waiting for it.-        Poll<TrailingMetadata> poll = promise_();+        Poll<TrailingMetadata> poll;+        {+          ScopedActivity activity(this);+          poll = promise_();+        }         if (auto* r = absl::get_if<TrailingMetadata>(&poll)) {-          GPR_ASSERT(recv_trailing_state_ == RecvTrailingState::kComplete);-          GPR_ASSERT(recv_trailing_metadata_ == UnwrapMetadata(std::move(*r)));-          recv_trailing_state_ = RecvTrailingState::kResponded;-          call_closure =-              absl::exchange(original_recv_trailing_metadata_ready_, nullptr);+          promise_ = ArenaPromise<TrailingMetadata>();+          auto* md = UnwrapMetadata(std::move(*r));+          bool destroy_md = true;+          switch (recv_trailing_state_) {+            case RecvTrailingState::kComplete:+              if (recv_trailing_metadata_ != md) {+                *recv_trailing_metadata_ = std::move(*md);+              } else {+                destroy_md = false;+              }+              recv_trailing_state_ = RecvTrailingState::kResponded;+              call_closure = absl::exchange(+                  original_recv_trailing_metadata_ready_, nullptr);+              break;+            case RecvTrailingState::kQueued:+            case RecvTrailingState::kForwarded: {+              GPR_ASSERT(*md->get_pointer(GrpcStatusMetadata()) !=+                         GRPC_STATUS_OK);+              grpc_error_handle error = grpc_error_set_int(+                  GRPC_ERROR_CREATE_FROM_STATIC_STRING(+                      ""early return from promise based filter""),+                  GRPC_ERROR_INT_GRPC_STATUS,+                  *md->get_pointer(GrpcStatusMetadata()));+              if (auto* message = md->get_pointer(GrpcMessageMetadata())) {+                error = grpc_error_set_str(error, GRPC_ERROR_STR_GRPC_MESSAGE,+                                           message->as_string_view());+              }+              if (recv_trailing_state_ == RecvTrailingState::kQueued) {+                GPR_ASSERT(send_initial_state_ == SendInitialState::kQueued);+                send_initial_state_ = SendInitialState::kCancelled;+                cancel_send_initial_metadata_error = error;+              } else {+                forward_batch =+                    grpc_make_transport_stream_op(GRPC_CLOSURE_CREATE(+                        [](void*, grpc_error_handle) {}, nullptr, nullptr));+                forward_batch->cancel_stream = true;+                forward_batch->payload->cancel_stream.cancel_error = error;+              }+              recv_trailing_state_ = RecvTrailingState::kCancelled;+            } break;+            case RecvTrailingState::kInitial:+              abort();  // unimplemented+            case RecvTrailingState::kResponded:+            case RecvTrailingState::kCancelled:+              abort();  // unreachable+          }+          if (destroy_md) {+            md->~grpc_metadata_batch();+          }         }       } break;       case SendInitialState::kInitial:       case SendInitialState::kCancelled:-        // If we get a response without sending anything, we just propagate that-        // up. (note: that situation isn't possible once we finish the promise-        // transition).+        // If we get a response without sending anything, we just propagate+        // that up. (note: that situation isn't possible once we finish the+        // promise transition).         if (recv_trailing_state_ == RecvTrailingState::kComplete) {           recv_trailing_state_ = RecvTrailingState::kResponded;           call_closure =               absl::exchange(original_recv_trailing_metadata_ready_, nullptr);         }         break;     }+    GRPC_CALL_STACK_REF(call_stack(), ""finish_poll"");     is_polling_ = false;+    bool in_combiner = true;+    if (forward_batch != nullptr) {+      in_combiner = false;+      forward_send_initial_metadata_ = false;+      grpc_call_next_op(elem(), forward_batch);+    }+    if (cancel_send_initial_metadata_error != GRPC_ERROR_NONE) {+      GPR_ASSERT(in_combiner);+      forward_send_initial_metadata_ = false;+      in_combiner = false;+      grpc_transport_stream_op_batch_finish_with_failure(+          absl::exchange(send_initial_metadata_batch_, nullptr),+          cancel_send_initial_metadata_error, call_combiner());+    }     if (absl::exchange(forward_send_initial_metadata_, false)) {+      GPR_ASSERT(in_combiner);+      in_combiner = false;       grpc_call_next_op(elem(),                         absl::exchange(send_initial_metadata_batch_, nullptr));     }     if (call_closure != nullptr) {+      GPR_ASSERT(in_combiner);+      in_combiner = false;       Closure::Run(DEBUG_LOCATION, call_closure, GRPC_ERROR_NONE);     }+    if (absl::exchange(repoll_, false)) {","If we're no longer holding the call combiner here, is it safe to access `repoll_`?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804187317,2022-02-10T22:34:07Z,src/core/lib/promise/activity.h,"@@ -332,15 +330,15 @@ class ActivityContexts : public ContextHolder<Contexts>... { // invoked, and that a given activity will not be concurrently scheduled again // until its RunScheduledWakeup() has been invoked. template <class F, class WakeupScheduler, class OnDone, typename... Contexts>-class PromiseActivity final : public Activity,+class PromiseActivity final : public FreestandingActivity,                               private ActivityContexts<Contexts...> {","Inheritence should be public.  Can we use a data member instead of inheritting?  (I realize in this case that it's an arbitrary number of contexts, but I'm thinking that your table code might be able to handle that.)",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804200011,2022-02-10T22:48:28Z,src/core/lib/security/credentials/iam/iam_credentials.cc,"@@ -28,27 +28,22 @@ #include <grpc/support/sync.h>  #include ""src/core/lib/gprpp/ref_counted_ptr.h""+#include ""src/core/lib/promise/promise.h"" #include ""src/core/lib/surface/api_trace.h"" -bool grpc_google_iam_credentials::get_request_metadata(-    grpc_polling_entity* /*pollent*/, grpc_auth_metadata_context /*context*/,-    grpc_core::CredentialsMetadataArray* md_array,-    grpc_closure* /*on_request_metadata*/, grpc_error_handle* /*error*/) {+grpc_core::ArenaPromise<absl::StatusOr<grpc_core::ClientInitialMetadata>>+grpc_google_iam_credentials::GetRequestMetadata(+    grpc_core::ClientInitialMetadata initial_metadata,+    grpc_core::AuthMetadataContext*) {   if (token_.has_value()) {-    md_array->emplace_back(grpc_core::Slice::FromStaticString(-                               GRPC_IAM_AUTHORIZATION_TOKEN_METADATA_KEY),-                           token_->Ref());+    initial_metadata->Append(+        GRPC_IAM_AUTHORIZATION_TOKEN_METADATA_KEY, token_->Ref(),+        [](absl::string_view, const grpc_core::Slice&) { abort(); });","Remind me what situations this will crash in?  Just want to make sure we're not introducing any new failure modes (e.g., where we crash if a user manually specifies that metadata key).",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804206125,2022-02-10T22:55:36Z,src/core/lib/security/transport/auth_filters.h,"@@ -24,14 +24,55 @@ #include <grpc/grpc_security.h>  #include ""src/core/lib/channel/channel_stack.h""+#include ""src/core/lib/security/credentials/credentials.h""+#include ""src/core/lib/security/security_connector/security_connector.h""+#include ""src/core/lib/transport/transport.h""  extern const grpc_channel_filter grpc_client_auth_filter; extern const grpc_channel_filter grpc_server_auth_filter; -void grpc_auth_metadata_context_build(-    const char* url_scheme, const grpc_slice& call_host,-    const grpc_slice& call_method, grpc_auth_context* auth_context,-    grpc_auth_metadata_context* auth_md_context);+namespace grpc_core {++class ClientAuthFilter final : private AuthMetadataContext {",Inheritence should be public.  Can we instead define a nested class that implements this interface?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804245691,2022-02-10T23:44:57Z,src/core/lib/security/credentials/plugin/plugin_credentials.h,"@@ -30,44 +30,39 @@ extern grpc_core::TraceFlag grpc_plugin_credentials_trace; // -Wmismatched-tags. struct grpc_plugin_credentials final : public grpc_call_credentials {  public:-  struct pending_request {-    bool cancelled;+  struct pending_request : public grpc_core::RefCounted<pending_request> {","I think this can now be private within `grpc_plugin_credentials`.Also, can we make this a fully idiomatic C++ class?  Specifically:- Let's call it `PendingRequest`.- I think it can use `InternallyRefCounted<>` instead of `RefCounted<>`.- I think we can make the `process_plugin_result()` and `plugin_md_request_metadata_ready()` functions in the .cc file private methods of this class.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804252911,2022-02-11T00:01:56Z,src/core/lib/security/credentials/plugin/plugin_credentials.cc,"@@ -128,133 +108,97 @@ static void plugin_md_request_metadata_ready(void* request,   grpc_core::ApplicationCallbackExecCtx callback_exec_ctx;   grpc_core::ExecCtx exec_ctx(GRPC_EXEC_CTX_FLAG_IS_FINISHED |                               GRPC_EXEC_CTX_FLAG_THREAD_RESOURCE_LOOP);-  grpc_plugin_credentials::pending_request* r =-      static_cast<grpc_plugin_credentials::pending_request*>(request);+  grpc_core::RefCountedPtr<grpc_plugin_credentials::pending_request> r(+      static_cast<grpc_plugin_credentials::pending_request*>(request));   if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {     gpr_log(GPR_INFO,             ""plugin_credentials[%p]: request %p: plugin returned ""             ""asynchronously"",-            r->creds, r);+            r->creds, r.get());   }-  // Remove request from pending list if not previously cancelled.-  r->creds->pending_request_complete(r);-  // If it has not been cancelled, process it.-  if (!r->cancelled) {-    grpc_error_handle error =-        process_plugin_result(r, md, num_md, status, error_details);-    grpc_core::ExecCtx::Run(DEBUG_LOCATION, r->on_request_metadata, error);-  } else if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {-    gpr_log(GPR_INFO,-            ""plugin_credentials[%p]: request %p: plugin was previously ""-            ""cancelled"",-            r->creds, r);+  for (size_t i = 0; i < num_md; i++) {+    grpc_metadata p;+    p.key = grpc_slice_ref_internal(md[i].key);+    p.value = grpc_slice_ref_internal(md[i].value);+    r->metadata.push_back(p);   }-  gpr_free(r);+  r->error_details = error_details == nullptr ? """" : error_details;+  r->status = status;+  r->ready.store(true, std::memory_order_release);+  r->waker.Wakeup(); } -bool grpc_plugin_credentials::get_request_metadata(-    grpc_polling_entity* /*pollent*/, grpc_auth_metadata_context context,-    grpc_core::CredentialsMetadataArray* md_array,-    grpc_closure* on_request_metadata, grpc_error_handle* error) {-  bool retval = true;  // Synchronous return.-  if (plugin_.get_metadata != nullptr) {-    // Create pending_request object.-    pending_request* request = grpc_core::Zalloc<pending_request>();-    request->creds = this;-    request->md_array = md_array;-    request->on_request_metadata = on_request_metadata;-    // Add it to the pending list.-    gpr_mu_lock(&mu_);-    if (pending_requests_ != nullptr) {-      pending_requests_->prev = request;-    }-    request->next = pending_requests_;-    pending_requests_ = request;-    gpr_mu_unlock(&mu_);-    // Invoke the plugin.  The callback holds a ref to us.+grpc_core::ArenaPromise<absl::StatusOr<grpc_core::ClientInitialMetadata>>+grpc_plugin_credentials::GetRequestMetadata(+    grpc_core::ClientInitialMetadata initial_metadata,+    grpc_core::AuthMetadataContext* auth_metadata_context) {+  if (plugin_.get_metadata == nullptr) {+    return grpc_core::Immediate(std::move(initial_metadata));+  }++  // Create pending_request object.+  auto request = grpc_core::MakeRefCounted<pending_request>();+  request->ready = false;+  request->waker = grpc_core::Activity::current()->MakeNonOwningWaker();+  request->creds = this;+  request->call_creds = Ref();+  request->context = auth_metadata_context->MakeLegacyContext(initial_metadata);+  request->md = std::move(initial_metadata);+  // Invoke the plugin.  The callback holds a ref to us.+  if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {+    gpr_log(GPR_INFO, ""plugin_credentials[%p]: request %p: invoking plugin"",+            this, request.get());+  }+  grpc_metadata creds_md[GRPC_METADATA_CREDENTIALS_PLUGIN_SYNC_MAX];+  size_t num_creds_md = 0;+  grpc_status_code status = GRPC_STATUS_OK;+  const char* error_details = nullptr;+  auto child_request = request->Ref();",Why is this temporary ref needed?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804253921,2022-02-11T00:04:24Z,src/core/lib/security/credentials/plugin/plugin_credentials.cc,"@@ -128,133 +108,97 @@ static void plugin_md_request_metadata_ready(void* request,   grpc_core::ApplicationCallbackExecCtx callback_exec_ctx;   grpc_core::ExecCtx exec_ctx(GRPC_EXEC_CTX_FLAG_IS_FINISHED |                               GRPC_EXEC_CTX_FLAG_THREAD_RESOURCE_LOOP);-  grpc_plugin_credentials::pending_request* r =-      static_cast<grpc_plugin_credentials::pending_request*>(request);+  grpc_core::RefCountedPtr<grpc_plugin_credentials::pending_request> r(+      static_cast<grpc_plugin_credentials::pending_request*>(request));   if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {     gpr_log(GPR_INFO,             ""plugin_credentials[%p]: request %p: plugin returned ""             ""asynchronously"",-            r->creds, r);+            r->creds, r.get());   }-  // Remove request from pending list if not previously cancelled.-  r->creds->pending_request_complete(r);-  // If it has not been cancelled, process it.-  if (!r->cancelled) {-    grpc_error_handle error =-        process_plugin_result(r, md, num_md, status, error_details);-    grpc_core::ExecCtx::Run(DEBUG_LOCATION, r->on_request_metadata, error);-  } else if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {-    gpr_log(GPR_INFO,-            ""plugin_credentials[%p]: request %p: plugin was previously ""-            ""cancelled"",-            r->creds, r);+  for (size_t i = 0; i < num_md; i++) {+    grpc_metadata p;+    p.key = grpc_slice_ref_internal(md[i].key);+    p.value = grpc_slice_ref_internal(md[i].value);+    r->metadata.push_back(p);   }-  gpr_free(r);+  r->error_details = error_details == nullptr ? """" : error_details;+  r->status = status;+  r->ready.store(true, std::memory_order_release);+  r->waker.Wakeup(); } -bool grpc_plugin_credentials::get_request_metadata(-    grpc_polling_entity* /*pollent*/, grpc_auth_metadata_context context,-    grpc_core::CredentialsMetadataArray* md_array,-    grpc_closure* on_request_metadata, grpc_error_handle* error) {-  bool retval = true;  // Synchronous return.-  if (plugin_.get_metadata != nullptr) {-    // Create pending_request object.-    pending_request* request = grpc_core::Zalloc<pending_request>();-    request->creds = this;-    request->md_array = md_array;-    request->on_request_metadata = on_request_metadata;-    // Add it to the pending list.-    gpr_mu_lock(&mu_);-    if (pending_requests_ != nullptr) {-      pending_requests_->prev = request;-    }-    request->next = pending_requests_;-    pending_requests_ = request;-    gpr_mu_unlock(&mu_);-    // Invoke the plugin.  The callback holds a ref to us.+grpc_core::ArenaPromise<absl::StatusOr<grpc_core::ClientInitialMetadata>>+grpc_plugin_credentials::GetRequestMetadata(+    grpc_core::ClientInitialMetadata initial_metadata,+    grpc_core::AuthMetadataContext* auth_metadata_context) {+  if (plugin_.get_metadata == nullptr) {+    return grpc_core::Immediate(std::move(initial_metadata));+  }++  // Create pending_request object.+  auto request = grpc_core::MakeRefCounted<pending_request>();+  request->ready = false;+  request->waker = grpc_core::Activity::current()->MakeNonOwningWaker();+  request->creds = this;+  request->call_creds = Ref();+  request->context = auth_metadata_context->MakeLegacyContext(initial_metadata);+  request->md = std::move(initial_metadata);+  // Invoke the plugin.  The callback holds a ref to us.+  if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {+    gpr_log(GPR_INFO, ""plugin_credentials[%p]: request %p: invoking plugin"",+            this, request.get());+  }+  grpc_metadata creds_md[GRPC_METADATA_CREDENTIALS_PLUGIN_SYNC_MAX];+  size_t num_creds_md = 0;+  grpc_status_code status = GRPC_STATUS_OK;+  const char* error_details = nullptr;+  auto child_request = request->Ref();+  if (!plugin_.get_metadata(plugin_.state, request->context,+                            plugin_md_request_metadata_ready,+                            child_request.get(), creds_md, &num_creds_md,+                            &status, &error_details)) {+    child_request.release();     if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {-      gpr_log(GPR_INFO, ""plugin_credentials[%p]: request %p: invoking plugin"",-              this, request);-    }-    Ref().release();-    grpc_metadata creds_md[GRPC_METADATA_CREDENTIALS_PLUGIN_SYNC_MAX];-    size_t num_creds_md = 0;-    grpc_status_code status = GRPC_STATUS_OK;-    const char* error_details = nullptr;-    if (!plugin_.get_metadata(-            plugin_.state, context, plugin_md_request_metadata_ready, request,-            creds_md, &num_creds_md, &status, &error_details)) {-      if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {-        gpr_log(GPR_INFO,-                ""plugin_credentials[%p]: request %p: plugin will return ""-                ""asynchronously"",-                this, request);-      }-      return false;  // Asynchronous return.+      gpr_log(GPR_INFO,+              ""plugin_credentials[%p]: request %p: plugin will return ""+              ""asynchronously"",+              this, request.get());     }-    // Returned synchronously.-    // Remove request from pending list if not previously cancelled.-    request->creds->pending_request_complete(request);-    // If the request was cancelled, the error will have been returned-    // asynchronously by plugin_cancel_get_request_metadata(), so return-    // false.  Otherwise, process the result.-    if (request->cancelled) {-      if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {-        gpr_log(GPR_INFO,-                ""plugin_credentials[%p]: request %p was cancelled, error ""-                ""will be returned asynchronously"",-                this, request);-      }-      retval = false;-    } else {-      if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {-        gpr_log(GPR_INFO,-                ""plugin_credentials[%p]: request %p: plugin returned ""-                ""synchronously"",-                this, request);+    return [request]() -> grpc_core::Poll<+                           absl::StatusOr<grpc_core::ClientInitialMetadata>> {+      if (!request->ready.load(std::memory_order_acquire)) {+        return grpc_core::Pending{};       }-      *error = process_plugin_result(request, creds_md, num_creds_md, status,-                                     error_details);-    }-    // Clean up.-    for (size_t i = 0; i < num_creds_md; ++i) {-      grpc_slice_unref_internal(creds_md[i].key);-      grpc_slice_unref_internal(creds_md[i].value);-    }-    gpr_free(const_cast<char*>(error_details));-    gpr_free(request);+      auto result = process_plugin_result(+          request.get(), request->metadata.data(), request->metadata.size(),+          request->status, request->error_details.c_str());+      return std::move(result);",Can we just directly say `return process_plugin_result(...)` instead of having this temporary variable?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804259889,2022-02-11T00:19:41Z,src/core/lib/security/credentials/plugin/plugin_credentials.cc,"@@ -128,133 +108,97 @@ static void plugin_md_request_metadata_ready(void* request,   grpc_core::ApplicationCallbackExecCtx callback_exec_ctx;   grpc_core::ExecCtx exec_ctx(GRPC_EXEC_CTX_FLAG_IS_FINISHED |                               GRPC_EXEC_CTX_FLAG_THREAD_RESOURCE_LOOP);-  grpc_plugin_credentials::pending_request* r =-      static_cast<grpc_plugin_credentials::pending_request*>(request);+  grpc_core::RefCountedPtr<grpc_plugin_credentials::pending_request> r(+      static_cast<grpc_plugin_credentials::pending_request*>(request));   if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {     gpr_log(GPR_INFO,             ""plugin_credentials[%p]: request %p: plugin returned ""             ""asynchronously"",-            r->creds, r);+            r->creds, r.get());   }-  // Remove request from pending list if not previously cancelled.-  r->creds->pending_request_complete(r);-  // If it has not been cancelled, process it.-  if (!r->cancelled) {-    grpc_error_handle error =-        process_plugin_result(r, md, num_md, status, error_details);-    grpc_core::ExecCtx::Run(DEBUG_LOCATION, r->on_request_metadata, error);-  } else if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {-    gpr_log(GPR_INFO,-            ""plugin_credentials[%p]: request %p: plugin was previously ""-            ""cancelled"",-            r->creds, r);+  for (size_t i = 0; i < num_md; i++) {+    grpc_metadata p;+    p.key = grpc_slice_ref_internal(md[i].key);+    p.value = grpc_slice_ref_internal(md[i].value);+    r->metadata.push_back(p);   }-  gpr_free(r);+  r->error_details = error_details == nullptr ? """" : error_details;+  r->status = status;+  r->ready.store(true, std::memory_order_release);+  r->waker.Wakeup(); } -bool grpc_plugin_credentials::get_request_metadata(-    grpc_polling_entity* /*pollent*/, grpc_auth_metadata_context context,-    grpc_core::CredentialsMetadataArray* md_array,-    grpc_closure* on_request_metadata, grpc_error_handle* error) {-  bool retval = true;  // Synchronous return.-  if (plugin_.get_metadata != nullptr) {-    // Create pending_request object.-    pending_request* request = grpc_core::Zalloc<pending_request>();-    request->creds = this;-    request->md_array = md_array;-    request->on_request_metadata = on_request_metadata;-    // Add it to the pending list.-    gpr_mu_lock(&mu_);-    if (pending_requests_ != nullptr) {-      pending_requests_->prev = request;-    }-    request->next = pending_requests_;-    pending_requests_ = request;-    gpr_mu_unlock(&mu_);-    // Invoke the plugin.  The callback holds a ref to us.+grpc_core::ArenaPromise<absl::StatusOr<grpc_core::ClientInitialMetadata>>+grpc_plugin_credentials::GetRequestMetadata(+    grpc_core::ClientInitialMetadata initial_metadata,+    grpc_core::AuthMetadataContext* auth_metadata_context) {+  if (plugin_.get_metadata == nullptr) {+    return grpc_core::Immediate(std::move(initial_metadata));+  }++  // Create pending_request object.+  auto request = grpc_core::MakeRefCounted<pending_request>();+  request->ready = false;+  request->waker = grpc_core::Activity::current()->MakeNonOwningWaker();+  request->creds = this;+  request->call_creds = Ref();+  request->context = auth_metadata_context->MakeLegacyContext(initial_metadata);+  request->md = std::move(initial_metadata);+  // Invoke the plugin.  The callback holds a ref to us.+  if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {+    gpr_log(GPR_INFO, ""plugin_credentials[%p]: request %p: invoking plugin"",+            this, request.get());+  }+  grpc_metadata creds_md[GRPC_METADATA_CREDENTIALS_PLUGIN_SYNC_MAX];","Do we still need the limitation of `GRPC_METADATA_CREDENTIALS_PLUGIN_SYNC_MAX` for synchronous returns?  I added that way back in #12374, and my comment at the time was that the need for this would go away soon with some metadata changes you had in mind. :)",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804262617,2022-02-11T00:26:34Z,src/core/lib/security/credentials/oauth2/oauth2_credentials.h,"@@ -73,11 +74,14 @@ struct grpc_credentials_metadata_request {   grpc_http_response response; }; -struct grpc_oauth2_pending_get_request_metadata {-  grpc_core::CredentialsMetadataArray* md_array;-  grpc_closure* on_request_metadata;+struct grpc_oauth2_pending_get_request_metadata","Similar comment as for `grpc_plugin_credentials::pending_request`: I think this can be private within `grpc_oauth2_token_fetcher_credentials`, and it can be a fully idiomatic C++ class, using `InternallyRefCounted<>`, etc.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28844,804811100,2022-02-11T16:27:20Z,src/core/lib/security/credentials/composite/composite_credentials.h,"@@ -56,6 +56,12 @@ class grpc_composite_channel_credentials : public grpc_channel_credentials {     return inner_creds_->update_arguments(args);   } +  int cmp(const grpc_channel_credentials* other) const override {+    // TODO(yashykt): Check if we can do something better here","I think this should delegate to comparing the underlying channel and call creds.  Note that this will require also adding a `cmp()` method to `grpc_call_credentials`.  Once you do that, the code here can be something like this:```int r = strcmp(type(), other->type());if (r != 0) return r;auto* o = static_cast<grpc_composite_channel_credentials*>(other);r = inner_->cmp(o->inner_.get());if (r != 0) return r;return call_creds_->cmp(o->call_creds_.get());```Also, note that basically every implementation will need those first two lines, so we should consider finding a way to make that happen automatically as part of the structure.  For example, maybe wrapping `cmp()` in a parent method that does that check and then calls `cmp()` only if the types are the same.  We would just need to make sure that it's clear which method is implemented by subclasses and which one is called by the external caller, so that we don't get that confused.  (That may be a little ugly; I'm open to other suggestions as well.)",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804960675,2022-02-11T19:51:08Z,test/core/security/credentials_test.cc,"@@ -464,52 +502,31 @@ class RequestMetadataState {       GPR_ASSERT(expected_error == actual_error);       GRPC_ERROR_UNREF(expected_error_);     }-    gpr_log(GPR_INFO, ""expected_size=%"" PRIdPTR "" actual_size=%"" PRIdPTR,-            expected_.size(), md_array_.size());-    GPR_ASSERT(md_array_.size() == expected_.size());-    CheckMetadata(expected_, &md_array_);-    delete this;-  }--  static void CheckMetadata(const std::map<std::string, std::string>& expected,-                            grpc_core::CredentialsMetadataArray* md_array) {-    for (auto const& i : expected) {-      size_t j;-      for (j = 0; j < md_array->size(); ++j) {-        absl::string_view actual_key = md_array->at(j).first.as_string_view();-        if (actual_key == i.first) {-          absl::string_view actual_value =-              md_array->at(j).second.as_string_view();-          GPR_ASSERT(actual_value == i.second);-          break;-        }-      }-      if (j == md_array->size()) {-        gpr_log(GPR_ERROR, ""key %s not found"", i.first.c_str());-        GPR_ASSERT(0);-      }-    }+    gpr_log(GPR_INFO, ""expected metadata: %s"", expected_.c_str());",We're not actually checking that the metadata is equal anymore?  Seems like that could cause the tests to pass when they shouldn't.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,804989395,2022-02-11T20:38:33Z,src/core/lib/security/credentials/credentials.h,"@@ -159,6 +161,22 @@ using CredentialsMetadataArray = std::vector<std::pair<Slice, Slice>>;  /* --- grpc_call_credentials. --- */ +namespace grpc_core {++// Abstract interface to obtain contextual data for some call credentials.+class AuthMetadataContext {","I'm wondering if we actually need this API.  It looks like almost all of the data it returns is actually coming from the initial metadata; the only exception is the URL scheme, which comes from the security connector.  Could we just pass the URL scheme in as a parameter to `GetRequestMetadata()` and dispense with this interface?The reason I ask is that it looks like both of the methods of this interface are used in exactly one place: `JwtServiceUrl()` is used only in JWT creds, and `MakeLegacyContext()` is used only in plugin creds.  Given that, it seems like it would make sense to just move the code from those methods directly into the credential types where they are needed, which would decrease the size of the API surface between the client_auth_filter and the credentials.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28767,805061260,2022-02-11T23:05:09Z,src/core/lib/channel/promise_based_filter.h,"@@ -325,39 +374,137 @@ class CallData<ChannelFilter, true> : public BaseCallData {     GPR_ASSERT(!is_polling_);     grpc_closure* call_closure = nullptr;     is_polling_ = true;+    grpc_error_handle cancel_send_initial_metadata_error = GRPC_ERROR_NONE;+    grpc_transport_stream_op_batch* forward_batch = nullptr;     switch (send_initial_state_) {       case SendInitialState::kQueued:       case SendInitialState::kForwarded: {         // Poll the promise once since we're waiting for it.-        Poll<TrailingMetadata> poll = promise_();+        Poll<TrailingMetadata> poll;+        {+          ScopedActivity activity(this);+          poll = promise_();+        }         if (auto* r = absl::get_if<TrailingMetadata>(&poll)) {-          GPR_ASSERT(recv_trailing_state_ == RecvTrailingState::kComplete);-          GPR_ASSERT(recv_trailing_metadata_ == UnwrapMetadata(std::move(*r)));-          recv_trailing_state_ = RecvTrailingState::kResponded;-          call_closure =-              absl::exchange(original_recv_trailing_metadata_ready_, nullptr);+          promise_ = ArenaPromise<TrailingMetadata>();+          auto* md = UnwrapMetadata(std::move(*r));+          bool destroy_md = true;+          switch (recv_trailing_state_) {+            case RecvTrailingState::kComplete:+              if (recv_trailing_metadata_ != md) {+                *recv_trailing_metadata_ = std::move(*md);+              } else {+                destroy_md = false;+              }+              recv_trailing_state_ = RecvTrailingState::kResponded;+              call_closure = absl::exchange(+                  original_recv_trailing_metadata_ready_, nullptr);+              break;+            case RecvTrailingState::kQueued:+            case RecvTrailingState::kForwarded: {+              GPR_ASSERT(*md->get_pointer(GrpcStatusMetadata()) !=+                         GRPC_STATUS_OK);+              grpc_error_handle error = grpc_error_set_int(+                  GRPC_ERROR_CREATE_FROM_STATIC_STRING(+                      ""early return from promise based filter""),+                  GRPC_ERROR_INT_GRPC_STATUS,+                  *md->get_pointer(GrpcStatusMetadata()));+              if (auto* message = md->get_pointer(GrpcMessageMetadata())) {+                error = grpc_error_set_str(error, GRPC_ERROR_STR_GRPC_MESSAGE,+                                           message->as_string_view());+              }+              if (recv_trailing_state_ == RecvTrailingState::kQueued) {+                GPR_ASSERT(send_initial_state_ == SendInitialState::kQueued);+                send_initial_state_ = SendInitialState::kCancelled;+                cancel_send_initial_metadata_error = error;+              } else {+                forward_batch =","Fairly certain we can't guarantee that (think a deadline filter: there's no guarantee of an op around when the deadline terminates), I've added the call combiner call.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28767,805068065,2022-02-11T23:26:07Z,src/core/lib/promise/activity.h,"@@ -332,15 +330,15 @@ class ActivityContexts : public ContextHolder<Contexts>... { // invoked, and that a given activity will not be concurrently scheduled again // until its RunScheduledWakeup() has been invoked. template <class F, class WakeupScheduler, class OnDone, typename... Contexts>-class PromiseActivity final : public Activity,+class PromiseActivity final : public FreestandingActivity,                               private ActivityContexts<Contexts...> {","Not easily, and I think this is the technically best and most readable solution.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28570,805109594,2022-02-12T03:12:25Z,third_party/libuv_test.patch,"@@ -0,0 +1,56 @@+diff --git a/test/test-timer.c b/test/test-timer.c","This looks like a general change to the libuv test code, nothing gRPC-specific. I don't see why we'd want to maintain patches for general improvements that they'd likely accept and maintain upstream. Patches are a maintenance burden best avoided, if possible. For example, if it does not apply cleanly onto the next libuv version, upgrading the libuv dependency now means we'd have to port a patch before we could do the upgrade.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28767,805515782,2022-02-14T05:32:46Z,src/core/lib/security/credentials/plugin/plugin_credentials.cc,"@@ -128,133 +108,97 @@ static void plugin_md_request_metadata_ready(void* request,   grpc_core::ApplicationCallbackExecCtx callback_exec_ctx;   grpc_core::ExecCtx exec_ctx(GRPC_EXEC_CTX_FLAG_IS_FINISHED |                               GRPC_EXEC_CTX_FLAG_THREAD_RESOURCE_LOOP);-  grpc_plugin_credentials::pending_request* r =-      static_cast<grpc_plugin_credentials::pending_request*>(request);+  grpc_core::RefCountedPtr<grpc_plugin_credentials::pending_request> r(+      static_cast<grpc_plugin_credentials::pending_request*>(request));   if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {     gpr_log(GPR_INFO,             ""plugin_credentials[%p]: request %p: plugin returned ""             ""asynchronously"",-            r->creds, r);+            r->creds, r.get());   }-  // Remove request from pending list if not previously cancelled.-  r->creds->pending_request_complete(r);-  // If it has not been cancelled, process it.-  if (!r->cancelled) {-    grpc_error_handle error =-        process_plugin_result(r, md, num_md, status, error_details);-    grpc_core::ExecCtx::Run(DEBUG_LOCATION, r->on_request_metadata, error);-  } else if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {-    gpr_log(GPR_INFO,-            ""plugin_credentials[%p]: request %p: plugin was previously ""-            ""cancelled"",-            r->creds, r);+  for (size_t i = 0; i < num_md; i++) {+    grpc_metadata p;+    p.key = grpc_slice_ref_internal(md[i].key);+    p.value = grpc_slice_ref_internal(md[i].value);+    r->metadata.push_back(p);   }-  gpr_free(r);+  r->error_details = error_details == nullptr ? """" : error_details;+  r->status = status;+  r->ready.store(true, std::memory_order_release);+  r->waker.Wakeup(); } -bool grpc_plugin_credentials::get_request_metadata(-    grpc_polling_entity* /*pollent*/, grpc_auth_metadata_context context,-    grpc_core::CredentialsMetadataArray* md_array,-    grpc_closure* on_request_metadata, grpc_error_handle* error) {-  bool retval = true;  // Synchronous return.-  if (plugin_.get_metadata != nullptr) {-    // Create pending_request object.-    pending_request* request = grpc_core::Zalloc<pending_request>();-    request->creds = this;-    request->md_array = md_array;-    request->on_request_metadata = on_request_metadata;-    // Add it to the pending list.-    gpr_mu_lock(&mu_);-    if (pending_requests_ != nullptr) {-      pending_requests_->prev = request;-    }-    request->next = pending_requests_;-    pending_requests_ = request;-    gpr_mu_unlock(&mu_);-    // Invoke the plugin.  The callback holds a ref to us.+grpc_core::ArenaPromise<absl::StatusOr<grpc_core::ClientInitialMetadata>>+grpc_plugin_credentials::GetRequestMetadata(+    grpc_core::ClientInitialMetadata initial_metadata,+    grpc_core::AuthMetadataContext* auth_metadata_context) {+  if (plugin_.get_metadata == nullptr) {+    return grpc_core::Immediate(std::move(initial_metadata));+  }++  // Create pending_request object.+  auto request = grpc_core::MakeRefCounted<pending_request>();+  request->ready = false;+  request->waker = grpc_core::Activity::current()->MakeNonOwningWaker();+  request->creds = this;+  request->call_creds = Ref();+  request->context = auth_metadata_context->MakeLegacyContext(initial_metadata);+  request->md = std::move(initial_metadata);+  // Invoke the plugin.  The callback holds a ref to us.+  if (GRPC_TRACE_FLAG_ENABLED(grpc_plugin_credentials_trace)) {+    gpr_log(GPR_INFO, ""plugin_credentials[%p]: request %p: invoking plugin"",+            this, request.get());+  }+  grpc_metadata creds_md[GRPC_METADATA_CREDENTIALS_PLUGIN_SYNC_MAX];",I don't know what I had in mind 5 years ago.I think we still need it for now.Let's kill the C API and then make ClientInitialMetadata part of surface.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28868,806087516,2022-02-14T17:39:01Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -521,6 +521,19 @@ grpc_millis NowFromCycleCounter() {   return grpc_timespec_to_millis_round_down(gpr_now(GPR_CLOCK_MONOTONIC)); } +// There can be milliseconds of difference between cycle time and time spec,+// when the difference is in the order of a few seconds, we need to know that so+// that our tests can account for the difference and reduce flakes due to the+// clock difference.+grpc_millis ClockDifference() {","I think we probably want a more convenient interface here, so that the adjustment logic is all here instead of in the individual tests.  Consider something like the following:```MATCHER_P2(AdjustedClockInRange, t1, t2) {  grpc_millis now = arg;  // ...apply whatever adjustment to now is appropriate given the clock difference...  bool ok = true;  ok &= ::testing::ExplainMatchResult(      ::testing::Ge(t1), arg, result_listener);  ok &= ::testing::ExplainMatchResult(      ::testing::Lt(t2), arg, result_listener);  return ok;}```Then, in the individual test, instead of doing this:```grpc_millis t1 = t0 + kTimeoutGrpcTimeoutHeaderMaxSecond * 1000 +                 kTimeoutMillis - clock_difference;grpc_millis t2 = t0 + kTimeoutMaxStreamDurationSecond * 1000 +                 kTimeoutMillis + clock_difference;// ...t0 = NowFromCycleCounter();EXPECT_GE(t0, t1);EXPECT_LT(t0, t2);```You can instead do something like this:```grpc_millis t1 = t0 + kTimeoutGrpcTimeoutHeaderMaxSecond * 1000 +                 kTimeoutMillis;grpc_millis t2 = t0 + kTimeoutMaxStreamDurationSecond * 1000 +                 kTimeoutMillis;// ...EXPECT_THAT(NowFromCycleCounter(), AdjustedClockInRange(t1, t2));```",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/28865,806151184,2022-02-14T19:01:12Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -83,6 +83,9 @@ def setUpClass(cls):         """"""Hook method for setting up class fixture before running tests in         the class.         """"""+        logger.info('----- Testing %s -----', cls.__name__)+        logger.info('Logs timezone: %s', time.localtime().tm_zone)","Emm... technically, we can know the system timezone. Like this PR, we can print before update.The general debug case demands PST, including how we seek help from control plane teams. It's beneficial that the timezone to be consistent across all runs.For extra functionality, like setting the timezone to a specific location, I don't know Kokoro's behavior, but if we want, we can add an environment variable to control it.This can reduce the cognitive burden for triage people to minimum about the timestamp. Comparing: 1. every time people need to locate this log line in the flie to validate timezone, including staging; 2. knowing that the default timezone will be a fixed value, if this run is not specially configured.What would be the experience we want to provide?",
1535036,tamird,https://api.github.com/repos/grpc/grpc/pulls/28889,807229313,2022-02-15T19:19:18Z,test/core/event_engine/test_suite/client_test.cc,"@@ -0,0 +1,33 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <random>+#include <thread>++#include <gmock/gmock.h>+#include <gtest/gtest.h>++#include ""absl/functional/bind_front.h""","seems like a lot of unused headers here, above, and below.",
19922103,CodingCanuck,https://api.github.com/repos/grpc/grpc/pulls/27507,807384888,2022-02-15T22:52:06Z,bazel/grpc_python_deps.bzl,"@@ -49,8 +49,10 @@ def grpc_python_deps():     if ""io_bazel_rules_python"" not in native.existing_rules():         http_archive(             name = ""io_bazel_rules_python"",-            url = ""https://github.com/bazelbuild/rules_python/releases/download/0.0.1/rules_python-0.0.1.tar.gz"",-            sha256 = ""aa96a691d3a8177f3215b14b0edc9641787abaaa30363a080165d06ab65e1161"",+            url = ""https://github.com/bazelbuild/rules_python/releases/download/0.4.0/rules_python-0.4.0.tar.gz"",+            sha256 = ""954aa89b491be4a083304a2cb838019c8b8c3720a7abb9c4cb81ac7a24230cea"",+            patches = [""//third_party:rules_python.patch""],","Does this break users of the gRPC library?I'm trying to upgrade a repository that uses gRPC to the latest gRPC release (1.44.0). This line seems to be causing an error (repo details omitted) when building with bazel 4.2.2:```bazel test ${MY_GRPC_TEST_TARGET}INFO: Repository io_bazel_rules_python instantiated at:    ${MY_REPO}/WORKSPACE.bazel:26:5: in <toplevel>    ${MY_REPO}/bazel/deps.bzl:59:14: in deps. /home/alexmc/.cache/bazel/_bazel_alexmc/e5e83162f61030880de4b3e9d73ac179/external/com_github_grpc_grpc/bazel/grpc_deps.bzl:459:21: in grpc_deps /home/alexmc/.cache/bazel/_bazel_alexmc/e5e83162f61030880de4b3e9d73ac179/external/com_github_grpc_grpc/bazel/grpc_python_deps.bzl:53:21: in grpc_python_depsRepository rule http_archive defined at:   /home/alexmc/.cache/bazel/_bazel_alexmc/e5e83162f61030880de4b3e9d73ac179/external/bazel_tools/tools/build_defs/repo/http.bzl:336:31: in <toplevel>INFO: Repository 'io_bazel_rules_python' used the following cache hits instead of downloading the corresponding file.* Hash '954aa89b491be4a083304a2cb838019c8b8c3720a7abb9c4cb81ac7a24230cea' for https://github.com/bazelbuild/rules_python/releases/download/0.4.0/rules_python-0.4.0.tar.gzIf the definition of 'io_bazel_rules_python' was updated, verify that the hashes were also updated.ERROR: An error occurred during the fetch of repository 'io_bazel_rules_python':   Traceback (most recent call last):        File ""/home/alexmc/.cache/bazel/_bazel_alexmc/e5e83162f61030880de4b3e9d73ac179/external/bazel_tools/tools/build_defs/repo/http.bzl"", line 121, column 10, in _http_archive_impl                patch(ctx)        File ""/home/alexmc/.cache/bazel/_bazel_alexmc/e5e83162f61030880de4b3e9d73ac179/external/bazel_tools/tools/build_defs/repo/utils.bzl"", line 135, column 22, in patch                ctx.patch(patchfile, strip)Error in patch: Unable to load package for //third_party:rules_python.patch: BUILD file not found in any of the following directories. Add a BUILD file to a directory to mark it as a package. - ${MY_REPO}/third_party```Specifically, it's looking for a third_party directory to exist as a sibling directory to the directory containing the grpc source archive.Is this expected? I'm wondering if this dependency on a `//target_path:target_name` doesn't work when this code is included from an external repository.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28893,807808727,2022-02-16T11:05:38Z,tools/run_tests/run_microbenchmark.py,"@@ -203,11 +203,27 @@ def run_summary(bm_name, cfg, base_json_name):   def collect_summary(bm_name, args):-    heading('Summary: %s [no counters]' % bm_name)-    text(run_summary(bm_name, 'opt', bm_name))-    heading('Summary: %s [with counters]' % bm_name)-    text(run_summary(bm_name, 'counters', bm_name))+    # no counters, run microbenchmark and add summary+    # both to HTML report and to console.+    nocounters_heading = 'Summary: %s [no counters]' % bm_name+    nocounters_summary = run_summary(bm_name, 'opt', bm_name)+    heading(nocounters_heading)+    text(nocounters_summary)+    print(nocounters_heading)+    print(nocounters_summary)++    # with counters, run microbenchmark and add summary+    # both to HTML report and to console.+    counter_heading = 'Summary: %s [with counters]' % bm_name+    counters_summary = run_summary(bm_name, 'counters', bm_name)+    heading(counter_heading)+    text(counters_summary)+    print(counters_heading)+    print(counters_summary)+     if args.bq_result_table:+        # TODO(jtattermusch): currently there is no way to differentiate","@ctiller  this actually seems problematic - AFAICT  there is no way of distinguishing the data from ""counters"" and ""nocounters"" runs in BQ.Would it make sense to stop uploading the data for ""counters"" to at least know what's stored in BQ?",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28893,808176227,2022-02-16T16:11:31Z,tools/run_tests/run_microbenchmark.py,"@@ -203,11 +203,27 @@ def run_summary(bm_name, cfg, base_json_name):   def collect_summary(bm_name, args):-    heading('Summary: %s [no counters]' % bm_name)-    text(run_summary(bm_name, 'opt', bm_name))-    heading('Summary: %s [with counters]' % bm_name)-    text(run_summary(bm_name, 'counters', bm_name))+    # no counters, run microbenchmark and add summary+    # both to HTML report and to console.+    nocounters_heading = 'Summary: %s [no counters]' % bm_name+    nocounters_summary = run_summary(bm_name, 'opt', bm_name)+    heading(nocounters_heading)+    text(nocounters_summary)+    print(nocounters_heading)+    print(nocounters_summary)++    # with counters, run microbenchmark and add summary+    # both to HTML report and to console.+    counter_heading = 'Summary: %s [with counters]' % bm_name+    counters_summary = run_summary(bm_name, 'counters', bm_name)+    heading(counter_heading)+    text(counters_summary)+    print(counters_heading)+    print(counters_summary)+     if args.bq_result_table:+        # TODO(jtattermusch): currently there is no way to differentiate","So we smash them together right now... this code calls bm2bq.py, which ultimately calls:https://github.com/grpc/grpc/blob/master/tools/profiling/microbenchmarks/bm_json.py#L181and that code smashes together the times from the non-counters run with the counters from the counters runand that combination is... probably what we actually want",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,808244796,2022-02-16T17:04:46Z,src/core/lib/channel/promise_based_filter.cc,"@@ -0,0 +1,50 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include ""src/core/lib/channel/promise_based_filter.h""++#include ""src/core/lib/channel/channel_stack.h""++namespace grpc_core {+namespace promise_filter_detail {++// We don't form ActivityPtr's to this type, and consequently don't need+// Orphan().+void BaseCallData::Orphan() { abort(); }++// For now we don't care about owning/non-owning wakers, instead just share+// implementation.+Waker BaseCallData::MakeNonOwningWaker() { return MakeOwningWaker(); }++Waker BaseCallData::MakeOwningWaker() {+  GRPC_CALL_STACK_REF(call_stack_, ""waker"");+  return Waker(this);+}++void BaseCallData::Wakeup() {+  auto wakeup = [](void* p, grpc_error_handle) {+    auto* self = static_cast<BaseCallData*>(p);+    self->OnWakeup();+    self->Drop();+  };+  auto* closure = GRPC_CLOSURE_CREATE(wakeup, this, nullptr);","What are the additional costs of going to sleep and waking up again?  It seems like that is basically the promise equivalent of an old-style filter doing an async operation and waiting for I/O.  In the latter case, we've always tried pretty hard to use an existing closure instead of allocating a new one each time.Could we avoid this by having a `grpc_closure` data member here?  I assume that there won't be multiple wakeups in flight at the same time.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,808339672,2022-02-16T18:37:08Z,src/core/lib/promise/activity.h,"@@ -332,15 +330,15 @@ class ActivityContexts : public ContextHolder<Contexts>... { // invoked, and that a given activity will not be concurrently scheduled again // until its RunScheduledWakeup() has been invoked. template <class F, class WakeupScheduler, class OnDone, typename... Contexts>-class PromiseActivity final : public Activity,+class PromiseActivity final : public FreestandingActivity,                               private ActivityContexts<Contexts...> {","Okay.  As above, please add a comment justifying the use of private inheritence (in this case, maybe both a pointer and the complexity of forwarding a list of template arguments in C++11?).",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,808348931,2022-02-16T18:48:51Z,src/core/lib/security/credentials/credentials.h,"@@ -159,6 +161,22 @@ using CredentialsMetadataArray = std::vector<std::pair<Slice, Slice>>;  /* --- grpc_call_credentials. --- */ +namespace grpc_core {++// Abstract interface to obtain contextual data for some call credentials.+class AuthMetadataContext {","Can we do that sharing by providing a common library of utility functions that shares code, rather than adding them to the credentials API surface?  It seems like we could just plumb the URL scheme and the initial metadata through the API surface here, and then have a separate library to generate both the JWT service URL and the plugin auth context from the URL scheme and the metadata.  That same library could be used by both the JWT creds and the plugin creds, so we'd get the same code reuse, but without needing to expand this API surface.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,808351590,2022-02-16T18:52:17Z,src/core/lib/promise/activity.h,"@@ -320,6 +225,99 @@ class ActivityContexts : public ContextHolder<Contexts>... {   }; }; +// A free standing activity: an activity that owns its own synchronization and+// memory.+// The alternative is an activity that's somehow tied into another system, for+// instance the type seen in promise_based_filter.h as we're transitioning from+// the old filter stack to the new system.+class FreestandingActivity : public Activity, private Wakeable {+ public:+  Waker MakeOwningWaker() final {+    Ref();+    return Waker(this);+  }+  Waker MakeNonOwningWaker() final;++  void Orphan() final {+    Cancel();+    Unref();+  }++  void ForceImmediateRepoll() final {+    mu_.AssertHeld();+    SetActionDuringRun(ActionDuringRun::kWakeup);+  }++ protected:+  // Action received during a run, in priority order.+  // If more than one action is received during a run, we use max() to resolve+  // which one to report (so Cancel overrides Wakeup).+  enum class ActionDuringRun : uint8_t {+    kNone,    // No action occured during run.+    kWakeup,  // A wakeup occured during run.+    kCancel,  // Cancel was called during run.+  };++  inline ~FreestandingActivity() override {+    if (handle_) {+      DropHandle();+    }+  }++  // All promise execution occurs under this mutex.+  Mutex mu_;","I actually agree that simply having an accessor method for the mutex is a little ugly.  Is there a way that we can restructure things such that the actions that have to be performed while holding the mutex in the subclasses are done via separate methods, so that all of the locking and unlocking are performed in the base class?If not, the way you have this now is fine.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,808367210,2022-02-16T19:11:49Z,src/core/lib/channel/promise_based_filter.h,"@@ -325,39 +374,137 @@ class CallData<ChannelFilter, true> : public BaseCallData {     GPR_ASSERT(!is_polling_);     grpc_closure* call_closure = nullptr;     is_polling_ = true;+    grpc_error_handle cancel_send_initial_metadata_error = GRPC_ERROR_NONE;+    grpc_transport_stream_op_batch* forward_batch = nullptr;     switch (send_initial_state_) {       case SendInitialState::kQueued:       case SendInitialState::kForwarded: {         // Poll the promise once since we're waiting for it.-        Poll<TrailingMetadata> poll = promise_();+        Poll<TrailingMetadata> poll;+        {+          ScopedActivity activity(this);+          poll = promise_();+        }         if (auto* r = absl::get_if<TrailingMetadata>(&poll)) {-          GPR_ASSERT(recv_trailing_state_ == RecvTrailingState::kComplete);-          GPR_ASSERT(recv_trailing_metadata_ == UnwrapMetadata(std::move(*r)));-          recv_trailing_state_ = RecvTrailingState::kResponded;-          call_closure =-              absl::exchange(original_recv_trailing_metadata_ready_, nullptr);+          promise_ = ArenaPromise<TrailingMetadata>();+          auto* md = UnwrapMetadata(std::move(*r));+          bool destroy_md = true;+          switch (recv_trailing_state_) {+            case RecvTrailingState::kComplete:+              if (recv_trailing_metadata_ != md) {+                *recv_trailing_metadata_ = std::move(*md);+              } else {+                destroy_md = false;+              }+              recv_trailing_state_ = RecvTrailingState::kResponded;+              call_closure = absl::exchange(+                  original_recv_trailing_metadata_ready_, nullptr);+              break;+            case RecvTrailingState::kQueued:+            case RecvTrailingState::kForwarded: {+              GPR_ASSERT(*md->get_pointer(GrpcStatusMetadata()) !=+                         GRPC_STATUS_OK);+              grpc_error_handle error = grpc_error_set_int(+                  GRPC_ERROR_CREATE_FROM_STATIC_STRING(+                      ""early return from promise based filter""),+                  GRPC_ERROR_INT_GRPC_STATUS,+                  *md->get_pointer(GrpcStatusMetadata()));+              if (auto* message = md->get_pointer(GrpcMessageMetadata())) {+                error = grpc_error_set_str(error, GRPC_ERROR_STR_GRPC_MESSAGE,+                                           message->as_string_view());+              }+              if (recv_trailing_state_ == RecvTrailingState::kQueued) {+                GPR_ASSERT(send_initial_state_ == SendInitialState::kQueued);+                send_initial_state_ = SendInitialState::kCancelled;+                cancel_send_initial_metadata_error = error;+              } else {+                forward_batch =+                    grpc_make_transport_stream_op(GRPC_CLOSURE_CREATE(+                        [](void*, grpc_error_handle) {}, nullptr, nullptr));+                forward_batch->cancel_stream = true;+                forward_batch->payload->cancel_stream.cancel_error = error;+              }+              recv_trailing_state_ = RecvTrailingState::kCancelled;+            } break;+            case RecvTrailingState::kInitial:+              abort();  // unimplemented+            case RecvTrailingState::kResponded:+            case RecvTrailingState::kCancelled:+              abort();  // unreachable+          }+          if (destroy_md) {+            md->~grpc_metadata_batch();+          }         }       } break;       case SendInitialState::kInitial:       case SendInitialState::kCancelled:-        // If we get a response without sending anything, we just propagate that-        // up. (note: that situation isn't possible once we finish the promise-        // transition).+        // If we get a response without sending anything, we just propagate+        // that up. (note: that situation isn't possible once we finish the+        // promise transition).         if (recv_trailing_state_ == RecvTrailingState::kComplete) {           recv_trailing_state_ = RecvTrailingState::kResponded;           call_closure =               absl::exchange(original_recv_trailing_metadata_ready_, nullptr);         }         break;     }+    GRPC_CALL_STACK_REF(call_stack(), ""finish_poll"");     is_polling_ = false;+    bool in_combiner = true;+    if (forward_batch != nullptr) {+      in_combiner = false;+      forward_send_initial_metadata_ = false;+      grpc_call_next_op(elem(), forward_batch);+    }+    if (cancel_send_initial_metadata_error != GRPC_ERROR_NONE) {+      GPR_ASSERT(in_combiner);+      forward_send_initial_metadata_ = false;+      in_combiner = false;+      grpc_transport_stream_op_batch_finish_with_failure(+          absl::exchange(send_initial_metadata_batch_, nullptr),+          cancel_send_initial_metadata_error, call_combiner());+    }     if (absl::exchange(forward_send_initial_metadata_, false)) {+      GPR_ASSERT(in_combiner);+      in_combiner = false;       grpc_call_next_op(elem(),                         absl::exchange(send_initial_metadata_batch_, nullptr));     }     if (call_closure != nullptr) {+      GPR_ASSERT(in_combiner);+      in_combiner = false;       Closure::Run(DEBUG_LOCATION, call_closure, GRPC_ERROR_NONE);     }+    if (absl::exchange(repoll_, false)) {+      if (in_combiner) {+        WakeInsideCombiner();+      } else {+        struct NextPoll : public grpc_closure {+          grpc_call_stack* call_stack;+          CallData* call_data;+        };+        auto run = [](void* p, grpc_error_handle) {+          auto* next_poll = static_cast<NextPoll*>(p);+          next_poll->call_data->WakeInsideCombiner();+          GRPC_CALL_STACK_UNREF(next_poll->call_stack, ""re-poll"");+          delete next_poll;+        };+        auto* p = new NextPoll;+        GRPC_CALL_STACK_REF(call_stack(), ""re-poll"");+        GRPC_CLOSURE_INIT(p, run, p, nullptr);+        GRPC_CALL_COMBINER_START(call_combiner(), p, GRPC_ERROR_NONE,+                                 ""re-poll"");+      }+    } else if (in_combiner) {+      GRPC_CALL_COMBINER_STOP(call_combiner(), ""poll paused"");","I think I must still be missing something in terms of how the promises interact with the existing filter stack machinery.Am I correct in understanding that this code will be used only for individual filters that are implemented as promises, not for the case where the entire filter stack has been converted to use promises?  It looks like the activity here is just for this one filter, not for the call as a whole.  If so, doesn't that mean that the promise will only ever be woken up when there's an async callback for some work done *while this filter is processing an op*?If so, then I don't understand why we ever need to yield or re-enter the call combiner in this code.  The existing filter code is designed to hold the call combiner while a given batch is transiting either down or up through the call stack, even if async work is done during that transit.  It's not clear to me why that model should change just because the async work is now being done via a promise instead of via callbacks.  I think the only time we should need to yield the call combiner would be when the call gets cancelled, and even then it will happen indirectly: I think we should just register a call combiner cancellation callback that cancels the promise and returns a failure for whatever batch we were processing, which will result in yielding the call combiner when that failure reaches the surface.I will certainly defer to your expertise in performance matters, and even if there is a performance hit here, I do realize that this code is temporary, so it probably doesn't matter in the long run.  But I'm struggling to understand how this approach of always yielding and re-entering the call combiner for async work could *not* cause performance problems, when this is exactly the behavior that I had to work so hard to avoid when I first implemented the call combiner.What am I missing here?  (Happy to chat offline if that will be easier.)",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,808368949,2022-02-16T19:14:08Z,src/core/lib/security/transport/auth_filters.h,"@@ -24,14 +24,55 @@ #include <grpc/grpc_security.h>  #include ""src/core/lib/channel/channel_stack.h""+#include ""src/core/lib/security/credentials/credentials.h""+#include ""src/core/lib/security/security_connector/security_connector.h""+#include ""src/core/lib/transport/transport.h""  extern const grpc_channel_filter grpc_client_auth_filter; extern const grpc_channel_filter grpc_server_auth_filter; -void grpc_auth_metadata_context_build(-    const char* url_scheme, const grpc_slice& call_host,-    const grpc_slice& call_method, grpc_auth_context* auth_context,-    grpc_auth_metadata_context* auth_md_context);+namespace grpc_core {++class ClientAuthFilter final : private AuthMetadataContext {","As elsewhere, please add a comment explaining this choice (the desire to avoid increasing the size of the class by an additional pointer).",
14166415,sanjaypujare,https://api.github.com/repos/grpc/grpc/pulls/28865,808384441,2022-02-16T19:33:10Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -83,6 +83,9 @@ def setUpClass(cls):         """"""Hook method for setting up class fixture before running tests in         the class.         """"""+        logger.info('----- Testing %s -----', cls.__name__)+        logger.info('Logs timezone: %s', time.localtime().tm_zone)","> Why is this the case? Won't UTC be more appropriate?Just wondering why you think UTC is more appropriate - other than the ""universal"" in UTC making us feel better. In terms of usability UTC is least useful since it is never the local time for anyone and the difference from your local time is dependent on standard vs daylight saving time - making UTC a real pain.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28767,808392364,2022-02-16T19:42:59Z,src/core/lib/security/credentials/credentials.h,"@@ -159,6 +161,22 @@ using CredentialsMetadataArray = std::vector<std::pair<Slice, Slice>>;  /* --- grpc_call_credentials. --- */ +namespace grpc_core {++// Abstract interface to obtain contextual data for some call credentials.+class AuthMetadataContext {","We'd still be plumbing something through though, so I'm not convinced it actually simplifies things? And I think it sets the defaults wrong: now if there's a new thing that needs to be passed through we'll prefer to do it eagerly even if it's thrown away, whereas if we expose this interface the default is to expand the interface, preferring lazy computation so that only credentials that need the new thing do the work.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/28895,808408794,2022-02-16T20:03:31Z,third_party/py/python_configure.bzl,"@@ -345,7 +345,7 @@ def _python_autoconf_impl(repository_ctx):         repository_ctx,         ""_python3"",         _PYTHON3_BIN_PATH,-        ""python3"",+        ""python3"" if not _is_windows(repository_ctx) else ""python.exe"",",Would `python3.exe` work here? Or is this naming convention not generally available on windows?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28881,808464647,2022-02-16T21:22:44Z,src/core/lib/resolver/resolver_registry.cc,"@@ -24,20 +24,21 @@  #include <vector> +#include ""absl/memory/memory.h"" #include ""absl/strings/str_cat.h"" #include ""absl/strings/str_format.h""  #include <grpc/support/alloc.h> #include <grpc/support/log.h> #include <grpc/support/string_util.h> -namespace grpc_core {+#include ""src/core/lib/resolver/resolver_registry.h"" -namespace {+namespace grpc_core { -class RegistryState {+class ResolverRegistry::State {","I sent you ctiller/grpc#451 to do some more cleanup here, which (amongst other things) removes the need for this class.  I think this is mostly a hold-over from before we could use STL.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28883,808493210,2022-02-16T22:05:51Z,src/core/lib/config/core_configuration.cc,"@@ -16,10 +16,13 @@  #include ""src/core/lib/config/core_configuration.h"" +#include <atomic>+ #include <grpc/support/log.h>  namespace grpc_core { +std::atomic<Mutex*> CoreConfiguration::builder_mu_;","Why do we need this mutex now, when we didn't before?If we are going to have a mutex here, do we still need atomics?  Could we just use `gpr_once_init()` to set a non-atomic global `Mutex*` variable, and then just use that mutex for the remaining data?Can we add lock annotations, so that it's clear to both readers and to compiler checks which variables are being guarded by this mutex?Can I have a sentence in this comment that is not in the form of a question? :)",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28883,808497220,2022-02-16T22:12:28Z,src/core/plugin_registry/grpc_plugin_registry.cc,"@@ -50,16 +50,12 @@ void grpc_resolver_sockaddr_shutdown(void); void grpc_message_size_filter_init(void);",I think these two forward declarations are no longer needed.,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28883,808549745,2022-02-16T23:47:36Z,src/core/lib/config/core_configuration.cc,"@@ -16,10 +16,13 @@  #include ""src/core/lib/config/core_configuration.h"" +#include <atomic>+ #include <grpc/support/log.h>  namespace grpc_core { +std::atomic<Mutex*> CoreConfiguration::builder_mu_;","We used to allow multiple builders to run concurrently and race to provide the finished configuration: since that happens relatively infrequently, and the configuration is relatively fast to construct, it seemed like a good trade-off.What changes here is that the service config code stores some of its configuration in static variables (specifically the indices of the configured service config slots).We could eliminate this mutex if we stored those indices as part of the configuration, which I considered... however that would mean some additional indirections I believe along the data path to load the core configuration and chase a pointer to get the relevant index. I guess if we had some scheme to locate those indices on the channel that might alleviate it and re-simplify this code.",
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/28900,809240848,2022-02-17T16:30:33Z,src/core/lib/http/httpcli_security_connector.cc,"@@ -192,10 +192,10 @@ class HttpRequestSSLCredentials : public grpc_channel_credentials {   }   private:-  int cmp_impl(const grpc_channel_credentials* other) const override {-    // TODO(yashykt): Check if we can do something better here-    return QsortCompare(static_cast<const grpc_channel_credentials*>(this),-                        other);+  int cmp_impl(const grpc_channel_credentials* /* other */) const override {+    // There's no differentiating factor between two HttpRequestSSLCredentials","If it is the case, should we also create a singleton object for `HttpRequestSslCredentials`? ",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28901,809371631,2022-02-17T18:59:04Z,include/grpcpp/impl/codegen/async_unary_call.h,"@@ -375,7 +374,7 @@ class ServerAsyncResponseWriter final   /// gRPC doesn't take ownership or a reference to status, so it is safe to   /// deallocate them once the Finish operation is complete (i.e. a result   /// arrives in the completion queue).-  void FinishWithError(const ::grpc::Status& status, void* tag) {+  void FinishWithError(const grpc::Status& status, void* tag) {","I'm not sure we can quantify it, but I'd imagine it's a small number. Ignoring `grpc::internal::*` and `grpc::testing::*`, which I very much hope nobody has co-opted ... if users have some `my::own::grpc` namespace, and inside of it, symbols such as `Server`, `Service` or `Channel`, there's a chance of conflicts.I don't think it's a breaking change, and it may be worth trying to land this improvement and see if indeed anyone takes issue. But I'm curious what others think. @ctiller, what's your take?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28847,809564781,2022-02-17T23:51:13Z,src/core/ext/transport/inproc/inproc_transport.cc,"@@ -710,12 +710,6 @@ void op_state_machine_locked(inproc_stream* s, grpc_error_handle error) {             .recv_initial_metadata->Set(grpc_core::GrpcTimeoutMetadata(),                                         s->deadline);       }-      if (s->recv_initial_md_op->payload->recv_initial_metadata",Why is this block no longer needed?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28847,809568157,2022-02-17T23:58:44Z,src/core/lib/surface/call.cc,"@@ -1030,6 +1030,12 @@ static void post_batch_completion(batch_control* bctl) {   if (bctl->op.send_trailing_metadata) {     call->send_trailing_metadata.Clear();   }+  if (bctl->op.recv_initial_metadata && !bctl->op.recv_trailing_metadata) {+    if (call->is_client && call->is_trailers_only) {+      error = grpc_error_add_child(error, GRPC_ERROR_CREATE_FROM_STATIC_STRING(","If we're going to just turn this into an error anyway, why not simply have the transport return the error directly, and just remove the `trailing_metadata_available` field from the `grpc_transport_stream_op_batch_payload` struct completely?Also, if we're going to return an error for this op back to through the surface API, then that implies that we can back out the changes in #26379.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28847,809568827,2022-02-18T00:00:12Z,src/core/ext/transport/inproc/inproc_transport.cc,"@@ -710,12 +710,6 @@ void op_state_machine_locked(inproc_stream* s, grpc_error_handle error) {             .recv_initial_metadata->Set(grpc_core::GrpcTimeoutMetadata(),                                         s->deadline);       }-      if (s->recv_initial_md_op->payload->recv_initial_metadata","And if it's not, doesn't that mean that we also don't need the similar block in chttp2?",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28901,809574050,2022-02-18T00:12:41Z,src/core/lib/gprpp/debug_location.h,"@@ -38,7 +38,7 @@ class DebugLocation {   const char* file_;   const int line_; };-#define DEBUG_LOCATION ::grpc_core::DebugLocation(__FILE__, __LINE__)+#define DEBUG_LOCATION grpc_core::DebugLocation(__FILE__, __LINE__)","Macros should always use the fully qualified name: we don't control where they'll be expanded.For instance... the following customer code would now fail:```#include ""src/core/lib/gprpp/debug_location.h""namespace sprockets_co {namespace grpc_core { class DebugLocation {}; };void foo() {   /* gets sprockets_co::grpc_core::DebugLocation(), not ::grpc_core::DebugLocation() */   CallSomething(DEBUG_LOCATION); }",X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/28847,809599658,2022-02-18T01:13:52Z,test/cpp/end2end/async_end2end_test.cc,"@@ -1332,6 +1332,28 @@ TEST_P(AsyncEnd2endTest, UnimplementedRpc) {   EXPECT_EQ("""", recv_status.error_message()); } +// Test a trailers-only response due to client cancellation",It gets structured as a trailers-only response since we just fake the trailing metadata in case of the cancellation,
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/28847,809603786,2022-02-18T01:25:04Z,src/core/lib/surface/call.cc,"@@ -1030,6 +1030,12 @@ static void post_batch_completion(batch_control* bctl) {   if (bctl->op.send_trailing_metadata) {     call->send_trailing_metadata.Clear();   }+  if (bctl->op.recv_initial_metadata && !bctl->op.recv_trailing_metadata) {+    if (call->is_client && call->is_trailers_only) {+      error = grpc_error_add_child(error, GRPC_ERROR_CREATE_FROM_STATIC_STRING(","If we have the transport return an error then that would result in the surface starting a cancel op down the stack which could overwrite the original fake status. For example, if the call original failed with deadline exceeded, it would now fail with a cancelled status> Also, if we're going to return an error for this op back to through the surface API, then that implies that we can back out the changes in #26379.I had considered this and I was going to remove it but then I considered the case where the batch that includes the initial metadata op also includes the recv status op, and in that case we set the error to GRPC_ERROR_NONE, so the upper layer won't know whether initial metadata was received or not.. which seems to be relevant information for the C++ callback API.",X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/28847,809606361,2022-02-18T01:31:47Z,src/core/ext/transport/inproc/inproc_transport.cc,"@@ -710,12 +710,6 @@ void op_state_machine_locked(inproc_stream* s, grpc_error_handle error) {             .recv_initial_metadata->Set(grpc_core::GrpcTimeoutMetadata(),                                         s->deadline);       }-      if (s->recv_initial_md_op->payload->recv_initial_metadata","This block sets trailing_metadata_available to true if the trailing metadata is available when we are receiving the initial metadata which fits the original metadata of the field, but with the updated definition, we are to only set it to true if we actually received a trailers-only response (or a fake status due to a cancellation).chttp2 doesn't have such a block.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,810287273,2022-02-18T19:32:50Z,src/core/lib/security/credentials/credentials.h,"@@ -159,6 +161,22 @@ using CredentialsMetadataArray = std::vector<std::pair<Slice, Slice>>;  /* --- grpc_call_credentials. --- */ +namespace grpc_core {++// Abstract interface to obtain contextual data for some call credentials.+class AuthMetadataContext {","I agree that we should not be eagerly computing anything here.  I think the pattern here should be to pass through whatever ""raw"" attributes we already have available that are needed by creds implementations, but make it the responsibility of the creds implementations to do whatever computation they need on those raw attributes.I think passing through the attributes that we already have available should be very straightforward; it can be done via an args struct, so that it's easily extensible.   Today, the args struct would contain only the initial metadata and the URL scheme, but we can add new fields to it as needed.  That seems like a much cleaner and less invasive API than this interface, which pushes computation needed by the implementations into the client auth filter, and it avoids the need for one of the cases where we're using private inheritance.If you'd like, I can send you a PR to change this?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,810291361,2022-02-18T19:39:30Z,src/core/lib/channel/promise_based_filter.h,"@@ -325,39 +374,137 @@ class CallData<ChannelFilter, true> : public BaseCallData {     GPR_ASSERT(!is_polling_);     grpc_closure* call_closure = nullptr;     is_polling_ = true;+    grpc_error_handle cancel_send_initial_metadata_error = GRPC_ERROR_NONE;+    grpc_transport_stream_op_batch* forward_batch = nullptr;     switch (send_initial_state_) {       case SendInitialState::kQueued:       case SendInitialState::kForwarded: {         // Poll the promise once since we're waiting for it.-        Poll<TrailingMetadata> poll = promise_();+        Poll<TrailingMetadata> poll;+        {+          ScopedActivity activity(this);+          poll = promise_();+        }         if (auto* r = absl::get_if<TrailingMetadata>(&poll)) {-          GPR_ASSERT(recv_trailing_state_ == RecvTrailingState::kComplete);-          GPR_ASSERT(recv_trailing_metadata_ == UnwrapMetadata(std::move(*r)));-          recv_trailing_state_ = RecvTrailingState::kResponded;-          call_closure =-              absl::exchange(original_recv_trailing_metadata_ready_, nullptr);+          promise_ = ArenaPromise<TrailingMetadata>();+          auto* md = UnwrapMetadata(std::move(*r));+          bool destroy_md = true;+          switch (recv_trailing_state_) {+            case RecvTrailingState::kComplete:+              if (recv_trailing_metadata_ != md) {+                *recv_trailing_metadata_ = std::move(*md);+              } else {+                destroy_md = false;+              }+              recv_trailing_state_ = RecvTrailingState::kResponded;+              call_closure = absl::exchange(+                  original_recv_trailing_metadata_ready_, nullptr);+              break;+            case RecvTrailingState::kQueued:+            case RecvTrailingState::kForwarded: {+              GPR_ASSERT(*md->get_pointer(GrpcStatusMetadata()) !=+                         GRPC_STATUS_OK);+              grpc_error_handle error = grpc_error_set_int(+                  GRPC_ERROR_CREATE_FROM_STATIC_STRING(+                      ""early return from promise based filter""),+                  GRPC_ERROR_INT_GRPC_STATUS,+                  *md->get_pointer(GrpcStatusMetadata()));+              if (auto* message = md->get_pointer(GrpcMessageMetadata())) {+                error = grpc_error_set_str(error, GRPC_ERROR_STR_GRPC_MESSAGE,+                                           message->as_string_view());+              }+              if (recv_trailing_state_ == RecvTrailingState::kQueued) {+                GPR_ASSERT(send_initial_state_ == SendInitialState::kQueued);+                send_initial_state_ = SendInitialState::kCancelled;+                cancel_send_initial_metadata_error = error;+              } else {+                forward_batch =+                    grpc_make_transport_stream_op(GRPC_CLOSURE_CREATE(+                        [](void*, grpc_error_handle) {}, nullptr, nullptr));+                forward_batch->cancel_stream = true;+                forward_batch->payload->cancel_stream.cancel_error = error;+              }+              recv_trailing_state_ = RecvTrailingState::kCancelled;+            } break;+            case RecvTrailingState::kInitial:+              abort();  // unimplemented+            case RecvTrailingState::kResponded:+            case RecvTrailingState::kCancelled:+              abort();  // unreachable+          }+          if (destroy_md) {+            md->~grpc_metadata_batch();+          }         }       } break;       case SendInitialState::kInitial:       case SendInitialState::kCancelled:-        // If we get a response without sending anything, we just propagate that-        // up. (note: that situation isn't possible once we finish the promise-        // transition).+        // If we get a response without sending anything, we just propagate+        // that up. (note: that situation isn't possible once we finish the+        // promise transition).         if (recv_trailing_state_ == RecvTrailingState::kComplete) {           recv_trailing_state_ = RecvTrailingState::kResponded;           call_closure =               absl::exchange(original_recv_trailing_metadata_ready_, nullptr);         }         break;     }+    GRPC_CALL_STACK_REF(call_stack(), ""finish_poll"");     is_polling_ = false;+    bool in_combiner = true;+    if (forward_batch != nullptr) {+      in_combiner = false;+      forward_send_initial_metadata_ = false;+      grpc_call_next_op(elem(), forward_batch);+    }+    if (cancel_send_initial_metadata_error != GRPC_ERROR_NONE) {+      GPR_ASSERT(in_combiner);+      forward_send_initial_metadata_ = false;+      in_combiner = false;+      grpc_transport_stream_op_batch_finish_with_failure(+          absl::exchange(send_initial_metadata_batch_, nullptr),+          cancel_send_initial_metadata_error, call_combiner());+    }     if (absl::exchange(forward_send_initial_metadata_, false)) {+      GPR_ASSERT(in_combiner);+      in_combiner = false;       grpc_call_next_op(elem(),                         absl::exchange(send_initial_metadata_batch_, nullptr));     }     if (call_closure != nullptr) {+      GPR_ASSERT(in_combiner);+      in_combiner = false;       Closure::Run(DEBUG_LOCATION, call_closure, GRPC_ERROR_NONE);     }+    if (absl::exchange(repoll_, false)) {+      if (in_combiner) {+        WakeInsideCombiner();+      } else {+        struct NextPoll : public grpc_closure {+          grpc_call_stack* call_stack;+          CallData* call_data;+        };+        auto run = [](void* p, grpc_error_handle) {+          auto* next_poll = static_cast<NextPoll*>(p);+          next_poll->call_data->WakeInsideCombiner();+          GRPC_CALL_STACK_UNREF(next_poll->call_stack, ""re-poll"");+          delete next_poll;+        };+        auto* p = new NextPoll;+        GRPC_CALL_STACK_REF(call_stack(), ""re-poll"");+        GRPC_CLOSURE_INIT(p, run, p, nullptr);+        GRPC_CALL_COMBINER_START(call_combiner(), p, GRPC_ERROR_NONE,+                                 ""re-poll"");+      }+    } else if (in_combiner) {+      GRPC_CALL_COMBINER_STOP(call_combiner(), ""poll paused"");","Just to leave a quick record of our offline discussion: This is indeed adding places where we yield and reenter the call combiner for every async operation.  However, we want the flexibility to be able to chain together multiple filters that implement promises, and in that case it may be possible for multiple operations to be going on in parallel, at which point it would be necessary to deal with the call combiner this way.  But we'll see how this goes; if it turns out that the performance overhead of this is too high, we can change the code here in promise_based_filter.h without affecting any individual filters that have been migrated to use this API.Marking this resolved.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28767,810292252,2022-02-18T19:41:05Z,src/core/lib/channel/promise_based_filter.h,"@@ -196,9 +228,23 @@ class CallData<ChannelFilter, true> : public BaseCallData {       if (recv_trailing_state_ == RecvTrailingState::kQueued) {         recv_trailing_state_ = RecvTrailingState::kCancelled;","Looks like you missed this comment again: I think we can now set `recv_trailing_state_` just once, not both here and below.Not a big deal if there's a reason not to do that, though -- just pinging the thread since github conversations are so damn hard to find.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28947,813035359,2022-02-23T15:46:55Z,src/core/plugin_registry/grpc_plugin_registry_extra.cc,"@@ -74,18 +74,44 @@ void grpc_register_extra_plugins() { }  namespace grpc_core {++extern void RegisterGoogleDefaultChannelCredsFactory(CoreConfiguration::Builder* builder);","I don't think this approach is going to work, because it will pull in the SSL dependency into the base gRPC build.  This would basically undo the benefit we got from Yihua's recent change where the cred types are all in their own build targets and are linked in only when explicitly used by the application.Instead, I suggest an approach whereby the creds types automatically register themselves (if not already registered) when they are instantiated.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28883,813100096,2022-02-23T16:48:29Z,src/core/lib/service_config/service_config_parser.cc,"@@ -26,10 +26,9 @@ ServiceConfigParser ServiceConfigParser::Builder::Build() {   return ServiceConfigParser(std::move(registered_parsers_)); } -size_t ServiceConfigParser::Builder::RegisterParser(+void ServiceConfigParser::Builder::RegisterParser(     std::unique_ptr<Parser> parser) {-  registered_parsers_.push_back(std::move(parser));-  return registered_parsers_.size() - 1;+  registered_parsers_.emplace_back(std::move(parser));","Please add a check here to ensure that the name of the new parser is not already used by a previously registered parser.  Otherwise, we'll wind up being blind to naming conflicts that will cause crashes when the calling code down-casts the resulting parsed config to the wrong type.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28883,813121792,2022-02-23T17:09:47Z,src/core/lib/config/core_configuration.h,"@@ -86,6 +93,8 @@ class CoreConfiguration {   template <typename BuildFunc>   static void BuildSpecialConfiguration(BuildFunc build) {     // Build bespoke configuration+    // We don't care about the builder lock here, since it's expected this call",I think this comment is no longer needed.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28915,813173229,2022-02-23T18:08:02Z,src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc,"@@ -215,12 +222,11 @@ class GrpcLb : public LoadBalancingPolicy {     // Created after the first serverlist is received.     RefCountedPtr<GrpcLbClientStats> client_stats_;     grpc_millis client_stats_report_interval_ = 0;-    grpc_timer client_load_report_timer_;+    EventEngine::TaskHandle client_load_report_handle_;     bool client_load_report_timer_callback_pending_ = false;     bool last_client_load_report_counters_were_zero_ = false;     bool client_load_report_is_due_ = false;-    // The closure used for either the load report timer or the callback for-    // completion of sending the load report.+    // The closure used for the completion of sending the load report.     grpc_closure client_load_report_closure_;",Suggest renaming this to `client_load_report_done_closure_`.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28915,813180455,2022-02-23T18:16:53Z,src/core/ext/filters/client_channel/lb_policy/grpclb/grpclb.cc,"@@ -215,12 +222,11 @@ class GrpcLb : public LoadBalancingPolicy {     // Created after the first serverlist is received.     RefCountedPtr<GrpcLbClientStats> client_stats_;     grpc_millis client_stats_report_interval_ = 0;-    grpc_timer client_load_report_timer_;+    EventEngine::TaskHandle client_load_report_handle_;     bool client_load_report_timer_callback_pending_ = false;","Now that we're not reusing a closure for this callback, I think we don't need this data member anymore.  Instead, we can just use `absl::optional<>` for the `client_load_report_handle_` field above.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28922,813190299,2022-02-23T18:28:44Z,include/grpc/impl/codegen/grpc_types.h,"@@ -533,10 +529,7 @@ typedef enum grpc_call_error {  /** Mask of all valid flags */ #define GRPC_INITIAL_METADATA_USED_MASK                  \-  (GRPC_INITIAL_METADATA_IDEMPOTENT_REQUEST |            \-   GRPC_INITIAL_METADATA_WAIT_FOR_READY |                \",Looks like you accidentally removed this one here.  I think it needs to stay.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28922,813201853,2022-02-23T18:41:30Z,src/core/ext/filters/http/server/http_server_filter.cc,"@@ -76,12 +73,6 @@ struct call_data {   uint32_t* recv_initial_metadata_flags;   bool seen_recv_initial_metadata_ready = false; -  // State for intercepting recv_message.",I think we also no longer need the `read_stream` or `have_read_stream` data members.,
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/28953,813264505,2022-02-23T19:54:36Z,tools/run_tests/xds_k8s_test_driver/bin/cleanup/cleanup.py,"@@ -337,8 +337,9 @@ def main(argv):     compute = gcp.compute.ComputeV1(gcp.api.GcpApiManager(), project)     leakedHealthChecks = []     for item in compute.list_health_check()['items']:-        if datetime.datetime.fromisoformat(-                item['creationTimestamp']) <= get_expire_timestamp():+        if datetime.datetime.strptime(+                item['creationTimestamp'],+                ""%Y-%m-%dT%H:%M:%S.%f%z"") <= get_expire_timestamp():","TIL neither `fromisoformat` nor strptime are good enough ISO time parsers.(1) `fromisoformat`: https://docs.python.org/3/library/datetime.html#datetime.datetime.fromisoformat> Caution This does not support parsing arbitrary ISO 8601 strings - it is only intended as the inverse operation of [datetime.isoformat()](https://docs.python.org/3/library/datetime.html#datetime.datetime.isoformat). A more full-featured ISO 8601 parser, [dateutil.parser.isoparse](https://dateutil.readthedocs.io/en/stable/parser.html#dateutil.parser.isoparse) is available in the third-party package dateutil.And it doesn't support Zulu time:```>>> datetime.fromisoformat(""2008-09-03T20:56:35.450686Z"")Traceback (most recent call last):  File ""<stdin>"", line 1, in <module>ValueError: Invalid isoformat string: '2008-09-03T20:56:35.450686Z'```(2) `strptime` does is pretty terrible with timezones in python3.6: https://stackoverflow.com/a/30696682/11697987```# zone with `:` separators not supported>>> datetime.strptime(""2008-09-03T20:56:35.450686+04:00"", ""%Y-%m-%dT%H:%M:%S.%f%z"")Traceback (most recent call last):  File ""<stdin>"", line 1, in <module>  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/_strptime.py"", line 565, in _strptime_datetime    tt, fraction = _strptime(data_string, format)  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/_strptime.py"", line 362, in _strptime    (data_string, format))ValueError: time data '2008-09-03T20:56:35.450686+04:00' does not match format '%Y-%m-%dT%H:%M:%S.%f%z'# Zulu not supported>>> datetime.strptime(""2008-09-03T20:56:35.450686Z"", ""%Y-%m-%dT%H:%M:%S.%f%z"")Traceback (most recent call last):  File ""<stdin>"", line 1, in <module>  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/_strptime.py"", line 565, in _strptime_datetime    tt, fraction = _strptime(data_string, format)  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/_strptime.py"", line 362, in _strptime    (data_string, format))ValueError: time data '2008-09-03T20:56:35.450686Z' does not match format '%Y-%m-%dT%H:%M:%S.%f%z'```Let's use [dateutil.parser.isoparse](https://dateutil.readthedocs.io/en/stable/parser.html#dateutil.parser.isoparse), as recommended in [python docs](https://docs.python.org/3/library/datetime.html#datetime.datetime.fromisoformat).",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/28953,813275799,2022-02-23T20:09:28Z,tools/run_tests/xds_k8s_test_driver/requirements.txt,"@@ -2,6 +2,7 @@ Mako~=1.1 PyYAML~=5.3 absl-py~=0.11 dataclasses~=0.8; python_version < '3.7'+dateutil~=2.8.2","```suggestiondateutil~=2.8```It's not exactly semver. To allow patch-level updates, the last component needs to be omitted. See https://www.python.org/dev/peps/pep-0440/#compatible-release```~= 2.2>= 2.2, == 2.*~= 1.4.5>= 1.4.5, == 1.4.*```",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28883,813366486,2022-02-23T22:08:22Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -1441,17 +1441,16 @@ void ClientChannel::RemoveResolverQueuedCall(ResolverQueuedCall* to_remove, void ClientChannel::UpdateServiceConfigInControlPlaneLocked(     RefCountedPtr<ServiceConfig> service_config,     RefCountedPtr<ConfigSelector> config_selector, const char* lb_policy_name) {-  UniquePtr<char> service_config_json(-      gpr_strdup(service_config->json_string().c_str()));+  std::string service_config_json(service_config->json_string());   if (GRPC_TRACE_FLAG_ENABLED(grpc_client_channel_routing_trace)) {     gpr_log(GPR_INFO,             ""chand=%p: resolver returned updated service config: \""%s\"""", this,-            service_config_json.get());+            service_config_json.c_str());   }   // Save service config.   saved_service_config_ = std::move(service_config);   // Swap out the data used by GetChannelInfo().-  UniquePtr<char> lb_policy_name_owned(gpr_strdup(lb_policy_name));+  std::string lb_policy_name_owned = lb_policy_name;","How about just changing this method to take the `lb_policy_name` parameter as a `std::string`, so it doesn't need to convert internally?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28922,814180352,2022-02-24T19:02:31Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -10503,95 +10503,6 @@ TEST_P(XdsRbacTestWithActionPermutations, MethodPostPermissionAnyPrincipal) {   SendRpc([this]() { return CreateInsecureChannel(); }, {}, {},           /*test_expects_failure=*/GetParam().rbac_action() == RBAC_Action_DENY,           grpc::StatusCode::PERMISSION_DENIED);-  // Test an RPC with a different method type","I want to make sure that we don't open ourselves to a security-related bug here.  [gRFC A41](https://github.com/grpc/proposal/blob/master/A41-xds-rbac.md) says:> For this design, `headers` can include `:method`, `:authority`, and `:path` matchers and they should match the values received on-the-wire independent of whether they are stored in Metadata or in separate APIs. `:method` can be hard-coded to `POST` if unavailable and a code audit confirms the server denies requests for all other method types.Do we have a guarantee that the transport will reject GET or PUT requests once this PR goes in?  If so, then that offers at least some assurance that the current behavior will be correct, even without the tests.  But it's really not the same as having a test.Perhaps more importantly, if we re-add support for GET or PUT in the future, how will we remember to re-add these tests?CC @ejona86 for his input as well.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28940,814378669,2022-02-25T00:19:15Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_verifier.cc,"@@ -30,6 +30,15 @@  namespace grpc_core { +namespace {+const char kTlsCertificateVerifierTypeExternal[] = ""external"";","Suggest making this a private static member of the `ExternalCertificateVerifier` class, so that it's not possible for an external impl to accidentally use it.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28940,814383950,2022-02-25T00:32:31Z,src/core/lib/security/credentials/xds/xds_credentials.cc,"@@ -61,44 +61,57 @@ bool XdsVerifySubjectAlternativeNames(   return false; } -class XdsCertificateVerifier : public grpc_tls_certificate_verifier {- public:-  XdsCertificateVerifier(-      RefCountedPtr<XdsCertificateProvider> xds_certificate_provider,-      std::string cluster_name)-      : xds_certificate_provider_(std::move(xds_certificate_provider)),-        cluster_name_(std::move(cluster_name)) {}--  bool Verify(grpc_tls_custom_verification_check_request* request,-              std::function<void(absl::Status)>,-              absl::Status* sync_status) override {-    GPR_ASSERT(request != nullptr);-    if (!XdsVerifySubjectAlternativeNames(-            request->peer_info.san_names.uri_names,-            request->peer_info.san_names.uri_names_size,-            xds_certificate_provider_->GetSanMatchers(cluster_name_)) &&-        !XdsVerifySubjectAlternativeNames(-            request->peer_info.san_names.ip_names,-            request->peer_info.san_names.ip_names_size,-            xds_certificate_provider_->GetSanMatchers(cluster_name_)) &&-        !XdsVerifySubjectAlternativeNames(-            request->peer_info.san_names.dns_names,-            request->peer_info.san_names.dns_names_size,-            xds_certificate_provider_->GetSanMatchers(cluster_name_))) {-      *sync_status = absl::Status(-          absl::StatusCode::kUnauthenticated,-          ""SANs from certificate did not match SANs from xDS control plane"");-    }-    return true; /* synchronous check */+const char kTlsCertificateVerifierTypeXds[] = ""xds"";++}  // namespace++//+// XdsCertificateVerifier+//++XdsCertificateVerifier::XdsCertificateVerifier(+    RefCountedPtr<XdsCertificateProvider> xds_certificate_provider,+    std::string cluster_name)+    : grpc_tls_certificate_verifier(kTlsCertificateVerifierTypeXds),+      xds_certificate_provider_(std::move(xds_certificate_provider)),+      cluster_name_(std::move(cluster_name)) {}++bool XdsCertificateVerifier::Verify(+    grpc_tls_custom_verification_check_request* request,+    std::function<void(absl::Status)>, absl::Status* sync_status) {+  GPR_ASSERT(request != nullptr);+  if (!XdsVerifySubjectAlternativeNames(+          request->peer_info.san_names.uri_names,+          request->peer_info.san_names.uri_names_size,+          xds_certificate_provider_->GetSanMatchers(cluster_name_)) &&+      !XdsVerifySubjectAlternativeNames(+          request->peer_info.san_names.ip_names,+          request->peer_info.san_names.ip_names_size,+          xds_certificate_provider_->GetSanMatchers(cluster_name_)) &&+      !XdsVerifySubjectAlternativeNames(+          request->peer_info.san_names.dns_names,+          request->peer_info.san_names.dns_names_size,+          xds_certificate_provider_->GetSanMatchers(cluster_name_))) {+    *sync_status = absl::Status(+        absl::StatusCode::kUnauthenticated,+        ""SANs from certificate did not match SANs from xDS control plane"");   }-  void Cancel(grpc_tls_custom_verification_check_request*) override {}+  return true; /* synchronous check */+} - private:-  RefCountedPtr<XdsCertificateProvider> xds_certificate_provider_;-  std::string cluster_name_;-};+void XdsCertificateVerifier::Cancel(+    grpc_tls_custom_verification_check_request*) {} -}  // namespace+int XdsCertificateVerifier::cmp_impl(+    const grpc_tls_certificate_verifier* other) const {+  auto* o = static_cast<const XdsCertificateVerifier*>(other);+  if (xds_certificate_provider_ == o->xds_certificate_provider_ &&+      cluster_name_ == o->cluster_name_) {+    return 0;+  }+  return QsortCompare(static_cast<const grpc_tls_certificate_verifier*>(this),","I don't think we need to fall back on comparing the pointer values if we're comparing all of the data members anyway.  Instead, I think we can do something like this:```int r = QsortCompare(xds_certificate_provider_, o->xds_certificate_provider_);if (r != 0) return r;return cluster_name_.compare(o->cluster_name_);```",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28940,814386304,2022-02-25T00:36:44Z,test/core/security/credentials_test.cc,"@@ -3701,6 +3693,72 @@ TEST(CredentialsTest, TestXdsCredentialsCompareFailure) {   grpc_channel_credentials_release(xds_creds_2); } +TEST(TlsCertificateVerifierTest, ComparingDifferentObjectTypesFails) {",The verifier tests belong in test/core/security/grpc_tls_certificate_verifier_test.cc.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28940,814389068,2022-02-25T00:42:30Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -7538,13 +7538,11 @@ class XdsSecurityTest : public BasicTest {     constexpr int kRetryCount = 100;     int num_tries = 0;     for (; num_tries < kRetryCount; num_tries++) {-      // Give some time for the updates to propagate.-      gpr_sleep_until(grpc_timeout_milliseconds_to_deadline(100));+      // Restart the servers to force a reconnection so that previously","Can you explain why this change is here?  I imagine it has something to do with the following TODO, but the exact reasoning isn't clear.https://github.com/grpc/grpc/blob/87068e02c3c4d34f51019b3ccdceea1230c92fea/src/core/lib/security/credentials/xds/xds_credentials.cc#L167Speaking of which, shouldn't we remove that TODO as part of this PR?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28847,814391990,2022-02-25T00:50:24Z,src/core/lib/surface/call.cc,"@@ -1030,6 +1030,12 @@ static void post_batch_completion(batch_control* bctl) {   if (bctl->op.send_trailing_metadata) {     call->send_trailing_metadata.Clear();   }+  if (bctl->op.recv_initial_metadata && !bctl->op.recv_trailing_metadata) {+    if (call->is_client && call->is_trailers_only) {+      error = grpc_error_add_child(error, GRPC_ERROR_CREATE_FROM_STATIC_STRING(","> If we have the transport return an error then that would result in the surface starting a cancel op down the stack which could overwrite the original fake status. For example, if the call original failed with deadline exceeded, it would now fail with a cancelled statusShouldn't the transport use whichever status happened first?> I had considered this and I was going to remove it but then I considered the case where the batch that includes the initial metadata op also includes the recv status op, and in that case we set the error to GRPC_ERROR_NONE, so the upper layer won't know whether initial metadata was received or not.. which seems to be relevant information for the C++ callback API.Does the callback API ever put the recv_initial_metadata and recv_trailing_metadata ops in the same batch?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28847,814392614,2022-02-25T00:52:01Z,test/cpp/end2end/async_end2end_test.cc,"@@ -1332,6 +1332,28 @@ TEST_P(AsyncEnd2endTest, UnimplementedRpc) {   EXPECT_EQ("""", recv_status.error_message()); } +// Test a trailers-only response due to client cancellation",Might be cleaner to call the test something like `FailRecvInitialMetadataUponCancellation`.  The behavior being tested here really has nothing to do with receiving a Trailers-Only response.,X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/28847,814404005,2022-02-25T01:21:54Z,src/core/lib/surface/call.cc,"@@ -1030,6 +1030,12 @@ static void post_batch_completion(batch_control* bctl) {   if (bctl->op.send_trailing_metadata) {     call->send_trailing_metadata.Clear();   }+  if (bctl->op.recv_initial_metadata && !bctl->op.recv_trailing_metadata) {+    if (call->is_client && call->is_trailers_only) {+      error = grpc_error_add_child(error, GRPC_ERROR_CREATE_FROM_STATIC_STRING(","> Shouldn't the transport use whichever status happened first?We could maybe, but we've got super old code that advocates the other way around https://github.com/grpc/grpc/blob/87068e02c3c4d34f51019b3ccdceea1230c92fea/src/core/ext/transport/chttp2/transport/chttp2_transport.cc#L2043 @ctiller might know the context here better. Also, I'm not sure if how that would with the new promise API ",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/28847,814409715,2022-02-25T01:36:50Z,src/core/lib/surface/call.cc,"@@ -1030,6 +1030,12 @@ static void post_batch_completion(batch_control* bctl) {   if (bctl->op.send_trailing_metadata) {     call->send_trailing_metadata.Clear();   }+  if (bctl->op.recv_initial_metadata && !bctl->op.recv_trailing_metadata) {+    if (call->is_client && call->is_trailers_only) {+      error = grpc_error_add_child(error, GRPC_ERROR_CREATE_FROM_STATIC_STRING(","> Does the callback API ever put the recv_initial_metadata and recv_trailing_metadata ops in the same batch?It doeshttps://github.com/grpc/grpc/blob/87068e02c3c4d34f51019b3ccdceea1230c92fea/include/grpcpp/impl/codegen/client_callback.h#L76Hmm, but this instance does not care for the success status of the op, so maybe it's ok to revert that change.",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/28940,814415001,2022-02-25T01:50:55Z,src/core/lib/security/credentials/tls/grpc_tls_credentials_options.h,"@@ -126,6 +126,25 @@ struct grpc_tls_credentials_options   // not enable CRL checking. Only supported for OpenSSL version > 1.1.   void set_crl_directory(std::string path) { crl_directory_ = std::move(path); } +  bool operator==(const grpc_tls_credentials_options& other) const {+    return cert_request_type_ == other.cert_request_type_ &&+           verify_server_cert_ == other.verify_server_cert_ &&+           min_tls_version_ == other.min_tls_version_ &&+           max_tls_version_ == other.max_tls_version_ &&+           (verifier_ == other.verifier_ ||+            (verifier_ != nullptr && other.verifier_ != nullptr &&+             verifier_->cmp(other.verifier_.get()) == 0)) &&+           check_call_host_ == other.check_call_host_ &&+           provider_ == other.provider_ &&",We need it in verifier because of an explicit need in XdsCertificateVerifier https://github.com/grpc/grpc/blob/87068e02c3c4d34f51019b3ccdceea1230c92fea/src/core/lib/security/credentials/xds/xds_credentials.cc#L163.. We don't have such a need for providers yet.,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28965,814583217,2022-02-25T08:53:06Z,tools/internal_ci/linux/grpc_bazel_test_in_docker.sh,"@@ -1,29 +0,0 @@-#!/usr/bin/env bash-# Copyright 2017 gRPC authors.-#-# Licensed under the Apache License, Version 2.0 (the ""License"");-# you may not use this file except in compliance with the License.-# You may obtain a copy of the License at-#-#     http://www.apache.org/licenses/LICENSE-2.0-#-# Unless required by applicable law or agreed to in writing, software-# distributed under the License is distributed on an ""AS IS"" BASIS,-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.-# See the License for the specific language governing permissions and-# limitations under the License.-#-# Test full Bazel-#-# NOTE: No empty lines should appear in this file before igncr is set!-set -ex -o igncr || set -ex--mkdir -p /var/local/git-git clone /var/local/jenkins/grpc /var/local/git/grpc-(cd /var/local/jenkins/grpc/ && git submodule foreach 'cd /var/local/git/grpc \-&& git submodule update --init --reference /var/local/jenkins/grpc/${name} \-${name}')-cd /var/local/git/grpc--bazel test //test/...-bazel test //test/cpp/end2end:admin_services_end2end_test --define=grpc_no_xds=true","The only place I know of where we could add this is https://github.com/grpc/grpc/blob/c05ecb8a9e99df3ff7cf40c1a947a85b3b4a56c6/tools/internal_ci/linux/grpc_bazel_build_in_docker.sh#L48(but note that that's a ""build only"" job, it doesn't run the test, just verifies it builds).AFAIK Currently we don't have any test that does ""bazel test"" on linux (except the RBE jobs of course).Please let me know what you wanna do.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28847,814876682,2022-02-25T15:45:43Z,src/core/lib/surface/call.cc,"@@ -1030,6 +1030,12 @@ static void post_batch_completion(batch_control* bctl) {   if (bctl->op.send_trailing_metadata) {     call->send_trailing_metadata.Clear();   }+  if (bctl->op.recv_initial_metadata && !bctl->op.recv_trailing_metadata) {+    if (call->is_client && call->is_trailers_only) {+      error = grpc_error_add_child(error, GRPC_ERROR_CREATE_FROM_STATIC_STRING(","> Also, I'm not sure if how that would with the new promise APICraig can jump in here, but from previous conversations with him, I don't think this will be a problem in the promise world, because there should no longer be a need for the surface to send down a cancel op when one of the batches fails; instead, cancellation will work by simply destroying the entire promise tree, at which point there will no longer be any opportunity for the call to even see the status returned by the transport (i.e., the cancellation status will always take precedence).  So I think the entire question of the transport deciding precedence will likely go away; whatever failure happens first will determine the status of the call.I suspect it would be fine to go ahead and change the transport to have this same first-failure-wins behavior now.  But again, I'll let Craig weigh in on that.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28847,814905649,2022-02-25T16:20:27Z,src/core/lib/surface/call.cc,"@@ -1030,6 +1030,12 @@ static void post_batch_completion(batch_control* bctl) {   if (bctl->op.send_trailing_metadata) {     call->send_trailing_metadata.Clear();   }+  if (bctl->op.recv_initial_metadata && !bctl->op.recv_trailing_metadata) {+    if (call->is_client && call->is_trailers_only) {+      error = grpc_error_add_child(error, GRPC_ERROR_CREATE_FROM_STATIC_STRING(","Captain context here!So Mark's right on the promise world: if we cancel somewhere, we simply destroy the lower part of the tree and choose a status at that point.This tends to be the most natural way to define error handling (higher layer things get more say in the decision) and is the reason for the at-first-odd looking rules in transport. Since we have to push errors down to the bottom of the stack to properly publish them (ugh) we need to choose a rule down at the bottom for which error wins that mimics what our top level intuition is.The temporally first error observed by the transport is not that: suppose the transport gets busy and then observes an write error, while concurrently the application decides to cancel the request due to an authentication problem - it's more useful for us to report the authentication problem to the application - so that should dominate.Hence the rule in the transport today: always prefer the structurally highest level error. This will be the behavior in the new promises world too (indeed its going to be terrifically hard to write anything else in that world).",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28847,814927418,2022-02-25T16:47:47Z,src/core/lib/surface/call.cc,"@@ -1030,6 +1030,12 @@ static void post_batch_completion(batch_control* bctl) {   if (bctl->op.send_trailing_metadata) {     call->send_trailing_metadata.Clear();   }+  if (bctl->op.recv_initial_metadata && !bctl->op.recv_trailing_metadata) {+    if (call->is_client && call->is_trailers_only) {+      error = grpc_error_add_child(error, GRPC_ERROR_CREATE_FROM_STATIC_STRING(","Thanks, Craig, that's useful.  Good point about us wanting the status from the highest point in the stack, not the one seen by the transport first.I'm now wondering if we should just defer this PR until after the promise conversion.  At that point, I think it will be much easier to make this change in a clean way.  We'll probably be able to just remove the `trailing_metadata_available` field completely from the transport API.",
28025951,HannahShiSFB,https://api.github.com/repos/grpc/grpc/pulls/28570,814964761,2022-02-25T17:37:38Z,third_party/libuv_test.patch,"@@ -0,0 +1,56 @@+diff --git a/test/test-timer.c b/test/test-timer.c+index c667da00..58693ac2 100644+--- a/test/test-timer.c++++ b/test/test-timer.c+@@ -25,6 +25,8 @@+ + static int once_cb_called = 0;+ static int once_close_cb_called = 0;++static int twice_cb_called = 0;++static int twice_close_cb_called = 0;+ static int repeat_cb_called = 0;+ static int repeat_close_cb_called = 0;+ static int order_cb_called = 0;+@@ -58,6 +60,27 @@ static void once_cb(uv_timer_t* handle) {+   uv_update_time(uv_default_loop());+ }+ ++static void twice_close_cb(uv_handle_t* handle) {++  printf(""TWICE_CLOSE_CB\n"");++++  ASSERT(handle != NULL);++  ASSERT(0 == uv_is_active(handle));++++  twice_close_cb_called++;++}++++static void twice_cb(uv_timer_t* handle) {++  printf(""TWICE_CB %d\n"", twice_cb_called);++++  ASSERT(handle != NULL);++  ASSERT(0 == uv_is_active((uv_handle_t*) handle));++++  twice_cb_called++;++++  uv_close((uv_handle_t*)handle, twice_close_cb);++}+++++ + static void repeat_close_cb(uv_handle_t* handle) {+   printf(""REPEAT_CLOSE_CB\n"");+@@ -144,12 +167,12 @@ TEST_IMPL(timer_start_twice) {+   ASSERT(r == 0);+   r = uv_timer_start(&once, never_cb, 86400 * 1000, 0);+   ASSERT(r == 0);+-  r = uv_timer_start(&once, once_cb, 10, 0);++  r = uv_timer_start(&once, twice_cb, 10, 0);+   ASSERT(r == 0);+   r = uv_run(uv_default_loop(), UV_RUN_DEFAULT);+   ASSERT(r == 0);+ +-  ASSERT(once_cb_called == 1);++  ASSERT(twice_cb_called == 1);","It’s for test case `timer_start_twice`, which is the named similar to other xx_cb variables. Can we keep it for now and let the libuv team decide when I send them the PR?",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28570,814979591,2022-02-25T17:58:11Z,third_party/libuv_test.patch,"@@ -0,0 +1,56 @@+diff --git a/test/test-timer.c b/test/test-timer.c","Thanks. Please collect the patches separately, and remove them from this PR. If it was a patch for something that we absolutely need for libuv to _function_ on iOS, the extra burden is justifiable. But this is not essential, and again, every patch we have to maintain adds a real maintenance burden for library upgrades.",X
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/28570,815222215,2022-02-25T23:39:41Z,third_party/libuv_test.patch,"@@ -0,0 +1,56 @@+diff --git a/test/test-timer.c b/test/test-timer.c","Chatted w/ Hannah offline, we have a solution to run iOS test without patching libuv via [ios_ui_test](https://github.com/bazelbuild/rules_apple/blob/master/doc/rules-ios.md#ios_ui_test). This allows us to re-launch the app per test case via [XCUIApplication.launch](https://developer.apple.com/documentation/xctest/xcuiapplication/1500467-launch?language=objc) if needed in order to simulate the same fork run behavior as other platform.   ",X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/28847,815301676,2022-02-26T11:35:39Z,src/core/lib/surface/call.cc,"@@ -1030,6 +1030,12 @@ static void post_batch_completion(batch_control* bctl) {   if (bctl->op.send_trailing_metadata) {     call->send_trailing_metadata.Clear();   }+  if (bctl->op.recv_initial_metadata && !bctl->op.recv_trailing_metadata) {+    if (call->is_client && call->is_trailers_only) {+      error = grpc_error_add_child(error, GRPC_ERROR_CREATE_FROM_STATIC_STRING(","Given that we've identified broken API, I'd prefer to do it sooner rather than later. (kinda afraid of forgetting about this too). If after we convert to promises, we are able to remove `trailing_metadata_available` field completely, then we could simply delete these six lines from call.cc",
6898909,musicinmybrain,https://api.github.com/repos/grpc/grpc/pulls/28589,816137773,2022-02-28T18:17:17Z,test/core/security/evaluate_args_test.cc,"@@ -32,9 +32,9 @@ class EvaluateArgsTest : public ::testing::Test {  TEST_F(EvaluateArgsTest, EmptyMetadata) {   EvaluateArgs args = util_.MakeEvaluateArgs();-  EXPECT_EQ(args.GetPath(), nullptr);-  EXPECT_EQ(args.GetMethod(), nullptr);-  EXPECT_EQ(args.GetAuthority(), nullptr);+  EXPECT_THAT(args.GetPath(), ::testing::IsEmpty());","I can make this change, but are you sure you want me to? This source file already consistently uses fully-qualified `::testing::` throughout, presumably to avoid ambiguity with `grpc::testing`.",X
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/28922,816208409,2022-02-28T19:58:46Z,src/objective-c/GRPCClient/GRPCCallLegacy.m,"@@ -147,12 +147,6 @@ + (void)setCallSafety:(GRPCCallSafety)callSafety host:(NSString *)host path:(NSS       case GRPCCallSafetyDefault:         callFlags[hostAndPath] = @0;         break;-      case GRPCCallSafetyIdempotentRequest:",we can get rid of the entire switch/case similar to other changes ```objc @synchronized(callFlags) {   callFlags[hostAndPath] = @0;}```,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28922,816212695,2022-02-28T20:05:28Z,src/objective-c/GRPCClient/GRPCTypes.h,"@@ -131,13 +131,6 @@ typedef NS_ENUM(NSUInteger, GRPCCallSafety) {    * state.    */   GRPCCallSafetyDefault = 0,","I think that would be reasonable to do!I'm not aware of any upcoming features that would want a similar API, @markdroth?",
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/28922,816222414,2022-02-28T20:21:23Z,src/objective-c/GRPCClient/GRPCTypes.h,"@@ -131,13 +131,6 @@ typedef NS_ENUM(NSUInteger, GRPCCallSafety) {    * state.    */   GRPCCallSafetyDefault = 0,-  /** Signal that the call is idempotent. gRPC is free to use PUT verb. */","Ideally on iOS, we split this into two steps ( deprecation -> removal), instead of doing a hard build break.  But looks like this API is a ""semi-public"" (private to bazel/blaze codegen users, but public for anyone directly using GRPCCallSafety).  Can we at least mark this as deprecated instead of removal ?   ```objc   GRPCCallSafetyIdempotentRequest __attribute__((deprecated(""GRPCCallSafetyIdempotentRequest is deprecated.""))) = 1,  GRPCCallSafetyCacheableRequest __attribute__((deprecated(""GRPCCallSafetyCacheableRequest is deprecated.""))) = 2,```g3 internal seems ok (cs shows no existing usage), but OSS side this at least given the devs a chance to fix their code when picking up the change. Thanks ! ",X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/28940,816322932,2022-02-28T23:12:22Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_provider.cc,"@@ -30,9 +30,12 @@  namespace grpc_core { +const char StaticDataCertificateProvider::kType[] = ""static"";","Suggest calling this ""static_data""..",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28986,816913377,2022-03-01T16:04:18Z,src/core/ext/filters/client_channel/resolver/xds/xds_resolver.cc,"@@ -648,13 +648,16 @@ ConfigSelector::CallConfig XdsResolver::XdsConfigSelector::GetCallConfig(   absl::optional<uint64_t> hash;   for (const auto& hash_policy : route_action->hash_policies) {     absl::optional<uint64_t> new_hash;+    std::string new_hash_string;","This can go under the `case` statement where it is used.  Just put braces around the body of the `case` statement.  In other words:```case XdsRouteConfigResource::Route::RouteAction::HashPolicy::CHANNEL_ID: {  std::string new_hash_string;  // ...code...  break;}```But that having been said, I'm not sure that taking the hash of the address is the best approach to begin with.  Instead, how about just computing a random value at `XdsResolver` instantiation time and then using that value here instead of the address of the `XdsResolver`?  It seems like that would give better distribution.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28922,816925256,2022-03-01T16:17:18Z,src/objective-c/GRPCClient/GRPCTypes.h,"@@ -131,13 +131,6 @@ typedef NS_ENUM(NSUInteger, GRPCCallSafety) {    * state.    */   GRPCCallSafetyDefault = 0,-  /** Signal that the call is idempotent. gRPC is free to use PUT verb. */",Is the proposal here to leave the enums in (marked deprecated) but silently change the behavior?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28987,816971378,2022-03-01T17:06:33Z,src/core/ext/filters/client_channel/global_subchannel_pool.h,"@@ -57,9 +47,12 @@ class GlobalSubchannelPool final : public SubchannelPoolInterface {       ABSL_LOCKS_EXCLUDED(mu_);   private:+  GlobalSubchannelPool() {}+  ~GlobalSubchannelPool() override {}+   // The singleton instance. (It's a pointer to RefCountedPtr so that this   // non-local static object can be trivially destructible.)-  static RefCountedPtr<GlobalSubchannelPool>* instance_;+  static std::atomic<RefCountedPtr<GlobalSubchannelPool>*> instance_;","I think we can remove this data member and instead make it a local static variable within the `instance()` method.  That way, it does not need to be atomic.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28922,817987629,2022-03-02T18:34:07Z,src/objective-c/GRPCClient/GRPCTypes.h,"@@ -131,13 +131,6 @@ typedef NS_ENUM(NSUInteger, GRPCCallSafety) {    * state.    */   GRPCCallSafetyDefault = 0,","No immediate plans, no.  But in the long run, I would not be surprised if we wind up re-adding GET support.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28940,818020308,2022-03-02T19:17:54Z,src/core/lib/security/credentials/tls/grpc_tls_certificate_verifier.h,"@@ -57,19 +61,20 @@ struct grpc_tls_certificate_verifier   // Compares this grpc_tls_certificate_verifier object with \a other.   // If this method returns 0, it means that gRPC can treat the two channel   // credentials as effectively the same.-  int cmp(const grpc_tls_certificate_verifier* other) const {+  int Compare(const grpc_tls_certificate_verifier* other) const {     GPR_ASSERT(other != nullptr);     int r = strcmp(type(), other->type());     if (r != 0) return r;-    return cmp_impl(other);+    return CompareImpl(other);   }    const char* type() const { return type_; }","As we've discussed offline, let's change the API for all of the classes involved here (credentials, verifier, provider, and anything else) such that instead of passing the type name pointer to the base class and requiring the base class to store it, we instead just make the `type()` method pure virtual, so that we can directly inline the string in that method in the subclasses.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28940,818038803,2022-03-02T19:43:19Z,src/core/lib/security/credentials/tls/grpc_tls_credentials_options.h,"@@ -126,6 +126,25 @@ struct grpc_tls_credentials_options   // not enable CRL checking. Only supported for OpenSSL version > 1.1.   void set_crl_directory(std::string path) { crl_directory_ = std::move(path); } +  bool operator==(const grpc_tls_credentials_options& other) const {","This looks great!Please also add these generated files to the [`.gitattributes`](https://github.com/grpc/grpc/blob/master/.gitattributes) file so that they don't show up in diffs by default.Also, please make sure this script is automatically run as part of the sanity check before PR submission, so that people don't accidentally manually edit these files and not notice that they actually need to do it via the script.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28985,818058415,2022-03-02T20:04:26Z,doc/server_side_auth.md,"@@ -12,6 +12,13 @@ The authentication context is structured as a multi-map of key-value pairs - the  The contents of the *auth properties* are populated by an *auth interceptor*. The interceptor also chooses which property key will act as the peer identity (e.g. for client certificate authentication this property will be `""x509_common_name""` or `""x509_subject_alternative_name""`). +Note that AuthContext is not modifiable, unless AuthMetadataProcessor is used([reference](https://github.com/grpc/grpc/blob/master/include/grpcpp/impl/codegen/security/auth_context.h#L89)). ","While you're in this doc, the note on line 5 is no longer correct.  Please consult @yihuazhang for better wording here, but we should instead talk about the security level agreement that has to be in place between the call creds and the security connector.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28985,818059059,2022-03-02T20:05:21Z,doc/server_side_auth.md,"@@ -12,6 +12,13 @@ The authentication context is structured as a multi-map of key-value pairs - the  The contents of the *auth properties* are populated by an *auth interceptor*. The interceptor also chooses which property key will act as the peer identity (e.g. for client certificate authentication this property will be `""x509_common_name""` or `""x509_subject_alternative_name""`). +Note that AuthContext is not modifiable, unless AuthMetadataProcessor is used([reference](https://github.com/grpc/grpc/blob/master/include/grpcpp/impl/codegen/security/auth_context.h#L89)). +When AuthContext is modified through AuthMetadataProcessor, we are able to see the modifications in all the subsequent calls. This is because AuthContext is a channel-level object which could be shared by multiple calls.++AuthContext contains the channel-level information, such as the peer identity, etc. +If it is modified through AuthMetadataProcessor, all",This sentence is incomplete.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28985,818062785,2022-03-02T20:10:30Z,include/grpcpp/security/auth_metadata_processor.h,"@@ -27,29 +27,39 @@  namespace grpc { -/// Interface allowing custom server-side authorization based on credentials-/// encoded in metadata.  Objects of this type can be passed to-/// \a ServerCredentials::SetAuthMetadataProcessor().+// Interface allowing custom server-side authorization based on credentials+// encoded in metadata.  Objects of this type can be passed to+// \a ServerCredentials::SetAuthMetadataProcessor().+// Please also check out grpc::experimental::Interceptor for another way to do+// customized operations on the information provided by a specific call. class AuthMetadataProcessor {  public:   typedef std::multimap<grpc::string_ref, grpc::string_ref> InputMetadata;   typedef std::multimap<std::string, std::string> OutputMetadata;    virtual ~AuthMetadataProcessor() {} -  /// If this method returns true, the \a Process function will be scheduled in-  /// a different thread from the one processing the call.+  // If this method returns true, the \a Process function will be scheduled in+  // a different thread from the one processing the call.   virtual bool IsBlocking() const { return true; } -  /// context is read/write: it contains the properties of the channel peer and-  /// it is the job of the Process method to augment it with properties derived-  /// from the passed-in auth_metadata.-  /// consumed_auth_metadata needs to be filled with metadata that has been-  /// consumed by the processor and will be removed from the call.-  /// response_metadata is the metadata that will be sent as part of the-  /// response.-  /// If the return value is not Status::OK, the rpc call will be aborted with-  /// the error code and error message sent back to the client.+  // Processes a Call associated with a Channel.+  // auth_metadata: the authentication metadata associated with the particular+  //   call+  // context: contains the channel-level info, e.g. the peer identity. This+  //   parameter is readable and writable. Note that since the information is+  //   shared for all calls associated with the channel, if we update the info+  //   in a specific call, all the subsequent calls will see the updates. A+  //   typical usage of context is to use |auth_metadata| to infer the peer+  //   identity, and augment it with properties.+  // consumed_auth_metadata: contains the metadata that users want to remove for","This comment is incorrect.  The metadata specified here is not removed from subsequent calls; it is removed from the current call, so that the metadata is no longer present when the server application sees it.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/28979,819329439,2022-03-04T07:35:44Z,src/core/ext/filters/client_channel/resolver/dns/c_ares/dns_resolver_ares.cc,"@@ -60,165 +55,95 @@ namespace grpc_core {  namespace { -class AresClientChannelDNSResolver : public Resolver {+class AresClientChannelDNSResolver : public PollingResolver {  public:-  explicit AresClientChannelDNSResolver(ResolverArgs args);+  AresClientChannelDNSResolver(ResolverArgs args,+                               const grpc_channel_args* channel_args); -  void StartLocked() override;+  OrphanablePtr<Orphanable> StartRequest() override; -  void RequestReresolutionLocked() override;+ private:+  class AresRequestWrapper : public InternallyRefCounted<AresRequestWrapper> {","A couple of structural comments (I won't block the PR on this, just want to consider):The thread safety around the combined request initialization and starting in `StartRequest` is somewhat subtle to reason about, since it relies on the `PollingResolver` to synchronize its access to the `Orphanable` returned from `StartRequest`.What do you think about changing the `OrphanablePtr<Orphanable> StartRequest` API to `OrphanablePtr<PollingResolver::Request> CreateRequest`, where `PollingResolver::Request` looks like:```class Request : InternallyRefCounted<Request> {  public:    Start();}```For c-ares, `PollingResolver::Request` would be implemented by `AresRequestWrapper` pretty much as is.For the native resolver, we could introduce `NativeRequestWrapper` which would hold a `RefCountedPtr<DNSResolver::Request> request_;`, as well as `RefCountedPtr<PollingResolver>` (getting rid of the manual ref in the native resolver).We could then go a step further and make `PollingResolver` a final class, and merge all of the remaining  `AresClientChannelDNSResolver` and `NativeClientChannelDNSResolver` methods into their corresponding `PollingResolver::Request` implementations. With `PollingResolver` e.g. taking a factory function in it's ctor for creating `PollingResolver::Request`.Getting rid of the `AresClientChannelDNSResolver` and `NativeClientChannelDNSResolver` sub-class types might improve readability, since they are pretty thin now anyways.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28979,819892236,2022-03-04T20:21:14Z,src/core/ext/filters/client_channel/resolver/dns/c_ares/dns_resolver_ares.cc,"@@ -60,165 +55,95 @@ namespace grpc_core {  namespace { -class AresClientChannelDNSResolver : public Resolver {+class AresClientChannelDNSResolver : public PollingResolver {  public:-  explicit AresClientChannelDNSResolver(ResolverArgs args);+  AresClientChannelDNSResolver(ResolverArgs args,+                               const grpc_channel_args* channel_args); -  void StartLocked() override;+  OrphanablePtr<Orphanable> StartRequest() override; -  void RequestReresolutionLocked() override;+ private:+  class AresRequestWrapper : public InternallyRefCounted<AresRequestWrapper> {","> The thread safety around the combined request initialization and starting in `StartRequest` is somewhat subtle to reason about, since it relies on the `PollingResolver` to synchronize its access to the `Orphanable` returned from `StartRequest`.That's true, but `PollingResolver` is explicitly designed to make that guarantee.  In any polling resolver, the request is going to complete asynchronously, and when it does, we're going to have to hop back into the `WorkSerializer` to process the result.  `PollingResolver` handles that so that subclasses don't need to worry about it, so this access will always be safe.Note that this is not intended to be a public API, so I'm fine with having it depend on the semantics of `WorkSerializer`.> > What do you think about changing the `OrphanablePtr<Orphanable> StartRequest` API to `OrphanablePtr<PollingResolver::Request> CreateRequest`, where `PollingResolver::Request` looks like:> > ```> class Request : InternallyRefCounted<Request> {>   public:>     Start();> }> ```I thought about that -- in fact, I had actually implemented something like that in an earlier draft of this PR -- but I decided against it, because I didn't want to force implementations to jump through hoops to avoid diamond dependency problems.The native DNS resolver is a prime example of that: the iomgr DNS resolution API already returns a `DNSResolver::Request` object, which itself inherits from `InternallyRefCounted<>`.  The native DNS resolver can't return an object that inherits from both `DNSResolver::Request` *and* from `PollingResolver::Request` without hitting a diamond dependency problem, so it would be forced to create a new wrapper object that inherits from `PollingResolver::Request` and contains the `DNSResolver::Request` object that does all the real work.  That seems unnecessarily complex and inefficient, especially given that `PollingResolver` doesn't actually care whether the request object is ref-counted or not; all it really cares about is that it's orphanable, so that it can cancel it.> > For c-ares, `PollingResolver::Request` would be implemented by `AresRequestWrapper` pretty much as is.In the long run, I don't think we really want to have `AresRequestWrapper` in the first place.  If #25108 hadn't been reverted (and I would still like to see it rolled forward at some point), the c-ares resolver would have been able to just drop in its existing `AresRequest` object here.  But if we went with the `PollingResolver::Request` approach, the c-ares resolver would have had to jump through exactly the same hoops as the native DNS resolver to avoid the diamond dependency problem.> > For the native resolver, we could introduce `NativeRequestWrapper` which would hold a `RefCountedPtr<DNSResolver::Request> request_;`, as well as `RefCountedPtr<PollingResolver>` (getting rid of the manual ref in the native resolver).I agree that having a way to avoid dealing with this ref manually would be better.  But once we can use C++14, we can do that anyway by making the completion callback a lambda that captures the `RefCountedPtr<>` using move semantics.  (Even with C++11, we could actually use the lambda-capture approach instead of dealing with the refs manually, but that would result in an extra unnecessay ref/unref pair due to the lack of lambda move-capture semantics.)> > We could then go a step further and make `PollingResolver` a final class, and merge all of the remaining `AresClientChannelDNSResolver` and `NativeClientChannelDNSResolver` methods into their corresponding `PollingResolver::Request` implementations. With `PollingResolver` e.g. taking a factory function in it's ctor for creating `PollingResolver::Request`.> > Getting rid of the `AresClientChannelDNSResolver` and `NativeClientChannelDNSResolver` sub-class types might improve readability, since they are pretty thin now anyways.I think that would force the interface for implementations to be a little too narrow, which would make it hard to expand it in the future if needed.Also, it's worth noting that once AJ moves the c-ares code inside of the iomgr API, we will have only one client channel DNS resolver implementation anyway, so any small amount of duplication here will go away at that point anyway.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28940,819912309,2022-03-04T20:56:40Z,src/core/lib/security/credentials/credentials.h,"@@ -53,11 +53,21 @@ typedef enum {   ""FakeTransportSecurity"" #define GRPC_CHANNEL_CREDENTIALS_TYPE_GOOGLE_DEFAULT ""GoogleDefault"" #define GRPC_CREDENTIALS_TYPE_INSECURE ""insecure""+#define GRPC_CREDENTIALS_TYPE_COMPOSITE ""Composite""","I don't think we want these macros.  Having them increases the risk that someone will reuse one of them somewhere they shouldn't.  And while that would wind up being a different instance of the string and therefore wouldn't affect the comparison, it could lead to human confusion.I think we can just inline the names in the `type()` method of each credentials type.  Or, alternatively, if we do actually need these constants to be externally accessible for some reason, then I think we should just have each creds class have a static method that returns them.  In that case, the virtual `type()` method can just call the static method.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28940,819961578,2022-03-04T22:33:50Z,tools/dockerfile/grpc_clang_format/clang_format_all_the_things.sh,"@@ -32,7 +32,7 @@ for dir in $DIRS do   for glob in $GLOB   do-    files=""$files `find ${CLANG_FORMAT_ROOT}/$dir -name $glob -and -not -name '*.generated.*' -and -not -name '*.upb.h' -and -not -name '*.upb.c' -and -not -name '*.upbdefs.h' -and -not -name '*.upbdefs.c' -and -not -name '*.pb.h' -and -not -name '*.pb.c' -and -not -name '*.pb.cc' -and -not -name '*.pbobjc.h' -and -not -name '*.pbobjc.m' -and -not -name '*.pbrpc.h' -and -not -name '*.pbrpc.m' -and -not -name end2end_tests.cc -and -not -name end2end_nosec_tests.cc -and -not -name public_headers_must_be_c89.c -and -not -name grpc_shadow_boringssl.h`""+    files=""$files `find ${CLANG_FORMAT_ROOT}/$dir -name $glob -and -not -name '*.generated.*' -and -not -name '*.upb.h' -and -not -name '*.upb.c' -and -not -name '*.upbdefs.h' -and -not -name '*.upbdefs.c' -and -not -name '*.pb.h' -and -not -name '*.pb.c' -and -not -name '*.pb.cc' -and -not -name '*.pbobjc.h' -and -not -name '*.pbobjc.m' -and -not -name '*.pbrpc.h' -and -not -name '*.pbrpc.m' -and -not -name end2end_tests.cc -and -not -name end2end_nosec_tests.cc -and -not -name public_headers_must_be_c89.c -and -not -name grpc_shadow_boringssl.h -and -not -name grpc_tls_credentials_options.h -and -not -name grpc_tls_credentials_options_comparator_test.cc`""","Why don't we want to clang-format these generated files?  Historically, we have done that sort of thing -- e.g., we used to do that with the static metadata file, before it was removed.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28979,819972959,2022-03-04T23:01:38Z,src/core/ext/filters/client_channel/resolver/polling_resolver.cc,"@@ -0,0 +1,201 @@+//+// Copyright 2015 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <grpc/support/port_platform.h>++#include ""src/core/ext/filters/client_channel/resolver/polling_resolver.h""++#include ""absl/strings/strip.h""++#include ""src/core/lib/backoff/backoff.h""+#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/iomgr/pollset_set.h""+#include ""src/core/lib/iomgr/timer.h""+#include ""src/core/lib/iomgr/work_serializer.h""++namespace grpc_core {++PollingResolver::PollingResolver(ResolverArgs args,+                                 const grpc_channel_args* channel_args,+                                 Duration min_time_between_resolutions,+                                 BackOff::Options backoff_options,+                                 TraceFlag* tracer)+    : authority_(args.uri.authority()),+      name_to_resolve_(absl::StripPrefix(args.uri.path(), ""/"")),+      channel_args_(grpc_channel_args_copy(channel_args)),+      work_serializer_(std::move(args.work_serializer)),+      result_handler_(std::move(args.result_handler)),+      tracer_(tracer),+      interested_parties_(args.pollset_set),+      min_time_between_resolutions_(min_time_between_resolutions),+      backoff_(backoff_options) {+  if (GPR_UNLIKELY(tracer_ != nullptr && tracer_->enabled())) {+    gpr_log(GPR_INFO, ""[polling resolver %p] created"", this);+  }+}++PollingResolver::~PollingResolver() {+  if (GPR_UNLIKELY(tracer_ != nullptr && tracer_->enabled())) {+    gpr_log(GPR_INFO, ""[polling resolver %p] destroying"", this);+  }+  grpc_channel_args_destroy(channel_args_);+}++void PollingResolver::StartLocked() { MaybeStartResolvingLocked(); }++void PollingResolver::RequestReresolutionLocked() {+  if (request_ == nullptr) {+    MaybeStartResolvingLocked();+  }+}++void PollingResolver::ResetBackoffLocked() {+  if (have_next_resolution_timer_) {+    grpc_timer_cancel(&next_resolution_timer_);+  }+  backoff_.Reset();+}++void PollingResolver::ShutdownLocked() {+  if (GPR_UNLIKELY(tracer_ != nullptr && tracer_->enabled())) {+    gpr_log(GPR_INFO, ""[polling resolver %p] shutting down"", this);+  }+  shutdown_ = true;+  if (have_next_resolution_timer_) {+    grpc_timer_cancel(&next_resolution_timer_);+  }+  request_.reset();+}++void PollingResolver::OnNextResolution(void* arg, grpc_error_handle error) {+  auto* self = static_cast<PollingResolver*>(arg);+  (void)GRPC_ERROR_REF(error);  // ref owned by lambda+  self->work_serializer_->Run(+      [self, error]() { self->OnNextResolutionLocked(error); }, DEBUG_LOCATION);+}++void PollingResolver::OnNextResolutionLocked(grpc_error_handle error) {+  if (GPR_UNLIKELY(tracer_ != nullptr && tracer_->enabled())) {+    gpr_log(GPR_INFO,+            ""[polling resolver %p] re-resolution timer fired: error=\""%s\"", ""+            ""request_=%p"",+            this, grpc_error_std_string(error).c_str(), request_.get());+  }+  have_next_resolution_timer_ = false;+  if (error == GRPC_ERROR_NONE && request_ == nullptr) {+    StartResolvingLocked();+  }+  Unref(DEBUG_LOCATION, ""retry-timer"");+  GRPC_ERROR_UNREF(error);+}++void PollingResolver::OnRequestComplete(Result result) {+  Ref(DEBUG_LOCATION, ""OnRequestComplete"").release();+  work_serializer_->Run(+      [this, result]() mutable { OnRequestCompleteLocked(std::move(result)); },+      DEBUG_LOCATION);+}++void PollingResolver::OnRequestCompleteLocked(Result result) {+  if (GPR_UNLIKELY(tracer_ != nullptr && tracer_->enabled())) {+    gpr_log(GPR_INFO, ""[polling resolver %p] request complete"", this);+  }+  request_.reset();+  if (!shutdown_) {+    if (result.service_config.ok() && result.addresses.ok()) {","The `service_config` field defaults to a null value (i.e., OK status):https://github.com/grpc/grpc/blob/fe913387da494713a945f7842669bd3bf3c27a03/src/core/lib/resolver/resolver.h#L57This means that `result.service_config.ok()` will be false only if the resolver explicitly returns an error for it.  I think if we get an explicit error for either service config or addresses, we should consider it a failure for the purposes of backoff.I don't think this will actually change the behavior of the c-ares resolver, because it does not explicitly return an error for the service config if it is not fetching the service config.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29014,819976919,2022-03-04T23:13:29Z,src/core/ext/xds/xds_client.cc,"@@ -1316,8 +1319,15 @@ void XdsClient::ChannelState::AdsCallState::OnStatusReceivedLocked(     // Try to restart the call.     parent_->OnCallFinishedLocked();     // Send error to all watchers.-    xds_client()->NotifyOnErrorLocked(-        GRPC_ERROR_CREATE_FROM_STATIC_STRING(""xds call failed""));+    char* status_details = grpc_slice_to_c_string(status_details_);","Instead, please use `StringViewFromSlice()`, to avoid an unnecessary allocation.https://github.com/grpc/grpc/blob/fe913387da494713a945f7842669bd3bf3c27a03/src/core/lib/slice/slice_internal.h#L92Note that `absl::StrFormat()` knows how to handle `absl::string_view` parameters, so you can probably just inline this into that call instead of storing it in a separate variable.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29014,819977632,2022-03-04T23:15:54Z,src/core/ext/xds/xds_client.cc,"@@ -1316,8 +1319,15 @@ void XdsClient::ChannelState::AdsCallState::OnStatusReceivedLocked(     // Try to restart the call.     parent_->OnCallFinishedLocked();     // Send error to all watchers.-    xds_client()->NotifyOnErrorLocked(-        GRPC_ERROR_CREATE_FROM_STATIC_STRING(""xds call failed""));+    char* status_details = grpc_slice_to_c_string(status_details_);+    xds_client()->NotifyOnErrorLocked(GRPC_ERROR_CREATE_FROM_COPIED_STRING(+        absl::StrFormat(+            ""xDS call failed: xDS server: %s, ADS call status code=%d, ""+            ""details='%s', error='%s'"",+            chand()->server_.server_uri.c_str(), status_code_, status_details,","`absl::StrFormat()` knows how to handle `std::string` arguments, so no need to call `.c_str()` on any parameters.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29014,819979505,2022-03-04T23:21:29Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -2439,6 +2439,21 @@ TEST_P(XdsResolverOnlyTest, KeepUsingLastDataIfBalancerGoesDown) {   WaitForBackend(1); } +TEST_P(XdsResolverOnlyTest, XdsStreamErrorPropagation) {+  const std::string kErrorMessage = ""test forced ADS stream failure"";+  balancer_->ads_service()->ForceADSFailure(+      Status(StatusCode::RESOURCE_EXHAUSTED, kErrorMessage));+  auto status = SendRpc();+  gpr_log(GPR_INFO,+          ""XdsStreamErrorPropagation test: RPC got error: code=%d message=%s"",+          status.error_code(), status.error_message().c_str());+  // TODO(roth): should the XDS stream error be re-packaged into an UNAVAILABLE","Yes, this is definitely a bug.  I think the right place to fix this is in the xds resolver, probably right here:https://github.com/grpc/grpc/blob/fe913387da494713a945f7842669bd3bf3c27a03/src/core/ext/filters/client_channel/resolver/xds/xds_resolver.cc#L880Instead of just converting the `grpc_error` returned by the `XdsClient`, we should do something like this:```result.service_config = absl::UnavailableError(absl::StrCat(    ""error obtaining xDS resources: "", grpc_error_std_string(error)));```",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28965,820467365,2022-03-07T08:18:41Z,tools/internal_ci/linux/grpc_bazel_test_in_docker.sh,"@@ -1,29 +0,0 @@-#!/usr/bin/env bash-# Copyright 2017 gRPC authors.-#-# Licensed under the Apache License, Version 2.0 (the ""License"");-# you may not use this file except in compliance with the License.-# You may obtain a copy of the License at-#-#     http://www.apache.org/licenses/LICENSE-2.0-#-# Unless required by applicable law or agreed to in writing, software-# distributed under the License is distributed on an ""AS IS"" BASIS,-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.-# See the License for the specific language governing permissions and-# limitations under the License.-#-# Test full Bazel-#-# NOTE: No empty lines should appear in this file before igncr is set!-set -ex -o igncr || set -ex--mkdir -p /var/local/git-git clone /var/local/jenkins/grpc /var/local/git/grpc-(cd /var/local/jenkins/grpc/ && git submodule foreach 'cd /var/local/git/grpc \-&& git submodule update --init --reference /var/local/jenkins/grpc/${name} \-${name}')-cd /var/local/git/grpc--bazel test //test/...-bazel test //test/cpp/end2end:admin_services_end2end_test --define=grpc_no_xds=true","If so, you can do it by basically duplicating these files https://github.com/grpc/grpc/blob/master/tools/internal_ci/linux/grpc_bazel_rbe_opt.cfghttps://github.com/grpc/grpc/blob/f8a909e76fcd947949502832a7ab8e2cba2b8e27/tools/internal_ci/linux/grpc_bazel_on_foundry_opt.sh(while setting --define=grpc_no_xds=true) and creating a new kokoro job for it.the name of the new job can be e.g. `grpc_bazel_rbe_no_xds`. Since so far we weren't running grpc_bazel_test job at all, I think it's enough to add that job for master only.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28872,820505623,2022-03-07T09:10:38Z,tools/internal_ci/windows/grpc_build_artifacts.bat,"@@ -25,7 +25,6 @@ endlocal @rem enter repo root cd /d %~dp0\..\..\.. -set PREPARE_BUILD_INSTALL_DEPS_CSHARP=true","it install the C# dotnet SDK, but that's not necessary for building the C# artifact (since C# artifact is only the grpc_csharp_ext native library and doesn't use C# at all).Since the prepare step takes some time (and can potentially fail), it make sense to remove it when not needed.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29037,820745281,2022-03-07T14:16:36Z,tools/internal_ci/linux/grpc_bloat_diff_in_docker.sh,"@@ -24,3 +24,8 @@ time python3 -m pip install --user -r tools/internal_ci/helper_scripts/requireme  tools/internal_ci/linux/run_if_c_cpp_modified.sh tools/profiling/bloat/bloat_diff.py \   -d ""origin/$KOKORO_GITHUB_PULL_REQUEST_TARGET_BRANCH""++tools/run_tests/start_port_server.py++tools/internal_ci/linux/run_if_c_cpp_modified.sh tools/profiling/memory/memory_diff.py \","as always with two subsequent tasks in the same job, this is a bit hacky. You probably need to use ` || FAILED=true` and `if [ ""$FAILED"" != """" ]` at the end of the script to make sure these two scripts are at least a bit independent (the second one runs if the first one fails)Should there be a completely separate job for memory_diff?e.g. here:https://github.com/grpc/grpc/blob/b0cdd3cba29928da3652bded596a64bda68c75c1/tools/internal_ci/linux/grpc_distribtests_csharp.sh#L64https://github.com/grpc/grpc/blob/b0cdd3cba29928da3652bded596a64bda68c75c1/tools/internal_ci/linux/grpc_distribtests_csharp.sh#L35",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29037,820745701,2022-03-07T14:17:05Z,tools/profiling/memory/memory_diff.py,"@@ -0,0 +1,109 @@+#!/usr/bin/env python3+#+# Copyright 2022 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import csv+import glob+import math+import multiprocessing+import os+import pathlib+import re+import shutil+import subprocess+import sys++sys.path.append(+    os.path.join(os.path.dirname(sys.argv[0]), '..', '..', 'run_tests',+                 'python_utils'))+import check_on_pr++argp = argparse.ArgumentParser(description='Perform diff on microbenchmarks')","nit: ""Perform diff on microbenchmarks""  no longer true.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29037,820748798,2022-03-07T14:20:19Z,tools/profiling/memory/memory_diff.py,"@@ -0,0 +1,109 @@+#!/usr/bin/env python3+#+# Copyright 2022 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import csv+import glob+import math+import multiprocessing+import os+import pathlib+import re+import shutil+import subprocess+import sys++sys.path.append(+    os.path.join(os.path.dirname(sys.argv[0]), '..', '..', 'run_tests',+                 'python_utils'))+import check_on_pr++argp = argparse.ArgumentParser(description='Perform diff on microbenchmarks')++argp.add_argument('-d',+                  '--diff_base',+                  type=str,+                  help='Commit or branch to compare the current one to')++argp.add_argument('-j', '--jobs', type=int, default=multiprocessing.cpu_count())++args = argp.parse_args()++_INTERESTING = {+    'client call':+        (rb'client call memory usage: ([0-9\.]+) bytes per call', float),+    'server call':+        (rb'server call memory usage: ([0-9\.]+) bytes per call', float),+}+++def _run():+    """"""Build with Bazel, then run, and extract interesting lines from the output.""""""+    subprocess.check_call([+        'tools/bazel', 'build', '-c', 'opt',+        'test/core/memory_usage/memory_usage_test'+    ])+    output = subprocess.check_output([+        'bazel-bin/test/core/memory_usage/memory_usage_test',+        '--warmup=10000',+        '--benchmark=50000',+    ])+    ret = {}+    for line in output.splitlines():+        for key, (pattern, conversion) in _INTERESTING.items():+            m = re.match(pattern, line)+            if m:+                ret[key] = conversion(m.group(1))+    return ret+++cur = _run()+new = None++print(cur)++if args.diff_base:+    where_am_i = subprocess.check_output(+        ['git', 'rev-parse', '--abbrev-ref', 'HEAD']).decode().strip()+    # checkout the diff base (=""old"")+    subprocess.check_call(['git', 'checkout', args.diff_base])+    subprocess.check_call(['git', 'submodule', 'update'])","nit: for bazel build, the ""submodule update"" is useless (also below)",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28975,821487956,2022-03-08T09:41:16Z,tools/run_tests/performance/scenario_config.py,"@@ -725,6 +726,133 @@ def scenarios(self):     def __str__(self):         return 'csharp' +class DotnetLanguage(Language):",The newly added grpc-dotnet scenarios are basically clones of the original CSharpLanguage scenarios.,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29037,821516724,2022-03-08T10:13:40Z,tools/profiling/memory/memory_diff.py,"@@ -0,0 +1,109 @@+#!/usr/bin/env python3+#+# Copyright 2022 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import argparse+import csv+import glob+import math+import multiprocessing+import os+import pathlib+import re+import shutil+import subprocess+import sys++sys.path.append(+    os.path.join(os.path.dirname(sys.argv[0]), '..', '..', 'run_tests',+                 'python_utils'))+import check_on_pr++argp = argparse.ArgumentParser(description='Perform diff on microbenchmarks')++argp.add_argument('-d',+                  '--diff_base',+                  type=str,+                  help='Commit or branch to compare the current one to')++argp.add_argument('-j', '--jobs', type=int, default=multiprocessing.cpu_count())++args = argp.parse_args()++_INTERESTING = {+    'client call':+        (rb'client call memory usage: ([0-9\.]+) bytes per call', float),+    'server call':+        (rb'server call memory usage: ([0-9\.]+) bytes per call', float),+}+++def _run():+    """"""Build with Bazel, then run, and extract interesting lines from the output.""""""+    subprocess.check_call([+        'tools/bazel', 'build', '-c', 'opt',",Basically clone and modify these files: tools/internal_ci/linux/grpc_bloat_diff_in_docker.sh -> tools/internal_ci/linux/grpc_memory_diff_in_docker.shtools/internal_ci/linux/grpc_bloat_diff.sh -> tools/internal_ci/linux/grpc_memory_diff.shI can then help with fine tuning stuff like choosing the right docker image and with creation of the kokoro job.,X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28970,821944074,2022-03-08T18:19:05Z,src/core/lib/security/transport/secure_endpoint.cc,"@@ -143,12 +163,51 @@ static void secure_endpoint_unref(secure_endpoint* ep) { static void secure_endpoint_ref(secure_endpoint* ep) { gpr_ref(&ep->ref); } #endif +static void post_reclaimer(secure_endpoint* ep) {",suggest `maybe_post_reclaimer` to signal this isn't done unconditionally,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28970,821948189,2022-03-08T18:24:09Z,src/core/lib/security/transport/secure_endpoint.cc,"@@ -270,11 +334,16 @@ static void endpoint_read(grpc_endpoint* secure_ep, grpc_slice_buffer* slices, }  static void flush_write_staging_buffer(secure_endpoint* ep, uint8_t** cur,","similarly, this whole function needs to be guarded by write_mu",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/28970,821949104,2022-03-08T18:25:12Z,src/core/lib/security/transport/secure_endpoint.cc,"@@ -284,8 +353,10 @@ static void endpoint_write(grpc_endpoint* secure_ep, grpc_slice_buffer* slices,   unsigned i;   tsi_result result = TSI_OK;   secure_endpoint* ep = reinterpret_cast<secure_endpoint*>(secure_ep);+  ep->write_mu.Lock();",this lock should extend until we're no longer manipulating cur/end,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/29034,822936832,2022-03-09T18:04:56Z,test/cpp/qps/driver.cc,"@@ -367,7 +370,8 @@ std::unique_ptr<ScenarioResult> RunScenario(     const std::string& qps_server_target_override,     const std::string& credential_type,     const std::map<std::string, std::string>& per_worker_credential_types,-    bool run_inproc, int32_t median_latency_collection_interval_millis) {+    bool run_inproc, int32_t median_latency_collection_interval_millis,+    std::string prometheus_query_time_file) {","Nit: Since there's nothing actually Prometheus-specific about the format being used here, I think we might want to change the naming to be more generic.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29011,823938922,2022-03-10T16:52:24Z,src/core/ext/transport/chttp2/transport/parsing.cc,"@@ -92,10 +96,12 @@ grpc_error_handle grpc_chttp2_perform_read(grpc_chttp2_transport* t,           return GRPC_ERROR_CREATE_FROM_CPP_STRING(absl::StrFormat(               ""Connect string mismatch: expected '%c' (%d) got '%c' (%d) ""               ""at byte %d"",-              GRPC_CHTTP2_CLIENT_CONNECT_STRING[t->deframe_state],+              get_utf8_safe_char(","Now that you're storing the value in the Status payload instead of the message field, I think there's no longer any need to strip out non-UTF-8 characters here.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/28735,823982246,2022-03-10T17:37:39Z,cmake/ccache.cmake,"@@ -20,6 +20,18 @@ if ($ENV{GRPC_BUILD_ENABLE_CCACHE})     message(STATUS ""Will use ccache as compiler launcher: ${gRPC_CCACHE_BINARY}"")     set(CMAKE_C_COMPILER_LAUNCHER   ${gRPC_CCACHE_BINARY})     set(CMAKE_CXX_COMPILER_LAUNCHER ${gRPC_CCACHE_BINARY})+","Out of curiosity, how does this pdb race condition happen? PDB is usually created at the linking time and linking is done by a single process.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/28735,824004579,2022-03-10T18:01:44Z,cmake/ccache.cmake,"@@ -20,6 +20,18 @@ if ($ENV{GRPC_BUILD_ENABLE_CCACHE})     message(STATUS ""Will use ccache as compiler launcher: ${gRPC_CCACHE_BINARY}"")     set(CMAKE_C_COMPILER_LAUNCHER   ${gRPC_CCACHE_BINARY})     set(CMAKE_CXX_COMPILER_LAUNCHER ${gRPC_CCACHE_BINARY})+","AFAIK msvc by default has different behavior by default. Instead of keeping the debug info in .obj files, it creates a single PDB file that acts as a ""database""  of all the debug symbols for a given compilation unit. That doesn't seem to play well with ccache.Also see the documentation for sccache [here](https://github.com/mozilla/sccache#usage) that mentions this problem and the cmake workaround. ",
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/29057,824254747,2022-03-10T23:15:42Z,tools/run_tests/performance/prom.py,"@@ -0,0 +1,249 @@+#!/usr/bin/env python3++# Copyright 2022 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+",Perhaps some description here of what this is for. Or also a description in the class Prom.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28860,826466490,2022-03-14T23:45:46Z,src/core/lib/channel/channel_args.h,"@@ -23,10 +23,170 @@  #include <string> +#include ""absl/strings/string_view.h""+#include ""absl/types/optional.h""+#include ""absl/types/variant.h""+ #include <grpc/impl/codegen/grpc_types.h> +#include ""src/core/lib/avl/avl.h""+#include ""src/core/lib/gpr/useful.h""+#include ""src/core/lib/gprpp/ref_counted.h""+#include ""src/core/lib/gprpp/ref_counted_ptr.h"" #include ""src/core/lib/surface/channel_stack_type.h"" +namespace grpc_core {++// Define a traits object for vtable lookup - allows us to integrate with+// existing code easily (just define the trait!) and allows some magic in+// ChannelArgs to automatically derive a vtable from a T*.+// To participate as a pointer, instances should expose the function:+//   // Gets the vtable for this type+//   static const grpc_channel_arg_vtable* vtable();+//   // Performs any mutations required for channel args to own a pointer+//   static void* take_unowned_pointer(T* p);+template <typename T, typename Ignored = void /* for SFINAE */>+struct ChannelArgTypeTraits;++template <typename T>+struct ChannelArgTypeTraits<+    T, absl::enable_if_t<std::is_base_of<RefCounted<T>, T>::value, void>> {+  static const grpc_arg_pointer_vtable* vtable() {+    static const grpc_arg_pointer_vtable tbl = {+        // copy+        [](void* p) -> void* { return static_cast<T*>(p)->Ref().release(); },+        // destroy+        [](void* p) { static_cast<T*>(p)->Unref(); },+        // compare+        [](void* p1, void* p2) {+          return QsortCompare(*static_cast<const T*>(p1),+                              *static_cast<const T*>(p2));+        },+    };+    return &tbl;+  };+};++class ChannelArgs {+ public:+  class Pointer {+   public:+    Pointer(void* p, const grpc_arg_pointer_vtable* vtable)+        : p_(p), vtable_(vtable == nullptr ? empty_vtable() : vtable) {}+    ~Pointer() { vtable_->destroy(p_); }++    Pointer(const Pointer& other)+        : p_(other.vtable_->copy(other.p_)), vtable_(other.vtable_) {}+    Pointer& operator=(Pointer other) {+      std::swap(p_, other.p_);+      std::swap(vtable_, other.vtable_);+      return *this;+    }+    Pointer(Pointer&& other) noexcept : p_(other.p_), vtable_(other.vtable_) {+      other.p_ = nullptr;+      other.vtable_ = empty_vtable();+    }+    Pointer& operator=(Pointer&& other) noexcept {+      std::swap(p_, other.p_);+      std::swap(vtable_, other.vtable_);+      return *this;+    }++    bool operator==(const Pointer& rhs) const;+    bool operator<(const Pointer& rhs) const;+    bool operator!=(const Pointer& rhs) const { return !(*this == rhs); }++    void* c_pointer() const { return p_; }++    const grpc_arg_pointer_vtable* c_vtable() const { return vtable_; }++   private:+    void* p_;+    const grpc_arg_pointer_vtable* vtable_;+    static const grpc_arg_pointer_vtable* empty_vtable() {",Please put methods before data members within each section.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28860,826476068,2022-03-15T00:09:36Z,src/core/lib/channel/channel_args.h,"@@ -23,10 +23,170 @@  #include <string> +#include ""absl/strings/string_view.h""+#include ""absl/types/optional.h""+#include ""absl/types/variant.h""+ #include <grpc/impl/codegen/grpc_types.h> +#include ""src/core/lib/avl/avl.h""+#include ""src/core/lib/gpr/useful.h""+#include ""src/core/lib/gprpp/ref_counted.h""+#include ""src/core/lib/gprpp/ref_counted_ptr.h"" #include ""src/core/lib/surface/channel_stack_type.h"" +namespace grpc_core {++// Define a traits object for vtable lookup - allows us to integrate with+// existing code easily (just define the trait!) and allows some magic in+// ChannelArgs to automatically derive a vtable from a T*.+// To participate as a pointer, instances should expose the function:+//   // Gets the vtable for this type+//   static const grpc_channel_arg_vtable* vtable();+//   // Performs any mutations required for channel args to own a pointer+//   static void* take_unowned_pointer(T* p);+template <typename T, typename Ignored = void /* for SFINAE */>+struct ChannelArgTypeTraits;++template <typename T>+struct ChannelArgTypeTraits<+    T, absl::enable_if_t<std::is_base_of<RefCounted<T>, T>::value, void>> {+  static const grpc_arg_pointer_vtable* vtable() {+    static const grpc_arg_pointer_vtable tbl = {+        // copy+        [](void* p) -> void* { return static_cast<T*>(p)->Ref().release(); },+        // destroy+        [](void* p) { static_cast<T*>(p)->Unref(); },+        // compare+        [](void* p1, void* p2) {+          return QsortCompare(*static_cast<const T*>(p1),+                              *static_cast<const T*>(p2));+        },+    };+    return &tbl;+  };+};++class ChannelArgs {+ public:+  class Pointer {+   public:+    Pointer(void* p, const grpc_arg_pointer_vtable* vtable)+        : p_(p), vtable_(vtable == nullptr ? empty_vtable() : vtable) {}+    ~Pointer() { vtable_->destroy(p_); }++    Pointer(const Pointer& other)+        : p_(other.vtable_->copy(other.p_)), vtable_(other.vtable_) {}+    Pointer& operator=(Pointer other) {+      std::swap(p_, other.p_);","The swaps here confused me for minute until I noticed that you're passing the parameter in by value instead of as a const reference, thus effectively using it as a convenient way to avoid explicitly calling `destroy()`.  While it is a couple of lines longer, I think this would be a bit easier to understand if written in a less clever way:```Pointer& operator=(const Pointer& other) {  if (this != &other) {    vtable_->destroy(p_);    p_ = other.vtable_->copy(other.p_);    vtable_ = other.vtable_;  }  return *this;}```",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28860,826476801,2022-03-15T00:11:35Z,src/core/lib/channel/channel_args.h,"@@ -23,10 +23,170 @@  #include <string> +#include ""absl/strings/string_view.h""+#include ""absl/types/optional.h""+#include ""absl/types/variant.h""+ #include <grpc/impl/codegen/grpc_types.h> +#include ""src/core/lib/avl/avl.h""+#include ""src/core/lib/gpr/useful.h""+#include ""src/core/lib/gprpp/ref_counted.h""+#include ""src/core/lib/gprpp/ref_counted_ptr.h"" #include ""src/core/lib/surface/channel_stack_type.h"" +namespace grpc_core {++// Define a traits object for vtable lookup - allows us to integrate with+// existing code easily (just define the trait!) and allows some magic in+// ChannelArgs to automatically derive a vtable from a T*.+// To participate as a pointer, instances should expose the function:+//   // Gets the vtable for this type+//   static const grpc_channel_arg_vtable* vtable();+//   // Performs any mutations required for channel args to own a pointer+//   static void* take_unowned_pointer(T* p);+template <typename T, typename Ignored = void /* for SFINAE */>+struct ChannelArgTypeTraits;++template <typename T>+struct ChannelArgTypeTraits<+    T, absl::enable_if_t<std::is_base_of<RefCounted<T>, T>::value, void>> {+  static const grpc_arg_pointer_vtable* vtable() {+    static const grpc_arg_pointer_vtable tbl = {+        // copy+        [](void* p) -> void* { return static_cast<T*>(p)->Ref().release(); },+        // destroy+        [](void* p) { static_cast<T*>(p)->Unref(); },+        // compare+        [](void* p1, void* p2) {+          return QsortCompare(*static_cast<const T*>(p1),+                              *static_cast<const T*>(p2));+        },+    };+    return &tbl;+  };+};++class ChannelArgs {+ public:+  class Pointer {+   public:+    Pointer(void* p, const grpc_arg_pointer_vtable* vtable)+        : p_(p), vtable_(vtable == nullptr ? empty_vtable() : vtable) {}+    ~Pointer() { vtable_->destroy(p_); }++    Pointer(const Pointer& other)+        : p_(other.vtable_->copy(other.p_)), vtable_(other.vtable_) {}+    Pointer& operator=(Pointer other) {+      std::swap(p_, other.p_);+      std::swap(vtable_, other.vtable_);+      return *this;+    }+    Pointer(Pointer&& other) noexcept : p_(other.p_), vtable_(other.vtable_) {+      other.p_ = nullptr;+      other.vtable_ = empty_vtable();+    }+    Pointer& operator=(Pointer&& other) noexcept {+      std::swap(p_, other.p_);","Similar comment here, but in this case, I think the swap could actually lead to a lifetime issue, since we don't actually have any guarantee that `other` will be destroyed immediately after this, which means that the originally pointed-to object could wind up living longer than we really want it to.I think this would be cleaner if written as:```if (this != &other) {  vtable_->destroy(p_);  p_ = absl::exchange(other.p_, nullptr);  vtable_ = absl::exchange(other.vtable_, empty_vtable());}```",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28860,826484726,2022-03-15T00:33:42Z,src/core/lib/channel/channel_args.cc,"@@ -401,46 +491,23 @@ const grpc_channel_args* UniquifyChannelArgKeys(const grpc_channel_args* src) {         concatenated_values[key].push_back(src->args[i].value.string);       }       continue;+    } else if (absl::StartsWith(key, ""grpc.internal."")) {","Is `RemoveGrpcInternalArgs()` (line 462 above) still needed now that we're doing this here?If we're combining the two, maybe rename this function to something like `ChannelArgsBuiltinPrecondition()`?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28860,827122809,2022-03-15T15:38:55Z,test/core/channel/channel_args_test.cc,"@@ -207,14 +253,10 @@ static void test_server_create_with_args(void) { }  int main(int argc, char** argv) {+  ::testing::InitGoogleTest(&argc, argv);   grpc::testing::TestEnvironment env(argc, argv);   grpc_init();-  test_create();-  test_channel_create_with_args();-  test_server_create_with_args();-  // This has to be the last test.-  // TODO(markdroth): re-enable this test once client_idle is re-enabled","I think this test should probably be re-enabled instead of being removed.  I don't remember why this test had to be disabled when the client idle filter was, but you fixed the client idle filter a while back, so whatever was blocking this is presumably no longer an issue.  And the `grpc_channel_args_set_client_channel_creation_mutator()` and `grpc_channel_args_get_client_channel_creation_mutator()` functions are actually used internally, so I think we should have a test to make sure they work.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29085,827193244,2022-03-15T16:41:59Z,src/python/grpcio_tests/tests/reflection/_reflection_client_test.py,"@@ -0,0 +1,151 @@+# Copyright 2022 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests of grpc_reflection.v1alpha.reflection.""""""++import unittest++from google.protobuf.descriptor_pool import DescriptorPool+import grpc+from grpc_reflection.v1alpha import reflection+from grpc_reflection.v1alpha.proto_reflection_descriptor_database import \+    ProtoReflectionDescriptorDatabase++from src.proto.grpc.testing import test_pb2+# Needed to load the EmptyWithExtensions message+from src.proto.grpc.testing.proto2 import empty2_extensions_pb2+from tests.unit import test_common++_PROTO_PACKAGE_NAME = ""grpc.testing""+_PROTO_FILE_NAME = ""src/proto/grpc/testing/test.proto""+_EMPTY_PROTO_FILE_NAME = ""src/proto/grpc/testing/empty.proto""+_INVALID_FILE_NAME = ""i-do-not-exist.proto""+_EMPTY_PROTO_SYMBOL_NAME = ""grpc.testing.Empty""+_INVALID_SYMBOL_NAME = ""IDoNotExist""+_EMPTY_EXTENSIONS_SYMBOL_NAME = ""grpc.testing.proto2.EmptyWithExtensions""+++class ReflectionClientTest(unittest.TestCase):++    def setUp(self):+        self._server = test_common.test_server()+        self._SERVICE_NAMES = (+            test_pb2.DESCRIPTOR.services_by_name[""TestService""].full_name,+            reflection.SERVICE_NAME,+        )+        reflection.enable_server_reflection(self._SERVICE_NAMES, self._server)+        port = self._server.add_insecure_port(""[::]:0"")+        self._server.start()++        self._channel = grpc.insecure_channel(""localhost:%d"" % port)++        self._reflection_db = ProtoReflectionDescriptorDatabase(self._channel)+        self.desc_pool = DescriptorPool(self._reflection_db)++    def tearDown(self):+        self._server.stop(None)+        self._channel.close()++    def testListServices(self):+        services = self._reflection_db.get_services()+        self.assertCountEqual(self._SERVICE_NAMES, services)++    def testReflectionServiceName(self):+        self.assertEqual(reflection.SERVICE_NAME,+                         ""grpc.reflection.v1alpha.ServerReflection"")++    def testFindFile(self):+        file_name = _PROTO_FILE_NAME+        file_desc = self.desc_pool.FindFileByName(file_name)+        self.assertEqual(file_name, file_desc.name)+        self.assertEqual(_PROTO_PACKAGE_NAME, file_desc.package)+        self.assertEqual(""proto3"", file_desc.syntax)+        self.assertIn(""TestService"", file_desc.services_by_name)++        file_name = _EMPTY_PROTO_FILE_NAME+        file_desc = self.desc_pool.FindFileByName(file_name)+        self.assertEqual(file_name, file_desc.name)+        self.assertEqual(_PROTO_PACKAGE_NAME, file_desc.package)+        self.assertEqual(""proto3"", file_desc.syntax)+        self.assertIn(""Empty"", file_desc.message_types_by_name)++    def testFindFileError(self):+        with self.assertRaises(KeyError):+            self.desc_pool.FindFileByName(_INVALID_FILE_NAME)++    def testFindMessage(self):+        message_name = _EMPTY_PROTO_SYMBOL_NAME+        message_desc = self.desc_pool.FindMessageTypeByName(message_name)+        self.assertEqual(message_name, message_desc.full_name)+        self.assertTrue(message_name.endswith(message_desc.name))++    def testFindMessageError(self):+        with self.assertRaises(KeyError):+            self.desc_pool.FindMessageTypeByName(_INVALID_SYMBOL_NAME)++    def testFindServiceFindMethod(self):+        service_name = self._SERVICE_NAMES[0]+        service_desc = self.desc_pool.FindServiceByName(service_name)+        self.assertEqual(service_name, service_desc.full_name)+        self.assertTrue(service_name.endswith(service_desc.name))+        file_name = _PROTO_FILE_NAME+        file_desc = self.desc_pool.FindFileByName(file_name)+        self.assertIs(file_desc, service_desc.file)++        method_name = ""EmptyCall""+        self.assertIn(method_name, service_desc.methods_by_name)++        method_desc = service_desc.FindMethodByName(method_name)+        self.assertIs(method_desc, service_desc.methods_by_name[method_name])+        self.assertIs(service_desc, method_desc.containing_service)+        self.assertEqual(method_name, method_desc.name)+        self.assertTrue(method_desc.full_name.endswith(method_name))++        empty_message_desc = self.desc_pool.FindMessageTypeByName(+            _EMPTY_PROTO_SYMBOL_NAME)+        self.assertEqual(empty_message_desc, method_desc.input_type)+        self.assertEqual(empty_message_desc, method_desc.output_type)++    def testFindServiceError(self):+        with self.assertRaises(KeyError):+            self.desc_pool.FindServiceByName(_INVALID_SYMBOL_NAME)++    def testFindMethodError(self):+        service_name = self._SERVICE_NAMES[0]+        service_desc = self.desc_pool.FindServiceByName(service_name)++        # FindMethodByName sometimes raises a KeyError, and sometimes returns None.+        # See https://github.com/protocolbuffers/protobuf/issues/9592+        with self.assertRaises(KeyError):+            res = service_desc.FindMethodByName(_INVALID_SYMBOL_NAME)+            if res is None:+                raise KeyError()","Thanks for the clear write-up in https://github.com/protocolbuffers/protobuf/issues/9592.Checking other find* methods ([Python code](https://github.com/protocolbuffers/protobuf/blob/349d74d92e8268c41fbb726b0a8292d588c63e20/python/google/protobuf/descriptor_database.py)), they favor the raise behavior. Do you know how the C-Extension implementation will behave? Based on [code](https://github.com/protocolbuffers/protobuf/blob/2d69d44cc18beaa0a3a62d353f80aead669abb5c/python/google/protobuf/pyext/descriptor_database.cc#L113), their return type is `bool`.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29088,827219336,2022-03-15T17:06:56Z,src/core/lib/surface/channel_init.h,"@@ -24,6 +24,7 @@ #include <functional> #include <vector> +#include ""src/core/lib/channel/channel_stack_builder.h"" #include ""src/core/lib/surface/channel_stack_type.h""",Looks like this include is no longer needed.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29088,827219750,2022-03-15T17:07:22Z,src/core/lib/surface/channel_init.h,"@@ -74,8 +73,7 @@ class ChannelInit {   /// Construct a channel stack of some sort: see channel_stack.h for details   /// \a type is the type of channel stack to create","This line can be removed, since the parameter is no longer present.",
1926539,tomerv,https://api.github.com/repos/grpc/grpc/pulls/29085,827242493,2022-03-15T17:31:14Z,src/python/grpcio_tests/tests/reflection/_reflection_client_test.py,"@@ -0,0 +1,151 @@+# Copyright 2022 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Tests of grpc_reflection.v1alpha.reflection.""""""++import unittest++from google.protobuf.descriptor_pool import DescriptorPool+import grpc+from grpc_reflection.v1alpha import reflection+from grpc_reflection.v1alpha.proto_reflection_descriptor_database import \+    ProtoReflectionDescriptorDatabase++from src.proto.grpc.testing import test_pb2+# Needed to load the EmptyWithExtensions message+from src.proto.grpc.testing.proto2 import empty2_extensions_pb2+from tests.unit import test_common++_PROTO_PACKAGE_NAME = ""grpc.testing""+_PROTO_FILE_NAME = ""src/proto/grpc/testing/test.proto""+_EMPTY_PROTO_FILE_NAME = ""src/proto/grpc/testing/empty.proto""+_INVALID_FILE_NAME = ""i-do-not-exist.proto""+_EMPTY_PROTO_SYMBOL_NAME = ""grpc.testing.Empty""+_INVALID_SYMBOL_NAME = ""IDoNotExist""+_EMPTY_EXTENSIONS_SYMBOL_NAME = ""grpc.testing.proto2.EmptyWithExtensions""+++class ReflectionClientTest(unittest.TestCase):++    def setUp(self):+        self._server = test_common.test_server()+        self._SERVICE_NAMES = (+            test_pb2.DESCRIPTOR.services_by_name[""TestService""].full_name,+            reflection.SERVICE_NAME,+        )+        reflection.enable_server_reflection(self._SERVICE_NAMES, self._server)+        port = self._server.add_insecure_port(""[::]:0"")+        self._server.start()++        self._channel = grpc.insecure_channel(""localhost:%d"" % port)++        self._reflection_db = ProtoReflectionDescriptorDatabase(self._channel)+        self.desc_pool = DescriptorPool(self._reflection_db)++    def tearDown(self):+        self._server.stop(None)+        self._channel.close()++    def testListServices(self):+        services = self._reflection_db.get_services()+        self.assertCountEqual(self._SERVICE_NAMES, services)++    def testReflectionServiceName(self):+        self.assertEqual(reflection.SERVICE_NAME,+                         ""grpc.reflection.v1alpha.ServerReflection"")++    def testFindFile(self):+        file_name = _PROTO_FILE_NAME+        file_desc = self.desc_pool.FindFileByName(file_name)+        self.assertEqual(file_name, file_desc.name)+        self.assertEqual(_PROTO_PACKAGE_NAME, file_desc.package)+        self.assertEqual(""proto3"", file_desc.syntax)+        self.assertIn(""TestService"", file_desc.services_by_name)++        file_name = _EMPTY_PROTO_FILE_NAME+        file_desc = self.desc_pool.FindFileByName(file_name)+        self.assertEqual(file_name, file_desc.name)+        self.assertEqual(_PROTO_PACKAGE_NAME, file_desc.package)+        self.assertEqual(""proto3"", file_desc.syntax)+        self.assertIn(""Empty"", file_desc.message_types_by_name)++    def testFindFileError(self):+        with self.assertRaises(KeyError):+            self.desc_pool.FindFileByName(_INVALID_FILE_NAME)++    def testFindMessage(self):+        message_name = _EMPTY_PROTO_SYMBOL_NAME+        message_desc = self.desc_pool.FindMessageTypeByName(message_name)+        self.assertEqual(message_name, message_desc.full_name)+        self.assertTrue(message_name.endswith(message_desc.name))++    def testFindMessageError(self):+        with self.assertRaises(KeyError):+            self.desc_pool.FindMessageTypeByName(_INVALID_SYMBOL_NAME)++    def testFindServiceFindMethod(self):+        service_name = self._SERVICE_NAMES[0]+        service_desc = self.desc_pool.FindServiceByName(service_name)+        self.assertEqual(service_name, service_desc.full_name)+        self.assertTrue(service_name.endswith(service_desc.name))+        file_name = _PROTO_FILE_NAME+        file_desc = self.desc_pool.FindFileByName(file_name)+        self.assertIs(file_desc, service_desc.file)++        method_name = ""EmptyCall""+        self.assertIn(method_name, service_desc.methods_by_name)++        method_desc = service_desc.FindMethodByName(method_name)+        self.assertIs(method_desc, service_desc.methods_by_name[method_name])+        self.assertIs(service_desc, method_desc.containing_service)+        self.assertEqual(method_name, method_desc.name)+        self.assertTrue(method_desc.full_name.endswith(method_name))++        empty_message_desc = self.desc_pool.FindMessageTypeByName(+            _EMPTY_PROTO_SYMBOL_NAME)+        self.assertEqual(empty_message_desc, method_desc.input_type)+        self.assertEqual(empty_message_desc, method_desc.output_type)++    def testFindServiceError(self):+        with self.assertRaises(KeyError):+            self.desc_pool.FindServiceByName(_INVALID_SYMBOL_NAME)++    def testFindMethodError(self):+        service_name = self._SERVICE_NAMES[0]+        service_desc = self.desc_pool.FindServiceByName(service_name)++        # FindMethodByName sometimes raises a KeyError, and sometimes returns None.+        # See https://github.com/protocolbuffers/protobuf/issues/9592+        with self.assertRaises(KeyError):+            res = service_desc.FindMethodByName(_INVALID_SYMBOL_NAME)+            if res is None:+                raise KeyError()","In the code you linked, the return type is `bool` but internally it calls a Python function (`PyObject_CallMethod(...)`). In this context we have a call chain that look like `Python -> C++ -> Python`, where the outer Python code is the one doing the call `desc_database.FindFileByName(...)` and the inner is the `FindFileByName` implementation. I'm not an expert on this, but I believe that if the inner Python code raises an exception then it is immediately caught by the outer Python code - it doesn't even do exception folding of that C++ function, just sort of steals the context.If the exception is not caught by the outer Python code, it looks like this:```Traceback (most recent call last):  File ""test.py"", line 12, in <module>    desc_pool.FindFileByName(""foo"")KeyError: ""Couldn't find file foo""```Note how all the traceback of the inner `C++ -> Python` is missing as result of this unusual flow.I hope this answers your question.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27976,827242551,2022-03-15T17:31:17Z,src/core/lib/json/json_object_loader.h,"@@ -0,0 +1,350 @@+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_JSON_JSON_OBJECT_LOADER_H+#define GRPC_CORE_LIB_JSON_JSON_OBJECT_LOADER_H++#include <grpc/support/port_platform.h>++#include <sys/types.h>++#include <cstddef>+#include <cstdint>+#include <cstring>+#include <string>+#include <vector>++#include ""absl/strings/string_view.h""++#include ""src/core/lib/json/json.h""++namespace grpc_core {++class ErrorList {+ public:+  void PushField(absl::string_view ext) GPR_ATTRIBUTE_NOINLINE;+  void PopField() GPR_ATTRIBUTE_NOINLINE;+  void AddError(absl::string_view error) GPR_ATTRIBUTE_NOINLINE;++  const std::vector<std::string>& errors() const { return errors_; }++ private:+  std::vector<std::string> errors_;+  std::vector<std::string> fields_;+};++class ScopedField {+ public:+  ScopedField(ErrorList* error_list, absl::string_view field_name)+      : error_list_(error_list) {+    error_list_->PushField(field_name);+  }+  ~ScopedField() { error_list_->PopField(); }++ private:+  ErrorList* error_list_;+};++namespace json_detail {++struct Element {+  enum Type : uint8_t {+    kInt32,+    kUint32,+    kString,+    // Vector should be the first thing after scalar types.+    kVector,+    kMap,+  };+  Element() = default;+  template <typename A, typename B>+  Element(B A::*p, Type type, bool optional, uint8_t type_data,+          const char* name)+      : member_offset(static_cast<uint16_t>(+            reinterpret_cast<uintptr_t>(&(static_cast<A*>(nullptr)->*p)))),+        type(type),+        optional(optional),+        type_data(type_data),+        name{} {+    strcpy(this->name, name);+  }+  uint16_t member_offset;+  Type type;+  bool optional;+  uint8_t type_data;+  char name[11];+};++struct TypeVtable {+  void* (*create)();+  void (*destroy)(void*);+  void (*push_to_vec)(void* ptr, void* vec);+  void (*insert_to_map)(std::string key, void* ptr, void* map);+};++template <typename T>+struct TypeVtableImpl {+  static const TypeVtable vtable;+};++template <typename T>+const TypeVtable TypeVtableImpl<T>::vtable{+    // create+    []() -> void* { return new T(); },+    // destroy+    [](void* ptr) { delete static_cast<T*>(ptr); },+    // push_to_vec+    [](void* ptr, void* vec) {+      auto* vec_ptr = static_cast<std::vector<T>*>(vec);+      auto* src_ptr = static_cast<T*>(ptr);+      vec_ptr->emplace_back(std::move(*src_ptr));+    },+    // insert_to_map+    [](std::string key, void* ptr, void* map) {+      auto* map_ptr = static_cast<std::map<std::string, T>*>(map);+      auto* src_ptr = static_cast<T*>(ptr);+      map_ptr->emplace(std::move(key), std::move(*src_ptr));+    },+};++class TypeRefProvider;++struct TypeRef {+  const TypeVtable* vtable;+  const Element* elements;+  size_t count;+  const TypeRefProvider* const* type_ref_providers;++  void Load(const Json::Object& json, void* dest,+            ErrorList* errors) const GPR_ATTRIBUTE_NOINLINE;+  using LoadFn = absl::FunctionRef<void(const Json& json, void* dest_ptr)>;+  void WithLoaderForTypeData(+      uint8_t tag, ErrorList* errors,+      absl::FunctionRef<void(const TypeVtable* vtable, LoadFn load)>) const+      GPR_ATTRIBUTE_NOINLINE;+  void LoadScalar(const Json& json, Element::Type type, void* dest,+                  ErrorList* errors) const GPR_ATTRIBUTE_NOINLINE;+  void LoadVector(const Json& json, uint8_t type_data, void* dest,+                  ErrorList* errors) const GPR_ATTRIBUTE_NOINLINE;+  void LoadMap(const Json& json, uint8_t type_data, void* dest,+               ErrorList* errors) const GPR_ATTRIBUTE_NOINLINE;+};++class TypeRefProvider {+ public:+  virtual void WithTypeRef(absl::FunctionRef<void(const TypeRef&)>) const = 0;++ protected:+  ~TypeRefProvider() = default;+};++template <typename T>+struct ElementTypeOf;+template <>+struct ElementTypeOf<int32_t> {+  static Element::Type type() { return Element::kInt32; }+};+template <>+struct ElementTypeOf<uint32_t> {+  static Element::Type type() { return Element::kUint32; }+};+template <>+struct ElementTypeOf<std::string> {+  static Element::Type type() { return Element::kString; }+};++// Vec<T, kSize> provides a constant array type that can be appended to by+// copying. It's setup so that most compilers can optimize away all of its+// operations.+template <typename T, size_t kSize>+class Vec {",Just pinging this comment to make sure it's on your radar.  Not a big deal either way; it just seems like it might be similar to things we have elsewhere.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27976,827246637,2022-03-15T17:35:44Z,src/core/lib/json/json_object_loader.h,"@@ -0,0 +1,387 @@+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_JSON_JSON_OBJECT_LOADER_H+#define GRPC_CORE_LIB_JSON_JSON_OBJECT_LOADER_H++#include <grpc/support/port_platform.h>++#include <sys/types.h>++#include <cstddef>+#include <cstdint>+#include <cstring>+#include <string>+#include <vector>++#include ""absl/strings/string_view.h""++#include ""src/core/lib/gprpp/time.h""+#include ""src/core/lib/json/json.h""++// Provides a means to load JSON objects into C++ objects, with the aim of+// minimizing object code size.+//+// Usage:+// Given struct Foo:+//   struct Foo {+//     int a;+//     int b;+//   };+// We add a member to Foo to declare how to load the object from JSON:+//   struct Foo {+//     int a;+//     int b;+//     static const JsonLoaderInterface* JsonLoader() {+//       static const auto loader = JsonObjectLoader<Foo>()+//           .Field(""a"", &Foo::a)+//           .Field(""b"", &Foo::b)+//           .Finish();+//       return &loader;+//     }+//   };+// Now we can load Foo objects from JSON:+//   ErrorList errors;+//   Foo foo = LoadFromJson<Foo>(json, &errors);+namespace grpc_core {++// A list of errors that occurred during JSON parsing.+// If a non-empty list occurs during parsing, the parsing failed.+class ErrorList {+ public:+  // Record that we're reading some field.+  void PushField(absl::string_view ext) GPR_ATTRIBUTE_NOINLINE;+  // Record that we've finished reading that field.+  void PopField() GPR_ATTRIBUTE_NOINLINE;+  // Record that we've encountered an error.+  void AddError(absl::string_view error) GPR_ATTRIBUTE_NOINLINE;++  // Return the list of errors.+  const std::vector<std::string>& errors() const { return errors_; }++  // Return true if there are no errors.+  bool ok() const { return errors_.empty(); }++ private:+  std::vector<std::string> errors_;+  std::vector<std::string> fields_;+};++// Note that we're reading a field, and remove it at the end of the scope.+class ScopedField {+ public:+  ScopedField(ErrorList* error_list, absl::string_view field_name)+      : error_list_(error_list) {+    error_list_->PushField(field_name);+  }+  ~ScopedField() { error_list_->PopField(); }++ private:+  ErrorList* error_list_;+};++namespace json_detail {++// An un-typed JSON loader.+class LoaderInterface {+ public:+  // Convert json value to whatever type we're loading at dst.+  // If errors occur, add them to error_list.+  virtual void LoadInto(const Json& json, void* dst,+                        ErrorList* errors) const = 0;++ protected:+  ~LoaderInterface() = default;+};++// Loads a scalar (string or number).+class LoadScalar : public LoaderInterface {+ public:+  void LoadInto(const Json& json, void* dst, ErrorList* errors) const override;++ protected:+  ~LoadScalar() = default;++ private:+  // true if we're loading a number, false if we're loading a string.+  // We use a virtual function to store this decision in a vtable instead of+  // needing an instance variable.+  virtual bool IsNumber() const = 0;++  virtual void LoadInto(const std::string& json, void* dst,+                        ErrorList* errors) const = 0;+};++// Load a number.+class LoadNumber : public LoadScalar {+ protected:+  ~LoadNumber() = default;++ private:+  bool IsNumber() const override;+};++// Load a duration+class LoadDuration : public LoadScalar {+ protected:+  ~LoadDuration() = default;++ private:+  bool IsNumber() const override;+  void LoadInto(const std::string& value, void* dst,+                ErrorList* errors) const override;+};++// Load a number of type T.+template <typename T>+class TypedLoadNumber : public LoadNumber {+ protected:+  ~TypedLoadNumber() = default;++ private:+  void LoadInto(const std::string& value, void* dst,+                ErrorList* errors) const override {+    if (!absl::SimpleAtoi(value, static_cast<T*>(dst))) {+      errors->AddError(""failed to parse number."");+    }+  }+};++// Load a string.+class LoadString : public LoadScalar {+ protected:+  ~LoadString() = default;++ private:+  bool IsNumber() const override;+  void LoadInto(const std::string& value, void* dst,+                ErrorList* errors) const override;+};++// Load a vector of some type.+class LoadVector : public LoaderInterface {+ public:+  void LoadInto(const Json& json, void* dst, ErrorList* errors) const override;++ protected:+  ~LoadVector() = default;++ private:+  virtual void LoadOne(const Json& json, void* dst,+                       ErrorList* errors) const = 0;+};++// Load a map of string->some type.+class LoadMap : public LoaderInterface {+ public:+  void LoadInto(const Json& json, void* dst, ErrorList* errors) const override;++ protected:+  ~LoadMap() = default;++ private:+  virtual void LoadOne(const Json& json, const std::string& name, void* dst,+                       ErrorList* errors) const = 0;+};++// Fetch a LoaderInterface for some type.+template <typename T>+const LoaderInterface* LoaderForType();++// AutoLoader implements LoaderInterface for a type.+// The default asks the type for its LoaderInterface and then uses that.+// Classes that load from objects should provide a:+// static const JsonLoaderInterface* JsonLoader();+template <typename T>+class AutoLoader final : public LoaderInterface {+ public:+  void LoadInto(const Json& json, void* dst, ErrorList* errors) const override {+    T::JsonLoader()->LoadInto(json, dst, errors);+  }+};++// Specializations of AutoLoader for basic types.+template <>+class AutoLoader<int32_t> final : public TypedLoadNumber<int32_t> {};+template <>+class AutoLoader<uint32_t> final : public TypedLoadNumber<uint32_t> {};+template <>+class AutoLoader<int64_t> final : public TypedLoadNumber<int64_t> {};+template <>+class AutoLoader<uint64_t> final : public TypedLoadNumber<uint64_t> {};+template <>+class AutoLoader<Duration> final : public LoadDuration {};+template <>+class AutoLoader<std::string> final : public LoadString {};++// Specializations of AutoLoader for vectors.+template <typename T>+class AutoLoader<std::vector<T>> final : public LoadVector {+ private:+  void LoadOne(const Json& json, void* dst, ErrorList* errors) const final {+    auto* vec = static_cast<std::vector<T>*>(dst);+    T value;+    LoaderForType<T>()->LoadInto(json, &value, errors);+    vec->push_back(std::move(value));+  }+};++// Specializations of AutoLoader for maps.+template <typename T>+class AutoLoader<std::map<std::string, T>> final : public LoadMap {+ private:+  void LoadOne(const Json& json, const std::string& name, void* dst,+               ErrorList* errors) const final {+    auto* map = static_cast<std::map<std::string, T>*>(dst);+    T value;+    LoaderForType<T>()->LoadInto(json, &value, errors);+    map->emplace(name, std::move(value));+  }+};++// Implementation of aforementioned LoaderForType.+// Simply keeps a static AutoLoader<T> and returns a pointer to that.+template <typename T>+const LoaderInterface* LoaderForType() {+  static const AutoLoader<T> loader;+  return &loader;+}++// Element describes one typed field to be loaded from a JSON object.+struct Element {+  Element() = default;+  template <typename A, typename B>+  Element(const char* name, bool optional, B A::*p,+          const LoaderInterface* loader)+      : loader(loader),+        member_offset(static_cast<uint16_t>(+            reinterpret_cast<uintptr_t>(&(static_cast<A*>(nullptr)->*p)))),+        optional(optional),+        name{} {+    strcpy(this->name, name);+  }+  // The loader for this field.+  const LoaderInterface* loader;+  // Offset into the destination object to store the field.+  uint16_t member_offset;+  // Is this field optional?+  bool optional;+  // The name of the field.+  char name[13];","We definitely have some field names that are longer than 13 characters.Instead of imposing any particular length limitation here, why not use `absl::string_view`?  I think the passed-in field names should basically always be string literals, so we shouldn't have to store a copy in the first place.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/29094,827298839,2022-03-15T18:36:02Z,src/core/ext/xds/xds_client.cc,"@@ -1204,6 +1204,7 @@ bool XdsClient::ChannelState::AdsCallState::OnResponseReceivedLocked() {             xds_client(), chand()->server_.server_uri.c_str(),             status.ToString().c_str());   } else {+    seen_response_ = true;","At first glance, this doesn't look quite the same as [Go](https://github.com/grpc/grpc-go/pull/5241), [Java](https://github.com/grpc/grpc-java/blob/87e13daf1a0a01afafb3e0f2818e02fe5c624ad1/xds/src/main/java/io/grpc/xds/AbstractXdsClient.java#L420), or [envoy](https://github.com/envoyproxy/envoy/blob/82b1f66df20d9577a24ea632f149481ba2f99edd/source/common/config/grpc_stream.h#L92), because we're resetting backoff only if the response message parsed.But I take it we are doing that here because those other implementations are implicitly doing that?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27976,827322767,2022-03-15T19:05:52Z,src/core/lib/json/json_object_loader.h,"@@ -0,0 +1,387 @@+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_JSON_JSON_OBJECT_LOADER_H+#define GRPC_CORE_LIB_JSON_JSON_OBJECT_LOADER_H++#include <grpc/support/port_platform.h>++#include <sys/types.h>++#include <cstddef>+#include <cstdint>+#include <cstring>+#include <string>+#include <vector>++#include ""absl/strings/string_view.h""++#include ""src/core/lib/gprpp/time.h""+#include ""src/core/lib/json/json.h""++// Provides a means to load JSON objects into C++ objects, with the aim of+// minimizing object code size.+//+// Usage:+// Given struct Foo:+//   struct Foo {+//     int a;+//     int b;+//   };+// We add a member to Foo to declare how to load the object from JSON:+//   struct Foo {+//     int a;+//     int b;+//     static const JsonLoaderInterface* JsonLoader() {+//       static const auto loader = JsonObjectLoader<Foo>()+//           .Field(""a"", &Foo::a)","One common use-case we have is when parsing the LB policy config for a non-leaf policy, where the config for the child policy is embedded in the config for the parent policy.  In that case, at the point where we're defining the config for the parent policy, we don't actually know what fields are going to be in the config for the child policy, so we can't define them inline.  Instead, what we want is to pass the `Json` object of the child's config to the LB policy registry, which will parse the config for us.  For example:https://github.com/grpc/grpc/blob/28db143b98682b06aa5c58059ffe53b42f28b395/src/core/ext/filters/client_channel/lb_policy/priority/priority.cc#L840For the majority of instances of this use-case, we just want to store the resulting `RefCountedPtr<LoadBalancingPolicy::Config>` object, so I think we can probably just define `AutoLoader<RefCountedPtr<LoadBalancingPolicy::Config>>`.  But we probably don't want to do that here in the JSON library, because we don't want the JSON library to depend on the LB policy API.  Is there a way we can inject a new `AutoLoader<>` type from somewhere in the LB policy library?Also, there's at least one edge case where we actually want to hang on to the child policy config in `Json` form rather than converting it to `RefCountedPtr<LoadBalancingPolicy::Config>`, since we need to modify it before we actually use it:https://github.com/grpc/grpc/blob/28db143b98682b06aa5c58059ffe53b42f28b395/src/core/ext/filters/client_channel/lb_policy/rls/rls.cc#L2501For that kind of case, is there a way that we can pass in a custom parsing function for a particular field, rather than having it be always inferred from the type?  Since the field we want to store here is type `Json`, that seems a little too general to key this solely by the type, so I think we need some sort of escape hatch here.  Or maybe the answer is to define a custom struct type that contains only a `Json` field, and then inject an `AutoLoader<>` implementation for that custom struct type?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/27976,827324302,2022-03-15T19:07:52Z,src/core/lib/json/json_object_loader.h,"@@ -0,0 +1,387 @@+// Copyright 2020 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_JSON_JSON_OBJECT_LOADER_H+#define GRPC_CORE_LIB_JSON_JSON_OBJECT_LOADER_H++#include <grpc/support/port_platform.h>++#include <sys/types.h>++#include <cstddef>+#include <cstdint>+#include <cstring>+#include <string>+#include <vector>++#include ""absl/strings/string_view.h""++#include ""src/core/lib/gprpp/time.h""+#include ""src/core/lib/json/json.h""++// Provides a means to load JSON objects into C++ objects, with the aim of+// minimizing object code size.+//+// Usage:+// Given struct Foo:+//   struct Foo {+//     int a;+//     int b;+//   };+// We add a member to Foo to declare how to load the object from JSON:+//   struct Foo {+//     int a;+//     int b;+//     static const JsonLoaderInterface* JsonLoader() {+//       static const auto loader = JsonObjectLoader<Foo>()+//           .Field(""a"", &Foo::a)+//           .Field(""b"", &Foo::b)+//           .Finish();","I think we may need some hooks for customizing validation for one-off use-cases.  One use-case for this is where we have a validation that needs to look at multiple fields.  For example, there's a case in the RLS LB policy where we clamp one field to the value of another field:https://github.com/grpc/grpc/blob/28db143b98682b06aa5c58059ffe53b42f28b395/src/core/ext/filters/client_channel/lb_policy/rls/rls.cc#L2380There's also a use-case where we need to do a custom check on the value of a string field:https://github.com/grpc/grpc/blob/28db143b98682b06aa5c58059ffe53b42f28b395/src/core/ext/filters/client_channel/lb_policy/rls/rls.cc#L2349In those two particular cases, it probably wouldn't be terrible to just do that validation after the parsing is done.  But I could imagine cases where we need to do that validation for an object that is nested deep within the JSON hierarchy in many different places, where it might make sense to do that validation after parsing each instance of the object.  Is there a way to add a post-parsing hook here that can look at the fields in the resulting struct and optionally add errors if there's a validation failure?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29094,827352531,2022-03-15T19:46:42Z,src/core/ext/xds/xds_client.cc,"@@ -1204,6 +1204,7 @@ bool XdsClient::ChannelState::AdsCallState::OnResponseReceivedLocked() {             xds_client(), chand()->server_.server_uri.c_str(),             status.ToString().c_str());   } else {+    seen_response_ = true;","Yes, I believe that in all of those other implementations, the protobuf deserialization will happen before they consider the message to be received; the read will basically fail if the protobuf deserialization fails.In any case, I think that's the right thing to do, because it's a completely reasonable sanity check that we've gotten a valid response from the server.  We don't have to care whether any of the resources in the response are valid, but we should care that the basic transport protocol semantics are being followed, or else we're not really in contact with a healthy server.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28667,828359689,2022-03-16T19:11:54Z,bazel/grpc_build_system.bzl,"@@ -271,7 +277,7 @@ def ios_cc_test(             deps = ios_test_deps,         ) -def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None, copts = [], linkstatic = None):+def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None, copts = [], linkstatic = None, exclude_pollers = [], uses_event_engine = True):",Why do we need `uses_event_engine` as a parameter?  Isn't that basically always going to be the same as `uses_polling`?  In what case will we set those two parameters to different values on the same test?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28667,828360055,2022-03-16T19:12:27Z,bazel/grpc_build_system.bzl,"@@ -271,7 +277,7 @@ def ios_cc_test(             deps = ios_test_deps,         ) -def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None, copts = [], linkstatic = None):+def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None, copts = [], linkstatic = None, exclude_pollers = [], uses_event_engine = True):",Note that any additional parameters here will also need to be added internally before this PR can be imported.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28667,828364972,2022-03-16T19:18:52Z,test/core/end2end/generate_tests.bzl,"@@ -431,35 +431,25 @@ def grpc_end2end_tests():         )          for t, topt in END2END_TESTS.items():-            #print(_compatible(fopt, topt), f, t, fopt, topt)             if not _compatible(fopt, topt):                 continue-             test_short_name = str(t) if not topt.short_name else topt.short_name-            native.sh_test(+            grpc_cc_test(                 name = ""%s_test@%s"" % (f, test_short_name),-                data = ["":%s_test"" % f],-                srcs = [""end2end_test.sh""],-                args = [-                    ""$(location %s_test)"" % f,-                    t,+                srcs = [""fixtures/%s.cc"" % f],+                data = ["":end2end_tests""],","Seems odd to have the same target listed in both deps and data.  Why is this needed?  If it's related to the TSI test certificate files above, maybe just move that data here instead?",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28667,828407893,2022-03-16T20:14:59Z,test/core/end2end/generate_tests.bzl,"@@ -431,35 +431,25 @@ def grpc_end2end_tests():         )          for t, topt in END2END_TESTS.items():-            #print(_compatible(fopt, topt), f, t, fopt, topt)             if not _compatible(fopt, topt):                 continue-             test_short_name = str(t) if not topt.short_name else topt.short_name-            native.sh_test(+            grpc_cc_test(                 name = ""%s_test@%s"" % (f, test_short_name),-                data = ["":%s_test"" % f],-                srcs = [""end2end_test.sh""],-                args = [-                    ""$(location %s_test)"" % f,-                    t,+                srcs = [""fixtures/%s.cc"" % f],+                data = ["":end2end_tests""],","As per my preference above, we'd be duplicating the TSI cert data to two targets if we did that. If (in a separate PR) we can remove the end2end test cc_binary target, then you're exactly right, all we'd need to do is move the data here.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29133,829446339,2022-03-17T19:53:30Z,src/python/grpcio/grpc/_server.py,"@@ -880,7 +880,11 @@ def _serve(state):         if state.server_deallocated:             _begin_shutdown_once(state)         if event.completion_type != cygrpc.CompletionType.queue_timeout:-            if not _process_event_and_continue(state, event):+            try:+                if not _process_event_and_continue(state, event):+                    return+            except RuntimeError:",Should we try to catch more exception types?,X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/29011,829508266,2022-03-17T21:00:25Z,src/core/ext/transport/chttp2/transport/parsing.cc,"@@ -92,10 +96,12 @@ grpc_error_handle grpc_chttp2_perform_read(grpc_chttp2_transport* t,           return GRPC_ERROR_CREATE_FROM_CPP_STRING(absl::StrFormat(               ""Connect string mismatch: expected '%c' (%d) got '%c' (%d) ""               ""at byte %d"",-              GRPC_CHTTP2_CLIENT_CONNECT_STRING[t->deframe_state],+              get_utf8_safe_char(",It's still valuable to keep this change because although we don't need to technically but having utf-8 compatible strings is worth it.,
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28959,829556378,2022-03-17T22:17:32Z,test/core/event_engine/test_suite/README.md,"@@ -37,3 +37,28 @@ you could depend on any subset of the following: * `//test/core/event_engine/test_suite:dns` * `//test/core/event_engine/test_suite:client` * `//test/core/event_engine/test_suite:server`+++## Breaking Changes++As the conformance test suite evolves, it may introduce changes that break your+implementation. We want to be able to improve the conformance test suite freely+without harming everyone's EventEngine builds or requiring lock-step engine+improvements, so a test rollout strategy is required.++New conformance tests will be added to `*_rc` (release candidate) targets,+such as:++* `//test/core/event_engine/test_suite:timer_rc`+* `//test/core/event_engine/test_suite:dns_rc`+* `//test/core/event_engine/test_suite:client_rc`+* `//test/core/event_engine/test_suite:server_rc`++Quarterly, these additional tests will be rolled up into the main test suite","@tamird  Regarding the naming scheme, I'm interested in what you think a clearer scheme might be. It's a common standard, for example, to have a production version `<name>` and a release candidate version named `<name>-rc1` or some such. gRPC versions its libraries that way. What I suggested is the same, but for bazel targets.Given that a new test may be added to the `_rc` target the day before a release, it may be better to have a plan such as:* ""base name"" targets make up the current production conformance test suite (e.g., `timer`, `dns`, etc)* new tests get added to brand new independent targets (e.g., `timer_dst_expectations`)* targets get tagged with a release version or a date that they're expected to land in production, and the production target name that they should be rolled up into (e.g., `timer`).* release automation will combine the targets that need to be rolled up (via adding its source and headers files, maybe), and delete the old independent targets",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29095,829668339,2022-03-18T03:22:36Z,src/core/lib/surface/call.cc,"@@ -62,170 +64,277 @@ #include ""src/core/lib/transport/error_utils.h"" #include ""src/core/lib/transport/transport.h"" -/** The maximum number of concurrent batches possible.-    Based upon the maximum number of individually queueable ops in the batch-    api:-      - initial metadata send-      - message send-      - status/close send (depending on client/server)-      - initial metadata recv-      - message recv-      - status/close recv (depending on client/server) */-#define MAX_CONCURRENT_BATCHES 6--struct batch_control {-  batch_control() = default;--  grpc_call* call = nullptr;-  grpc_transport_stream_op_batch op;-  /* Share memory for cq_completion and notify_tag as they are never needed-     simultaneously. Each byte used in this data structure count as six bytes-     per call, so any savings we can make are worthwhile,--     We use notify_tag to determine whether or not to send notification to the-     completion queue. Once we've made that determination, we can reuse the-     memory for cq_completion. */-  union {-    grpc_cq_completion cq_completion;-    struct {-      /* Any given op indicates completion by either (a) calling a closure or-         (b) sending a notification on the call's completion queue.  If-         \a is_closure is true, \a tag indicates a closure to be invoked;-         otherwise, \a tag indicates the tag to be used in the notification to-         be sent to the completion queue. */-      void* tag;-      bool is_closure;-    } notify_tag;-  } completion_data;-  grpc_closure start_batch;-  grpc_closure finish_batch;-  std::atomic<intptr_t> steps_to_complete{0};-  AtomicError batch_error;-  void set_num_steps_to_complete(uintptr_t steps) {-    steps_to_complete.store(steps, std::memory_order_release);+grpc_core::TraceFlag grpc_call_error_trace(false, ""call_error"");+grpc_core::TraceFlag grpc_compression_trace(false, ""compression"");++namespace grpc_core {++class Call : public CppImplOf<Call, grpc_call> {","So when we have C types that we want to implement in C++ we get some design choices...Choice 1:```struct grpc_call {  grpc_call(...);  void foo();   private:  int member1_;};void grpc_call_foo(grpc_call* c) { c->foo(); }```Here we land the C++ implementation directly in the C struct... we end up with something that performs nicely, but the implementation is a confusing mixture of styles, and the resulting code looks like someone vomited a great deal of `grpc_core::`'s all over the place.Choice 2:```namespace grpc_core {class Call { public:  void foo(); private:  grpc_call* outer_;};struct grpc_call {  unique_ptr<grpc_core::Call> inner;};void grpc_call_foo(grpc_call* c) { c->inner->foo(); }```Here we wrap the inner C++ type with a C struct, and use a member variable to get back out again. It's fiddly to construct, but more importantly costs an extra allocation and two additional pointers for each wrapped type - and necessitates touching two cachelines for every access to the object from outside. We've made the code more idiomatic and typesafe - great for us - but we've potentially sacrificed some performance for users.Choice 3:```struct grpc_call;namespace grpc_core {class Call { ... };}void grpc_call_foo(grpc_call* c) { reinterpret_cast<grpc_core::Call*>(c)->foo(); }```Here we say that grpc_call is just a tag, and we reinterpet_cast back and forth everywhere we might need.It works... and most of the code ends up looking nice and idiomatic, except at our boundary points - reasonable people avoid writing reinterpret_cast frequently - one might cast to the wrong type with disastrous consequences, it's just ugly, and let's not do that.This is on par with the best possible performance though, so we're getting closer!And so finally, Choice 4:```struct grpc_call;namespace grpc_core {class Call : public CppImplOf<Call, grpc_call> { ... };}void grpc_call_foo(grpc_call* c) { grpc_core::Call::FromC(c)->foo(); }```We've essentially taken Choice 3, and buried the reinterpret_casts inside CppImplOf.In doing so, we've limited the number of ways we can mess things up with reinterpret_cast (it's quite hard to misuse), and our code looks pretty dang idiomatic - AND we don't have the dual allocation/cache pollution problems of Choice 2.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29050,829676834,2022-03-18T03:50:52Z,src/core/ext/transport/chttp2/transport/chttp2_transport.cc,"@@ -1759,18 +1759,123 @@ void grpc_chttp2_ack_ping(grpc_chttp2_transport* t, uint64_t id) {   } } +namespace {++// Fire and forget (deletes itself on completion). Does a graceful shutdown by","My understanding of this type is that it keeps the transport alive an extra 20 seconds unconditionally - I think this is problematic -- if we signal GOAWAY and wait one RTT, then once the number of requests on a transport drops to zero we are able to complete this dance deterministically and release the (significant) memory used by the transport much earlier.",X
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/29050,829691600,2022-03-18T04:43:16Z,src/core/ext/transport/chttp2/transport/chttp2_transport.cc,"@@ -1759,18 +1759,123 @@ void grpc_chttp2_ack_ping(grpc_chttp2_transport* t, uint64_t id) {   } } +namespace {++// Fire and forget (deletes itself on completion). Does a graceful shutdown by","No, it's not an unconditional 20 second wait. It only waits 20 seconds if the peer does not respond to the ping. If there are no streams and the RTT completes with a ping ack, then we would immediately send the final goaway and close the transport. (Some of our tests fail on an unconditional wait.)",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/29050,829692455,2022-03-18T04:46:16Z,src/core/ext/transport/chttp2/transport/chttp2_transport.cc,"@@ -1759,18 +1759,123 @@ void grpc_chttp2_ack_ping(grpc_chttp2_transport* t, uint64_t id) {   } } +namespace {++// Fire and forget (deletes itself on completion). Does a graceful shutdown by+// sending a GOAWAY frame with the last stream id set to 2^31-1, sending a ping+// and waiting for an ack (effective waiting for an RTT) and then sending a+// final GOAWAY freame with an updated last stream identifier. This helps ensure+// that a connection can be cleanly shut down without losing requests.+// In the event, that the client does not respond to the ping for some reason,+// we add a 20 second deadline, after which we send the second goaway.+class GracefulGoaway : public grpc_core::InternallyRefCounted<GracefulGoaway> {+ public:+  static void Start(grpc_chttp2_transport* t) { new GracefulGoaway(t); }++  ~GracefulGoaway() override {+    GRPC_CHTTP2_UNREF_TRANSPORT(t_, ""graceful goaway"");+  }++ private:+  explicit GracefulGoaway(grpc_chttp2_transport* t) : t_(t) {+    t->sent_goaway_state = GRPC_CHTTP2_GRACEFUL_GOAWAY;+    GRPC_CHTTP2_REF_TRANSPORT(t_, ""graceful goaway"");+    grpc_chttp2_goaway_append((1u << 31) - 1, 0, grpc_empty_slice(), &t->qbuf);+    send_ping_locked(+        t, nullptr, GRPC_CLOSURE_INIT(&on_ping_ack_, OnPingAck, this, nullptr));+    grpc_chttp2_initiate_write(t, GRPC_CHTTP2_INITIATE_WRITE_GOAWAY_SENT);+    Ref().release();  // Ref for the timer+    grpc_timer_init(+        &timer_,+        grpc_core::ExecCtx::Get()->Now() + grpc_core::Duration::Seconds(20),+        GRPC_CLOSURE_INIT(&on_timer_, OnTimer, this, nullptr));+  }++  // Unused+  void Orphan() override { GPR_ASSERT(0); }++  void MaybeSendFinalGoawayLocked() {+    if (t_->sent_goaway_state != GRPC_CHTTP2_GRACEFUL_GOAWAY) {","No, this handles a race condition between the timer and the ping ack. Consider a scenario where the timer fires but we also receive the ping ack around the same time.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29141,829703961,2022-03-18T05:26:08Z,tools/internal_ci/linux/grpc_distribtests_php.sh,"@@ -33,7 +33,7 @@ cp -r artifacts/* input_artifacts/ || true rm -rf artifacts_from_build_artifacts_step mv artifacts artifacts_from_build_artifacts_step || true -# This step mostly just copies artifacts from input_artifacts (but it also does some wheel stripping)+# This step simply collects php artifacts from subdirectories of input_artifacts/ and copies them to artifacts/ tools/run_tests/task_runner.py -f package linux php -x build_packages/sponge_log.xml || FAILED=""true""","kind of, for some languages the package phase is just ""collect the artifacts from subdirectories"" (so that the per-language release processes can expect the packages being found in the right place).For some languages (like C# and ruby's grpc-tools), we will still need a second phase (""build packages"") but that's more of an exception.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/28959,830161329,2022-03-18T16:25:10Z,test/core/event_engine/test_suite/README.md,"@@ -37,3 +37,28 @@ you could depend on any subset of the following: * `//test/core/event_engine/test_suite:dns` * `//test/core/event_engine/test_suite:client` * `//test/core/event_engine/test_suite:server`+++## Breaking Changes++As the conformance test suite evolves, it may introduce changes that break your+implementation. We want to be able to improve the conformance test suite freely+without harming everyone's EventEngine builds or requiring lock-step engine+improvements, so a test rollout strategy is required.++New conformance tests will be added to `*_rc` (release candidate) targets,+such as:++* `//test/core/event_engine/test_suite:timer_rc`+* `//test/core/event_engine/test_suite:dns_rc`+* `//test/core/event_engine/test_suite:client_rc`+* `//test/core/event_engine/test_suite:server_rc`++Quarterly, these additional tests will be rolled up into the main test suite","I see. The change would have to be scheduled for minor releases. It's a bit of a sticky subject, but given that this low-level API is expected to be sparsely used in the public and primarily by ""power users"", after the EventEngine API is baked for a while and promoted from experimental, we intend to provide best effort API stability without a major version bump for necessary changes. And we're aiming to design the API to allow some changes to be made in a backwards-compatible ways.Last I talked with the team about it, there is strong sentiment against doing a gRPC 2.0 (has been for years, anyway), so your suggestion would prevent us from improving the test suite at all. We do a minor release every 6 weeks, with a pre-release a week before. You can read more about the release process here: https://grpc.io/docs/what-is-grpc/faq/#how-long-are-grpc-releases-supported-for",X
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/29160,830397105,2022-03-18T23:05:07Z,src/core/lib/debug/stats_data.cc,"@@ -281,7 +276,6 @@ const char* grpc_stats_histogram_doc[GRPC_STATS_HISTOGRAM_COUNT] = {     ""Number of streams whose payload was written per TCP write"",     ""Number of streams terminated per TCP write"",     ""Number of flow control updates written per TCP write"",-    // NOLINTNEXTLINE(bugprone-suspicious-missing-comma)","Two things we can do. i) modifying `tools/codegen/core/gen_stats_data.py` to have this comment (I didn't know this is auto-generated, either) or ii) modifying tools/dockerfile/grpc_clang_tidy/clang_tidy_all_the_things.sh to exclude this file. I prefer 1 since it'd be valuable to run clang-tidy even on generated files since the generator could be buggy. But i'm not strongly opposing to the second option, either.",X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/29160,831275528,2022-03-21T16:01:03Z,src/core/lib/iomgr/ev_posix.cc,"@@ -126,12 +125,12 @@ const grpc_event_engine_vtable* init_non_polling(bool explicit_request) { // environment variable if that variable is set (which should be a // comma-separated list of one or more event engine names) static event_engine_factory g_factories[] = {-    {ENGINE_HEAD_CUSTOM, nullptr},        {ENGINE_HEAD_CUSTOM, nullptr},-    {ENGINE_HEAD_CUSTOM, nullptr},        {ENGINE_HEAD_CUSTOM, nullptr},-    {""epollex"", grpc_init_epollex_linux}, {""epoll1"", grpc_init_epoll1_linux},-    {""poll"", grpc_init_poll_posix},       {""none"", init_non_polling},-    {ENGINE_TAIL_CUSTOM, nullptr},        {ENGINE_TAIL_CUSTOM, nullptr},-    {ENGINE_TAIL_CUSTOM, nullptr},        {ENGINE_TAIL_CUSTOM, nullptr},+    {ENGINE_HEAD_CUSTOM, nullptr},      {ENGINE_HEAD_CUSTOM, nullptr},+    {ENGINE_HEAD_CUSTOM, nullptr},      {ENGINE_HEAD_CUSTOM, nullptr},+    {""epoll1"", grpc_init_epoll1_linux}, {""poll"", grpc_init_poll_posix},","Because GRPC_POLL_STRATEGY is an exposed environment variable, I believe that this change has API implications in that previously running code will no longer have a match in this array. Do they fail or just accept a different implementation in that case?",
85715732,yousukseung,https://api.github.com/repos/grpc/grpc/pulls/29169,831303062,2022-03-21T16:27:01Z,src/core/lib/address_utils/sockaddr_utils.cc,"@@ -183,14 +183,39 @@ void grpc_sockaddr_make_wildcard6(int port, }  std::string grpc_sockaddr_to_string(const grpc_resolved_address* resolved_addr,-                                    bool normalize) {","That sounds better, but are we OK to make a big change that updates everywhere and/or is it safe to make a ""breaking change"" of an existing util? Also I wonder what Mark thinks.In case we decide to change: In some places I see we don't check for an error where I believe we should, but I guess we should make a safe change in this PR to always return either the converted string when OK or the error message which will be the same as the error string that we return today.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29169,831307897,2022-03-21T16:31:50Z,src/core/lib/address_utils/sockaddr_utils.cc,"@@ -183,14 +183,39 @@ void grpc_sockaddr_make_wildcard6(int port, }  std::string grpc_sockaddr_to_string(const grpc_resolved_address* resolved_addr,-                                    bool normalize) {",Internal APIs (such as this) I think it's better to make a big change and get the API right rather than have a mess of partially changed APIs to program against.We'll need to update this repo and probably cherry-pick the change across rather than use the usual import process (assuming there's non-zero internal usage of this API),X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29133,831378703,2022-03-21T17:38:56Z,src/python/grpcio_tests/tests_py3_only/unit/_simple_stubs_test.py,"@@ -409,6 +409,42 @@ def test_default_timeout(self):         for case in cases:             with self.subTest(**case):                 self.assert_times_out(case)+    +    def test_main_thread_quit(self):+        def _get_server(+            credentials: Optional[grpc.ServerCredentials]+        ) -> Tuple[grpc.Server, int]:+            server = test_common.test_server()+            target = '[::]:0'+            if credentials is None:+                port = server.add_insecure_port(target)+            else:+                port = server.add_secure_port(target, credentials)+            server.add_generic_rpc_handlers((_GenericHandler(),))+            server.start()+            return server, port++        rpc_end_event = threading.Event()+        server, port = _get_server(None)+        target = f'localhost:{port}'++        def _send_rpc():+            try:+                grpc.experimental.unary_unary(_REQUEST,+                                            target,+                                            _UNARY_UNARY,+                                            timeout=3,+                                            insecure=True)+            except:+                rpc_end_event.set()+++        server._state.thread_pool._shutdown = True+        threading.Thread(target=_send_rpc).start()+        end_flag = rpc_end_event.wait(1)","If possible, please use existing constants:https://github.com/grpc/grpc/blob/master/src/python/grpcio_tests/tests/unit/framework/common/test_constants.pyOr define your own on the top of the test.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29133,831383401,2022-03-21T17:44:07Z,src/python/grpcio_tests/tests_py3_only/unit/_simple_stubs_test.py,"@@ -409,6 +409,42 @@ def test_default_timeout(self):         for case in cases:             with self.subTest(**case):                 self.assert_times_out(case)+    +    def test_main_thread_quit(self):","Technically, in the PR description, the issue was that a process refused to end due to the thread pool not closed. This test doesn't seem to be as powerful as your snippet in description. I would suggest to use subprocess to test the exit behavior.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29178,832388691,2022-03-22T16:34:54Z,tools/run_tests/artifacts/build_artifact_python.sh,"@@ -113,6 +113,7 @@ clean_non_source_files() { tar xzf ""${GRPCIO_TAR_GZ}"" -C ""${GRPCIO_STRIP_TEMPDIR}"" ( cd ""${GRPCIO_STRIP_TEMPDIR}""   find . -type d -name .git -exec rm -fr {} \; || true+  rm -f src/python/grpcio/grpc/_cython/cygrpc.cpp || true","The content of a source wheel is dictated by [MANIFEST](https://github.com/grpc/grpc/blob/master/PYTHON-MANIFEST.in). I don't see how a cpp file might be included. Can you provide more evidence?If there is a bug in source wheel file list, I would propose to correct the MANIFEST file.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29178,832518979,2022-03-22T18:51:26Z,tools/run_tests/artifacts/build_artifact_python.sh,"@@ -113,6 +113,7 @@ clean_non_source_files() { tar xzf ""${GRPCIO_TAR_GZ}"" -C ""${GRPCIO_STRIP_TEMPDIR}"" ( cd ""${GRPCIO_STRIP_TEMPDIR}""   find . -type d -name .git -exec rm -fr {} \; || true+  rm -f src/python/grpcio/grpc/_cython/cygrpc.cpp || true","Thanks for the official guide pointer, and checking the wheel content. I see the cpp file is got included, and that is on us. The guide didn't mention how extension generated files will get special treatment. Maybe we can try to exclude this file explicitly: https://github.com/grpc/grpc/blob/master/PYTHON-MANIFEST.in#L2Besides, if we update MANIFEST to fix the problem, we should be able to decouple the building of correct source wheel from this script.",X
87434895,niyas-sait,https://api.github.com/repos/grpc/grpc/pulls/29178,833211527,2022-03-23T12:37:01Z,tools/run_tests/artifacts/build_artifact_python.sh,"@@ -113,6 +113,7 @@ clean_non_source_files() { tar xzf ""${GRPCIO_TAR_GZ}"" -C ""${GRPCIO_STRIP_TEMPDIR}"" ( cd ""${GRPCIO_STRIP_TEMPDIR}""   find . -type d -name .git -exec rm -fr {} \; || true+  rm -f src/python/grpcio/grpc/_cython/cygrpc.cpp || true",I think the exclude issue is specific to windows. It seems to work on my mac. Looks like `setuptools` is struggling with pattern matching for windows (possibly forward slashes etc.). Since we are running the script form a Linux box I think it should be fine to add it to MANIFEST file. I've reverted previous change and added the entry to MANIFEST. I think it should work. but If anyone can try it on their setup and confirm that would be great.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29175,834352762,2022-03-24T14:12:13Z,test/distrib/bazel/cpp/greeter_client.cc,"@@ -0,0 +1,104 @@+/*","is this a verbatim copy of greeter_client.cc from examples? if so, we should at least add a TODO to deduplicate.I don't think we want to have multiple copies of greeter_client.cc in the repo.Same for greeter_server.cc and the .proto file.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29175,834359265,2022-03-24T14:18:05Z,test/distrib/bazel/test_single_bazel_version.sh,"@@ -45,3 +45,7 @@ export OVERRIDE_BAZEL_VERSION=""$VERSION"" # when running under bazel docker image, the workspace is read only. export OVERRIDE_BAZEL_WRAPPER_DOWNLOAD_DIR=/tmp bazel build -- //... ""${EXCLUDED_TARGETS[@]}""++cd test/distrib/bazel/cpp/++bazel test //:all","nit: for a test that has 2 actions (`bazel build -- //... ""${EXCLUDED_TARGETS[@]}""` and `bazel test //:all`, running the in a sequence is generally speaking an antipattern  (if the first action fails, the other one will never run).but in this case, since the first step tries to build grpc, it is perhaps reasonable to assume that if grpc build fails, there is no point of trying to build greeter on top of grpc.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29191,834372029,2022-03-24T14:29:31Z,test/distrib/bazel/run_bazel_distrib_test.sh,"@@ -17,15 +17,11 @@ set -ex  cd ""$(dirname ""$0"")"" -# TODO(jtattermusch): make build work with bazel 2.2.0 and bazel 1.2.1 if that's reasonably simple.-SUPPORTED_VERSIONS=(-  ""3.7.2""-  ""4.0.0""-  ""5.0.0""-)+# Ignore comment lines.","since you already have a buildgen plugin to read the list of supported bazel versions, you could have a template the creates this shell script and renders the `SUPPORTED_VERSIONS=(    )`. Right now you have two places where you're stripping comment from the file and both are fragile.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29191,834374096,2022-03-24T14:31:19Z,test/distrib/bazel/supported_versions.txt,"@@ -0,0 +1,4 @@+# TODO(jtattermusch): make build work with bazel 2.2.0 and bazel 1.2.1 if that's reasonably simple.",I don't think the supported_versions.txt file needs to be under `test/distrib/bazel` (since the list of supported bazel versions isn't logically part of the distribtest).,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29174,834606825,2022-03-24T18:26:19Z,src/core/ext/filters/client_channel/lb_policy/xds/xds_cluster_resolver.cc,"@@ -510,6 +489,35 @@ void XdsClusterResolverLb::LogicalDNSDiscoveryMechanism::ResolverResultHandler::       discovery_mechanism_->index(), std::move(update)); } +//+// XdsClusterResolverLb::DiscoveryMechanismEntry+//++const XdsClusterResolverLbConfig::DiscoveryMechanism&+XdsClusterResolverLb::DiscoveryMechanismEntry::config() const {+  return discovery_mechanism->parent()+      ->config_->discovery_mechanisms()[discovery_mechanism->index()];+}++std::string XdsClusterResolverLb::DiscoveryMechanismEntry::GetChildPolicyName(+    size_t priority) const {+  const size_t child_number = priority_child_numbers[priority];+  const auto& discovery_config = config();+  absl::InlinedVector<absl::string_view, 9> parts = {+      ""{cluster="", discovery_config.cluster_name, "",""};","The EDS service name is set in the CDS resource, not the EDS resource -- it's basically the way that the CDS resource tells the client which EDS resource to use.  And it is certainly possible to get a CDS update that changes the EDS service name.The config of the xds_cluster_resolver policy includes a list of discovery mechanisms, and each discovery mechanism includes the [EDS service name](https://github.com/grpc/grpc-proto/blob/ab96cf12ec7ce135e03d6ea91d96213fa4cb02af/grpc/service_config/service_config.proto#L399) and [LRS server name](https://github.com/grpc/grpc-proto/blob/ab96cf12ec7ce135e03d6ea91d96213fa4cb02af/grpc/service_config/service_config.proto#L383).  When the xds_cluster_resolver policy gets a config update that changes the list of discovery mechanisms, it will [gracefully swap itself out](https://github.com/grpc/grpc/blob/8f3cd544cdd95092d514ae30bb67a037132d2e35/src/core/ext/filters/client_channel/lb_policy/xds/xds_cluster_resolver.cc#L1240) -- i.e., instead of updating the existing instance, it will create a whole new instance of the policy, and it will do a graceful swap over to that new instance.",X
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/29174,834626159,2022-03-24T18:51:10Z,src/core/ext/filters/client_channel/lb_policy/xds/xds_cluster_resolver.cc,"@@ -790,139 +768,117 @@ RefCountedPtr<LoadBalancingPolicy::Config> XdsClusterResolverLb::CreateChildPolicyConfigLocked() {   Json::Object priority_children;   Json::Array priority_priorities;-  // Setting up index to iterate through the discovery mechanisms and keeping-  // track the discovery_mechanism each priority belongs to.-  size_t discovery_index = 0;-  // Setting up num_priorities_remaining to track the priorities in each-  // discovery_mechanism.-  size_t num_priorities_remaining_in_discovery =-      discovery_mechanisms_[discovery_index].num_priorities;-  for (size_t priority = 0; priority < priority_list_.size(); ++priority) {-    Json child_policy;-    if (!discovery_mechanisms_[discovery_index]-             .discovery_mechanism->override_child_policy()-             .empty()) {-      child_policy = discovery_mechanisms_[discovery_index]-                         .discovery_mechanism->override_child_policy();-    } else {-      const auto& xds_lb_policy = config_->xds_lb_policy().object_value();-      if (xds_lb_policy.find(""ROUND_ROBIN"") != xds_lb_policy.end()) {-        const auto& localities = priority_list_[priority].localities;-        Json::Object weighted_targets;-        for (const auto& p : localities) {-          XdsLocalityName* locality_name = p.first;-          const auto& locality = p.second;-          // Construct JSON object containing locality name.-          Json::Object locality_name_json;-          if (!locality_name->region().empty()) {-            locality_name_json[""region""] = locality_name->region();-          }-          if (!locality_name->zone().empty()) {-            locality_name_json[""zone""] = locality_name->zone();-          }-          if (!locality_name->sub_zone().empty()) {-            locality_name_json[""sub_zone""] = locality_name->sub_zone();+  for (const auto& discovery_entry : discovery_mechanisms_) {+    const auto& discovery_config = discovery_entry.config();+    for (size_t priority = 0;+         priority < discovery_entry.latest_update->priorities.size();+         ++priority) {+      const auto& priority_entry =+          discovery_entry.latest_update->priorities[priority];+      Json child_policy;+      if (!discovery_entry.discovery_mechanism->override_child_policy()+               .empty()) {+        child_policy =+            discovery_entry.discovery_mechanism->override_child_policy();+      } else {+        const auto& xds_lb_policy = config_->xds_lb_policy().object_value();+        if (xds_lb_policy.find(""ROUND_ROBIN"") != xds_lb_policy.end()) {+          const auto& localities = priority_entry.localities;+          Json::Object weighted_targets;+          for (const auto& p : localities) {+            XdsLocalityName* locality_name = p.first;+            const auto& locality = p.second;+            // Construct JSON object containing locality name.+            Json::Object locality_name_json;",nit: is `locality_name_json` unused?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29174,834647213,2022-03-24T19:18:05Z,src/core/ext/filters/client_channel/lb_policy/xds/xds_cluster_resolver.cc,"@@ -790,139 +768,117 @@ RefCountedPtr<LoadBalancingPolicy::Config> XdsClusterResolverLb::CreateChildPolicyConfigLocked() {   Json::Object priority_children;   Json::Array priority_priorities;-  // Setting up index to iterate through the discovery mechanisms and keeping-  // track the discovery_mechanism each priority belongs to.-  size_t discovery_index = 0;-  // Setting up num_priorities_remaining to track the priorities in each-  // discovery_mechanism.-  size_t num_priorities_remaining_in_discovery =-      discovery_mechanisms_[discovery_index].num_priorities;-  for (size_t priority = 0; priority < priority_list_.size(); ++priority) {-    Json child_policy;-    if (!discovery_mechanisms_[discovery_index]-             .discovery_mechanism->override_child_policy()-             .empty()) {-      child_policy = discovery_mechanisms_[discovery_index]-                         .discovery_mechanism->override_child_policy();-    } else {-      const auto& xds_lb_policy = config_->xds_lb_policy().object_value();-      if (xds_lb_policy.find(""ROUND_ROBIN"") != xds_lb_policy.end()) {-        const auto& localities = priority_list_[priority].localities;-        Json::Object weighted_targets;-        for (const auto& p : localities) {-          XdsLocalityName* locality_name = p.first;-          const auto& locality = p.second;-          // Construct JSON object containing locality name.-          Json::Object locality_name_json;-          if (!locality_name->region().empty()) {-            locality_name_json[""region""] = locality_name->region();-          }-          if (!locality_name->zone().empty()) {-            locality_name_json[""zone""] = locality_name->zone();-          }-          if (!locality_name->sub_zone().empty()) {-            locality_name_json[""sub_zone""] = locality_name->sub_zone();+  for (const auto& discovery_entry : discovery_mechanisms_) {+    const auto& discovery_config = discovery_entry.config();+    for (size_t priority = 0;+         priority < discovery_entry.latest_update->priorities.size();+         ++priority) {+      const auto& priority_entry =+          discovery_entry.latest_update->priorities[priority];+      Json child_policy;+      if (!discovery_entry.discovery_mechanism->override_child_policy()+               .empty()) {+        child_policy =+            discovery_entry.discovery_mechanism->override_child_policy();+      } else {+        const auto& xds_lb_policy = config_->xds_lb_policy().object_value();+        if (xds_lb_policy.find(""ROUND_ROBIN"") != xds_lb_policy.end()) {+          const auto& localities = priority_entry.localities;+          Json::Object weighted_targets;+          for (const auto& p : localities) {+            XdsLocalityName* locality_name = p.first;+            const auto& locality = p.second;+            // Construct JSON object containing locality name.+            Json::Object locality_name_json;","Good catch.  Looks like this dead code has been here forever, but it's not needed.  Removed.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29174,834647602,2022-03-24T19:18:37Z,src/core/ext/filters/client_channel/lb_policy/xds/xds_cluster_resolver.cc,"@@ -790,139 +768,117 @@ RefCountedPtr<LoadBalancingPolicy::Config> XdsClusterResolverLb::CreateChildPolicyConfigLocked() {   Json::Object priority_children;   Json::Array priority_priorities;-  // Setting up index to iterate through the discovery mechanisms and keeping-  // track the discovery_mechanism each priority belongs to.-  size_t discovery_index = 0;-  // Setting up num_priorities_remaining to track the priorities in each-  // discovery_mechanism.-  size_t num_priorities_remaining_in_discovery =-      discovery_mechanisms_[discovery_index].num_priorities;-  for (size_t priority = 0; priority < priority_list_.size(); ++priority) {-    Json child_policy;-    if (!discovery_mechanisms_[discovery_index]-             .discovery_mechanism->override_child_policy()-             .empty()) {-      child_policy = discovery_mechanisms_[discovery_index]-                         .discovery_mechanism->override_child_policy();-    } else {-      const auto& xds_lb_policy = config_->xds_lb_policy().object_value();-      if (xds_lb_policy.find(""ROUND_ROBIN"") != xds_lb_policy.end()) {-        const auto& localities = priority_list_[priority].localities;-        Json::Object weighted_targets;-        for (const auto& p : localities) {-          XdsLocalityName* locality_name = p.first;-          const auto& locality = p.second;-          // Construct JSON object containing locality name.-          Json::Object locality_name_json;-          if (!locality_name->region().empty()) {-            locality_name_json[""region""] = locality_name->region();-          }-          if (!locality_name->zone().empty()) {-            locality_name_json[""zone""] = locality_name->zone();-          }-          if (!locality_name->sub_zone().empty()) {-            locality_name_json[""sub_zone""] = locality_name->sub_zone();+  for (const auto& discovery_entry : discovery_mechanisms_) {+    const auto& discovery_config = discovery_entry.config();+    for (size_t priority = 0;+         priority < discovery_entry.latest_update->priorities.size();+         ++priority) {+      const auto& priority_entry =+          discovery_entry.latest_update->priorities[priority];+      Json child_policy;+      if (!discovery_entry.discovery_mechanism->override_child_policy()+               .empty()) {+        child_policy =+            discovery_entry.discovery_mechanism->override_child_policy();+      } else {+        const auto& xds_lb_policy = config_->xds_lb_policy().object_value();+        if (xds_lb_policy.find(""ROUND_ROBIN"") != xds_lb_policy.end()) {+          const auto& localities = priority_entry.localities;+          Json::Object weighted_targets;+          for (const auto& p : localities) {+            XdsLocalityName* locality_name = p.first;+            const auto& locality = p.second;+            // Construct JSON object containing locality name.+            Json::Object locality_name_json;+            if (!locality_name->region().empty()) {+              locality_name_json[""region""] = locality_name->region();+            }+            if (!locality_name->zone().empty()) {+              locality_name_json[""zone""] = locality_name->zone();+            }+            if (!locality_name->sub_zone().empty()) {+              locality_name_json[""sub_zone""] = locality_name->sub_zone();+            }+            // Add weighted target entry.+            weighted_targets[locality_name->AsHumanReadableString()] =+                Json::Object{+                    {""weight"", locality.lb_weight},+                    {""childPolicy"",+                     Json::Array{+                         Json::Object{+                             {""round_robin"", Json::Object()},+                         },+                     }},+                };           }-          // Add weighted target entry.-          weighted_targets[locality_name->AsHumanReadableString()] =+          // Construct locality-picking policy.+          // Start with field from our config and add the ""targets"" field.+          child_policy = Json::Array{               Json::Object{-                  {""weight"", locality.lb_weight},-                  {""childPolicy"",-                   Json::Array{-                       Json::Object{-                           {""round_robin"", Json::Object()},-                       },+                  {""weighted_target_experimental"",+                   Json::Object{+                       {""targets"", Json::Object()},                    }},-              };+              },+          };+          Json::Object& config =+              *(*child_policy.mutable_array())[0].mutable_object();+          auto it = config.begin();+          GPR_ASSERT(it != config.end());+          (*it->second.mutable_object())[""targets""] =+              std::move(weighted_targets);+        } else {+          auto it = xds_lb_policy.find(""RING_HASH"");+          GPR_ASSERT(it != xds_lb_policy.end());",The CDS policy won't currently pass down any value other than RING_HASH or ROUND_ROBIN:https://github.com/grpc/grpc/blob/8f3cd544cdd95092d514ae30bb67a037132d2e35/src/core/ext/filters/client_channel/lb_policy/xds/cds.cc#L413,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/29203,834676666,2022-03-24T19:59:06Z,src/core/ext/filters/client_channel/lb_policy/weighted_target/weighted_target.cc,"@@ -401,6 +413,53 @@ void WeightedTargetLb::UpdateStateLocked() {                                         std::move(picker)); } +//+// WeightedTargetLb::WeightedChild::DelayedRemovalTimer+//++WeightedTargetLb::WeightedChild::DelayedRemovalTimer::DelayedRemovalTimer(+    RefCountedPtr<WeightedTargetLb::WeightedChild> weighted_child)+    : weighted_child_(std::move(weighted_child)) {+  GRPC_CLOSURE_INIT(&on_timer_, OnTimer, this, nullptr);+  Ref().release();+  grpc_timer_init(&timer_, ExecCtx::Get()->Now() + kChildRetentionInterval,+                  &on_timer_);+}++void WeightedTargetLb::WeightedChild::DelayedRemovalTimer::Orphan() {+  if (timer_pending_) {+    if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_weighted_target_trace)) {+      gpr_log(GPR_INFO,+              ""[weighted_target_lb %p] WeightedChild %p %s: cancelling ""+              ""delayed removal timer"",+              weighted_child_->weighted_target_policy_.get(),+              weighted_child_.get(), weighted_child_->name_.c_str());+    }+    timer_pending_ = false;+    grpc_timer_cancel(&timer_);+  }+}++void WeightedTargetLb::WeightedChild::DelayedRemovalTimer::OnTimer(+    void* arg, grpc_error_handle error) {+  auto* self = static_cast<DelayedRemovalTimer*>(arg);+  (void)GRPC_ERROR_REF(error);  // ref owned by lambda+  self->weighted_child_->weighted_target_policy_->work_serializer()->Run(+      [self, error]() { self->OnTimerLocked(error); }, DEBUG_LOCATION);+}++void WeightedTargetLb::WeightedChild::DelayedRemovalTimer::OnTimerLocked(+    grpc_error_handle error) {+  if (error == GRPC_ERROR_NONE && timer_pending_ &&+      !weighted_child_->shutdown_ && weighted_child_->weight_ == 0) {+    timer_pending_ = false;","nit: any reason not to set `timer_pending_ = false`, unconditionally from this method?Otherwise, I think there would be no bug, but `Orphan` would still log `cancelling delayed removal timer`, which might be confusing.Same suggestion for timers in priority lb PR too, actually: https://github.com/grpc/grpc/pull/29188/files",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29188,834716664,2022-03-24T20:51:09Z,src/core/ext/filters/client_channel/lb_policy/priority/priority.cc,"@@ -628,122 +765,25 @@ void PriorityLb::ChildPriority::OnConnectivityStateUpdateLocked(   // If READY or IDLE or TRANSIENT_FAILURE, cancel failover timer.   if (state == GRPC_CHANNEL_READY || state == GRPC_CHANNEL_IDLE ||       state == GRPC_CHANNEL_TRANSIENT_FAILURE) {-    MaybeCancelFailoverTimerLocked();+    failover_timer_.reset();   }   // Notify the parent policy.   priority_policy_->HandleChildConnectivityStateChangeLocked(this); }  void PriorityLb::ChildPriority::StartFailoverTimerLocked() {-  if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_priority_trace)) {-    gpr_log(-        GPR_INFO,-        ""[priority_lb %p] child %s (%p): starting failover timer for %"" PRId64-        ""ms"",-        priority_policy_.get(), name_.c_str(), this,-        priority_policy_->child_failover_timeout_.millis());-  }-  Ref(DEBUG_LOCATION, ""ChildPriority+OnFailoverTimerLocked"").release();-  grpc_timer_init(-      &failover_timer_,-      ExecCtx::Get()->Now() + priority_policy_->child_failover_timeout_,-      &on_failover_timer_);-  failover_timer_callback_pending_ = true;-}--void PriorityLb::ChildPriority::MaybeCancelFailoverTimerLocked() {-  if (failover_timer_callback_pending_) {-    if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_priority_trace)) {-      gpr_log(GPR_INFO,-              ""[priority_lb %p] child %s (%p): cancelling failover timer"",-              priority_policy_.get(), name_.c_str(), this);-    }-    grpc_timer_cancel(&failover_timer_);-    failover_timer_callback_pending_ = false;-  }-}--void PriorityLb::ChildPriority::OnFailoverTimer(void* arg,-                                                grpc_error_handle error) {-  ChildPriority* self = static_cast<ChildPriority*>(arg);-  (void)GRPC_ERROR_REF(error);  // ref owned by lambda-  self->priority_policy_->work_serializer()->Run(-      [self, error]() { self->OnFailoverTimerLocked(error); }, DEBUG_LOCATION);-}--void PriorityLb::ChildPriority::OnFailoverTimerLocked(grpc_error_handle error) {-  if (error == GRPC_ERROR_NONE && failover_timer_callback_pending_ &&-      !priority_policy_->shutting_down_) {-    if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_priority_trace)) {-      gpr_log(GPR_INFO,-              ""[priority_lb %p] child %s (%p): failover timer fired, ""-              ""reporting TRANSIENT_FAILURE"",-              priority_policy_.get(), name_.c_str(), this);-    }-    failover_timer_callback_pending_ = false;-    OnConnectivityStateUpdateLocked(-        GRPC_CHANNEL_TRANSIENT_FAILURE,-        absl::Status(absl::StatusCode::kUnavailable, ""failover timer fired""),-        nullptr);-  }-  Unref(DEBUG_LOCATION, ""ChildPriority+OnFailoverTimerLocked"");-  GRPC_ERROR_UNREF(error);+  failover_timer_ = MakeOrphanable<FailoverTimer>(Ref()); }  void PriorityLb::ChildPriority::DeactivateLocked() {   // If already deactivated, don't do it again.-  if (deactivation_timer_callback_pending_) return;-  if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_priority_trace)) {-    gpr_log(GPR_INFO,-            ""[priority_lb %p] child %s (%p): deactivating -- will remove in ""-            ""%"" PRId64 ""ms."",-            priority_policy_.get(), name_.c_str(), this,-            kChildRetentionInterval.millis());-  }-  MaybeCancelFailoverTimerLocked();-  // Start a timer to delete the child.-  Ref(DEBUG_LOCATION, ""ChildPriority+timer"").release();-  grpc_timer_init(&deactivation_timer_,-                  ExecCtx::Get()->Now() + kChildRetentionInterval,-                  &on_deactivation_timer_);-  deactivation_timer_callback_pending_ = true;+  if (deactivation_timer_ != nullptr) return;+  failover_timer_.reset();+  deactivation_timer_ = MakeOrphanable<DeactivationTimer>(Ref()); }  void PriorityLb::ChildPriority::MaybeReactivateLocked() {-  if (deactivation_timer_callback_pending_) {-    if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_priority_trace)) {-      gpr_log(GPR_INFO, ""[priority_lb %p] child %s (%p): reactivating"",-              priority_policy_.get(), name_.c_str(), this);-    }-    deactivation_timer_callback_pending_ = false;-    grpc_timer_cancel(&deactivation_timer_);-  }-}--void PriorityLb::ChildPriority::OnDeactivationTimer(void* arg,-                                                    grpc_error_handle error) {-  ChildPriority* self = static_cast<ChildPriority*>(arg);-  (void)GRPC_ERROR_REF(error);  // ref owned by lambda-  self->priority_policy_->work_serializer()->Run(-      [self, error]() { self->OnDeactivationTimerLocked(error); },-      DEBUG_LOCATION);-}--void PriorityLb::ChildPriority::OnDeactivationTimerLocked(-    grpc_error_handle error) {-  if (error == GRPC_ERROR_NONE && deactivation_timer_callback_pending_ &&-      !priority_policy_->shutting_down_) {-    if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_priority_trace)) {-      gpr_log(GPR_INFO,-              ""[priority_lb %p] child %s (%p): deactivation timer fired, ""-              ""deleting child"",-              priority_policy_.get(), name_.c_str(), this);-    }-    deactivation_timer_callback_pending_ = false;-    priority_policy_->DeleteChild(this);-  }-  Unref(DEBUG_LOCATION, ""ChildPriority+timer"");-  GRPC_ERROR_UNREF(error);+  deactivation_timer_.reset();","Can't do that with this one, because it gets called from outside of this object, so the caller doesn't have direct access to the `deactivation_timer_` data member.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29188,834736117,2022-03-24T21:13:47Z,src/core/ext/filters/client_channel/lb_policy/priority/priority.cc,"@@ -501,6 +518,136 @@ void PriorityLb::SelectPriorityLocked(uint32_t priority) {                                         child->GetPicker()); } +//+// PriorityLb::ChildPriority::DeactivationTimer+//++PriorityLb::ChildPriority::DeactivationTimer::DeactivationTimer(+    RefCountedPtr<PriorityLb::ChildPriority> child_priority)+    : child_priority_(std::move(child_priority)) {+  if (GRPC_TRACE_FLAG_ENABLED(grpc_lb_priority_trace)) {+    gpr_log(GPR_INFO,+            ""[priority_lb %p] child %s (%p): deactivating -- will remove in ""+            ""%"" PRId64 ""ms"",+            child_priority_->priority_policy_.get(),+            child_priority_->name_.c_str(), child_priority_.get(),+            kChildRetentionInterval.millis());+  }+  GRPC_CLOSURE_INIT(&on_timer_, OnTimer, this, nullptr);+  Ref(DEBUG_LOCATION, ""Timer"").release();","I think we have consensus that we want to get to a future where we do time caching on the data plane but not anywhere on the control plane.  However, we need to find a reasonable API for doing that; it is not sustainable to expect all timer callers to explicitly call `InvalidateNow()`, and I would be opposed to that as a general approach to addressing this problem.  I think we are likely to find a better approach as part of the promise conversion work that Craig is doing.In any case, this discussion is far outside of the scope of this PR.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29194,834765008,2022-03-24T21:57:11Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -2261,30 +2270,27 @@ grpc_error_handle ClientChannel::CallData::ApplyServiceConfigToCallLocked( void ClientChannel::CallData::     RecvTrailingMetadataReadyForConfigSelectorCommitCallback(         void* arg, grpc_error_handle error) {-  auto* self = static_cast<CallData*>(arg);+  auto* elem = static_cast<grpc_call_element*>(arg);+  auto* chand = static_cast<ClientChannel*>(elem->channel_data);+  auto* calld = static_cast<CallData*>(elem->call_data);   auto* service_config_call_data =       static_cast<ClientChannelServiceConfigCallData*>(-          self->call_context_[GRPC_CONTEXT_SERVICE_CONFIG_CALL_DATA].value);+          calld->call_context_[GRPC_CONTEXT_SERVICE_CONFIG_CALL_DATA].value);+  if (GRPC_TRACE_FLAG_ENABLED(grpc_client_channel_call_trace)) {+    gpr_log(GPR_INFO,+            ""chand=%p calld=%p: got recv_trailing_metadata_ready: error=%s ""+            ""service_config_call_data=%p"",+            chand, calld, grpc_error_std_string(error).c_str(),+            service_config_call_data);+  }   if (service_config_call_data != nullptr) {     service_config_call_data->call_dispatch_controller()->Commit();   }   // Chain to original callback.-  Closure::Run(DEBUG_LOCATION, self->original_recv_trailing_metadata_ready_,+  Closure::Run(DEBUG_LOCATION, calld->original_recv_trailing_metadata_ready_,                GRPC_ERROR_REF(error)); } -void ClientChannel::CallData::","It doesn't strictly need to be here, but it makes sense to be.  If I'd left this method in place, I would have had to add the `grpc_call_element*` parameter to it, since I needed that to be the parameter for the intercepted callback, so that the callback can log `chand`.  Since I already had all of the state in the caller, I just moved the code there to avoid some boilerplate.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29206,834828899,2022-03-24T23:31:27Z,src/core/lib/iomgr/tcp_client_posix.cc,"@@ -50,18 +51,174 @@  extern grpc_core::TraceFlag grpc_tcp_trace; -struct async_connect {-  gpr_mu mu;-  grpc_fd* fd;-  grpc_timer alarm;-  grpc_closure on_alarm;-  int refs;-  grpc_closure write_closure;-  grpc_pollset_set* interested_parties;-  std::string addr_str;-  grpc_endpoint** ep;-  grpc_closure* closure;-  grpc_channel_args* channel_args;+class AsyncConnect {+ public:+  AsyncConnect(grpc_fd* fd, grpc_pollset_set* interested_parties,+               const grpc_resolved_address* addr, grpc_endpoint** ep,+               grpc_closure* closure, const grpc_channel_args* channel_args,+               const grpc_core::Timestamp& deadline)+      : fd_(fd),+        refs_(2),+        interested_parties_(interested_parties),+        addr_str_(grpc_sockaddr_to_uri(addr)),+        ep_(ep),+        closure_(closure),+        channel_args_(grpc_channel_args_copy(channel_args)) {+    GRPC_CLOSURE_INIT(&write_closure_, OnWritable, this,+                      grpc_schedule_on_exec_ctx);+    GRPC_CLOSURE_INIT(&on_alarm_, OnAlarm, this, grpc_schedule_on_exec_ctx);++    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_INFO, ""CLIENT_CONNECT: %s: asynchronously connecting fd %p"",+              addr_str_.c_str(), fd_);+    }++    mu_.Lock();+    grpc_timer_init(&alarm_, deadline, &on_alarm_);+    grpc_fd_notify_on_write(fd_, &write_closure_);+    mu_.Unlock();+  }++  ~AsyncConnect() { grpc_channel_args_destroy(channel_args_); }++ private:+  static void OnAlarm(void* acp, grpc_error_handle error) {+    int done;+    AsyncConnect* ac = static_cast<AsyncConnect*>(acp);+    ac->mu_.Lock();+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_INFO, ""CLIENT_CONNECT: %s: on_alarm: error=%s"",+              ac->addr_str_.c_str(), grpc_error_std_string(error).c_str());+    }+    if (ac->fd_ != nullptr) {+      grpc_fd_shutdown(+          ac->fd_, GRPC_ERROR_CREATE_FROM_STATIC_STRING(""connect() timed out""));+    }+    done = (--ac->refs_ == 0);+    ac->mu_.Unlock();+    if (done) {+      delete ac;+    }+  }++  static void OnWritable(void* acp, grpc_error_handle error) {+    AsyncConnect* ac = static_cast<AsyncConnect*>(acp);+    int so_error = 0;+    socklen_t so_error_size;+    int err;+    int done;+    grpc_endpoint** ep = ac->ep_;+    grpc_closure* closure = ac->closure_;+    grpc_fd* fd;+    std::string addr_str = ac->addr_str_;++    (void)GRPC_ERROR_REF(error);++    ac->mu_.Lock();+    if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {+      gpr_log(GPR_INFO, ""CLIENT_CONNECT: %s: on_writable: error=%s"",+              ac->addr_str_.c_str(), grpc_error_std_string(error).c_str());+    }+    GPR_ASSERT(ac->fd_);+    fd = ac->fd_;+    ac->fd_ = nullptr;+    ac->mu_.Unlock();++    grpc_timer_cancel(&ac->alarm_);++    ac->mu_.Lock();+    if (error != GRPC_ERROR_NONE) {+      error = grpc_error_set_str(error, GRPC_ERROR_STR_OS_ERROR,+                                 ""Timeout occurred"");+      goto finish;+    }++    do {+      so_error_size = sizeof(so_error);+      err = getsockopt(grpc_fd_wrapped_fd(fd), SOL_SOCKET, SO_ERROR, &so_error,+                       &so_error_size);+    } while (err < 0 && errno == EINTR);+    if (err < 0) {+      error = GRPC_OS_ERROR(errno, ""getsockopt"");+      goto finish;+    }++    switch (so_error) {+      case 0:+        grpc_pollset_set_del_fd(ac->interested_parties_, fd);+        *ep = grpc_tcp_client_create_from_fd(fd, ac->channel_args_,+                                             ac->addr_str_);+        fd = nullptr;+        break;+      case ENOBUFS:+        /* We will get one of these errors if we have run out of+           memory in the kernel for the data structures allocated+           when you connect a socket.  If this happens it is very+           likely that if we wait a little bit then try again the+           connection will work (since other programs or this+           program will close their network connections and free up+           memory).  This does _not_ indicate that there is anything+           wrong with the server we are connecting to, this is a+           local problem.++           If you are looking at this code, then chances are that+           your program or another program on the same computer+           opened too many network connections.  The ""easy"" fix:+           don't do that! */+        gpr_log(GPR_ERROR, ""kernel out of buffers"");+        ac->mu_.Unlock();+        grpc_fd_notify_on_write(fd, &ac->write_closure_);+        return;+      case ECONNREFUSED:+        /* This error shouldn't happen for anything other than connect(). */+        error = GRPC_OS_ERROR(so_error, ""connect"");+        break;+      default:+        /* We don't really know which syscall triggered the problem here,+           so punt by reporting getsockopt(). */+        error = GRPC_OS_ERROR(so_error, ""getsockopt(SO_ERROR)"");+        break;+    }++  finish:+    if (fd != nullptr) {+      grpc_pollset_set_del_fd(ac->interested_parties_, fd);+      grpc_fd_orphan(fd, nullptr, nullptr, ""tcp_client_orphan"");+      fd = nullptr;+    }+    done = (--ac->refs_ == 0);+    ac->mu_.Unlock();+    if (error != GRPC_ERROR_NONE) {+      std::string str;+      bool ret = grpc_error_get_str(error, GRPC_ERROR_STR_DESCRIPTION, &str);+      GPR_ASSERT(ret);+      std::string description =+          absl::StrCat(""Failed to connect to remote host: "", str);+      error =+          grpc_error_set_str(error, GRPC_ERROR_STR_DESCRIPTION, description);+      error =+          grpc_error_set_str(error, GRPC_ERROR_STR_TARGET_ADDRESS, addr_str);+    }+    if (done) {+      delete ac;+    }+    // Push async connect closure to the executor since this may actually be+    // called during the shutdown process, in which case a deadlock could form+    // between the core shutdown mu and the connector mu (b/188239051)+    grpc_core::Executor::Run(closure, error);+  }++  grpc_core::Mutex mu_;+  grpc_fd* fd_ ABSL_GUARDED_BY(&mu_);+  grpc_timer alarm_ ABSL_GUARDED_BY(&mu_);+  grpc_closure on_alarm_ ABSL_GUARDED_BY(&mu_);+  int refs_ ABSL_GUARDED_BY(&mu_);+  grpc_closure write_closure_ ABSL_GUARDED_BY(&mu_);+  grpc_pollset_set* interested_parties_ ABSL_GUARDED_BY(&mu_);+  std::string const addr_str_;","stylistically, the rest of the codebase puts `const` before the type. See ""Where to put the const"" in https://google.github.io/styleguide/cppguide.html#Use_of_const. Same for the types below.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29206,834836299,2022-03-24T23:39:45Z,src/core/lib/iomgr/tcp_client_posix.cc,"@@ -297,28 +324,8 @@ void grpc_tcp_client_create_from_prepared_fd(    grpc_pollset_set_add_fd(interested_parties, fdobj); -  async_connect* ac = new async_connect();-  ac->closure = closure;-  ac->ep = ep;-  ac->fd = fdobj;-  ac->interested_parties = interested_parties;-  ac->addr_str = grpc_sockaddr_to_uri(addr);-  gpr_mu_init(&ac->mu);-  ac->refs = 2;-  GRPC_CLOSURE_INIT(&ac->write_closure, on_writable, ac,-                    grpc_schedule_on_exec_ctx);-  ac->channel_args = grpc_channel_args_copy(channel_args);--  if (GRPC_TRACE_FLAG_ENABLED(grpc_tcp_trace)) {-    gpr_log(GPR_INFO, ""CLIENT_CONNECT: %s: asynchronously connecting fd %p"",-            ac->addr_str.c_str(), fdobj);-  }--  gpr_mu_lock(&ac->mu);-  GRPC_CLOSURE_INIT(&ac->on_alarm, tc_on_alarm, ac, grpc_schedule_on_exec_ctx);-  grpc_timer_init(&ac->alarm, deadline, &ac->on_alarm);-  grpc_fd_notify_on_write(ac->fd, &ac->write_closure);-  gpr_mu_unlock(&ac->mu);+  new AsyncConnect(fdobj, interested_parties, addr, ep, closure, channel_args,","This is an odd API from the caller's perspective, it works entirely by side effect. It would be nice if the usage conveyed that the object needs no owner and is self-deleting. A RefCounted type is probably the answer here given that it has a `refs_` member. And consider maybe changing the class name or adding a factory method to convey it's self-deleting nature.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29175,835318189,2022-03-25T14:22:17Z,test/distrib/bazel/cpp/greeter_client.cc,"@@ -0,0 +1,104 @@+/*","I still think there's value in de-duplicating copies of greeter_*.cc, even though this is the ""simplest possible"" usage, it's still 100 LOC of not entirely trivial code.But we can leave as is for now (I'd appreciate ideas for how to deduplicate in the future though, having a symlink doesn't seem that bad actually, but it would have to be for all files - client, server and the .proto file)",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29165,835408367,2022-03-25T16:01:37Z,src/core/lib/channel/channel_args.h,"@@ -131,6 +132,12 @@ class ChannelArgs {   const Value* Get(absl::string_view name) const { return args_.Lookup(name); }   GRPC_MUST_USE_RESULT ChannelArgs Set(absl::string_view name,                                        Value value) const;+  GRPC_MUST_USE_RESULT ChannelArgs Set(absl::string_view name,","There's an implicit conversion from `std::string` -> `Value` (being an `absl::variant`), and the noted `absl::string_view` implicit conversions, and so there's an overload ambiguity that needs to be resolved here.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29202,836549562,2022-03-28T15:17:48Z,CMakeLists.txt,"@@ -12200,44 +12199,6 @@ target_link_libraries(interop_server )  -endif()-if(gRPC_BUILD_TESTS)-if(_gRPC_PLATFORM_LINUX OR _gRPC_PLATFORM_MAC OR _gRPC_PLATFORM_POSIX)--  add_executable(interop_test-    test/cpp/interop/interop_test.cc-    third_party/googletest/googletest/src/gtest-all.cc-    third_party/googletest/googlemock/src/gmock-all.cc-  )--  target_include_directories(interop_test",any reason why interop_test be no longer buildable with cmake? Or is that an oversight?,
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29202,836620347,2022-03-28T16:26:01Z,bazel/grpc_build_system.bzl,"@@ -389,14 +450,55 @@ def grpc_generate_one_off_targets(): def grpc_generate_objc_one_off_targets():     pass -def grpc_sh_test(name, srcs, args = [], data = [], tags = []):-    native.sh_test(-        name = name,-        srcs = srcs,-        args = args,-        data = data,-        tags = tags,-    )+def grpc_sh_test(name, srcs = [], args = [], data = [], uses_polling = True, size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None, exclude_pollers = [], test_each_engine = True):","You missed its usage in `test/core/end2end/generate_tests.bzl`, PTAL. The sh_tests need to be parameterized for the product of pollers and engines, same as cc_test.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29176,836678083,2022-03-28T17:32:12Z,src/core/tsi/ssl_transport_security.h,"@@ -217,6 +217,15 @@ tsi_result tsi_ssl_client_handshaker_factory_create_handshaker(     tsi_ssl_client_handshaker_factory* factory,     const char* server_name_indication, tsi_handshaker** handshaker); +/* Creates a client handshaker with a custom BIO pair.+ * The default size of each BIO buffer is 0, which translates to 17KB in+ * boringSSL. */+tsi_result+tsi_ssl_client_handshaker_factory_create_handshaker_with_custom_bio_pair(","Instead of creating a duplicate API here, let's just add the new parameters to the existing API and update the callers.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29176,836678281,2022-03-28T17:32:25Z,src/core/tsi/ssl_transport_security.h,"@@ -354,6 +363,14 @@ tsi_result tsi_create_ssl_server_handshaker_factory_with_options( tsi_result tsi_ssl_server_handshaker_factory_create_handshaker(     tsi_ssl_server_handshaker_factory* factory, tsi_handshaker** handshaker); +/* Creates a server handshaker with a custom BIO pair.+ * The default size of each BIO buffer is 0, which translates to 17KB in+ * boringSSL. */+tsi_result+tsi_ssl_server_handshaker_factory_create_handshaker_with_custom_bio_pair(",Same here: Let's just add the new parameters to the existing API.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29202,836829481,2022-03-28T20:38:43Z,test/core/bad_ssl/generate_tests.bzl,"@@ -42,7 +42,10 @@ def grpc_bad_ssl_tests():         grpc_cc_binary(             name = ""bad_ssl_%s_server"" % t,             srcs = [""servers/%s.cc"" % t],-            deps = ["":bad_ssl_test_server""],+            deps = [+                "":bad_ssl_test_server"",+                ""//test/core/util:grpc_test_util"",",Why is this dependency needed?,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29202,836842731,2022-03-28T20:55:21Z,bazel/grpc_build_system.bzl,"@@ -282,17 +352,18 @@ def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data         flaky: Whether this test is flaky.         copts: Add these to the compiler invocation.         linkstatic: link the binary in static mode+        exclude_pollers: list of poller names to exclude for this set of tests.+        test_each_engine: set to False if the test is not sensitive to","How about calling this parameter `uses_event_engine`?  That seems like a much more intuitive name from the perspective of someone reading a test definition.Also, I suspect that there are a lot of tests that are not setting this to False that should be.  Just from a quick scan, I suspect that almost everything under test/core *aside* from the end2end tests should be doing this.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29202,836870205,2022-03-28T21:35:35Z,bazel/grpc_build_system.bzl,"@@ -282,17 +352,18 @@ def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data         flaky: Whether this test is flaky.         copts: Add these to the compiler invocation.         linkstatic: link the binary in static mode+        exclude_pollers: list of poller names to exclude for this set of tests.+        test_each_engine: set to False if the test is not sensitive to","Done. I used that name in a previous PR, but decided that it was misleading: most tests will certainly _use_ an EventEngine, but many of those tests may not care if they're exercised against every EventEngine implementation. It's not a big problem, though.> Also, I suspect that there are a lot of tests that are not setting this to False that should be.Agreed. I caught a handful already in the promise and avl code. We can catch more as we go along. It might make sense to have this set to False by default, but that would conflict with how the poller expansion works today.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29202,836887848,2022-03-28T22:04:45Z,bazel/grpc_build_system.bzl,"@@ -282,17 +352,18 @@ def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data         flaky: Whether this test is flaky.         copts: Add these to the compiler invocation.         linkstatic: link the binary in static mode+        exclude_pollers: list of poller names to exclude for this set of tests.+        test_each_engine: set to False if the test is not sensitive to","It's probably a sufficient heuristic at the moment to add `uses_event_engine=False` wherever `uses_polling=False` is set. There will be some exceptions to that, such as the EventEngine conformance test suite, but that may be a good first pass. WDYT?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29176,836908634,2022-03-28T22:38:25Z,src/core/tsi/ssl_transport_security.h,"@@ -209,19 +209,13 @@ tsi_result tsi_create_ssl_client_handshaker_factory_with_options(   - server_name_indication indicates the name of the server the client is     trying to connect to which will be relayed to the server using the SNI     extension.+  - network_bio_buf_size and ssl_bio_buf_size represent BIO pair buffers used in","Wait... are we not actually passing in a non-zero value here from any of the callers?  If not, why do we need to add these parameters in the first place?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29176,836913198,2022-03-28T22:46:00Z,src/core/tsi/ssl_transport_security.h,"@@ -209,19 +209,13 @@ tsi_result tsi_create_ssl_client_handshaker_factory_with_options(   - server_name_indication indicates the name of the server the client is     trying to connect to which will be relayed to the server using the SNI     extension.+  - network_bio_buf_size and ssl_bio_buf_size represent BIO pair buffers used in","Ah, okay.In the future, when we convert this code to a C++ API, we should consider making an args struct, so that callers need to specify only the parameters that are getting non-default values.  But this is fine for now.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29202,836924870,2022-03-28T23:11:15Z,bazel/grpc_build_system.bzl,"@@ -282,17 +352,18 @@ def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data         flaky: Whether this test is flaky.         copts: Add these to the compiler invocation.         linkstatic: link the binary in static mode+        exclude_pollers: list of poller names to exclude for this set of tests.+        test_each_engine: set to False if the test is not sensitive to","I did a first pass on the ~150 tests where `uses_polling=False`. Most are isolated tests that also do not need to exercise all EventEngines, with only a few exceptions. I also ensured that the `message_compress` tests are not parameterized on `poller x engines`, I don't think they need to be.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29202,837306098,2022-03-29T10:16:34Z,bazel/grpc_build_system.bzl,"@@ -259,7 +262,74 @@ def ios_cc_test(             deps = ios_test_deps,         ) -def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None, copts = [], linkstatic = None):+def expand_tests_for_each_poller_and_engine(name, srcs, deps, tags, args, exclude_pollers, uses_event_engine):+    """"""Common logic used to parameterize tests for every poller and EventEngine.++    Args:+        name: base name of the test+        srcs: source files+        deps: base deps+        tags: base tags+        args: base args+        exclude_pollers: list of poller names to exclude for this set of tests.+        uses_event_engine: set to False if the test is not sensitive to+            EventEngine implementation differences++    Returns:+        A list of dictionaries containing modified values of name, srcs, deps, tags, and args.+    """"""+    poller_config = []++    # On linux we run the same test with the default EventEngine, once for each+    # poller+    for poller in POLLERS:+        if poller in exclude_pollers:+            continue+        poller_config.append({+            ""name"": name + ""@poller="" + poller,+            ""srcs"": srcs,+            ""deps"": deps,+            ""tags"": (tags + EVENT_ENGINES[""default""][""tags""] + [+                ""no_windows"",+                ""no_mac"",+                ""bazel_only"",+            ]),+            ""args"": args + [""--poller="" + poller],+        })++    # Now generate one test for each subsequent EventEngine, all using the+    # default poller.+    if not uses_event_engine:+        # The poller tests exercise the default engine on Linux. This test+        # handles other platforms.+        poller_config.append({+            ""name"": name,+            ""srcs"": srcs,+            ""deps"": deps,+            ""tags"": tags + [""no_linux""],+            ""args"": args,+        })+    else:+        for engine_name, engine in EVENT_ENGINES.items():","I looked at the list of bazel tests that ran on this PR and I never saw the ""@engine="" in the name of a test, which seems suspicious?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29202,837319849,2022-03-29T10:33:13Z,bazel/grpc_build_system.bzl,"@@ -259,7 +262,74 @@ def ios_cc_test(             deps = ios_test_deps,         ) -def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None, copts = [], linkstatic = None):+def expand_tests_for_each_poller_and_engine(name, srcs, deps, tags, args, exclude_pollers, uses_event_engine):","suggestion: to make sure this logic here is correct and give the expected behavior, it might be useful to use `bazel query` to generate a list of tests we'd run on linux and  on non-linux  (use the tags to filter them) and then post the diff before and after.That would make it clear which tests were added (if any) and which tests were removed (which could be intentional or by mistake). This would also give us a good idea whats the diff on number of tests targets.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29202,837611550,2022-03-29T15:24:16Z,test/core/address_utils/parse_address_test.cc,"@@ -129,7 +129,7 @@ static void test_grpc_parse_ipv6_invalid(const char* uri_text) { }  int main(int argc, char** argv) {-  grpc::testing::TestEnvironment env(argc, argv);+  grpc::testing::TestEnvironment env(&argc, argv);","> It was a small number of tests, single digits I think. `test/core/fling/server.cc` is another example that would need some cleanup.If there are only a small number of tests affected, I think it would be cleaner to fix those tests instead of making this broader change.> This `argc` mutation approach seemed simple, I'm not sure why it's controversial. I don't buy the argument that broad changes should be inherently controversial, I see them in this codebase all the time. This one looks harmless.I don't think broad changes are inherently controversial, but I do think that the more invasive a change is, the higher the bar needed to justify the change.  In this case, the change seems to make the API a bit less intuitive, so it's not clear to me that it's the best approach.> If we did a broader cleanup to remove unknown argument sensitivities, we might be able to use absl::Flags instead, which would be another improvement, and another fairly trivial change to this TestEnvironment constructor definition (probably no need for argc and argv). I'd be happy to put that cleanup on a ""things to do while compiling"" queue. But I don't think that cleanup should block this work.I think changing all of our code to use absl::Flags is a great idea!  It would make things more unofirm across all of our tests, and it would eliminte the dependency on the legacy gflags library.  But I agree that that should be a separate change.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29202,837715118,2022-03-29T17:10:45Z,bazel/grpc_build_system.bzl,"@@ -259,7 +262,74 @@ def ios_cc_test(             deps = ios_test_deps,         ) -def grpc_cc_test(name, srcs = [], deps = [], external_deps = [], args = [], data = [], uses_polling = True, language = ""C++"", size = ""medium"", timeout = None, tags = [], exec_compatible_with = [], exec_properties = {}, shard_count = None, flaky = None, copts = [], linkstatic = None):+def expand_tests_for_each_poller_and_engine(name, srcs, deps, tags, args, exclude_pollers, uses_event_engine):+    """"""Common logic used to parameterize tests for every poller and EventEngine.++    Args:+        name: base name of the test+        srcs: source files+        deps: base deps+        tags: base tags+        args: base args+        exclude_pollers: list of poller names to exclude for this set of tests.+        uses_event_engine: set to False if the test is not sensitive to+            EventEngine implementation differences++    Returns:+        A list of dictionaries containing modified values of name, srcs, deps, tags, and args.+    """"""+    poller_config = []++    # On linux we run the same test with the default EventEngine, once for each+    # poller+    for poller in POLLERS:+        if poller in exclude_pollers:+            continue+        poller_config.append({+            ""name"": name + ""@poller="" + poller,+            ""srcs"": srcs,+            ""deps"": deps,+            ""tags"": (tags + EVENT_ENGINES[""default""][""tags""] + [+                ""no_windows"",+                ""no_mac"",+                ""bazel_only"",+            ]),+            ""args"": args + [""--poller="" + poller],+        })++    # Now generate one test for each subsequent EventEngine, all using the+    # default poller.+    if not uses_event_engine:+        # The poller tests exercise the default engine on Linux. This test+        # handles other platforms.+        poller_config.append({+            ""name"": name,+            ""srcs"": srcs,+            ""deps"": deps,+            ""tags"": tags + [""no_linux""],+            ""args"": args,+        })+    else:+        for engine_name, engine in EVENT_ENGINES.items():","This works as designed, `default` is not added to the target name for backwards compatibility. If you add another engine to the list, you'll see something like:```➜  grpc git:(ee_e2e_one_more_time) ✗ bazel query test/core/end2end:all | grep -E 'h2_full_test@cancel_in_a_vacuum'...//test/core/end2end:h2_full_test@cancel_in_a_vacuum@poller=poll//test/core/end2end:h2_full_test@cancel_in_a_vacuum@poller=epoll1//test/core/end2end:h2_full_test@cancel_in_a_vacuum@engine=jtatt-engine//test/core/end2end:h2_full_test@cancel_in_a_vacuum```",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29202,837835132,2022-03-29T19:26:34Z,CMakeLists.txt,"@@ -12200,44 +12199,6 @@ target_link_libraries(interop_server )  -endif()-if(gRPC_BUILD_TESTS)-if(_gRPC_PLATFORM_LINUX OR _gRPC_PLATFORM_MAC OR _gRPC_PLATFORM_POSIX)--  add_executable(interop_test-    test/cpp/interop/interop_test.cc-    third_party/googletest/googletest/src/gtest-all.cc-    third_party/googletest/googlemock/src/gmock-all.cc-  )--  target_include_directories(interop_test","> We can't just make changes that have side effects without understanding what is causing thoseYou're right to point it out, absolutely, and having a fruitful discussion is important. > to me it's a red flag that the PR does something we don't quite understand.Not the case, the mechanics are clear. This target was a unique case beforehand, that's not new. It's plausible that this target was added into cmake accidentally in the first place, though I'm not sure our CI would be sensitive to that (our master builds would, at least). `interop_client` and `interop_server` still exist in the cmake build, which is important because I _did_ find uses of them via cmake in our master builds. If this target is relied upon via cmake, it's not obvious where or how. Putting in extra effort to keep what _may be_ a mistake is not the right move in my opinion. If some tests break due to non-obvious usages, that's a problem that this PR would help identify. All in all, a clear improvement is presented here, a best effort was made, our builds and various test environments are complex, and in any case rollbacks happen frequently on this repo in the normal course of operation due to issues that can be difficult to predict. I'd be happy to be shown it's needed, and put in the work required to document that somewhere obvious, and handle this edge case in the cmake export.",X
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/29057,837939500,2022-03-29T21:50:30Z,tools/run_tests/performance/prometheus.py,"@@ -0,0 +1,301 @@+#!/usr/bin/env python3++# Copyright 2022 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# A script to fetch total cpu seconds and memory data from prometheus.+# example usage: python3 prometheus.py+# --url=http://prometheus.prometheus.svc.cluster.local:9090+# --pod_type=driver --pod_type=clients --container_name=main+# --container_name=sidecar+""""""Perform prometheus range queries to obtain cpu and memory data.++This module performs range queries through Prometheus API to obtain+total cpu seconds and memory during a tets run for given container+in given pods. The cpu data obtained is total cpu second used within+given period of time. The memory data was the instant memory at the+query time.+""""""+import argparse+import json+import logging+import statistics+from typing import Any, Dict, List++from dateutil import parser+import requests+++class Prometheus:+    """"""Objects which holds the start and end time, query URL+    for a query.""""""++    def __init__(+        self,+        url: str,+        start: str,+        end: str,+    ):+        self.url = url+        self.start = start+        self.end = end++    def _fetch_by_query(self, query: str) -> Dict[str, Any]:+        """"""Fetches the given query with time range.+        +        Fetch the given query within a time range. The pulling +        interval is every 5s, the actual data from the query is+        a time serier.+        """"""+        resp = requests.get(+            self.url + '/api/v1/query_range',+            {+                'query': query,+                'start': self.start,+                'end': self.end,+                'step': 5+            },+        )+        resp.raise_for_status()+        return resp.json()++    def _fetch_cpu_for_pod(self, container_matcher: str,+                           pod_name: str) -> Dict[str, List[float]]:+        """"""Fetches the cpu data for each pod.++        Fetch total cpu seconds during the time range specified in the Prometheus instance+        for a pod. After obtain the cpu seconds, the data are trimmed from time serier to+        a data list and saved in the Dict that keyed by the container names.++        Args:+            container_matcher:  A string consist one or more container name separated by |.+        """"""+        query = (+            'container_cpu_usage_seconds_total{job=""kubernetes-cadvisor"",pod=""'+            + pod_name + '"",container=' + container_matcher + '}')+        logging.debug('running prometheus query for cpu: %s', query)+        cpu_data = self._fetch_by_query(query)+        logging.debug('raw cpu data: %s', str(cpu_data))+        cpu_container_name_to_data_list = get_data_list_from_timeseries(+            cpu_data)+        return cpu_container_name_to_data_list++    def _fetch_memory_for_pod(self, container_matcher: str,+                              pod_name: str) -> Dict[str, List[float]]:+        """"""Fetches memory data for each pod.++        Fetch total memory data during the time range specified in the Prometheus instance+        for a pod. After obtain the memory data, the data are trimmed from time serier to+        a data list and saved in the Dict that keyed by the container names.++        Args:+            container_matcher:  A string consist one or more container name separated by |.+        """"""+        query = (+            'container_memory_usage_bytes{job=""kubernetes-cadvisor"",pod=""' ++            pod_name + '"",container=' + container_matcher + ""}"")++        logging.debug('running prometheus query for memory: %s', query)+        memory_data = self._fetch_by_query(query)++        logging.debug('raw memory data: %s', str(memory_data))+        memory_container_name_to_data_list = get_data_list_from_timeseries(+            memory_data)++        return memory_container_name_to_data_list++    def fetch_cpu_and_memory_data(+            self, container_list: List[str],+            pod_dict: Dict[str, List[str]]) -> Dict[str, Any]:+        """"""Fetch total cpu seconds and memory data for a multiple pods.++        Args:+            container_list: A list of container names to fetch the data for.+            pod_dict: the pods to fetch data for, the pod_dict is keyed by +                      role of the pod: clients, driver and servers. The values+                      for the pod_dict are the list of pod names that consist +                      the same role specified in the key.+        """"""+        container_matcher = construct_container_matcher(container_list)+        processed_data = {}+        for role, pod_names in pod_dict.items():+            pod_data = {}+            for pod in pod_names:+                container_data = {}+                for container, data in self._fetch_cpu_for_pod(+                        container_matcher, pod).items():+                    container_data[container] = {}+                    container_data[container][+                        'cpuSeconds'] = compute_total_cpu_seconds(data)++                for container, data in self._fetch_memory_for_pod(+                        container_matcher, pod).items():+                    container_data[container][+                        'memoryMean'] = compute_average_memory_usage(data)++                pod_data[pod] = container_data+            processed_data[role] = pod_data+        return processed_data+++def construct_container_matcher(container_list: List[str]) -> str:+    """"""Constructs the container matching string used in the+    prometheus query.""""""+    if len(container_list) == 0:+        raise Exception('no container name provided')++    containers_to_fetch = '""'+    if len(container_list) == 1:+        containers_to_fetch = container_list[0]+    else:+        containers_to_fetch = '~""' + container_list[0]+        for container in container_list[1:]:+            containers_to_fetch = containers_to_fetch + '|' + container+        containers_to_fetch = containers_to_fetch + '""'+    return containers_to_fetch+++def get_data_list_from_timeseries(data: Any) -> Dict[str, List[float]]:+    """"""Constructs a Dict as keys are the container names and+    values are a list of data taken from given timeserie data.""""""+    if data['status'] != 'success':+        raise Exception('command failed: ' + data['status'] + str(data))+    if data['data']['resultType'] != 'matrix':+        raise Exception('resultType is not matrix: ' ++                        data['data']['resultType'])++    container_name_to_data_list = {}+    for res in data[""data""][""result""]:+        container_name = res[""metric""][""container""]+        container_data_timeserie = res[""values""]++        container_data = []+        for d in container_data_timeserie:+            container_data.append(float(d[1]))+        container_name_to_data_list[container_name] = container_data+    return container_name_to_data_list+++def compute_total_cpu_seconds(cpu_data_list: List[float]) -> float:+    """"""Computes the total cpu seconds by CPUs[end]-CPUs[start].""""""+    return cpu_data_list[len(cpu_data_list) - 1] - cpu_data_list[0]+++def compute_average_memory_usage(memory_data_list: List[float]) -> float:+    """"""Computes the mean and for given list of data.""""""","""a"" given list of data.",
98778259,rburns1293,https://api.github.com/repos/grpc/grpc/pulls/23003,838077279,2022-03-30T03:07:13Z,src/csharp/Grpc.Core/Internal/AsyncCall.cs,"@@ -78,7 +78,12 @@ public TResponse UnaryCall(TRequest msg)             var profiler = Profilers.ForCurrentThread();              using (profiler.NewScope(""AsyncCall.UnaryCall""))-            using (CompletionQueueSafeHandle cq = CompletionQueueSafeHandle.CreateSync())++            // Create a pluckable completion queue for the call. Avoid creating a completion queue when we know the channel has already+            // been shutdown. In such case, the call will fail with ObjectDisposedException immediately anyway and creating / destroying+            // a completion queue would lead to crash if this was the last channel in the application (and thus GrpcEnvironment has been shutdown).+            // See https://github.com/grpc/grpc/issues/19090+            using (CompletionQueueSafeHandle cq = details.Channel.Handle.IsClosed ?  null : CompletionQueueSafeHandle.CreateSync())","The race condition was an issue even without this change. The old line: ```using (CompletionQueueSafeHandle cq = CompletionQueueSafeHandle.CreateSync())``` could still complete and then have the channel deleted by another thread before the destructor is called. Since this also doesn't add any new races and resolves the user's issue, I think it's best to push this change in and address the race separately as it isn't new. I can't see any other way to fix the race without using some form of a global lock. Since the plan is to deprecate this library, creating the PR and monitoring for performance is probably more effort than it's worth. ",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29101,838478103,2022-03-30T12:23:42Z,tools/bazel,"@@ -42,7 +42,7 @@ fi  # IMPORTANT: if you update the version here, other parts of infrastructure might needs updating as well # (e.g. sanity checks, bazel toolchains etc.)-VERSION=${OVERRIDE_BAZEL_VERSION:-4.2.1}+VERSION=${OVERRIDE_BAZEL_VERSION:-5.0.0}",note that the protoc and grpc_charp_plugin binaries aren't being built by bazel. See https://github.com/grpc/grpc/blob/3c558dd7962f2f2aea6b6a5759f65fa6f11db847/tools/internal_ci/macos/grpc_distribtests_csharp.sh for more info.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29245,839583038,2022-03-31T13:06:38Z,src/core/ext/filters/client_channel/retry_filter.cc,"@@ -1508,6 +1508,8 @@ void RetryFilter::CallData::CallAttempt::BatchData::RecvMessageReady(   // If this attempt has been abandoned, then we're not going to use the   // result of this recv_message op, so do nothing.   if (call_attempt->abandoned_) {+    // Cleanup the byte stream in case there is any pending data.","Please change this comment to say ""The transport will not invoke recv_trailing_metadata_ready until the byte stream for any recv_message op is orphaned, so we do that here to ensure that any pending recv_trailing_metadata op can complete.""",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29177,840197909,2022-04-01T03:13:28Z,src/core/ext/filters/channel_idle/channel_idle_filter.cc,"@@ -80,123 +79,30 @@ struct MaxAgeConfig {     return max_connection_age != Duration::Infinity() ||            max_connection_idle != Duration::Infinity();   }-};--/* A random jitter of +/-10% will be added to MAX_CONNECTION_AGE to spread out-   connection storms. Note that the MAX_CONNECTION_AGE option without jitter-   would not create connection storms by itself, but if there happened to be a-   connection storm it could cause it to repeat at a fixed period. */-MaxAgeConfig GetMaxAgeConfig(ChannelArgs args) {-  const Duration args_max_age =-      args.GetDurationFromIntMillis(GRPC_ARG_MAX_CONNECTION_AGE_MS)-          .value_or(kDefaultMaxConnectionAge);-  const Duration args_max_idle =-      args.GetDurationFromIntMillis(GRPC_ARG_MAX_CONNECTION_IDLE_MS)-          .value_or(kDefaultMaxConnectionIdle);-  const Duration args_max_age_grace =-      args.GetDurationFromIntMillis(GRPC_ARG_MAX_CONNECTION_AGE_GRACE_MS)-          .value_or(kDefaultMaxConnectionAgeGrace);-  /* generate a random number between 1 - kMaxConnectionAgeJitter and-   1 + kMaxConnectionAgeJitter */-  const double multiplier = rand() * kMaxConnectionAgeJitter * 2.0 / RAND_MAX +-                            1.0 - kMaxConnectionAgeJitter;-  /* GRPC_MILLIS_INF_FUTURE - 0.5 converts the value to float, so that result-     will not be cast to int implicitly before the comparison. */-  return MaxAgeConfig{args_max_age * multiplier, args_max_idle,-                      args_max_age_grace};-}--class ChannelIdleFilter : public ChannelFilter {",Most of this change is lifting this class declaration into the header so that it can be instantiated by the fuzzer.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29275,840764485,2022-04-01T16:54:28Z,test/distrib/bazel/test_single_bazel_version.sh,"@@ -41,16 +41,25 @@ EXCLUDED_TARGETS=(   ""-//examples/android/binder/..."" ) +TEST_DIRECTORIES=(+  ""cpp""+  ""python""+)+ FAILED_TESTS=""""  export OVERRIDE_BAZEL_VERSION=""$VERSION"" # when running under bazel docker image, the workspace is read only. export OVERRIDE_BAZEL_WRAPPER_DOWNLOAD_DIR=/tmp bazel build -- //... ""${EXCLUDED_TARGETS[@]}"" || FAILED_TESTS=""${FAILED_TESTS}Build "" -cd test/distrib/bazel/cpp/+for TEST_DIRECTORY in ""${TEST_DIRECTORIES[@]}""; do+  pushd ""test/distrib/bazel/$TEST_DIRECTORY/""++  bazel test --test_output=errors //:all || FAILED_TESTS=""${FAILED_TESTS}${TEST_DIRECTORY} Distribtest""","Since this Bazel is not connected to GCP's ResultStore endpoint, we might completely lost access to test logs for okay runs. Suggest to `--test_output=all`, in case we need to compare good runs and bad runs.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29281,841250550,2022-04-03T17:02:00Z,tools/remote_build/linux.bazelrc,"@@ -13,42 +13,27 @@ # See the License for the specific language governing permissions and # limitations under the License. -# bazelrc with Foundry setting common to both manual run and runs started by Kokoro-# see https://github.com/bazelbuild/bazel-toolchains/tree/master/bazelrc-# for examples and more documentation+# bazelrc file for running gRPC tests with RBE (on Linux) -startup --host_jvm_args=-Dbazel.DigestFunction=SHA256+import %workspace%/tools/remote_build/include/rbe_remote_execution.bazelrc +# Next section is linux-specific RBE configuration build --crosstool_top=@rbe_default//cc:toolchain build --extra_toolchains=@rbe_default//config:cc-toolchain # Use custom execution platforms defined in third_party/toolchains build --extra_execution_platforms=@rbe_default//config:platform build --host_platform=@rbe_default//config:platform build --platforms=@rbe_default//config:platform -build --spawn_strategy=remote-build --strategy=Javac=remote-build --strategy=Closure=remote-build --genrule_strategy=remote-build --remote_timeout=7200  # very large value to avoid problems like https://github.com/grpc/grpc/issues/20777--build --remote_instance_name=projects/grpc-testing/instances/default_instance--build --verbose_failures=true--build --experimental_strict_action_env=true-build --action_env=BAZEL_DO_NOT_DETECT_CPP_TOOLCHAIN=1--# don't use port server-build --define GRPC_PORT_ISOLATED_RUNTIME=1-# without verbose gRPC logs the test outputs are not very useful-test --test_env=GRPC_VERBOSITY=debug- # we assume the default bazel RBE build is on linux, # so filter out stuff that should not be built or run there. build --test_tag_filters=-no_linux build --build_tag_filters=-no_linux +import %workspace%/tools/remote_build/include/test_config_common.bazelrc++build --jobs=100","Yes, that's 100 parallel jobs when running on RBE cluster, when started as a ""manual"" RBE run from developer's workstation. When RBE build is started from CI, we use 200 parallel jobs. The idea is that sometimes folks run jobs with lots of repeated tests (e.g. `--runs_per_test 10000` when deflaking a test) and we don't want them to negatively impact the throughput of the CI, so they get let lower ""quota"" than the CI.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29177,842115968,2022-04-04T20:31:34Z,test/core/filters/filter_fuzzer.cc,"@@ -0,0 +1,571 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <map>++#include ""absl/memory/memory.h""++#include ""src/core/ext/filters/channel_idle/channel_idle_filter.h""+#include ""src/core/ext/filters/http/client/http_client_filter.h""+#include ""src/core/ext/filters/http/client_authority_filter.h""+#include ""src/core/ext/filters/load_reporting/server_load_reporting_filter.h""+#include ""src/core/lib/gpr/env.h""+#include ""src/core/lib/iomgr/executor.h""+#include ""src/core/lib/iomgr/timer_manager.h""+#include ""src/core/lib/resource_quota/resource_quota.h""+#include ""src/core/lib/security/authorization/grpc_server_authz_filter.h""+#include ""src/core/lib/security/transport/auth_filters.h""+#include ""src/core/lib/transport/transport_impl.h""+#include ""src/libfuzzer/libfuzzer_macro.h""+#include ""test/core/filters/filter_fuzzer.pb.h""++bool squelch = true;++static void dont_log(gpr_log_func_args* /*args*/) {}++static gpr_timespec g_now;+extern gpr_timespec (*gpr_now_impl)(gpr_clock_type clock_type);++static gpr_timespec now_impl(gpr_clock_type clock_type) {+  GPR_ASSERT(clock_type != GPR_TIMESPAN);+  gpr_timespec ts = g_now;+  ts.clock_type = clock_type;+  return ts;+}++namespace grpc_core {+namespace {++const grpc_transport_vtable kFakeTransportVTable = {+    // sizeof_stream+    0,+    // name+    ""fake_transport"",+    // init_stream+    [](grpc_transport*, grpc_stream*, grpc_stream_refcount*, const void*,+       Arena*) -> int { abort(); },+    // make_call_promise+    [](grpc_transport*,+       ClientMetadataHandle) -> ArenaPromise<ServerMetadataHandle> { abort(); },+    // set_pollset+    [](grpc_transport*, grpc_stream*, grpc_pollset*) { abort(); },+    // set_pollset_set+    [](grpc_transport*, grpc_stream*, grpc_pollset_set*) { abort(); },+    // perform_stream_op+    [](grpc_transport*, grpc_stream*, grpc_transport_stream_op_batch*) {+      abort();+    },+    // perform_op+    [](grpc_transport*, grpc_transport_op*) { abort(); },+    // destroy_stream+    [](grpc_transport*, grpc_stream*, grpc_closure*) { abort(); },+    // destroy+    [](grpc_transport*) { abort(); },+    // get_endpoint+    [](grpc_transport*) -> grpc_endpoint* { abort(); },+};++class FakeChannelSecurityConnector final+    : public grpc_channel_security_connector {+ public:+  FakeChannelSecurityConnector()+      : grpc_channel_security_connector(""fake"", nullptr, nullptr) {}++  void check_peer(tsi_peer, grpc_endpoint*, RefCountedPtr<grpc_auth_context>*,+                  grpc_closure*) override {+    abort();+  }++  void cancel_check_peer(grpc_closure*, grpc_error_handle) override { abort(); }++  int cmp(const grpc_security_connector*) const override { abort(); }++  ArenaPromise<absl::Status> CheckCallHost(absl::string_view,+                                           grpc_auth_context*) override {+    uint32_t qry = next_check_call_host_qry_++;+    return [this, qry]() -> Poll<absl::Status> {+      auto it = check_call_host_results_.find(qry);+      if (it == check_call_host_results_.end()) return Pending{};+      return it->second;+    };+  }++  void add_handshakers(const grpc_channel_args*, grpc_pollset_set*,+                       HandshakeManager*) override {+    abort();+  }++  void FinishCheckCallHost(uint32_t qry, absl::Status status) {+    check_call_host_results_.emplace(qry, std::move(status));+    check_call_host_wakers_[qry].Wakeup();+  }++ private:+  uint32_t next_check_call_host_qry_ = 0;+  std::map<uint32_t, Waker> check_call_host_wakers_;+  std::map<uint32_t, absl::Status> check_call_host_results_;+};++class ConstAuthorizationEngine final : public AuthorizationEngine {+ public:+  explicit ConstAuthorizationEngine(AuthorizationEngine::Decision decision)+      : decision_(decision) {}++  Decision Evaluate(const EvaluateArgs&) const override { return decision_; }++ private:+  Decision decision_;+};++class FakeAuthorizationPolicyProvider final+    : public grpc_authorization_policy_provider {+ public:+  explicit FakeAuthorizationPolicyProvider(AuthorizationEngines engines)+      : engines_(engines) {}+  void Orphan() override {}+  AuthorizationEngines engines() override { return engines_; }++ private:+  AuthorizationEngines engines_;+};++struct GlobalObjects {+  ResourceQuotaRefPtr resource_quota = MakeResourceQuota(""test"");+  grpc_transport transport{&kFakeTransportVTable};+  RefCountedPtr<FakeChannelSecurityConnector> channel_security_connector{+      MakeRefCounted<FakeChannelSecurityConnector>()};++  void Perform(const filter_fuzzer::GlobalObjectAction& action) {+    switch (action.type_case()) {+      case filter_fuzzer::GlobalObjectAction::TYPE_NOT_SET:+        break;+      case filter_fuzzer::GlobalObjectAction::kSetResourceQuota:+        resource_quota->memory_quota()->SetSize(action.set_resource_quota());+        break;+      case filter_fuzzer::GlobalObjectAction::kFinishCheckCallHost:+        channel_security_connector->FinishCheckCallHost(+            action.finish_check_call_host().qry(),+            absl::Status(+                absl::StatusCode(action.finish_check_call_host().status()),+                action.finish_check_call_host().message()));+        break;+    }+  }+};++struct Filter {+  absl::string_view name;+  absl::StatusOr<std::unique_ptr<ChannelFilter>> (*create)(+      ChannelArgs channel_args, ChannelFilter::Args filter_args);++  template <typename T>+  static Filter* Make(absl::string_view name) {+    return new Filter{+        name,+        [](ChannelArgs channel_args, ChannelFilter::Args filter_args)+            -> absl::StatusOr<std::unique_ptr<ChannelFilter>> {+          auto r = T::Create(channel_args, filter_args);+          if (!r.ok()) return r.status();+          return std::unique_ptr<ChannelFilter>(new T(std::move(*r)));+        }};+  }+};++RefCountedPtr<AuthorizationEngine> LoadAuthorizationEngine(+    const filter_fuzzer::AuthorizationEngine& engine) {+  switch (engine.engine_case()) {+    case filter_fuzzer::AuthorizationEngine::kAlwaysAllow:+      return MakeRefCounted<ConstAuthorizationEngine>(+          AuthorizationEngine::Decision{+              AuthorizationEngine::Decision::Type::kAllow,+              engine.always_allow()});+    case filter_fuzzer::AuthorizationEngine::kAlwaysDeny:+      return MakeRefCounted<ConstAuthorizationEngine>(+          AuthorizationEngine::Decision{+              AuthorizationEngine::Decision::Type::kDeny,+              engine.always_deny()});+    case filter_fuzzer::AuthorizationEngine::ENGINE_NOT_SET:+      break;+  }+  return MakeRefCounted<ConstAuthorizationEngine>(AuthorizationEngine::Decision{+      AuthorizationEngine::Decision::Type::kAllow, engine.always_allow()});+}++template <typename FuzzerChannelArgs>+ChannelArgs LoadChannelArgs(const FuzzerChannelArgs& fuzz_args,+                            GlobalObjects* globals) {+  ChannelArgs args = ChannelArgs().SetObject(ResourceQuota::Default());","yes-ish... if it's something we always guarantee to be present we should add it here,if it's something we don't, then those filters should fail to instantiate",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29303,843086849,2022-04-05T17:25:02Z,test/distrib/gcf/python/cleanup.sh,"@@ -0,0 +1,22 @@+#!/bin/bash+# Copyright 2022 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++# Shellcheck cant find the included file.+# shellcheck disable=SC1091+source common.sh",It might be a good idea to change directory to ensure the work directory is as expected. Like https://github.com/grpc/grpc/blob/master/tools/run_tests/helper_scripts/build_python.sh#L19,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/29303,843127204,2022-04-05T18:14:03Z,tools/run_tests/artifacts/artifact_targets.py,"@@ -433,8 +433,8 @@ def targets():         CSharpExtArtifact('macos', 'ios', presubmit=True),         PythonArtifact('manylinux2014', 'x64', 'cp36-cp36m', presubmit=True),         PythonArtifact('manylinux2014', 'x64', 'cp37-cp37m'),-        PythonArtifact('manylinux2014', 'x64', 'cp38-cp38'),-        PythonArtifact('manylinux2014', 'x64', 'cp39-cp39'),+        PythonArtifact('manylinux2014', 'x64', 'cp38-cp38', presubmit=True),+        PythonArtifact('manylinux2014', 'x64', 'cp39-cp39', presubmit=True),","This is my biggest undecided point about this. It would be _nice_ to know on a triggering PR whether or not it will break GCF, but we don't know yet how flaky it will be.As discussed offline, we'll start this off as a presubmit test, since that gets us the most value. If flakiness becomes a problem, we'll move it to post-submit.",X
4540365,sai-sunder-s,https://api.github.com/repos/grpc/grpc/pulls/29296,843173521,2022-04-05T19:12:14Z,src/core/lib/security/credentials/external/aws_external_account_credentials.cc,"@@ -133,13 +143,83 @@ void AwsExternalAccountCredentials::RetrieveSubjectToken(   }   ctx_ = ctx;   cb_ = cb;+  if (imdsv2_session_token_url_.empty()) {+    if (signer_ != nullptr) {+      BuildSubjectToken();+    } else {+      RetrieveRegion();+    }+  } else {+    RetrieveImdsV2SessionToken();+  }+}++void AwsExternalAccountCredentials::RetrieveImdsV2SessionToken() {+  absl::StatusOr<URI> uri = URI::Parse(imdsv2_session_token_url_);+  if (!uri.ok()) {+    return;+  }+  grpc_http_header* headers =+      static_cast<grpc_http_header*>(gpr_malloc(sizeof(grpc_http_header)));+  headers[0].key = gpr_strdup(""x-aws-ec2-metadata-token-ttl-seconds"");+  headers[0].value = gpr_strdup(""300"");+  grpc_http_request request;+  memset(&request, 0, sizeof(grpc_http_request));+  request.hdr_count = 1;+  request.hdrs = headers;+  grpc_http_response_destroy(&ctx_->response);+  ctx_->response = {};+  GRPC_CLOSURE_INIT(&ctx_->closure, OnRetrieveImdsV2SessionToken, this,+                    nullptr);+  RefCountedPtr<grpc_channel_credentials> http_request_creds;+  if (uri->scheme() == ""http"") {+    http_request_creds = RefCountedPtr<grpc_channel_credentials>(+        grpc_insecure_credentials_create());+  } else {+    http_request_creds = CreateHttpRequestSSLCredentials();+  }+  http_request_ =+      HttpRequest::Put(std::move(*uri), nullptr /* channel args */,+                       ctx_->pollent, &request, ctx_->deadline, &ctx_->closure,+                       &ctx_->response, std::move(http_request_creds));+  http_request_->Start();+  grpc_http_request_destroy(&request);+}++void AwsExternalAccountCredentials::OnRetrieveImdsV2SessionToken(+    void* arg, grpc_error_handle error) {+  AwsExternalAccountCredentials* self =+      static_cast<AwsExternalAccountCredentials*>(arg);+  self->OnRetrieveImdsV2SessionTokenInternal(GRPC_ERROR_REF(error));+}++void AwsExternalAccountCredentials::OnRetrieveImdsV2SessionTokenInternal(+    grpc_error_handle error) {+  if (error != GRPC_ERROR_NONE) {+    FinishRetrieveSubjectToken("""", error);+    return;+  }+  imdsv2_session_token_ =+      std::string(ctx_->response.body, ctx_->response.body_length);+   if (signer_ != nullptr) {     BuildSubjectToken();   } else {     RetrieveRegion();   } } +void AwsExternalAccountCredentials::AddHeaders(grpc_http_request* request) {",Renamed to AddMetadataRequestHeaders to make it consistent with other libraries,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29323,844078926,2022-04-06T15:19:50Z,src/core/lib/address_utils/sockaddr_utils.cc,"@@ -51,11 +52,13 @@ static std::string grpc_sockaddr_to_uri_unix_if_possible(   }   const auto* unix_addr = reinterpret_cast<const struct sockaddr_un*>(addr);   if (unix_addr->sun_path[0] == '\0' && unix_addr->sun_path[1] != '\0') {","A better option here would be to use our [URI library](https://github.com/grpc/grpc/blob/master/src/core/lib/uri/uri_parser.h), which will also handle percent-encoding any other special characters that may need it:```std::string scheme;std::string path;if (unix_addr->sun_path[0] == '\0' && unix_addr->sun_path[1] != '\0') {  scheme = ""unix-abstract"";  path = std::string(unix_addr->sun_path + 1, resolved_addr->len - sizeof(unix_addr->sun_family) - 1);} else {  scheme = ""unix"";  path = unix_addr->sun_path;}absl::StatusOr<URI> uri = URI::Create(    std::move(scheme), /*authority=*/"""", std::move(path),    /*query_parameter_pairs=*/{}, /*fragment=*/"""");if (!uri.ok()) return """";return uri->ToString();```We should probably do the same thing in `grpc_sockaddr_to_uri()`.Feel free to make this a TODO assigned to me if you don't want to tackle this now.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29323,844084038,2022-04-06T15:24:24Z,test/core/address_utils/sockaddr_utils_test.cc,"@@ -206,6 +209,33 @@ TEST(SockAddrUtilsTest, SockAddrToString) {   EXPECT_TRUE(grpc_sockaddr_to_uri(&phony).empty()); } +#ifdef GRPC_HAVE_UNIX_SOCKET++grpc_resolved_address MakeAddrUnix(std::string path) {","Instead of writing your own function to do this, please use [`UnixSockaddrPopulate()`](https://github.com/grpc/grpc/blob/ab2193d8c178c41d2b097dc8b35a54bfdd9e928b/src/core/lib/address_utils/parse_address.h#L72) and [`UnixAbstractSockaddrPopulate()`](https://github.com/grpc/grpc/blob/ab2193d8c178c41d2b097dc8b35a54bfdd9e928b/src/core/lib/address_utils/parse_address.h#L77).",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/29254,844215458,2022-04-06T17:37:51Z,CMakeLists.txt,"@@ -333,49 +414,59 @@ file(MAKE_DIRECTORY ${_gRPC_PROTO_GENS_DIR}) #   Add custom commands to process ``.proto`` files to C++ using protoc and #   GRPC plugin:: #-#     protobuf_generate_grpc_cpp [<ARGN>...]+#     protobuf_generate_grpc_cpp <FILE_LOCATION> <IMPORT_PATH>","Yeah, my recommendation would be to introduce a new function with the new signature and leave this current function the way it is. Unless there's some way to add named parameters in CMake.Alternatively, there might be some way to use named parameters or variadic functions to add the new arguments in in a backward-compatible manner.",X
4210911,anicr7,https://api.github.com/repos/grpc/grpc/pulls/29323,844250575,2022-04-06T18:18:21Z,src/core/lib/address_utils/sockaddr_utils.cc,"@@ -51,11 +52,13 @@ static std::string grpc_sockaddr_to_uri_unix_if_possible(   }   const auto* unix_addr = reinterpret_cast<const struct sockaddr_un*>(addr);   if (unix_addr->sun_path[0] == '\0' && unix_addr->sun_path[1] != '\0') {","Moved the logic in grpc_sockaddr_to_uri_unix_if_possible to use the URI library.For grpc_sockaddr_to_uri, migrating to URI library causes some issues because of the square brackets around the IPV6 addresses. The URI library encodes the ""["" with %5B  and ""]"" with %5D. This seems like a bug in URI parser library as brackets around host should be fine. WDYT?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29323,844351636,2022-04-06T20:03:26Z,src/core/lib/address_utils/sockaddr_utils.cc,"@@ -51,11 +52,13 @@ static std::string grpc_sockaddr_to_uri_unix_if_possible(   }   const auto* unix_addr = reinterpret_cast<const struct sockaddr_un*>(addr);   if (unix_addr->sun_path[0] == '\0' && unix_addr->sun_path[1] != '\0') {","No, the URI parser implementation is correct.  `[` and `]` are legal characters in the authority part of a URI (see [RFC-3986 section 3.2.2](https://datatracker.ietf.org/doc/html/rfc3986#section-3.2.2), but not in the path part ([RFC-3986 section 3.3](https://datatracker.ietf.org/doc/html/rfc3986#section-3.3), and here we are encoding the address in the path.I think we should fix callers to deal with this properly.  They can parse the URI string using `URI::Parse()` and then access the percent-decoded address via `uri->path()`.But as I said before, feel free to leave this as a TODO for now if you don't want to fix all the callers.",
16571023,ananda1066,https://api.github.com/repos/grpc/grpc/pulls/29334,844401326,2022-04-06T20:36:38Z,src/core/lib/iomgr/tcp_posix.cc,"@@ -836,11 +844,21 @@ static void maybe_make_read_slices(grpc_tcp* tcp) {     int target_length = static_cast<int>(tcp->target_length);     int extra_wanted =         target_length - static_cast<int>(tcp->incoming_buffer->length);+    if (tcp->last_read_completed) {+      // Set it to false again to start the next block of reads+      tcp->last_read_completed = false;+      // Reset last_min_read_chunk_size for the next block of reads+      tcp->last_min_read_chunk_size = tcp->min_read_chunk_size;",nit: maybe rename `curr_min_read_chunk_size` and `starting_min_read_chunk_size` or something similar?,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/29332,844444455,2022-04-06T21:39:05Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -6429,6 +6430,75 @@ TEST_P(CdsTest, AggregateClusterFallBackFromRingHashAtStartup) {   EXPECT_TRUE(found); } +TEST_P(CdsTest, AggregateClusterFallBackFromRingHashToLogicalDnsAtStartup) {+  ScopedExperimentalEnvVar env_var(+      ""GRPC_XDS_EXPERIMENTAL_ENABLE_AGGREGATE_AND_LOGICAL_DNS_CLUSTER"");+  CreateAndStartBackends(1);+  const char* kEdsClusterName = ""eds_cluster"";+  const char* kLogicalDNSClusterName = ""logical_dns_cluster"";+  // Populate EDS resource.+  EdsResourceArgs args({+      {""locality0"",+       {MakeNonExistantEndpoint(), MakeNonExistantEndpoint()},+       kDefaultLocalityWeight,+       0},+      {""locality1"",+       {MakeNonExistantEndpoint(), MakeNonExistantEndpoint()},+       kDefaultLocalityWeight,+       1},+  });+  balancer_->ads_service()->SetEdsResource(BuildEdsResource(args));+  // Populate new CDS resources.+  Cluster eds_cluster = default_cluster_;+  eds_cluster.set_name(kEdsClusterName);+  balancer_->ads_service()->SetCdsResource(eds_cluster);+  // Populate LOGICAL_DNS cluster.+  auto logical_dns_cluster = default_cluster_;+  logical_dns_cluster.set_name(kLogicalDNSClusterName);+  logical_dns_cluster.set_type(Cluster::LOGICAL_DNS);+  auto* address = logical_dns_cluster.mutable_load_assignment()+                      ->add_endpoints()+                      ->add_lb_endpoints()+                      ->mutable_endpoint()+                      ->mutable_address()+                      ->mutable_socket_address();+  address->set_address(kServerName);+  address->set_port_value(443);+  balancer_->ads_service()->SetCdsResource(logical_dns_cluster);+  // Create Aggregate Cluster+  auto cluster = default_cluster_;+  cluster.set_lb_policy(Cluster::RING_HASH);+  CustomClusterType* custom_cluster = cluster.mutable_cluster_type();+  custom_cluster->set_name(""envoy.clusters.aggregate"");+  ClusterConfig cluster_config;+  cluster_config.add_clusters(kEdsClusterName);+  cluster_config.add_clusters(kLogicalDNSClusterName);+  custom_cluster->mutable_typed_config()->PackFrom(cluster_config);+  balancer_->ads_service()->SetCdsResource(cluster);+  // Set up route with channel id hashing+  auto new_route_config = default_route_config_;+  auto* route = new_route_config.mutable_virtual_hosts(0)->mutable_routes(0);+  auto* hash_policy = route->mutable_route()->add_hash_policy();+  hash_policy->mutable_filter_state()->set_key(""io.grpc.channel_id"");+  SetListenerAndRouteConfiguration(balancer_.get(), default_listener_,+                                   new_route_config);+  // Set Logical DNS result+  {+    grpc_core::ExecCtx exec_ctx;+    grpc_core::Resolver::Result result;+    result.addresses = CreateAddressListFromPortList(GetBackendPorts());+    logical_dns_cluster_resolver_response_generator_->SetResponse(+        std::move(result));+  }+  // Inject connection delay to make this act more realistically.","why do we need this delay? Do we actually need 500ms to trigger a certain previously bad behavior? Or, is the goal to make sure that TCP connect fails asynchronously?Note that in the integration test for the bug I originally filed, connections for ring hash connections actually fail immediately since the addresses are unroutable. The address in the logical_dns_cluster of the integration test may take a bit longer to connect, though.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29332,844462333,2022-04-06T22:10:03Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -6429,6 +6430,75 @@ TEST_P(CdsTest, AggregateClusterFallBackFromRingHashAtStartup) {   EXPECT_TRUE(found); } +TEST_P(CdsTest, AggregateClusterFallBackFromRingHashToLogicalDnsAtStartup) {+  ScopedExperimentalEnvVar env_var(+      ""GRPC_XDS_EXPERIMENTAL_ENABLE_AGGREGATE_AND_LOGICAL_DNS_CLUSTER"");+  CreateAndStartBackends(1);+  const char* kEdsClusterName = ""eds_cluster"";+  const char* kLogicalDNSClusterName = ""logical_dns_cluster"";+  // Populate EDS resource.+  EdsResourceArgs args({+      {""locality0"",+       {MakeNonExistantEndpoint(), MakeNonExistantEndpoint()},+       kDefaultLocalityWeight,+       0},+      {""locality1"",+       {MakeNonExistantEndpoint(), MakeNonExistantEndpoint()},+       kDefaultLocalityWeight,+       1},+  });+  balancer_->ads_service()->SetEdsResource(BuildEdsResource(args));+  // Populate new CDS resources.+  Cluster eds_cluster = default_cluster_;+  eds_cluster.set_name(kEdsClusterName);+  balancer_->ads_service()->SetCdsResource(eds_cluster);+  // Populate LOGICAL_DNS cluster.+  auto logical_dns_cluster = default_cluster_;+  logical_dns_cluster.set_name(kLogicalDNSClusterName);+  logical_dns_cluster.set_type(Cluster::LOGICAL_DNS);+  auto* address = logical_dns_cluster.mutable_load_assignment()+                      ->add_endpoints()+                      ->add_lb_endpoints()+                      ->mutable_endpoint()+                      ->mutable_address()+                      ->mutable_socket_address();+  address->set_address(kServerName);+  address->set_port_value(443);+  balancer_->ads_service()->SetCdsResource(logical_dns_cluster);+  // Create Aggregate Cluster+  auto cluster = default_cluster_;+  cluster.set_lb_policy(Cluster::RING_HASH);+  CustomClusterType* custom_cluster = cluster.mutable_cluster_type();+  custom_cluster->set_name(""envoy.clusters.aggregate"");+  ClusterConfig cluster_config;+  cluster_config.add_clusters(kEdsClusterName);+  cluster_config.add_clusters(kLogicalDNSClusterName);+  custom_cluster->mutable_typed_config()->PackFrom(cluster_config);+  balancer_->ads_service()->SetCdsResource(cluster);+  // Set up route with channel id hashing+  auto new_route_config = default_route_config_;+  auto* route = new_route_config.mutable_virtual_hosts(0)->mutable_routes(0);+  auto* hash_policy = route->mutable_route()->add_hash_policy();+  hash_policy->mutable_filter_state()->set_key(""io.grpc.channel_id"");+  SetListenerAndRouteConfiguration(balancer_.get(), default_listener_,+                                   new_route_config);+  // Set Logical DNS result+  {+    grpc_core::ExecCtx exec_ctx;+    grpc_core::Resolver::Result result;+    result.addresses = CreateAddressListFromPortList(GetBackendPorts());+    logical_dns_cluster_resolver_response_generator_->SetResponse(+        std::move(result));+  }+  // Inject connection delay to make this act more realistically.","Without the fix, the test does not fail without this delay, so it's not a useful test.  The problem is that we need the subchannels to report CONNECTING for a sufficiently long period of time to allow the right sequence of events to be seen by the priority policy.  That's what I saw happening in the log you posted on the internal bug.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/29320,844475408,2022-04-06T22:35:45Z,test/cpp/end2end/connection_delay_injector.cc,"@@ -0,0 +1,116 @@+// Copyright 2016 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""test/cpp/end2end/connection_delay_injector.h""++#include <atomic>+#include <memory>++#include ""absl/memory/memory.h""+#include ""absl/utility/utility.h""++#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/gprpp/time.h""+#include ""src/core/lib/iomgr/tcp_client.h""+#include ""src/core/lib/iomgr/timer.h""++// defined in tcp_client.cc+extern grpc_tcp_client_vtable* grpc_tcp_client_impl;++namespace grpc {+namespace testing {++namespace {++grpc_tcp_client_vtable* g_original_vtable = nullptr;++std::atomic<grpc_core::Duration> g_delay;++class InjectedDelay {+ public:+  InjectedDelay(grpc_closure* closure, grpc_endpoint** ep,+                grpc_pollset_set* interested_parties,+                const grpc_channel_args* channel_args,+                const grpc_resolved_address* addr,+                grpc_core::Timestamp deadline)+      : closure_(closure),+        endpoint_(ep),+        interested_parties_(interested_parties),+        channel_args_(grpc_channel_args_copy(channel_args)),+        deadline_(deadline) {+    memcpy(&address_, addr, sizeof(grpc_resolved_address));+    GRPC_CLOSURE_INIT(&timer_callback_, TimerCallback, this, nullptr);+    auto duration = g_delay.load();+    deadline_ += duration;+    grpc_timer_init(&timer_, grpc_core::ExecCtx::Get()->Now() + duration,+                    &timer_callback_);+  }++  ~InjectedDelay() { grpc_channel_args_destroy(channel_args_); }++ private:+  static void TimerCallback(void* arg, grpc_error_handle /*error*/) {+    auto* self = static_cast<InjectedDelay*>(arg);+    g_original_vtable->connect(self->closure_, self->endpoint_,+                               self->interested_parties_, self->channel_args_,+                               &self->address_, self->deadline_);","For least surprising behavior, though, shouldn't line 55 really be removed?Also shouldn't `duration` be set to something like `max(duration, deadline - Now())` ?Otherwise, if subchannel connection timeout is e.g. 20 seconds, and we inject a 30 second delay, the subchannel won't actually receive it's timeout until 30 seconds later?",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29303,845410401,2022-04-07T17:42:44Z,test/distrib/gcf/python/README.md,"@@ -0,0 +1,12 @@+# Python Google Cloud Functions Distribtest++This distribtest acts as a smoke test for usage of the `grpcio` Python wheel in+the GCF environment. This test is dependent on two long-lived GCP resources:++- `gcf-distribtest-topic` Pub Sub Topic with default configuration.+- `grpc-gcf-distribtest` GCS Bucket. This bucket has 1 day TTL on all artifacts.+++All Functions _should_ be deleted by the test under normal circumstances. In","qq: is it ok for multiple instance of the same test to run in parallel. That can easily happen (e.g. multiple adhoc runs started manually or if we run the test on PRs), so it would be good to make it clear if that's allowed or not.If it's not allowed, we'd need to think a bit about how to make sure that only once instance of the test jobs ever runs.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29320,845419388,2022-04-07T17:53:37Z,test/cpp/end2end/connection_delay_injector.cc,"@@ -0,0 +1,116 @@+// Copyright 2016 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""test/cpp/end2end/connection_delay_injector.h""++#include <atomic>+#include <memory>++#include ""absl/memory/memory.h""+#include ""absl/utility/utility.h""++#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/gprpp/time.h""+#include ""src/core/lib/iomgr/tcp_client.h""+#include ""src/core/lib/iomgr/timer.h""++// defined in tcp_client.cc+extern grpc_tcp_client_vtable* grpc_tcp_client_impl;++namespace grpc {+namespace testing {++namespace {++grpc_tcp_client_vtable* g_original_vtable = nullptr;++std::atomic<grpc_core::Duration> g_delay;++class InjectedDelay {+ public:+  InjectedDelay(grpc_closure* closure, grpc_endpoint** ep,+                grpc_pollset_set* interested_parties,+                const grpc_channel_args* channel_args,+                const grpc_resolved_address* addr,+                grpc_core::Timestamp deadline)+      : closure_(closure),+        endpoint_(ep),+        interested_parties_(interested_parties),+        channel_args_(grpc_channel_args_copy(channel_args)),+        deadline_(deadline) {+    memcpy(&address_, addr, sizeof(grpc_resolved_address));+    GRPC_CLOSURE_INIT(&timer_callback_, TimerCallback, this, nullptr);+    auto duration = g_delay.load();+    deadline_ += duration;+    grpc_timer_init(&timer_, grpc_core::ExecCtx::Get()->Now() + duration,+                    &timer_callback_);+  }++  ~InjectedDelay() { grpc_channel_args_destroy(channel_args_); }++ private:+  static void TimerCallback(void* arg, grpc_error_handle /*error*/) {+    auto* self = static_cast<InjectedDelay*>(arg);+    g_original_vtable->connect(self->closure_, self->endpoint_,+                               self->interested_parties_, self->channel_args_,+                               &self->address_, self->deadline_);","Good point.  I had thought there must have been some reason the existing code worked this way, but I traced it back to #13494, and I don't see anything in that (long) review that explains why it's done this way.  And the tests do seem to pass without line 55.  I'll remove that line in a subsequent PR.I don't think we need to subtract from deadline, because the deadline is a timestamp, not a duration.  If the delay is longer than `deadline - Now()`, then deadline will be in the past when the connection attempt starts, in which case it will fail immediately.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29320,845420178,2022-04-07T17:54:32Z,test/cpp/end2end/connection_delay_injector.cc,"@@ -0,0 +1,116 @@+// Copyright 2016 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include ""test/cpp/end2end/connection_delay_injector.h""++#include <atomic>+#include <memory>++#include ""absl/memory/memory.h""+#include ""absl/utility/utility.h""++#include ""src/core/lib/channel/channel_args.h""+#include ""src/core/lib/gprpp/time.h""+#include ""src/core/lib/iomgr/tcp_client.h""+#include ""src/core/lib/iomgr/timer.h""++// defined in tcp_client.cc+extern grpc_tcp_client_vtable* grpc_tcp_client_impl;++namespace grpc {+namespace testing {++namespace {++grpc_tcp_client_vtable* g_original_vtable = nullptr;++std::atomic<grpc_core::Duration> g_delay;++class InjectedDelay {+ public:+  InjectedDelay(grpc_closure* closure, grpc_endpoint** ep,+                grpc_pollset_set* interested_parties,+                const grpc_channel_args* channel_args,+                const grpc_resolved_address* addr,+                grpc_core::Timestamp deadline)+      : closure_(closure),+        endpoint_(ep),+        interested_parties_(interested_parties),+        channel_args_(grpc_channel_args_copy(channel_args)),+        deadline_(deadline) {+    memcpy(&address_, addr, sizeof(grpc_resolved_address));+    GRPC_CLOSURE_INIT(&timer_callback_, TimerCallback, this, nullptr);+    auto duration = g_delay.load();+    deadline_ += duration;+    grpc_timer_init(&timer_, grpc_core::ExecCtx::Get()->Now() + duration,+                    &timer_callback_);+  }++  ~InjectedDelay() { grpc_channel_args_destroy(channel_args_); }++ private:+  static void TimerCallback(void* arg, grpc_error_handle /*error*/) {+    auto* self = static_cast<InjectedDelay*>(arg);+    g_original_vtable->connect(self->closure_, self->endpoint_,+                               self->interested_parties_, self->channel_args_,+                               &self->address_, self->deadline_);","Oh wait, sorry, you were saying the opposite: you were saying we should cap the delay by the deadline.  Yeah, that's probably a good idea.  I'll make that change as well.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29303,845420232,2022-04-07T17:54:36Z,tools/internal_ci/linux/grpc_gcp_distribtests_python.cfg,"@@ -0,0 +1,26 @@+# Copyright 2021 The gRPC Authors","nit: I think grpc_distribtests_gcp_python better matches the current naming pattern (""from most general to more specific"")",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29254,846216190,2022-04-08T15:08:26Z,cmake/download_archive.cmake,"@@ -0,0 +1,39 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set(_download_archive_TEMPORARY_DIR ${CMAKE_BINARY_DIR}/http_archives)+file(MAKE_DIRECTORY ${_download_archive_TEMPORARY_DIR})++# This is basically Bazel's http_archive.+# Note that strip_prefix strips the directory path prefix of the extracted+# archive content, and it may strip multiple directories.+function(download_archive destination url hash strip_prefix)","Idea: perhaps you e.g. pass the URLs as a single ""url"" param separated e.g. by semicolon?""https://storage.googleapis.com/grpc-bazel-mirror/github.com/envoyproxy/data-plane-api/archive/9c42588c956220b48eb3099d186487c2f04d32ec.tar.gz;https://github.com/envoyproxy/data-plane-api/archive/9c42588c956220b48eb3099d186487c2f04d32ec.tar.gz""",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29254,846234463,2022-04-08T15:19:29Z,tools/buildgen/plugins/check_attrs.py,"@@ -89,6 +89,13 @@ def subset_of(values):         'vs_proj_dir': anything(),         'zlib': one_of((True,)),     },+    'external_library': {","you're calling this ""external_library"", but the only instances we currently plan to have are all ""external proto libraries"", so let's give some thought to how this should be named. Also the ""proto_prefix"" attributes signifies that this has something to do with protos.From some other perspective, these are ""external archives"" to download rather than libraries.Btw, we currently import the entire third_party/upb subtree into our git repo (because ubp isn't installable on its own so we embed it in our build), so perhaps we could consider making upb and ""external library"" as well.CC @veblush ",X
26934891,yihuazhang,https://api.github.com/repos/grpc/grpc/pulls/28985,846343512,2022-04-08T17:30:56Z,doc/server_side_auth.md,"@@ -2,7 +2,8 @@ Server-side API for Authenticating Clients ==========================================  NOTE: This document describes how server-side authentication works in C-core based gRPC implementations only. In gRPC Java and Go, server side authentication is handled differently.-NOTE2: `CallCredentials` class is only valid for secure channels in C-Core. So, for connections under insecure channels, features below might not be available.++NOTE2: `CallCredentials` class is only valid if its security level is less than or equal to the security level of the associated connection. See the [gRFC](https://github.com/grpc/proposal/blob/master/L62-core-call-credential-security-level.md) for more.","LGTM. But I will probably rephrase it as follows: ""`CallCredentials` class is only valid if the security level it requires is less than or equal to the security level of connection used to transfer it. See the [gRFC](https://github.com/grpc/proposal/blob/master/L62-core-call-credential-security-level.md) for more information.""",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29353,846506380,2022-04-08T21:42:51Z,tools/run_tests/python_utils/bazel_report_helper.py,"@@ -0,0 +1,252 @@+#!/usr/bin/env python3+# Copyright 2022 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Helps with running bazel with extra settings to generate structured test reports in CI.""""""++import argparse+import os+import platform+import sys+import uuid++_ROOT = os.path.abspath(os.path.join(os.path.dirname(sys.argv[0]), '../../..'))+os.chdir(_ROOT)+++def _platform_string():+    """"""Detect current platform""""""+    if platform.system() == 'Windows':+        return 'windows'+    elif platform.system()[:7] == 'MSYS_NT':+        return 'windows'+    elif platform.system() == 'Darwin':+        return 'mac'+    elif platform.system() == 'Linux':+        return 'linux'+    else:+        return 'posix'+++def _append_to_kokoro_bazel_invocations(invocation_id):+    """"""Kokoro can display ""Bazel"" result link on kokoro jobs if told so.""""""+    # to get ""bazel"" link for kokoro build, we need to upload+    # the ""bazel_invocation_ids"" file with bazel invocation ID as artifact.+    kokoro_artifacts_dir = os.getenv('KOKORO_ARTIFACTS_DIR')+    if kokoro_artifacts_dir:+        # append the bazel invocation UUID to the bazel_invocation_ids file.+        with open(os.path.join(kokoro_artifacts_dir, 'bazel_invocation_ids'),+                  'a') as f:+            f.write(invocation_id + '\n')+        print(+            'Added invocation ID %s to kokoro ""bazel_invocation_ids"" artifact' %+            invocation_id,+            file=sys.stderr)+    else:+        print(+            'Skipped adding invocation ID %s to kokoro ""bazel_invocation_ids"" artifact'+            % invocation_id,+            file=sys.stderr)+        pass+++def _generate_junit_report_string(report_suite_name, invocation_id, success):+    """"""Generate sponge_log.xml formatted report, that will make the bazel invocation reachable as a target in resultstore UI / sponge.""""""+    bazel_invocation_url = 'https://source.cloud.google.com/results/invocations/%s' % invocation_id+    package_name = report_suite_name+    # set testcase name to invocation URL. That way, the link will be displayed in some form+    # resultstore UI and sponge even in case the bazel invocation succeeds.+    testcase_name = bazel_invocation_url+    if success:+        # unfortunately, neither resultstore UI nor sponge display the ""system-err"" output (or any other tags)+        # on a passing test case. But at least we tried.+        test_output_tag = '<system-err>PASSED. See invocation results here: %s</system-err>' % bazel_invocation_url+    else:+        # The failure output will be displayes in both resultstore UI and sponge when clicking on the failing testcase.+        test_output_tag = '<failure message=""Failure"">FAILED. See bazel invocation results here: %s</failure>' % bazel_invocation_url++    lines = [+        '<testsuites>',+        '<testsuite id=""1"" name=""%s"" package=""%s"">' %+        (report_suite_name, package_name),+        '<testcase name=""%s"">' % testcase_name,+        test_output_tag,+        '</testcase>'+        '</testsuite>',+        '</testsuites>',+    ]+    return '\n'.join(lines)+++def _create_bazel_wrapper(report_path, report_suite_name, invocation_id,+                          upload_results):+    """"""Create a ""bazel wrapper"" script that will execute bazel with extra settings and postprocessing.""""""++    os.makedirs(report_path, exist_ok=True)++    bazel_wrapper_filename = os.path.join(report_path, 'bazel_wrapper')+    bazel_wrapper_bat_filename = bazel_wrapper_filename + '.bat'+    bazel_rc_filename = os.path.join(report_path, 'bazel_rc')++    # put xml reports in a separate directory if requested by GRPC_TEST_REPORT_BASE_DIR+    report_base_dir = os.getenv('GRPC_TEST_REPORT_BASE_DIR', None)+    xml_report_path = os.path.abspath(+        os.path.join(report_base_dir, report_path+                    ) if report_base_dir else report_path)+    os.makedirs(xml_report_path, exist_ok=True)++    failing_report_filename = os.path.join(xml_report_path, 'sponge_log.xml')+    success_report_filename = os.path.join(xml_report_path,+                                           'success_log_to_rename.xml')++    if _platform_string() == 'windows':+        workspace_status_command = 'tools/remote_build/workspace_status_kokoro.bat'+    else:+        workspace_status_command = 'tools/remote_build/workspace_status_kokoro.sh'++    # generate RC file with the bazel flags we want to use apply.+    # Using an RC file solves problems with flag ordering in the wrapper.+    # (e.g. some flags need to come after the build/test command)+    with open(bazel_rc_filename, 'w') as f:+        f.write('build --invocation_id=""%s""\n' % invocation_id)+        f.write('build --workspace_status_command=""%s""\n' %+                workspace_status_command)++    # generate ""failing"" and ""success"" report+    # the ""failing"" is named as ""sponge_log.xml"", which is the name picked up by sponge/resultstore+    # so the failing report will be used by default (unless we later replace the report with+    # one that says ""success""). That way if something goes wrong before bazel is run,+    # there will at least be a ""failing"" target that indicates that (we really don't want silent failures).+    with open(failing_report_filename, 'w') as f:+        f.write(+            _generate_junit_report_string(report_suite_name,+                                          invocation_id,+                                          success=False))+    with open(success_report_filename, 'w') as f:+        f.write(+            _generate_junit_report_string(report_suite_name,+                                          invocation_id,+                                          success=True))++    # generate the bazel wrapper for linux/macos+    with open(bazel_wrapper_filename, 'w') as f:+        intro_lines = [+            '#!/bin/bash',+            'set -ex',+            '',+            'tools/bazel --bazelrc=""%s"" ""$@"" || FAILED=true' %+            bazel_rc_filename,+            '',+        ]++        if upload_results:+            upload_results_lines = [+                '# Sleep to let ResultStore finish writing results before querying',+                'sleep 60',+                'PYTHONHTTPSVERIFY=0 python3 ./tools/run_tests/python_utils/upload_rbe_results.py --invocation_id=""%s""'+                % invocation_id,+                '',+            ]+        else:+            upload_results_lines = []++        outro_lines = [+            'if [ ""$FAILED"" != """" ]',+            'then',+            '  exit 1',+            'else',+            '  # success: plant the pre-generated xml report that says ""success""',+            '  mv -f %s %s' %+            (success_report_filename, failing_report_filename),+            'fi',+        ]++        lines = [+            line + '\n'+            for line in intro_lines + upload_results_lines + outro_lines+        ]+        f.writelines(lines)+    os.chmod(bazel_wrapper_filename, 0o775)  # make the unix wrapper executable++    # generate bazel wrapper for windows+    with open(bazel_wrapper_bat_filename, 'w') as f:+        intro_lines = [+            '@echo on',+            '',+            'bazel --bazelrc=""%s"" %%*' % bazel_rc_filename,+            'set BAZEL_EXITCODE=%errorlevel%',+            '',+        ]++        if upload_results:+            upload_results_lines = [+                '@rem Sleep to let ResultStore finish writing results before querying',+                'sleep 60',+                'python3 tools/run_tests/python_utils/upload_rbe_results.py --invocation_id=""%s"" || exit /b 1'+                % invocation_id,+                '',+            ]+        else:+            upload_results_lines = []++        outro_lines = [+            'if %BAZEL_EXITCODE% == 0 (',+            '  @rem success: plant the pre-generated xml report that says ""success""',+            '  mv -f %s %s' %+            (success_report_filename, failing_report_filename),+            ')',+            'exit /b %BAZEL_EXITCODE%',+        ]++        lines = [+            line + '\n'+            for line in intro_lines + upload_results_lines + outro_lines+        ]+        f.writelines(lines)++    print('Bazel invocation ID: %s' % invocation_id, file=sys.stderr)+    print('Upload test results to BigQuery after bazel runs: %s' %+          upload_results,+          file=sys.stderr)+    print('Generated bazel wrapper: %s' % bazel_wrapper_filename,+          file=sys.stderr)+    print('Generated bazel wrapper: %s' % bazel_wrapper_bat_filename,+          file=sys.stderr)+++# parse command line+argp = argparse.ArgumentParser(+    description='Generate bazel wrapper to help with bazel test reports in CI.')+argp.add_argument(+    '--report_path',+    required=True,+    type=str,+    help=+    'Path under which the bazel wrapper and other files are going to be generated'+)+argp.add_argument('--report_suite_name',+                  default='bazel_invocations',+                  type=str,+                  help='Test suite name to use in generated XML report')+args = argp.parse_args()++# generate new bazel invocation ID+invocation_id = str(uuid.uuid4())++report_path = args.report_path+report_suite_name = args.report_suite_name+upload_results = True if os.getenv('UPLOAD_TEST_RESULTS') else False++_append_to_kokoro_bazel_invocations(invocation_id)+_create_bazel_wrapper(report_path, report_suite_name, invocation_id,+                      upload_results)","Let's create a main function, and protect this file with:```pyif __name__ == '__main__':    # Execute when the module is not initialized from an import statement.    ...```https://docs.python.org/3/library/__main__.html#idiomatic-usage",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29254,846537014,2022-04-08T23:24:13Z,cmake/download_archive.cmake,"@@ -0,0 +1,39 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set(_download_archive_TEMPORARY_DIR ${CMAKE_BINARY_DIR}/http_archives)+file(MAKE_DIRECTORY ${_download_archive_TEMPORARY_DIR})++# This is basically Bazel's http_archive.+# Note that strip_prefix strips the directory path prefix of the extracted+# archive content, and it may strip multiple directories.+function(download_archive destination url hash strip_prefix)","I'm taking a slightly more verbose approach here. In the CMake template, we will iterate through all download URLs and generate a `download_archive` call for each URL:```cmake# Setup external proto library at third_party/opencensus-proto/src with 2 download URLsif (NOT EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/third_party/opencensus-proto/src)  # Download the archive via HTTP, validate the checksum, and extract to third_party/opencensus-proto/src.  download_archive(    ${CMAKE_CURRENT_SOURCE_DIR}/third_party/opencensus-proto/src    https://storage.googleapis.com/grpc-bazel-mirror/github.com/census-instrumentation/opencensus-proto/archive/v0.3.0.tar.gz    b7e13f0b4259e80c3070b583c2f39e53153085a6918718b1c710caf7037572b0    opencensus-proto-0.3.0/src  )endif()if (NOT EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/third_party/opencensus-proto/src)  # Download the archive via HTTP, validate the checksum, and extract to third_party/opencensus-proto/src.  download_archive(    ${CMAKE_CURRENT_SOURCE_DIR}/third_party/opencensus-proto/src    https://github.com/census-instrumentation/opencensus-proto/archive/v0.3.0.tar.gz    b7e13f0b4259e80c3070b583c2f39e53153085a6918718b1c710caf7037572b0    opencensus-proto-0.3.0/src  )endif()```",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29254,846538642,2022-04-08T23:31:38Z,CMakeLists.txt,"@@ -318,11 +318,92 @@ include(cmake/ssl.cmake) include(cmake/upb.cmake) include(cmake/xxhash.cmake) include(cmake/zlib.cmake)+include(cmake/download_archive.cmake)++# Setup external proto library at third_party/envoy-api with 2 download URLs+if (NOT EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/third_party/envoy-api)+  # Download the archive via HTTP, validate the checksum, and extract to third_party/envoy-api.+  download_archive(","I see 2 alternatives here:1. Make all dependencies here sub-tree (like what we did for upb, xxhash: https://github.com/grpc/grpc/tree/master/third_party) (our codebase size will bloat, we will wait longer for cloning);2. Stop supporting C++ xDS related targets in CMake (CSDS, Admin, and several tests) (if this is acceptable, I'm happy to discard the Bazel-CMake introduced in this PR).Between the 3 options (current, 2 above), I feel like not supporting offline build is less harmful.WDYT?",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29254,846549484,2022-04-09T00:25:43Z,tools/buildgen/plugins/check_attrs.py,"@@ -89,6 +89,13 @@ def subset_of(values):         'vs_proj_dir': anything(),         'zlib': one_of((True,)),     },+    'external_library': {","Renamed to `external_proto_library` (feel free to suggest other names).Yep, I do plan to look into the possibility of reducing our subtrees after the copied xds protos are gone. I recall one of the blocker: supporting `Make` without subtree is hard.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29254,847075826,2022-04-11T08:33:48Z,cmake/download_archive.cmake,"@@ -0,0 +1,39 @@+# Copyright 2021 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set(_download_archive_TEMPORARY_DIR ${CMAKE_BINARY_DIR}/http_archives)+file(MAKE_DIRECTORY ${_download_archive_TEMPORARY_DIR})++# This is basically Bazel's http_archive.+# Note that strip_prefix strips the directory path prefix of the extracted+# archive content, and it may strip multiple directories.+function(download_archive destination url hash strip_prefix)","I think ExternalProject has been available much earlier than cmake 3.7 (unless you need some specific features that were introduced in 3.7. do you?), but there might be other issues with using ExternalProject.Generally, the problem with ExternalProject is that it only download the dependencies in the ""build"" phase, which might be too late in some cases (e.g. when you need the external project to do something in the ""configure"" phase). This might be ok for the external proto libraries, since they don't have a CMakeLists.txt themselves, but it would not work in some more complex cases (e.g. download UPB dependency and than include it in a build).IIRC the `ExternalProject` isn't very popular due to its significant limitations.In cmake 3.11 version, the `FetchContent` was introduced, which is much more flexible and usable as AFAICT, it's now the ""recommended"" one to use over the `ExternalProject`. https://cmake.org/cmake/help/latest/module/FetchContent.htmlQuoting from the docs:> This module enables populating content at configure time via any method supported by the [ExternalProject](https://cmake.org/cmake/help/latest/module/ExternalProject.html#module:ExternalProject) module. Whereas [ExternalProject_Add()](https://cmake.org/cmake/help/latest/module/ExternalProject.html#command:externalproject_add) downloads at build time, the FetchContent module makes content available immediately, allowing the configure step to use the content in commands like [add_subdirectory()](https://cmake.org/cmake/help/latest/command/add_subdirectory.html#command:add_subdirectory), [include()](https://cmake.org/cmake/help/latest/command/include.html#command:include) or [file()](https://cmake.org/cmake/help/latest/command/file.html#command:file) operations.See e.g. https://bewagner.net/programming/2020/05/02/cmake-fetchcontent/ for more info.TL;DR from this is that for now we can keep the current `file(DOWNLOAD ` and maybe in the future we can look into converting our build to use FetchContent (some time in the future, when requiring e.g. cmake 3.14 isn't an issue for us), that has some potential for simplifying our build.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29254,847096395,2022-04-11T08:56:13Z,tools/buildgen/extract_metadata_from_bazel_xml.py,"@@ -45,6 +45,48 @@ BuildDict = Dict[str, BuildMetadata] BuildYaml = Dict[str, Any] +BuildMetadata = Dict[str, Any]+BuildDict = Dict[str, BuildMetadata]+BuildYaml = Dict[str, Any]+++# This is basically a Python dict with predefined fields and types+@dataclass()+class ExternalProtoLibrary:+    # The relative path of this proto library should be. Preferably, it should+    # match the submodule path.+    destination: str+    # The prefix to remove in order to insure the proto import is correct. For+    # more info, see description of https://github.com/grpc/grpc/pull/25272.+    proto_prefix: str+    # Following 3 fields should be filled by build metadata from Bazel.+    urls: List[str] = field(default_factory=list)+    hash: str = ''+    strip_prefix: str = ''+++EXTERNAL_PROTO_LIBRARIES = {+    'envoy_api':+        ExternalProtoLibrary(destination='third_party/envoy-api',+                             proto_prefix='third_party/envoy-api/'),+    'com_google_googleapis':+        ExternalProtoLibrary(destination='third_party/googleapis',+                             proto_prefix='third_party/googleapis/'),+    'com_github_cncf_udpa':+        ExternalProtoLibrary(destination='third_party/xds',+                             proto_prefix='third_party/xds/'),+    'opencensus_proto':+        ExternalProtoLibrary(destination='third_party/opencensus-proto/src',","nit: I noticed that we currently have third_party/opencensus-proto  git submodule, but we're only using the `src` subdirectory of it because how the http_archive is defined in grpc_deps.bzl:https://github.com/grpc/grpc/blob/61b34dfaee80d36b505fbb488cc73b2afd803c2e/bazel/grpc_deps.bzl#L433Can we clean this inconsistency up in the future?Also, it seem that we currently don't have a check in place to make sure that the git submodules for the proto libraries will always be set to the same version as what's declared in grpc_deps.bzl (and I think it's important to ensure they always stay in sync).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29254,847111954,2022-04-11T09:13:29Z,templates/CMakeLists.txt.template,"@@ -416,6 +452,67 @@     endforeach()   endfunction() +  #  protobuf_generate_grpc_cpp_with_import_path+  #  --------------------------+  #+  #   Add custom commands to process ``.proto`` files to C++ using protoc and+  #   GRPC plugin::+  #+  #     protobuf_generate_grpc_cpp_with_import_path <FILE_LOCATION> <IMPORT_PATH>+  #+  #   ``FILE_LOCATION``+  #     The relative path of the ``.proto`` file to the project root+  #   ``IMPORT_PATH``+  #     The proto import path that itself expected to be placed in. For+  #     example, a ""bar.proto"" file wants to be imported as+  #     `import ""foo/bar.proto""`. Then we should place it under+  #     ""<ProtoBuf_Include_Path>/foo/bar.proto"" instead of+  #     ""<ProtoBuf_Include_Path>/third_party/foo/bar.proto"". This ensures+  #     correct symbol being generated and C++ include path being correct.+  #     More info can be found at https://github.com/grpc/grpc/pull/25272.+  #+  function(protobuf_generate_grpc_cpp_with_import_path FILE_LOCATION IMPORT_PATH)","Looks like this functions is 95% the same as the original `protobuf_generate_grpc_cpp` you copied.Instead of duplicating the code (bad for maintainability), can you adjust the original function to be able to handle the case that requires putting protos into the centralized location?Btw, I think many of our .proto in the grpc/grpc repo had their import statements modified to workaround the same issue as you're trying to workaround here.Perhaps there should be some cleanup afterwards to make sure the import statements in all .proto files (inside and outside of grpc/grpc) are used consistently?",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29254,847131148,2022-04-11T09:33:59Z,templates/CMakeLists.txt.template,"@@ -416,6 +452,67 @@     endforeach()   endfunction() +  #  protobuf_generate_grpc_cpp_with_import_path+  #  --------------------------+  #+  #   Add custom commands to process ``.proto`` files to C++ using protoc and+  #   GRPC plugin::+  #+  #     protobuf_generate_grpc_cpp_with_import_path <FILE_LOCATION> <IMPORT_PATH>+  #+  #   ``FILE_LOCATION``+  #     The relative path of the ``.proto`` file to the project root+  #   ``IMPORT_PATH``+  #     The proto import path that itself expected to be placed in. For+  #     example, a ""bar.proto"" file wants to be imported as+  #     `import ""foo/bar.proto""`. Then we should place it under+  #     ""<ProtoBuf_Include_Path>/foo/bar.proto"" instead of+  #     ""<ProtoBuf_Include_Path>/third_party/foo/bar.proto"". This ensures+  #     correct symbol being generated and C++ include path being correct.+  #     More info can be found at https://github.com/grpc/grpc/pull/25272.+  #+  function(protobuf_generate_grpc_cpp_with_import_path FILE_LOCATION IMPORT_PATH)","To Illustrate the hacks we've done to the .proto files in our repository, see the same proto in grpc/grpc and grpc-java:https://github.com/grpc/grpc/blob/61b34dfaee80d36b505fbb488cc73b2afd803c2e/src/proto/grpc/testing/control.proto#L17https://github.com/grpc/grpc-java/blob/78ccc81fd5c3a9c94c8a202f141e477b946a263b/benchmarks/src/main/proto/grpc/testing/control.proto#L16as you can see, grpc-java uses the ""canonical import path"" (`grpc/testing/payloads.proto` ""as in google3""), but C-core uses the tweaked one: `src/proto/grpc/testing/payloads.proto`.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29353,847297094,2022-04-11T12:57:00Z,tools/run_tests/python_utils/bazel_report_helper.py,"@@ -0,0 +1,252 @@+#!/usr/bin/env python3+# Copyright 2022 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+""""""Helps with running bazel with extra settings to generate structured test reports in CI.""""""++import argparse+import os+import platform+import sys+import uuid++_ROOT = os.path.abspath(os.path.join(os.path.dirname(sys.argv[0]), '../../..'))+os.chdir(_ROOT)+++def _platform_string():+    """"""Detect current platform""""""+    if platform.system() == 'Windows':+        return 'windows'+    elif platform.system()[:7] == 'MSYS_NT':+        return 'windows'+    elif platform.system() == 'Darwin':+        return 'mac'+    elif platform.system() == 'Linux':+        return 'linux'+    else:+        return 'posix'+++def _append_to_kokoro_bazel_invocations(invocation_id):+    """"""Kokoro can display ""Bazel"" result link on kokoro jobs if told so.""""""+    # to get ""bazel"" link for kokoro build, we need to upload+    # the ""bazel_invocation_ids"" file with bazel invocation ID as artifact.+    kokoro_artifacts_dir = os.getenv('KOKORO_ARTIFACTS_DIR')+    if kokoro_artifacts_dir:+        # append the bazel invocation UUID to the bazel_invocation_ids file.+        with open(os.path.join(kokoro_artifacts_dir, 'bazel_invocation_ids'),+                  'a') as f:+            f.write(invocation_id + '\n')+        print(+            'Added invocation ID %s to kokoro ""bazel_invocation_ids"" artifact' %+            invocation_id,+            file=sys.stderr)+    else:+        print(+            'Skipped adding invocation ID %s to kokoro ""bazel_invocation_ids"" artifact'+            % invocation_id,+            file=sys.stderr)+        pass+++def _generate_junit_report_string(report_suite_name, invocation_id, success):+    """"""Generate sponge_log.xml formatted report, that will make the bazel invocation reachable as a target in resultstore UI / sponge.""""""+    bazel_invocation_url = 'https://source.cloud.google.com/results/invocations/%s' % invocation_id+    package_name = report_suite_name+    # set testcase name to invocation URL. That way, the link will be displayed in some form+    # resultstore UI and sponge even in case the bazel invocation succeeds.+    testcase_name = bazel_invocation_url+    if success:+        # unfortunately, neither resultstore UI nor sponge display the ""system-err"" output (or any other tags)+        # on a passing test case. But at least we tried.+        test_output_tag = '<system-err>PASSED. See invocation results here: %s</system-err>' % bazel_invocation_url+    else:+        # The failure output will be displayes in both resultstore UI and sponge when clicking on the failing testcase.+        test_output_tag = '<failure message=""Failure"">FAILED. See bazel invocation results here: %s</failure>' % bazel_invocation_url++    lines = [+        '<testsuites>',+        '<testsuite id=""1"" name=""%s"" package=""%s"">' %+        (report_suite_name, package_name),+        '<testcase name=""%s"">' % testcase_name,+        test_output_tag,+        '</testcase>'+        '</testsuite>',+        '</testsuites>',+    ]+    return '\n'.join(lines)+++def _create_bazel_wrapper(report_path, report_suite_name, invocation_id,+                          upload_results):+    """"""Create a ""bazel wrapper"" script that will execute bazel with extra settings and postprocessing.""""""++    os.makedirs(report_path, exist_ok=True)++    bazel_wrapper_filename = os.path.join(report_path, 'bazel_wrapper')+    bazel_wrapper_bat_filename = bazel_wrapper_filename + '.bat'+    bazel_rc_filename = os.path.join(report_path, 'bazel_rc')++    # put xml reports in a separate directory if requested by GRPC_TEST_REPORT_BASE_DIR+    report_base_dir = os.getenv('GRPC_TEST_REPORT_BASE_DIR', None)+    xml_report_path = os.path.abspath(+        os.path.join(report_base_dir, report_path+                    ) if report_base_dir else report_path)+    os.makedirs(xml_report_path, exist_ok=True)++    failing_report_filename = os.path.join(xml_report_path, 'sponge_log.xml')+    success_report_filename = os.path.join(xml_report_path,+                                           'success_log_to_rename.xml')++    if _platform_string() == 'windows':+        workspace_status_command = 'tools/remote_build/workspace_status_kokoro.bat'+    else:+        workspace_status_command = 'tools/remote_build/workspace_status_kokoro.sh'++    # generate RC file with the bazel flags we want to use apply.+    # Using an RC file solves problems with flag ordering in the wrapper.+    # (e.g. some flags need to come after the build/test command)+    with open(bazel_rc_filename, 'w') as f:+        f.write('build --invocation_id=""%s""\n' % invocation_id)+        f.write('build --workspace_status_command=""%s""\n' %+                workspace_status_command)++    # generate ""failing"" and ""success"" report+    # the ""failing"" is named as ""sponge_log.xml"", which is the name picked up by sponge/resultstore+    # so the failing report will be used by default (unless we later replace the report with+    # one that says ""success""). That way if something goes wrong before bazel is run,+    # there will at least be a ""failing"" target that indicates that (we really don't want silent failures).+    with open(failing_report_filename, 'w') as f:+        f.write(+            _generate_junit_report_string(report_suite_name,+                                          invocation_id,+                                          success=False))+    with open(success_report_filename, 'w') as f:+        f.write(+            _generate_junit_report_string(report_suite_name,+                                          invocation_id,+                                          success=True))++    # generate the bazel wrapper for linux/macos+    with open(bazel_wrapper_filename, 'w') as f:","I considered having wrapper scripts at first, but I wasn't happy with the result. What I ran into:- we need the logic for both a .sh wrapper and .bat wrapper. if there's a common python script, most of the ""work"" (e.g. generate the UUID, generate the rc file, generate the report etc.) can be done by the python script (with good comments and readability), while we just bake the hardcoded results into the generated wrapper. In bash/bat, even ""trivial"" task quickly become a pain  and the resulting ""code"" has bad readability and also is duplicated.- in python, coming up with future tweaks is much simpler than in .sh/.bat- with pre-existing bash scripts, passing the configuration to the wrapper script isn't easy. Since we need the wrapper to forward all the command line arguments to bazel, we are left with setting environment variables to pass values like the invocation ID to the script, at which point simply baking the arguments into generated code just comes off as easier, especially since we're already generating other files (e.g. the bazelrc and the xml report).I've given this quite a bit of thought and the current version is what I believe makes the most sense. I'm definitely open to improvements in the future, but I think it's going to be hard to come up with major simplifications. But yeah, often times simplifying stuff improves our ability to see even more possible improvements, so I'll be looking whether we can improve this even further (as followup work).",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29367,847603078,2022-04-11T18:06:23Z,include/grpc/event_engine/memory_allocator.h,"@@ -25,23 +25,90 @@  #include <grpc/event_engine/internal/memory_allocator_impl.h> #include <grpc/slice.h>--// forward-declaring an internal struct, not used publicly.-struct grpc_slice_buffer;+#include <grpc/slice_buffer.h>  namespace grpc_event_engine { namespace experimental { -// TODO(nnoble): needs implementation+/// A Wrapper around \a grpc_slice_buffer pointer.+///+/// A slice buffer holds the memory for a collection of slices.+/// The SliceBuffer object itself is meant to only hide the C-style API,+/// and won't hold the data itself. In terms of lifespan, the+/// grpc_slice_buffer ought to be kept somewhere inside the caller's objects,+/// like a transport or an endpoint.+///+/// This lifespan rule is likely to change in the future, as we may+/// collapse the grpc_slice_buffer structure straight into this class.+///+/// The SliceBuffer API is basically a replica of the grpc_slice_buffer's,+/// and its documentation will move here once we remove the C structure,+/// which should happen before the Event Engine's API is no longer+/// an experimental API. class SliceBuffer {  public:-  SliceBuffer() { abort(); }-  explicit SliceBuffer(grpc_slice_buffer*) { abort(); }+  explicit SliceBuffer(grpc_slice_buffer* slice_buffer)+      : slice_buffer_(slice_buffer) {}+  SliceBuffer(const SliceBuffer& other) : slice_buffer_(other.slice_buffer_) {}+  SliceBuffer(SliceBuffer&& other) noexcept+      : slice_buffer_(other.slice_buffer_) {+    other.slice_buffer_ = nullptr;+  }++  /// Adds a new slice into the SliceBuffer and makes an attempt to merge+  /// this slice with the last slice in the SliceBuffer.+  void Add(grpc_slice slice) { grpc_slice_buffer_add(slice_buffer_, slice); }++  /// Adds a new slice into the SliceBuffer at the next available index.+  size_t AddIndexed(grpc_slice slice) {+    return grpc_slice_buffer_add_indexed(slice_buffer_, slice);+  }++  /// Removes the first slice in the SliceBuffer.+  void Pop() { grpc_slice_buffer_pop(slice_buffer_); }++  /// Returns the number of slices held by the SliceBuffer.+  size_t Count() { return slice_buffer_->count; }++  /// Removes/deletes the last n bytes in the SliceBuffer.+  void TrimEnd(size_t n) {+    grpc_slice_buffer_trim_end(slice_buffer_, n, nullptr);+  }++  /// Move the first n bytes of the SliceBuffer into a memory pointed to by dst.+  void MoveFirstIntoBuffer(size_t n, void* dst) {+    grpc_slice_buffer_move_first_into_buffer(slice_buffer_, n, dst);+  }++  /// Removes and unrefs all slices in the SliceBuffer.+  void Clear() { grpc_slice_buffer_reset_and_unref(slice_buffer_); }++  /// Removes the first slice in the SliceBuffer and returns it.+  grpc_slice TakeFirst() { return grpc_slice_buffer_take_first(slice_buffer_); }++  /// Prepends the slice to the the front of the SliceBuffer.+  void UndoTakeFirst(grpc_slice slice) {+    grpc_slice_buffer_undo_take_first(slice_buffer_, slice);+  }++  /// Increased the ref-count of slice at the specified index and returns the+  /// associated slice.+  grpc_slice Ref(size_t index) {",`RefSlice` or similar? `Ref` has special meaning in our codebase,
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/29354,847618659,2022-04-11T18:27:06Z,tools/internal_ci/macos/grpc_run_bazel_cpp_ios_tests.sh,"@@ -21,14 +21,25 @@ source $(dirname $0)/../../../tools/internal_ci/helper_scripts/move_src_tree_and # change to grpc repo root cd $(dirname $0)/../../.. +source tools/internal_ci/helper_scripts/prepare_build_macos_rc++# make sure bazel is available+tools/bazel version+ ./tools/run_tests/start_port_server.py -dirs=(end2end server client common codegen util grpclb test)-for dir in ${dirs[*]}; do-  echo $dir-  out=`tools/bazel query ""kind(ios_unit_test, tests(//test/cpp/$dir/...))"" 2>/dev/null | grep '^//'`-  for test in $out; do-    echo ""Running: $test""-    tools/bazel test --test_summary=detailed --test_output=all $test-  done-done+# only select ObjC test from the following subdirs+# TODO(jtattermusch): start running selected tests from //test/core too.+test_pattern=""//test/cpp/end2end/... + //test/cpp/server/... + //test/cpp/client/... + //test/cpp/common/... + //test/cpp/codegen/... + //test/cpp/util/... + //test/cpp/grpclb/... + //test/cpp/test/...""",nit: can iOS share & reuse a test pattern list from cpp or other lang ? or is this need to be iOS specific ?,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29254,847705178,2022-04-11T20:23:01Z,CMakeLists.txt,"@@ -318,18 +318,103 @@ include(cmake/ssl.cmake) include(cmake/upb.cmake) include(cmake/xxhash.cmake) include(cmake/zlib.cmake)+include(cmake/download_archive.cmake)++# Setup external proto library at third_party/envoy-api with 2 download URLs+if (NOT EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/third_party/envoy-api)+  # Download the archive via HTTP, validate the checksum, and extract to third_party/envoy-api.+  download_archive(+    ${CMAKE_CURRENT_SOURCE_DIR}/third_party/envoy-api+    https://storage.googleapis.com/grpc-bazel-mirror/github.com/envoyproxy/data-plane-api/archive/9c42588c956220b48eb3099d186487c2f04d32ec.tar.gz+    c5807010b67033330915ca5a20483e30538ae5e689aa14b3631d6284beca4630+    data-plane-api-9c42588c956220b48eb3099d186487c2f04d32ec+  )+endif()+if (NOT EXISTS ${CMAKE_CURRENT_SOURCE_DIR}/third_party/envoy-api)","With `EXPECTED_HASH` field set in the `file(DOWNLOAD ...)` command, the CMake script will throw an error if the download failed in certain way. The behavior I observed for CMake 3.19.4 is that if the first download attempt failed, no folder will be created, the second download attempt (with different url) will start. However, the configuration phase will be stopped with message ""-- Configuring incomplete, errors occurred!"".As a workaround, I added following log line to ask users to re-run the configuration stage:```  message(STATUS ""Downloading from ${url}, if failed, please try configuring again"")```The actual console output looks like:```$ cmake buildCMake Warning at third_party/abseil-cpp/CMakeLists.txt:74 (message):  A future Abseil release will default ABSL_PROPAGATE_CXX_STD to ON for CMake  3.8 and up.  We recommend enabling this option to ensure your project still  builds correctly.-- Failed to find LLVM FileCheck-- git version: v1.6.0-6-g0baacde3 normalized to 1.6.0.6-- Version: 1.6.0.6-- Performing Test HAVE_THREAD_SAFETY_ATTRIBUTES -- failed to compile-- Performing Test HAVE_STD_REGEX -- success-- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile-- Performing Test HAVE_POSIX_REGEX -- success-- Performing Test HAVE_STEADY_CLOCK -- success---- 3.19.4.0CMake Deprecation Warning at third_party/zlib/CMakeLists.txt:1 (cmake_minimum_required):  Compatibility with CMake < 2.8.12 will be removed from a future version of  CMake.  Update the VERSION argument <min> value or use a ...<max> suffix to tell  CMake that the project does not need compatibility with older versions.-- Downloading from https://storage.333googleapis.com/grpc-bazel-mirror/github.com/envoyproxy/data-plane-api/archive/9c42588c956220b48eb3099d186487c2f04d32ec.tar.gz, if failed, please try configuring againCMake Error at cmake/download_archive.cmake:25 (file):  file DOWNLOAD HASH mismatch    for file: [/usr/local/google/home/lidiz/src/grpc/cmake/build/http_archives/data-plane-api-9c42588c956220b48eb3099d186487c2f04d32ec.tar.gz]      expected hash: [c5807010b67033330915ca5a20483e30538ae5e689aa14b3631d6284beca4630]        actual hash: [e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855]             status: [6;""Couldn't resolve host name""]Call Stack (most recent call first):  CMakeLists.txt:326 (download_archive)-- Downloading from https://github.com/envoyproxy/data-plane-api/archive/9c42588c956220b48eb3099d186487c2f04d32ec.tar.gz, if failed, please try configuring again-- Configuring incomplete, errors occurred!```",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29254,847715619,2022-04-11T20:37:19Z,templates/CMakeLists.txt.template,"@@ -416,6 +452,67 @@     endforeach()   endfunction() +  #  protobuf_generate_grpc_cpp_with_import_path+  #  --------------------------+  #+  #   Add custom commands to process ``.proto`` files to C++ using protoc and+  #   GRPC plugin::+  #+  #     protobuf_generate_grpc_cpp_with_import_path <FILE_LOCATION> <IMPORT_PATH>+  #+  #   ``FILE_LOCATION``+  #     The relative path of the ``.proto`` file to the project root+  #   ``IMPORT_PATH``+  #     The proto import path that itself expected to be placed in. For+  #     example, a ""bar.proto"" file wants to be imported as+  #     `import ""foo/bar.proto""`. Then we should place it under+  #     ""<ProtoBuf_Include_Path>/foo/bar.proto"" instead of+  #     ""<ProtoBuf_Include_Path>/third_party/foo/bar.proto"". This ensures+  #     correct symbol being generated and C++ include path being correct.+  #     More info can be found at https://github.com/grpc/grpc/pull/25272.+  #+  function(protobuf_generate_grpc_cpp_with_import_path FILE_LOCATION IMPORT_PATH)","New method renamed to `protobuf_generate_grpc_cpp_with_import_path_correction`, the old method will be a thin wrapper around the new method as suggested. Manually tested that the old method works (able to compile existing protos under src/proto).For user's usage (https://github.com/search?q=protobuf_generate_grpc_cpp&type=code):* gRPC as Submodule: https://github.com/IvanSafonov/grpc-cmake-example* ProtoBuf provides a more generic command: https://www.f-ax.de/dev/2020/11/08/grpc-plugin-cmake-support.html* Copied the implementation ([example](https://github.com/sandvikcode/pubsuber/blob/1083c20e71fc057accf1bc0e7fbb31008efaf885/cmake/grpc.cmake))As discussed, we might want to continue our support for the submodule use case, so keeping the original signature might be worthwhile. I didn't find any usage of using this method from the CMake gRPC target (probably because we don't support this approach yet).Given that the ProtoBuf repo did provide a plugin command, we can recommend users to use that instead of our CMake ""internal"" command.---For the import path tweak, I will work on that in follow-up PRs.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29354,848187154,2022-04-12T09:08:32Z,tools/internal_ci/macos/grpc_run_bazel_cpp_ios_tests.sh,"@@ -21,14 +21,25 @@ source $(dirname $0)/../../../tools/internal_ci/helper_scripts/move_src_tree_and # change to grpc repo root cd $(dirname $0)/../../.. +source tools/internal_ci/helper_scripts/prepare_build_macos_rc++# make sure bazel is available+tools/bazel version+ ./tools/run_tests/start_port_server.py -dirs=(end2end server client common codegen util grpclb test)-for dir in ${dirs[*]}; do-  echo $dir-  out=`tools/bazel query ""kind(ios_unit_test, tests(//test/cpp/$dir/...))"" 2>/dev/null | grep '^//'`-  for test in $out; do-    echo ""Running: $test""-    tools/bazel test --test_summary=detailed --test_output=all $test-  done-done+# only select ObjC test from the following subdirs+# TODO(jtattermusch): start running selected tests from //test/core too.+test_pattern=""//test/cpp/end2end/... + //test/cpp/server/... + //test/cpp/client/... + //test/cpp/common/... + //test/cpp/codegen/... + //test/cpp/util/... + //test/cpp/grpclb/... + //test/cpp/test/...""","So this is up to you to decide. For now, I'm keeping the test targets as they were (and we can later look into which targets should be run by ObjC.Our regular c++ tests simply use `//test/...`, but my guess the ios specific selection of test patterns to run has to do with the fact that simply running `//test/...` wouldn't work. This is something to investigate as a followup.(but it needs to be decided by the ObjC & iOS owner - i.e. you, not me).",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29323,848850236,2022-04-12T20:39:39Z,src/core/lib/address_utils/sockaddr_utils.cc,"@@ -51,11 +52,13 @@ static std::string grpc_sockaddr_to_uri_unix_if_possible(   }   const auto* unix_addr = reinterpret_cast<const struct sockaddr_un*>(addr);   if (unix_addr->sun_path[0] == '\0' && unix_addr->sun_path[1] != '\0') {","I'd really rather not have two different methods to maintain here.  The API really makes no sense, and this just prolongs the agony of having this broken.How many callers would have to be updated if we just fixed the behavior of the existing `grpc_sockaddr_to_uri()` function, and how hard would that be to do?  Remember that there may be some internal callers, so you'll have to check more than just the OSS code.If this isn't hard, let's just fix it.  If it is too hard to do right now, then let's just leave `grpc_sockaddr_to_uri()` the way it is now, but with a TODO indicating that it needs to be fixed, since I don't think fixing this is a blocker for your other PR.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29355,849357718,2022-04-13T11:06:14Z,tools/internal_ci/linux/arm64/grpc_bazel_test_c_cpp_in_docker.sh,"@@ -25,5 +25,13 @@ cd /var/local/git/grpc # tests require port server to be running python3 tools/run_tests/start_port_server.py +python3 tools/run_tests/python_utils/bazel_report_helper.py --report_path bazel_test_c_cpp+ # test gRPC C/C++ with bazel-tools/bazel test --config=opt --test_output=errors --test_tag_filters=-no_linux,-no_arm64 --build_tag_filters=-no_linux,-no_arm64 --flaky_test_attempts=1 --runs_per_test=1 //test/...+bazel_test_c_cpp/bazel_wrapper \+  --bazelrc=tools/remote_build/include/test_locally_with_resultstore_results.bazelrc \+  test --config=opt \","yes, those flags were initially used to assess the level of test flakiness on ARM64, but we're past that point and we no longer need them. I'm trying to make the arm64 config as close to the x64 config as possible.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29254,849518304,2022-04-13T13:59:45Z,templates/CMakeLists.txt.template,"@@ -382,40 +418,72 @@       return()     endif() -    set(_protobuf_include_path -I . -I <%text>${_gRPC_PROTOBUF_WELLKNOWN_INCLUDE_DIR}</%text>)     foreach(FIL <%text>${ARGN}</%text>)-      get_filename_component(ABS_FIL <%text>${FIL}</%text> ABSOLUTE)-      get_filename_component(FIL_WE <%text>${FIL}</%text> NAME_WE)-      file(RELATIVE_PATH REL_FIL <%text>${CMAKE_CURRENT_SOURCE_DIR}</%text> <%text>${ABS_FIL}</%text>)-      get_filename_component(REL_DIR <%text>${REL_FIL}</%text> DIRECTORY)-      set(RELFIL_WE ""<%text>${REL_DIR}/${FIL_WE}</%text>"")--      #if cross-compiling, find host plugin-      if(CMAKE_CROSSCOMPILING)-        find_program(_gRPC_CPP_PLUGIN grpc_cpp_plugin)-      else()-        set(_gRPC_CPP_PLUGIN $<TARGET_FILE:grpc_cpp_plugin>)-      endif()--      add_custom_command(-        OUTPUT <%text>""${_gRPC_PROTO_GENS_DIR}/${RELFIL_WE}.grpc.pb.cc""</%text>-               <%text>""${_gRPC_PROTO_GENS_DIR}/${RELFIL_WE}.grpc.pb.h""</%text>-               <%text>""${_gRPC_PROTO_GENS_DIR}/${RELFIL_WE}_mock.grpc.pb.h""</%text>-               <%text>""${_gRPC_PROTO_GENS_DIR}/${RELFIL_WE}.pb.cc""</%text>-               <%text>""${_gRPC_PROTO_GENS_DIR}/${RELFIL_WE}.pb.h""</%text>-        COMMAND <%text>${_gRPC_PROTOBUF_PROTOC_EXECUTABLE}</%text>-        ARGS --grpc_out=<%text>generate_mock_code=true:${_gRPC_PROTO_GENS_DIR}</%text>-             --cpp_out=<%text>${_gRPC_PROTO_GENS_DIR}</%text>-             --plugin=protoc-gen-grpc=<%text>${_gRPC_CPP_PLUGIN}</%text>-             <%text>${_protobuf_include_path}</%text>-             <%text>${REL_FIL}</%text>-        DEPENDS <%text>${ABS_FIL}</%text> <%text>${_gRPC_PROTOBUF_PROTOC}</%text> <%text>${_gRPC_CPP_PLUGIN}</%text>-        WORKING_DIRECTORY <%text>${CMAKE_CURRENT_SOURCE_DIR}</%text>-        COMMENT ""Running gRPC C++ protocol buffer compiler on <%text>${FIL}</%text>""-        VERBATIM)+      protobuf_generate_grpc_cpp_with_import_path_correction(<%text>${FIL}</%text> <%text>${FIL}</%text>)     endforeach()   endfunction() +  #  protobuf_generate_grpc_cpp_with_import_path_correction+  #  --------------------------+  #+  #   Add custom commands to process ``.proto`` files to C++ using protoc and+  #   GRPC plugin::+  #+  #     protobuf_generate_grpc_cpp_with_import_path_correction <FILE_LOCATION> <IMPORT_PATH>+  #+  #   ``FILE_LOCATION``+  #     The relative path of the ``.proto`` file to the project root+  #   ``IMPORT_PATH``+  #     The proto import path that itself expected to be placed in. For+  #     example, a ""bar.proto"" file wants to be imported as+  #     `import ""foo/bar.proto""`. Then we should place it under+  #     ""<ProtoBuf_Include_Path>/foo/bar.proto"" instead of+  #     ""<ProtoBuf_Include_Path>/third_party/foo/bar.proto"". This ensures+  #     correct symbol being generated and C++ include path being correct.+  #     More info can be found at https://github.com/grpc/grpc/pull/25272.+  #+  function(protobuf_generate_grpc_cpp_with_import_path_correction FILE_LOCATION IMPORT_PATH)+    if(NOT FILE_LOCATION)+      message(SEND_ERROR ""Error: PROTOBUF_GENERATE_GRPC_CPP() called without any proto files"")+      return()+    endif()++    set(_protobuf_include_path -I . -I <%text>${_gRPC_PROTOBUF_WELLKNOWN_INCLUDE_DIR}</%text>)+    # compute the relative path+    get_filename_component(ABS_FIL <%text>${_gRPC_PROTO_SRCS_DIR}/${IMPORT_PATH}</%text> ABSOLUTE)+    get_filename_component(FIL_WE <%text>${_gRPC_PROTO_SRCS_DIR}/${IMPORT_PATH}</%text> NAME_WE)","nit: note that the ""NAME_WE"" semantics is quite cryptic, so perhaps explaining what it does (see the comment above) is worthwhile.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29383,849797967,2022-04-13T18:54:13Z,test/cpp/end2end/server_load_reporting_end2end_test.cc,"@@ -142,39 +142,50 @@ TEST_F(ServerLoadReportingEnd2endTest, BasicReport) {   const std::string& lb_id = response.initial_response().load_balancer_id();   gpr_log(GPR_INFO, ""Initial response received (lb_id: %s)."", lb_id.c_str());   ClientMakeEchoCalls(lb_id, ""LB_TAG"", kOkMessage, 1);-  while (true) {++  unsigned loadCount = 0;+  bool gotInProgress = false;+  bool gotOrphaned = false;+  bool gotCalls = false;+  while (loadCount < 3) {     stream->Read(&response);-    if (!response.load().empty()) {-      ASSERT_EQ(response.load().size(), 3) << response.DebugString();-      for (const auto& load : response.load()) {-        if (load.in_progress_report_case()) {-          // The special load record that reports the number of in-progress-          // calls.-          ASSERT_EQ(load.num_calls_in_progress(), 1);-        } else if (load.orphaned_load_case()) {-          // The call from the balancer doesn't have any valid LB token.-          ASSERT_EQ(load.orphaned_load_case(), load.kLoadKeyUnknown);-          ASSERT_EQ(load.num_calls_started(), 1);-          ASSERT_EQ(load.num_calls_finished_without_error(), 0);-          ASSERT_EQ(load.num_calls_finished_with_error(), 0);-        } else {-          // This corresponds to the calls from the client.-          ASSERT_EQ(load.num_calls_started(), 1);-          ASSERT_EQ(load.num_calls_finished_without_error(), 1);-          ASSERT_EQ(load.num_calls_finished_with_error(), 0);-          ASSERT_GE(load.total_bytes_received(), sizeof(kOkMessage));-          ASSERT_GE(load.total_bytes_sent(), sizeof(kOkMessage));-          ASSERT_EQ(load.metric_data().size(), 1);-          ASSERT_EQ(load.metric_data().Get(0).metric_name(), kMetricName);-          ASSERT_EQ(load.metric_data().Get(0).num_calls_finished_with_metric(),-                    1);-          ASSERT_EQ(load.metric_data().Get(0).total_metric_value(),-                    kMetricValue);-        }+    for (const auto& load : response.load()) {+      loadCount++;+      if (load.in_progress_report_case()) {+        // The special load record that reports the number of in-progress+        // calls.+        ASSERT_EQ(load.num_calls_in_progress(), 1);","Not directly related to this PR, but as long as you're in here, these `ASSERT`s should probably all be `EXPECT`s instead.  In general, `ASSERT`s should be used only in cases where a failure in the expected condition makes it undesirable to continue with the test -- e.g., if you're testing that a pointer is non-null before testing the value that it points to, you may want to bail out if the pointer is null, since the test will crash if it proceeds beyond that point.https://google.github.io/googletest/primer.html#assertions",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29397,849954730,2022-04-13T22:47:29Z,src/core/ext/filters/http/client/http_client_filter.cc,"@@ -125,17 +129,23 @@ ArenaPromise<ServerMetadataHandle> HttpClientFilter::MakeCallPromise( }  HttpClientFilter::HttpClientFilter(HttpSchemeMetadata::ValueType scheme,-                                   Slice user_agent)-    : scheme_(scheme), user_agent_(std::move(user_agent)) {}+                                   Slice user_agent,+                                   bool test_only_use_put_requests)+    : scheme_(scheme),+      user_agent_(std::move(user_agent)),+      test_only_use_put_requests_(test_only_use_put_requests) {}  absl::StatusOr<HttpClientFilter> HttpClientFilter::Create(ChannelArgs args,                                                           ChannelFilter::Args) {   auto* transport = args.GetObject<grpc_transport>();   if (transport == nullptr) {     return absl::InvalidArgumentError(""HttpClientFilter needs a transport"");   }-  return HttpClientFilter(SchemeFromArgs(args),-                          UserAgentFromArgs(args, transport->vtable->name));+  absl::optional<int> use_put_requests =",This can be:```bool use_put_requests =    args.GetInt(GRPC_ARG_TEST_ONLY_USE_PUT_REQUESTS).value_or(false);```You can probably even just avoid the local variable and inline this expression in the `HttpClientFilter` instantiation below.,
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/29332,850719091,2022-04-14T18:53:22Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -5769,6 +5974,79 @@ TEST_P(CdsTest, RingHashIdleToReady) {   EXPECT_EQ(GRPC_CHANNEL_READY, channel_->GetState(false)); } +// Test that the channel will transition to READY once it starts+// connecting even if there are no RPCs being sent to the picker.+TEST_P(CdsTest, RingHashContinuesConnectingWithoutPicks) {+  // Create EDS resource.+  CreateAndStartBackends(1);+  auto non_existant_endpoint = MakeNonExistantEndpoint();+  EdsResourceArgs args(+      {{""locality0"", {non_existant_endpoint, CreateEndpoint(0)}}});+  balancer_->ads_service()->SetEdsResource(BuildEdsResource(args));+  // Change CDS resource to use RING_HASH.+  auto cluster = default_cluster_;+  cluster.set_lb_policy(Cluster::RING_HASH);+  balancer_->ads_service()->SetCdsResource(cluster);+  // Add hash policy to RDS resource.+  auto new_route_config = default_route_config_;+  auto* route = new_route_config.mutable_virtual_hosts(0)->mutable_routes(0);+  auto* hash_policy = route->mutable_route()->add_hash_policy();+  hash_policy->mutable_header()->set_header_name(""address_hash"");+  SetListenerAndRouteConfiguration(balancer_.get(), default_listener_,+                                   new_route_config);+  // A long-running RPC, just used to send the RPC in another thread.+  LongRunningRpc rpc;+  // A connection injector that cancels the RPC after seeing the+  // connection attempt for the non-existant endpoint.+  class ConnectionInjector : public ConnectionAttemptInjector {+   public:+    ConnectionInjector(int port, LongRunningRpc* rpc)+        : port_(port), rpc_(rpc) {}++    void HandleConnection(grpc_closure* closure, grpc_endpoint** ep,+                          grpc_pollset_set* interested_parties,+                          const grpc_channel_args* channel_args,+                          const grpc_resolved_address* addr,+                          grpc_core::Timestamp deadline) override {+      {+        grpc_core::MutexLock lock(&mu_);+        const int port = grpc_sockaddr_get_port(addr);+        gpr_log(GPR_INFO, ""==> HandleConnection(): seen_port_=%d, port=%d"",+                seen_port_, port);+        // Initial attempt should be for port0_, which should fail.+        // Cancel the RPC at this point, so that it's no longer+        // queued when the LB policy updates the picker.+        if (!seen_port_ && port == port_) {+          gpr_log(GPR_INFO, ""*** CANCELLING RPC"");+          rpc_->CancelWithoutJoining();","Hmm, what if we instead changed the logic to be something like the following:1) Use a ConnectionAttemptInjector which captures connection attempts and queues them2) Test starts the RPC, causing connection to `port0_` to begin but pause.3) Test cancels the RPC and retrieves its status to make sure it's been cancelled4) Test resumes connection attempt to `port0_`5) Test calls `channel_->WaitForConnected(grpc_timeout_seconds_to_deadline(5)`",X
8864912,vjpai,https://api.github.com/repos/grpc/grpc/pulls/29404,850773508,2022-04-14T20:16:34Z,examples/cpp/route_guide/route_guide_callback_server.cc,"@@ -201,25 +201,27 @@ class RouteGuideImpl final : public RouteGuide::CallbackService {         StartRead(&note_);       }       void OnDone() override {-        // Collect the read_starter thread if needed-        if (read_starter_.joinable()) {-          read_starter_.join();-        }         delete this;       }       void OnReadDone(bool ok) override {         if (ok) {-          // We may need to wait an arbitary amount of time on this mutex-          // and we cannot delay the reaction, so start it in a thread-          // Collect the previous read_starter thread if needed-          if (read_starter_.joinable()) {-            read_starter_.join();-          }-          read_starter_ = std::thread([this] {-            mu_->lock();-            notes_iterator_ = received_notes_->begin();-            NextWrite();-          });+          // Unlike the other example in this directory that's not using+          // the reactor pattern, we can't grab a local lock to secure the+          // access to the notes vector, because the reactor will most likely+          // make us jump threads, so we'll have to use a different locking+          // strategy. We'll grab the lock locally to build a copy of the+          // list of nodes we're going to send, then we'll grab the lock+          // again to append the received note to the existing vector.+          mu_->lock();","BTW one of the side reasons for this example was to use a thread inside a callback since we should have some example of what to do with long blocking ops. So even if this code wasn't the right one for doing so, maybe there should be one somewhere.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29411,851316035,2022-04-15T14:56:29Z,test/cpp/end2end/xds/xds_end2end_test_lib.cc,"@@ -175,6 +176,11 @@ void XdsEnd2endTest::ServerThread::Serve(grpc_core::Mutex* mu,     builder.experimental().set_drain_grace_time(         test_obj_->xds_drain_grace_time_ms_);     builder.AddListeningPort(server_address, Credentials());+    // Allow gRPC Core's HTTP server to accept PUT requests for testing+    // purposes.+    builder.AddChannelArgument(+        GRPC_ARG_INTERNAL_ONLY_DO_NOT_USE_UNLESS_YOU_HAVE_PERMISSION_FROM_ALLOW_GRPC_TEAM_ALLOW_BROKEN_PUT_REQUESTS,","Instead of hard-coding this here, how about adding a parameter to `CreateBackends()` that will enable this, so that we can use it only in the tests that need it?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29408,851319135,2022-04-15T15:02:20Z,test/cpp/end2end/xds/xds_end2end_test.cc,"@@ -6071,11 +6090,7 @@ class XdsSecurityTest : public XdsEnd2endTest {     balancer_->ads_service()->SetEdsResource(BuildEdsResource(args));   } -  void TearDown() override {-    g_fake1_cert_data_map = nullptr;-    g_fake2_cert_data_map = nullptr;-    XdsEnd2endTest::TearDown();-  }+  void TearDown() override { XdsEnd2endTest::TearDown(); }",This override isn't needed at all anymore if all it's going to do is delegate to the base class.,
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29221,851433025,2022-04-15T18:47:39Z,test/cpp/end2end/xds/BUILD,"@@ -64,18 +71,26 @@ grpc_cc_library(         ""//src/proto/grpc/testing:echo_messages_proto"",         ""//src/proto/grpc/testing:echo_proto"",         ""//src/proto/grpc/testing/duplicate:echo_duplicate_proto"",-        ""//src/proto/grpc/testing/xds:ads_for_test_proto"",-        ""//src/proto/grpc/testing/xds:lrs_for_test_proto"",-        ""//src/proto/grpc/testing/xds/v3:ads_proto"",-        ""//src/proto/grpc/testing/xds/v3:cluster_proto"",-        ""//src/proto/grpc/testing/xds/v3:discovery_proto"",-        ""//src/proto/grpc/testing/xds/v3:endpoint_proto"",-        ""//src/proto/grpc/testing/xds/v3:http_connection_manager_proto"",-        ""//src/proto/grpc/testing/xds/v3:http_filter_rbac_proto"",-        ""//src/proto/grpc/testing/xds/v3:listener_proto"",-        ""//src/proto/grpc/testing/xds/v3:lrs_proto"",-        ""//src/proto/grpc/testing/xds/v3:route_proto"",-        ""//src/proto/grpc/testing/xds/v3:router_proto"",+        ""//src/proto/grpc/lookup/v1:rls_config_proto"",","Jan and I had some discussion about using the grpc/grpc-proto protos. The benefits are: 1. (as mentioned) remove the copy; 2. remove the hacks we did to override the proto package path (e.g., health.proto supposed to be located at grpc/health/v1/health.proto, but we put it under src/protos).That's probably a bigger issue. It's on my todo list.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29411,851447347,2022-04-15T19:23:05Z,src/core/ext/filters/http/server/http_server_filter.h,"@@ -26,10 +26,10 @@ /* Processes metadata on the server side for HTTP2 transports */ extern const grpc_channel_filter grpc_http_server_filter; -namespace grpc_core {-// Temporary code that allows servers to accept PUT requests. DO NOT USE WITHOUT-// PERMISSION.-void InternalOnlyDoNotUseUnlessYouHavePermissionFromGrpcTeamAllowBrokenPutRequests();-}  // namespace grpc_core+// A Temporary channel arg that allows servers to accept PUT requests. DO NOT+// USE WITHOUT PERMISSION.+#define GRPC_ARG_INTERNAL_ONLY_DO_NOT_USE_UNLESS_YOU_HAVE_PERMISSION_FROM_ALLOW_GRPC_TEAM_ALLOW_BROKEN_PUT_REQUESTS \",I think the way we do this is by adding a visibility tag like the following to the BUILD target:https://github.com/grpc/grpc/blob/55e870dae61ead220e0c74213410d90f8d21179f/BUILD#L715We then provide a definition for that tag here:https://github.com/grpc/grpc/blob/55e870dae61ead220e0c74213410d90f8d21179f/bazel/grpc_build_system.bzl#L103And we can provide a different definition for the same tag internally.We should probably add the OSS pieces here as part of this PR.,X
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/29422,852226794,2022-04-18T16:10:05Z,src/core/ext/filters/client_channel/lb_policy/ring_hash/ring_hash.cc,"@@ -701,11 +724,20 @@ void RingHash::RingHashSubchannelData::ProcessConnectivityChangeLocked(     }     p->channel_control_helper()->RequestReresolution();   }+  // Update bookkeeping if we were finished an internally triggered+  // connection attempt.+  bool internal_connection_attempt_complete =+      internal_connection_attempt_pending_ &&","Oh, I see the there were the two fields now. Okay, that's actually what I expected to see given our conversation. And it did seem to be valid, though the `internal_connection_attempt_complete` middleman does make it less obvious. I had only suggested what I did because I thought you were avoiding the per-subchannel state, but it does turn out easier to verify.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29411,852426683,2022-04-18T21:26:50Z,test/cpp/end2end/xds/xds_end2end_test_lib.h,"@@ -232,10 +232,12 @@ class XdsEnd2endTest : public ::testing::TestWithParam<XdsTestType> {      // If use_xds_enabled_server is true, the server will use xDS.     explicit ServerThread(XdsEnd2endTest* test_obj,-                          bool use_xds_enabled_server = false)+                          bool use_xds_enabled_server = false,+                          bool allow_put_requests = false)","Seems like we no longer need this parameter here, in `CreateBackends()`, in `CreateAndStartBackends()`, or in the `BackendServerThread` ctor.",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/29423,852504133,2022-04-19T00:34:08Z,src/python/grpcio_tests/tests_py3_only/interop/xds_interop_client.py,"@@ -294,18 +295,17 @@ def _run_single_channel(config: _ChannelConfiguration) -> None:                     continue                 else:                     duration_per_query = 1.0 / float(config.qps)-            request_id = None-            with _global_lock:-                request_id = _global_rpc_id-                _global_rpc_id += 1-                _global_rpcs_started[config.method] += 1-            start = time.time()-            end = start + duration_per_query-            with config.condition:+                request_id = None+                with _global_lock:+                    request_id = _global_rpc_id+                    _global_rpc_id += 1+                    _global_rpcs_started[config.method] += 1+                start = time.time()+                end = start + duration_per_query                 _start_rpc(config.method, config.metadata, request_id, stub,                            float(config.rpc_timeout_sec), futures)-            with config.condition:-                _remove_completed_rpcs(futures, config.print_response)+                print_response = config.print_response+            _remove_completed_rpcs(futures, config.print_response)","> In _remove_completed_rpcs, we are treating the RPCs with old spec and the RPCs with new spec the same way. The tear might still be a problem for slow-responding RPCs.> There is an alternative of cleaning the in-flight RPCs before making configuration changes. WDYT?I considered this as a potential cause of the problem and actually implemented the cancellation you're suggesting in an attempt to fix the bug. But in practice, this is not a problem because all RPCs are created with a timeout defined by the test harness. If we cancel RPCs, then we'll end up with another bad situation -- RPCs with cancellation status codes in the accumulated stats.If we decide to add this cancellation behavior, it should be added to the spec and to all other test clients.",X
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/29336,853514073,2022-04-19T21:48:24Z,tools/run_tests/performance/loadtest_config.py,"@@ -118,17 +118,54 @@ def gen_run_indices(runs_per_test: int) -> Iterable[str]:         yield index_fmt.format(i)  +def scenario_transform_function(+    client_channels: int, offered_loads: Iterable[int],+    async_server_threads: int+) -> Optional[Callable[[Iterable[Mapping[str, Any]]], Iterable[Mapping[str,+                                                                       Any]]]]:+    """"""Returns a transform to be applied to a list of scenarios.""""""+    if client_channels == 0 or len(","Perhaps instead of this, we should skip only ""if not any(client_channels, async_server_threads, len(offered_loads))"". I would list the arguments client_channels, async_server_threads and offered_loads in this order, since only the last one is a list. I might call the second one ""server_threads"" for short.",
6579971,paulosjca,https://api.github.com/repos/grpc/grpc/pulls/29336,853521171,2022-04-19T22:00:53Z,tools/run_tests/performance/loadtest_config.py,"@@ -378,6 +417,19 @@ def main() -> None:                       '--output',                       type=str,                       help='Output file name. Output to stdout if not set.')+    argp.add_argument(+        '-offered_loads_list',+        nargs=""*"",+        type=int,+        default=[],+        help='A list of offered loads that the tests are running with.')+    argp.add_argument('-client_channel',+                      type=int,+                      help='Number of client channel opened for the test')+    argp.add_argument(","I would call the argument '--server_threads' and in the description write 'number of async server threads'.I would also list the arguments in the logical order: client_channels, server_threads, offered_loads.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29448,853587472,2022-04-19T23:26:58Z,doc/xds-test-descriptions.md,"@@ -93,6 +93,35 @@ The test client changes its behavior right after receiving the `ClientConfigureRequest`. Currently it only supports configuring the type(s)  of RPCs sent by the test client, metadata attached to each type of RPCs, and the timeout. +### XdsUpdateServerConfigureService++The xDS test server's behavior can be dynamically changed in the middle of tests.+This is achieved by invoking the `XdsUpdateClientConfigureService` gRPC service+on the test server.++```+message EchoStatus {+  int32 code = 1;+  string message = 2;+}++message ServerConfigureRequest {+  // The status code with which to respond to all requests+  EchoStatus response_status = 1;",optional: use the canonical RPC status proto `google.rpc.status` to represent the response status: https://github.com/googleapis/googleapis/blob/ba7f434b7797df88d71545e9a6049949439f2698/google/rpc/status.proto#L35,X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29266,853663392,2022-04-20T01:59:43Z,src/core/lib/surface/channel.h,"@@ -92,68 +80,96 @@ struct CallRegistrationTable {   int method_registration_attempts ABSL_GUARDED_BY(mu) = 0; }; -}  // namespace grpc_core--struct grpc_channel {-  int is_client;-  grpc_compression_options compression_options;--  gpr_atm call_size_estimate;--  // TODO(vjpai): Once the grpc_channel is allocated via new rather than malloc,-  //              expand the members of the CallRegistrationTable directly into-  //              the grpc_channel. For now it is kept separate so that all the-  //              manual constructing can be done with a single call rather than-  //              a separate manual construction for each field.-  grpc_core::ManualConstructor<grpc_core::CallRegistrationTable>-      registration_table;-  grpc_core::RefCountedPtr<grpc_core::channelz::ChannelNode> channelz_node;-  grpc_core::ManualConstructor<grpc_core::MemoryAllocator> allocator;--  grpc_core::ManualConstructor<std::string> target;+class Channel : public RefCounted<Channel>,+                public CppImplOf<Channel, grpc_channel> {+ public:+  ~Channel() override;++  static absl::StatusOr<RefCountedPtr<Channel>> Create(+      const char* target, ChannelArgs args,+      grpc_channel_stack_type channel_stack_type,+      grpc_transport* optional_transport);++  static absl::StatusOr<RefCountedPtr<Channel>> CreateWithBuilder(+      ChannelStackBuilder* builder);++  grpc_channel_stack* channel_stack() const { return channel_stack_.get(); }++  grpc_compression_options compression_options() const {+    return compression_options_;+  }++  channelz::ChannelNode* channelz_node() const { return channelz_node_.get(); }++  size_t CallSizeEstimate() {+    // We round up our current estimate to the NEXT value of kRoundUpSize.+    // This ensures:+    //  1. a consistent size allocation when our estimate is drifting slowly+    //     (which is common) - which tends to help most allocators reuse memory+    //  2. a small amount of allowed growth over the estimate without hitting+    //     the arena size doubling case, reducing overall memory usage+    static constexpr size_t kRoundUpSize = 256;+    return (call_size_estimate_.load(std::memory_order_relaxed) ++            2 * kRoundUpSize) &+           ~(kRoundUpSize - 1);+  }++  void UpdateCallSizeEstimate(size_t size);+  absl::string_view target() const { return target_; }+  MemoryAllocator* allocator() { return &allocator_; }+  bool is_client() const { return is_client_; }+  RegisteredCall* RegisterCall(const char* method, const char* host);++  int TestOnlyRegisteredCalls() {+    MutexLock lock(&registration_table_.mu);+    return registration_table_.map.size();+  }++  int TestOnlyRegistrationAttempts() {+    MutexLock lock(&registration_table_.mu);+    return registration_table_.method_registration_attempts;+  }++ private:+  Channel(bool is_client, std::string target, ChannelArgs channel_args,+          grpc_compression_options compression_options,+          RefCountedPtr<grpc_channel_stack> channel_stack);++  const bool is_client_;+  const grpc_compression_options compression_options_;+  std::atomic<size_t> call_size_estimate_;+  CallRegistrationTable registration_table_;",maybe... or maybe we should make them real classes and have a hierarchy of state here... either way I think it should be a follow-up,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29433,854036722,2022-04-20T11:42:30Z,tools/internal_ci/helper_scripts/prepare_build_linux_rc,"@@ -41,4 +41,29 @@ export DOCKERHUB_ORGANIZATION=grpctesting  git submodule update --init +if [ ""${PREPARE_BUILD_INSTALL_DEPS_PYTHON}"" == ""true"" ]","We currently don't have the ""PREPARE_BUILD_INSTALL_*"" section for any of the languages for linux and I don't think we should have one.For vast majority of our linux builds, we rely on all the necessary dependencies being available in the docker images we have. We only use ""PREPARE_BUILD_INSTALL_*"" on macos and windows, where we can't use docker.The problem of these per-language setup snippets is that they 1. add complexity 2. take time (= slow down the builds) 3. increase flakiness (unless a lot of extra care is given).- Building python from source is slow and we really should not do that at the beginning of any of our linux jobs (there must be a better solution)- Strictly speaking, the name ""PREPARE_BUILD_INSTALL_DEPS_PYTHON"" doesn't actually fit your use case - since you're not actually installing dependencies for the grpc-python build, but dependencies for our build/test scripts (in this case for the extract_metadata_from_bazel.py). Since any of these scripts might potentially use type annotations (requires python3.6), it also means that you'd eventually need a slow ""build python from source"" step for almost all of our linux jobs (and I don't think that's acceptable).",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/29336,854427222,2022-04-20T18:18:55Z,tools/run_tests/performance/loadtest_config.py,"@@ -378,6 +440,18 @@ def main() -> None:                       '--output',                       type=str,                       help='Output file name. Output to stdout if not set.')+    argp.add_argument('--client_channels',+                      type=int,+                      help='Number of client channels.')+    argp.add_argument('--server_threads',+                      type=int,+                      help='Number of async server threads.')+    argp.add_argument(+        '--offered_loads',+        nargs=""*"",+        type=int,+        default=[],+        help='A list of offered loads that the tests are running with.')","This is a bit vague. How about ""A list of QPS values at which each load test scenario will be run.""?",X
5564114,Vignesh2208,https://api.github.com/repos/grpc/grpc/pulls/29470,855424171,2022-04-21T17:29:12Z,src/core/lib/iomgr/tcp_server_posix.cc,"@@ -221,7 +224,13 @@ static void on_read(void* arg, grpc_error_handle err) {     }      if (sp->server->memory_quota->IsMemoryPressureHigh()) {-      gpr_log(GPR_INFO, ""Drop incoming connection: high memory pressure"");+      int64_t dropped_connections_count = ++num_dropped_connections;",Sure I can add craig. I don't think it matters because the ++ operator is equivalent to fetch_add and we can probably tolerate race conditions also. I made it explicit now anyway,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29475,855576783,2022-04-21T20:40:57Z,doc/xds-test-descriptions.md,"@@ -17,6 +17,11 @@ Server should accept these arguments:     *   When set to true it uses XdsServerCredentials with the test server for security test cases.         In case of secure mode, port and maintenance_port should be different. +In addition, when handling requests, if the initial request metadata contains the `rpc-behavior` key, it should modify its handling of the request as follows:++ - If the value matches `sleep-<number>`, the server should wait the specified number of seconds before responding.+ - If the value matches `fail-on=<hostname>`, and the specified hostname matches the server's hostname, the server should respond to the request with an error with the code `ABORTED`.","Recommend to have a pre-defined error message, and the driver or client needs to check. This avoids this code being thrown by other part of the library, and we ignores it.",
961599,murgatroid99,https://api.github.com/repos/grpc/grpc/pulls/29475,855591571,2022-04-21T21:02:51Z,doc/xds-test-descriptions.md,"@@ -17,6 +17,11 @@ Server should accept these arguments:     *   When set to true it uses XdsServerCredentials with the test server for security test cases.         In case of secure mode, port and maintenance_port should be different. +In addition, when handling requests, if the initial request metadata contains the `rpc-behavior` key, it should modify its handling of the request as follows:",I didn't explicitly mention the CSV format because the protocol spec already says that separate metadata entries with the same key are semantically equivalent to a comma-separated list of values in a single metadata entry.,
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/29475,855638118,2022-04-21T22:29:27Z,doc/xds-test-descriptions.md,"@@ -17,6 +17,11 @@ Server should accept these arguments:     *   When set to true it uses XdsServerCredentials with the test server for security test cases.         In case of secure mode, port and maintenance_port should be different. +In addition, when handling requests, if the initial request metadata contains the `rpc-behavior` key, it should modify its handling of the request as follows:","I don't see anything that says this field is a CSV. And I see now we don't say we support multivalue headers. It is just ""multiple values"" but in a generic sense. We could say that `rpc-behavior` is a comma-separated list and multiple headers must be supported.The RFC does say how to combine multiple headers into one, but I don't think it would be clear to implementors of this that we actually expect that to work unless we explicitly mention it. The RFC does not say that every header must support the concatenation semantics. It just says that if there are duplicate keys, they can be combined together without impacting their semantics.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29394,856283493,2022-04-22T14:22:18Z,src/core/ext/transport/chttp2/transport/hpack_parser_table.h,"@@ -30,6 +30,40 @@  namespace grpc_core { +using Memento = ParsedMetadata<grpc_metadata_batch>;","This is probably too generic of a name to leave at the top level of the namespace.Since we're having some build problems anyway, let's remove it and make `MementoRingBuffer` a child class of `HPackTable`.Build problem:```var/local/git/grpc/src/core/ext/transport/chttp2/transport/hpack_parser_table.h:79:9: error: declaration of ‘using Memento = using Memento = class grpc_core::ParsedMetadata<grpc_metadata_batch>’ changes meaning of ‘Memento’ [-fpermissive]   79 |   using Memento = Memento;      |         ^~~~~~~/var/local/git/grpc/src/core/ext/transport/chttp2/transport/hpack_parser_table.h:33:7: note: ‘Memento’ declared here as ‘using Memento = class grpc_core::ParsedMetadata<grpc_metadata_batch>’   33 | using Memento = ParsedMetadata<grpc_metadata_batch>;      |       ^~~~~~~```",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29485,856515269,2022-04-22T19:31:16Z,include/grpc/event_engine/event_engine.h,"@@ -331,8 +336,11 @@ class EventEngine {   // de-experimentalize this API.   virtual bool IsWorkerThread() = 0; -  /// Creates and returns an instance of a DNSResolver.-  virtual std::unique_ptr<DNSResolver> GetDNSResolver() = 0;+  /// Creates and returns an instance of a DNSResolver, optionally configured by+  /// the \a options struct.+  virtual std::unique_ptr<DNSResolver> GetDNSResolver(+      const DNSResolver::ResolverOptions& options =",@ctiller tends to really dislike default-value parameters.  Might be better to just make this required; it's not that onerous to require the caller to instantiate an empty struct when they call this method.,X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29485,856531309,2022-04-22T19:56:47Z,include/grpc/event_engine/event_engine.h,"@@ -261,6 +261,11 @@ class EventEngine {     struct LookupTaskHandle {       intptr_t key[2];     };+    /// Optional configuration for DNSResolvers.+    struct ResolverOptions {+      /// Override the DNSResolver's default authority.","Good question. I'm in favor of limiting it to IP:port for YAGNI reasons: it avoids the added complexity of supporting a feature that can't be used today. Changing gRPC's URI policy later would technically mean a breaking change for the engine API, but one that most applications will probably never encounter. WDYT?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29344,858019302,2022-04-25T20:52:29Z,src/core/ext/filters/client_channel/retry_filter.cc,"@@ -1693,6 +1693,8 @@ void RetryFilter::CallData::CallAttempt::BatchData::RunClosuresForCompletedCall(  void RetryFilter::CallData::CallAttempt::BatchData::RecvTrailingMetadataReady(     void* arg, grpc_error_handle error) {+  RefCountedPtr<grpc_call_stack> owning_call_stack =","In principle, I don't think this should be needed.  The `BatchData` object is already holding a ref to the call stack (it takes it [here](https://github.com/grpc/grpc/blob/d2ff2bb75e6a3f14a30a021e4576fc35ae68d85f/src/core/ext/filters/client_channel/retry_filter.cc#L1319) and releases it [here](https://github.com/grpc/grpc/blob/d2ff2bb75e6a3f14a30a021e4576fc35ae68d85f/src/core/ext/filters/client_channel/retry_filter.cc#L1333)), and we're already holding a ref to the `BatchData` struct for the duration of this method, so I think we're guaranteed to already be holding a ref to the call stack here.The only difference I can see that this would make is that we would not release the last ref to the call stack until after the `BatchData` object is destroyed.  Is that what's causing the problem?  If so, why is it being triggered now when it wasn't being triggered before?Maybe the problem is just that the `BatchData` dtor is dropping its ref to the call stack before it drops its ref to the `CallAttempt` object?  Could we solve this by just reversing the order of those two lines in the `BatchData` dtor?",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29344,858084244,2022-04-25T22:49:45Z,src/core/lib/channel/promise_based_filter.cc,"@@ -70,6 +72,119 @@ void BaseCallData::Wakeup() {  void BaseCallData::Drop() { GRPC_CALL_STACK_UNREF(call_stack_, ""waker""); } +///////////////////////////////////////////////////////////////////////////////+// BaseCallData::CapturedBatch++namespace {+uintptr_t* RefCountField(grpc_transport_stream_op_batch* b) {+  return &b->handler_private.closure.error_data.scratch;+}+}  // namespace++BaseCallData::CapturedBatch::CapturedBatch() : batch_(nullptr) {}++BaseCallData::CapturedBatch::CapturedBatch(+    grpc_transport_stream_op_batch* batch) {+  *RefCountField(batch) = 1;+  batch_ = batch;+}++BaseCallData::CapturedBatch::~CapturedBatch() {+  if (batch_ == nullptr) return;+  // A ref can be dropped by destruction, but it must not release the batch+  uintptr_t& refcnt = *RefCountField(batch_);+  if (refcnt == 0) return;  // refcnt==0 ==> cancelled+  --refcnt;+  GPR_ASSERT(refcnt != 0);+}++BaseCallData::CapturedBatch::CapturedBatch(const CapturedBatch& rhs)+    : batch_(rhs.batch_) {+  uintptr_t& refcnt = *RefCountField(batch_);+  if (refcnt == 0) return;  // refcnt==0 ==> cancelled+  ++refcnt;+}++BaseCallData::CapturedBatch& BaseCallData::CapturedBatch::operator=(+    const CapturedBatch& b) {+  CapturedBatch temp(b);+  Swap(&temp);+  return *this;+}++BaseCallData::CapturedBatch::CapturedBatch(CapturedBatch&& rhs) noexcept+    : batch_(rhs.batch_) {+  rhs.batch_ = nullptr;+}++BaseCallData::CapturedBatch& BaseCallData::CapturedBatch::operator=(+    CapturedBatch&& b) noexcept {+  Swap(&b);+  return *this;+}++void BaseCallData::CapturedBatch::Release(Flusher* releaser) {+  auto* batch = absl::exchange(batch_, nullptr);+  GPR_ASSERT(batch != nullptr);+  uintptr_t& refcnt = *RefCountField(batch);+  if (refcnt == 0) return;  // refcnt==0 ==> cancelled+  if (--refcnt == 0) {+    releaser->Release(batch);+  }+}++void BaseCallData::CapturedBatch::Cancel(grpc_error_handle error,+                                         Flusher* releaser) {+  auto* batch = absl::exchange(batch_, nullptr);+  GPR_ASSERT(batch != nullptr);+  uintptr_t& refcnt = *RefCountField(batch);+  if (refcnt == 0) return;  // refcnt==0 ==> cancelled+  refcnt = 0;+  releaser->Cancel(batch, error);+}++///////////////////////////////////////////////////////////////////////////////+// BaseCallData::Flusher++BaseCallData::Flusher::Flusher(BaseCallData* call) : call_(call) {+  GRPC_CALL_STACK_REF(call_->call_stack(), ""flusher"");+}++BaseCallData::Flusher::~Flusher() {+  if (release_.empty()) {+    if (call_closures_.size() == 0) {+      GRPC_CALL_COMBINER_STOP(call_->call_combiner(), ""nothing to flush"");+      GRPC_CALL_STACK_UNREF(call_->call_stack(), ""flusher"");+      return;+    }+    call_closures_.RunClosures(call_->call_combiner());+    GRPC_CALL_STACK_UNREF(call_->call_stack(), ""flusher"");+    return;+  }+  for (size_t i = 1; i < release_.size(); i++) {+    struct CallNextOp {","We don't actually need this struct, since we could instead use the batch's `handler_private.closure` and `handler_private.extra_arg` fields for the closure and the `BaseCallData` pointer.  But if you don't care about the extra overhead of the dynamic allocation in this temporary code, it's fine as-is.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29344,858219235,2022-04-26T03:18:49Z,src/core/lib/channel/promise_based_filter.cc,"@@ -70,6 +72,119 @@ void BaseCallData::Wakeup() {  void BaseCallData::Drop() { GRPC_CALL_STACK_UNREF(call_stack_, ""waker""); } +///////////////////////////////////////////////////////////////////////////////+// BaseCallData::CapturedBatch++namespace {+uintptr_t* RefCountField(grpc_transport_stream_op_batch* b) {+  return &b->handler_private.closure.error_data.scratch;+}+}  // namespace++BaseCallData::CapturedBatch::CapturedBatch() : batch_(nullptr) {}++BaseCallData::CapturedBatch::CapturedBatch(+    grpc_transport_stream_op_batch* batch) {+  *RefCountField(batch) = 1;+  batch_ = batch;+}++BaseCallData::CapturedBatch::~CapturedBatch() {+  if (batch_ == nullptr) return;+  // A ref can be dropped by destruction, but it must not release the batch+  uintptr_t& refcnt = *RefCountField(batch_);+  if (refcnt == 0) return;  // refcnt==0 ==> cancelled+  --refcnt;+  GPR_ASSERT(refcnt != 0);+}++BaseCallData::CapturedBatch::CapturedBatch(const CapturedBatch& rhs)+    : batch_(rhs.batch_) {+  uintptr_t& refcnt = *RefCountField(batch_);+  if (refcnt == 0) return;  // refcnt==0 ==> cancelled+  ++refcnt;+}++BaseCallData::CapturedBatch& BaseCallData::CapturedBatch::operator=(+    const CapturedBatch& b) {+  CapturedBatch temp(b);+  Swap(&temp);+  return *this;+}++BaseCallData::CapturedBatch::CapturedBatch(CapturedBatch&& rhs) noexcept+    : batch_(rhs.batch_) {+  rhs.batch_ = nullptr;+}++BaseCallData::CapturedBatch& BaseCallData::CapturedBatch::operator=(+    CapturedBatch&& b) noexcept {+  Swap(&b);+  return *this;+}++void BaseCallData::CapturedBatch::Release(Flusher* releaser) {+  auto* batch = absl::exchange(batch_, nullptr);+  GPR_ASSERT(batch != nullptr);+  uintptr_t& refcnt = *RefCountField(batch);+  if (refcnt == 0) return;  // refcnt==0 ==> cancelled+  if (--refcnt == 0) {+    releaser->Release(batch);+  }+}++void BaseCallData::CapturedBatch::Cancel(grpc_error_handle error,+                                         Flusher* releaser) {+  auto* batch = absl::exchange(batch_, nullptr);+  GPR_ASSERT(batch != nullptr);+  uintptr_t& refcnt = *RefCountField(batch);+  if (refcnt == 0) return;  // refcnt==0 ==> cancelled+  refcnt = 0;+  releaser->Cancel(batch, error);+}++///////////////////////////////////////////////////////////////////////////////+// BaseCallData::Flusher++BaseCallData::Flusher::Flusher(BaseCallData* call) : call_(call) {+  GRPC_CALL_STACK_REF(call_->call_stack(), ""flusher"");+}++BaseCallData::Flusher::~Flusher() {+  if (release_.empty()) {+    if (call_closures_.size() == 0) {+      GRPC_CALL_COMBINER_STOP(call_->call_combiner(), ""nothing to flush"");+      GRPC_CALL_STACK_UNREF(call_->call_stack(), ""flusher"");+      return;+    }+    call_closures_.RunClosures(call_->call_combiner());+    GRPC_CALL_STACK_UNREF(call_->call_stack(), ""flusher"");+    return;+  }+  for (size_t i = 1; i < release_.size(); i++) {+    struct CallNextOp {+      grpc_closure closure;+      grpc_transport_stream_op_batch* batch;+      BaseCallData* call;++      static void Run(void* p, grpc_error_handle) {+        auto* self = static_cast<CallNextOp*>(p);+        grpc_call_next_op(self->call->elem(), self->batch);+        GRPC_CALL_STACK_UNREF(self->call->call_stack(), ""flusher_batch"");+        delete self;+      }+    };+    auto* op = absl::make_unique<CallNextOp>().release();",clang-tidy worries about exceptions being raised in destructors. It disappears with your other suggestion.,
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/29443,858255302,2022-04-26T04:24:43Z,BUILD,"@@ -4325,23 +4325,49 @@ grpc_cc_library(     ], ) +grpc_cc_library(+    name = ""tsi_ssl_session_cache"",+    srcs = [+        ""src/core/tsi/ssl/session_cache/ssl_session_boringssl.cc"",+        ""src/core/tsi/ssl/session_cache/ssl_session_cache.cc"",+        ""src/core/tsi/ssl/session_cache/ssl_session_openssl.cc"",+    ],+    hdrs = [+        ""src/core/tsi/ssl/session_cache/ssl_session.h"",+        ""src/core/tsi/ssl/session_cache/ssl_session_cache.h"",+    ],+    external_deps = [+        ""absl/strings"",+        ""libssl"",+        ""libcrypto"",+    ],+    language = ""c++"",+    visibility = [""@grpc:public""],+    deps = [",Overall this is looking great! Did you get a chance to clean up the unused dependencies here as we discussed last time?,X
1535036,tamird,https://api.github.com/repos/grpc/grpc/pulls/29498,858850489,2022-04-26T15:18:51Z,tools/run_tests/sanity/check_banned_filenames.py,"@@ -0,0 +1,27 @@+#!/usr/bin/env python3++# Copyright 2022 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++import os+import sys++BANNED_FILENAMES = [+    'BUILD.gn',+]++for filename in BANNED_FILENAMES:+    if os.path.exists(filename):+        print('%s should not exist' % filename)+        sys.exit(1)",consider adding a boolean - this will list only the first banned file in case multiple appear,
4210911,anicr7,https://api.github.com/repos/grpc/grpc/pulls/29478,858928054,2022-04-26T16:32:03Z,test/core/address_utils/sockaddr_utils_fuzzer_test.cc,"@@ -0,0 +1,43 @@+//+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#include <stdbool.h>+#include <stdint.h>+#include <string.h>++#include <grpc/grpc.h>++#include ""src/core/lib/address_utils/sockaddr_utils.h""+#include ""src/core/lib/iomgr/resolve_address.h""+#include ""src/core/lib/uri/uri_parser.h""++bool squelch = true;++extern ""C"" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {+  if (size > GRPC_MAX_SOCKADDR_SIZE) return 0;+  grpc_resolved_address address;+  memset(&address, 0, sizeof(address));+  memcpy(address.addr, data, size);+  address.len = size;++  absl::StatusOr<std::string> uri = grpc_sockaddr_to_uri(&address);+  if (!uri.ok()) return 0;+  absl::StatusOr<grpc_core::URI> parsed_uri =+      grpc_core::URI::Parse(uri.value());++  GPR_ASSERT(parsed_uri.ok());","Craig, This sounds like a good idea. I tried these out locally and I see a lot of failures now. A lot of these failures I feel are majorly due to the test setup. Currently we arbitrary allow any data that the fuzzer creates but grpc_parse_uri performs validation when converting the URI back to resolved address. For example for IPV6 addresses the scope has to be valid which while constructing the resolved_address, the test didn't confirm it was as the grpc_resolved_address is just memcpy of the data.I can change the logic to be more sophisticated here and only proceed with  grpc_parse_uri if the address was truly valid. If there are better ways for me to construct the resolved_address let me know.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29419,859814115,2022-04-27T13:39:40Z,src/csharp/Grpc.IntegrationTesting/UnobservedTaskExceptionTest.cs,"@@ -96,5 +96,31 @@ public async Task NoUnobservedTaskExceptionForAbandonedStreamingResponse()              Assert.AreEqual(0, unobservedTaskExceptionCounter.Count);         }+        [Test]+        public async Task NoUnobservedTaskExceptionForUnavailableServers()+        {+            TaskScheduler.UnobservedTaskException += (sender, args) => {+                Assert.Fail(""UnobservedTaskException hit."");+            };+            var chan = new Channel(""localhost:100"", ChannelCredentials.Insecure);+            var client = new TestService.TestServiceClient(chan);+            for (int i = 0; i < 3; i++)+                try+                {+                    var call = client.FullDuplexCall();+                    using (call)+                    {+                        var req = call.RequestStream;+                        await req.WriteAsync(new StreamingOutputCallRequest());+                    }+                }+                catch (RpcException err)+                {+                    Console.WriteLine(err.Status.StatusCode);+                }+            await chan.ShutdownAsync();+            GC.Collect();+            GC.WaitForPendingFinalizers();+        }",the other test checks that there we no unobserved exceptions with a counter. Isn't that something that this test should do as well?,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29419,859842493,2022-04-27T14:04:14Z,src/csharp/Grpc.Core/Internal/AsyncCall.cs,"@@ -648,13 +648,13 @@ private void HandleFinished(bool success, ClientSideStatus receivedStatus)             if (status.StatusCode != StatusCode.OK)             {                 streamingResponseCallFinishedTcs.SetException(new RpcException(status, receivedStatus.Trailers));-                if (status.StatusCode == StatusCode.Cancelled || origCancelRequested)+                if (status.StatusCode == StatusCode.Cancelled || status.StatusCode == StatusCode.Unavailable || origCancelRequested)                 {                     // Make sure the exception set to the Task is observed,","For extra context, here's my understanding of why we're seeing the UnobservedException at all. I think the issue is with the way the streaming responses are processed.The call.ResponseStream.MoveNext() reads the next response message and only once it  gets reaches a null response,it observes the result of  `call.StreamingResponseCallFinishedTask` here:https://github.com/grpc/grpc/blob/1cd6e69347cbf62a012477fe184ee6fa8f25d32c/src/csharp/Grpc.Core/Internal/ClientResponseStream.cs#L59So if MoveNext() is not running (or if it doesn't exhaust the response stream) the exception coming from call.StreamingResponseCallFinishedTask will never be observed (which later shows up in finalizer as unobserved exception).Note that the call.StreamingResponseCallFinishedTask ""starts"" as soon a the call is started (but there may or may not be a call to responseStream.MoveNext().",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/28985,859928684,2022-04-27T15:16:44Z,doc/server_side_auth.md,"@@ -12,6 +12,9 @@ The authentication context is structured as a multi-map of key-value pairs - the  The contents of the *auth properties* are populated by an *auth interceptor*. The interceptor also chooses which property key will act as the peer identity (e.g. for client certificate authentication this property will be `""x509_common_name""` or `""x509_subject_alternative_name""`). +Note that AuthContext is not modifiable, unless AuthMetadataProcessor is used([reference](https://github.com/grpc/grpc/blob/master/include/grpcpp/impl/codegen/security/auth_context.h#L89)). ","Suggest writing this paragraph as follows:""""""Note that AuthContext is generally not modifiable, except when used via an AuthMetadataProcessor.  However, because the AuthContext is a connection-level object, when it is modified via an AuthMetadataProcessor, the modifications will be visible on all subsequent calls on the same connection.""""""",
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/29475,860337196,2022-04-28T00:13:42Z,doc/xds-test-descriptions.md,"@@ -20,8 +20,12 @@ Server should accept these arguments: In addition, when handling requests, if the initial request metadata contains the `rpc-behavior` key, it should modify its handling of the request as follows:   - If the value matches `sleep-<int>`, the server should wait the specified number of seconds before responding.- - If the value matches `fail=<status code as int>`, the server should respond with the specified status code.- - A value can have a prefix `hostname=<string>` followed by a space. In that case, the rest of the value should only be applied if the specified hostname matches the server's hostname. + - If the value matches `keep-open`, the server should never respond to the request.+ - If the value matches `success-on-retry-attempt-<int>`, the server should force an OK response if the value of the `grpc-previous-rpc-attempts` metadata field is equal to the specified number.+ - If the value matches `error-code-<int>`, the server should respond with the specified status code.+ - A value can have a prefix `hostname=<string>` followed by a space. In that case, the rest of the value should only be applied if the specified hostname matches the server's hostname.++If a request has multiple `rpc-behavior` metadata values, they should be applied in the order listed.","I don't think this is sufficient. You really have to specify how to parse a single entry, because that is key-specific. The HTTP spec does not specify the format for a single-value key, even if it was created by concatenating a multi-value key.> Each `rpc-behavior` header value (as there may be multiple) should be split on `,` and the resulting actions should be processed in-order.To contrast, we aren't supporting `hostname=hello\,world` or `hostname=""bad,name""`, which for some HTTP multi-valued headers would be allowed.",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/28666,861581984,2022-04-29T08:39:18Z,src/ruby/lib/grpc/grpc.rb,"@@ -16,9 +16,9 @@   ruby_version_dirname = /(\d+\.\d+)/.match(RUBY_VERSION).to_s   distrib_lib_dir = File.expand_path(ruby_version_dirname,                                      File.dirname(__FILE__))-  if File.directory?(distrib_lib_dir)+  begin","The only problem with this change is that if someone downloaded the cross-compiled package but for some reason it fails to load, then that error will be silently `rescue`'d and most likely they will get the second `LoadError` exception, which will have much less useful debug info (since what they most likely want to see is the first `LoadError`).How about we keep the original logic, but just change `File.directory?` check to:```if !Dir.glob(""#{distrib_lib_dir}/grpc_c*"").empty?```",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/29476,861993926,2022-04-29T17:11:15Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/network_security.py,"@@ -88,8 +88,9 @@ def from_response(cls, name: str,                    rules=response.get('rules', []))  -class _NetworkSecurityBase(gcp.api.GcpStandardCloudApiResource,-                           metaclass=abc.ABCMeta):+class _NetworkSecurityBase(  # pylint: disable=abstract-method","That's strange. This is an abstract class on it's own. Apparently pylint is bad at detecting it: https://stackoverflow.com/questions/39256350/pylint-cannot-handle-abstract-subclasses-of-abstract-base-classesLet's put `pylint: disable` inside the class, and add the comment why this is disabled, with a TODO of removing it  once pylint is better at recognizing abstract classes inheriting from other abstract classes.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/29476,862007265,2022-04-29T17:30:21Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_testcase.py,"@@ -427,9 +425,13 @@ def assertNumEndpoints(self, xds_config: DumpedXdsConfig, k: int) -> None:             f'insufficient endpoints in EDS: want={k} seen={xds_config.endpoints}'         ) -    def assertRpcStatusCode(self, test_client: XdsTestClient, *,-                            expected: Iterable[ExpectedResult], length: int,-                            tolerance: float) -> None:+    def assertRpcStatusCode(+            self,+            test_client: XdsTestClient,+            *,  # pylint: disable=too-many-locals",Why in the middle of declaring arguments?,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29476,862082944,2022-04-29T19:17:32Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/iam.py,"@@ -54,7 +53,7 @@ def _replace_binding(policy: 'Policy', binding: 'Policy.Binding',     new_bindings = set(policy.bindings)     new_bindings.discard(binding)     new_bindings.add(new_binding)-    return dataclasses.replace(policy, bindings=frozenset(new_bindings))+    return dataclasses.replace(policy, bindings=frozenset(new_bindings))  # pylint: disable=too-many-function-args",Good catch. This is still a mystery to me. Here is the raw pylint complaint:```tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/iam.py:56:11: E1121: Too many positional arguments for function call (too-many-function-args)```Our usage complies with official doc: https://docs.python.org/3/library/dataclasses.html#dataclasses.replaceI suspect it might be a PyLint issue with `dataclasses` library.,
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/29475,862117809,2022-04-29T20:13:54Z,doc/xds-test-descriptions.md,"@@ -20,8 +20,12 @@ Server should accept these arguments: In addition, when handling requests, if the initial request metadata contains the `rpc-behavior` key, it should modify its handling of the request as follows:   - If the value matches `sleep-<int>`, the server should wait the specified number of seconds before responding.- - If the value matches `fail=<status code as int>`, the server should respond with the specified status code.- - A value can have a prefix `hostname=<string>` followed by a space. In that case, the rest of the value should only be applied if the specified hostname matches the server's hostname. + - If the value matches `keep-open`, the server should never respond to the request.+ - If the value matches `success-on-retry-attempt-<int>`, the server should force an OK response if the value of the `grpc-previous-rpc-attempts` metadata field is equal to the specified number.","I don't think it is ""don't do `error-code-`"" as much as ""don't process any more behaviors."" It is a terminal state, essentially. As written now it sounds like if an `error-code-` exists earlier in the list this will retroactively ignore it.Does something like this work better? - If the value matches `sleep-<int>`, the server should wait the specified number of seconds before resuming behavior matching and RPC processing. - If the value matches `keep-open`, the server should never respond to the request and behavior matching ends. - If the value matches `error-code-<int>`, the server should respond with the specified status code and behavior matching ends. - If the value matches `success-on-retry-attempt-<int>` and the value of the `grpc-previous-rpc-attempts` metadata field is equal to the specified number, the normal RPC processing should resume and behavior matching ends.I think there's some other ways this could be phrased, but I was trying to sort of match the existing style. Some rearrangements could reduce the verbosity, but we've already spent more time on wording this than we were hoping for.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29476,862134465,2022-04-29T20:35:47Z,tools/run_tests/xds_k8s_test_driver/framework/infrastructure/gcp/network_security.py,"@@ -88,8 +88,9 @@ def from_response(cls, name: str,                    rules=response.get('rules', []))  -class _NetworkSecurityBase(gcp.api.GcpStandardCloudApiResource,-                           metaclass=abc.ABCMeta):+class _NetworkSecurityBase(  # pylint: disable=abstract-method","TIL this is pylint failed to detect whether a class is abstract or not. The disable note is moved into the class scope (TIL we can do that).Surprisingly I couldn't find a matching GitHub issue in PyLint repo, created https://github.com/grpc/grpc/issues/29532.Same changes applied to another `network_services.py` as well.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29476,862139680,2022-04-29T20:45:05Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_testcase.py,"@@ -362,13 +360,13 @@ def tearDownClass(cls):      def _fetch_and_check_xds_config(self):         # Cleanup state for this attempt-        self._xds_json_config = None+        self._xds_json_config = None  # pylint: disable=attribute-defined-outside-init","Todo added.I attempted that approach, the problem is `unittest.TestCase` contains too much magic. Theoretically, the variable declaration should happen in the constructor, but the constructor is invoked by `unittest`'s internal code. To keep magic working, we might make the `__init__` function overly complex. There might be an elegant way to pass the last-seen config to caller across the retry-layer. Feel free to suggest.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/29476,862186160,2022-04-29T21:43:05Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_testcase.py,"@@ -362,13 +360,13 @@ def tearDownClass(cls):      def _fetch_and_check_xds_config(self):         # Cleanup state for this attempt-        self._xds_json_config = None+        self._xds_json_config = None  # pylint: disable=attribute-defined-outside-init","Wait, can we add it as a class field? By adding this to line 248?```py_xds_json_config: Optional[dict] = None```",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29476,862257602,2022-04-29T23:35:09Z,tools/run_tests/xds_k8s_test_driver/framework/xds_url_map_testcase.py,"@@ -362,13 +360,13 @@ def tearDownClass(cls):      def _fetch_and_check_xds_config(self):         # Cleanup state for this attempt-        self._xds_json_config = None+        self._xds_json_config = None  # pylint: disable=attribute-defined-outside-init","We might not able to. Technically, this is a per-instance state. A class field might be a global state (since there is only one class for each test). On the other hand, the UrlMapTestCase class only have one test case per child class. It will be down to how Python store class variables:- If all class variables are stored in the child class, the suggested script will work and we can lift this ""disable"" comment- If not, the suggested script might encounter data race.People suggests to not put mutable objects as class variables: https://stackoverflow.com/questions/13404476/inherited-class-variable-modification-in-python",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29476,862258074,2022-04-29T23:37:26Z,tools/run_tests/xds_k8s_test_driver/tests/failover_test.py,"@@ -70,14 +70,14 @@ def test_failover(self) -> None:         with self.subTest('04_create_forwarding_rule'):             self.td.create_forwarding_rule(self.server_xds_port) +        default_test_servers: List[_XdsTestServer] = []+        alternate_test_servers: List[_XdsTestServer] = []","TIL it will end up with ""not defined"" instead of giving it a value... This is very different than other languages. I will update all test cases to use the suggested pattern.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29523,864261953,2022-05-03T21:07:33Z,src/core/lib/transport/metadata_batch.h,"@@ -553,6 +548,17 @@ struct GrpcStatusContext {  namespace metadata_detail { +// Build a key/value formatted debug string.","Needs further explanation. It's not obvious from the class definition that this produces `""key1: value1, key2: value2""`",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29574,865084322,2022-05-04T17:18:41Z,src/core/lib/transport/metadata_batch.h,"@@ -22,19 +22,38 @@ #include <grpc/support/port_platform.h>  #include <stdbool.h>+#include <stdlib.h>+#include <string.h> +#include <algorithm> #include <cstdint> #include <limits>+#include <string>+#include <type_traits>+#include <utility>++#include ""absl/container/inlined_vector.h""","A bunch yes, I think this is a mostly disjoint set that we're actually using.And here's the value in this little project - once we actually remove usage IWYU will remove the headers and we'll know we're actually done (and we'll notice in a code review when a new dependency appears)",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29591,866925211,2022-05-06T15:11:32Z,src/core/lib/event_engine/memory_allocator.cc,"@@ -41,7 +41,7 @@ class SliceRefCount : public grpc_slice_refcount {   static void Destroy(grpc_slice_refcount* p) {     auto* rc = static_cast<SliceRefCount*>(p);     rc->~SliceRefCount();-    gpr_free(rc);","We were accidentally getting `grpc/support/alloc.h` via another path and so it was either add that or change these, and we get no value at this point from the gpr variants...",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29593,867257830,2022-05-06T22:58:24Z,test/distrib/bazel/test_single_bazel_version.sh,"@@ -51,12 +51,15 @@ FAILED_TESTS="""" export OVERRIDE_BAZEL_VERSION=""$VERSION"" # when running under bazel docker image, the workspace is read only. export OVERRIDE_BAZEL_WRAPPER_DOWNLOAD_DIR=/tmp-bazel build -- //... ""${EXCLUDED_TARGETS[@]}"" || FAILED_TESTS=""${FAILED_TESTS}Build ""+# clean the caches and downloaded archives to make sure we build from scratch+tools/bazel clean --expunge+tools/bazel build -- //... ""${EXCLUDED_TARGETS[@]}"" || FAILED_TESTS=""${FAILED_TESTS}Build ""+# tools/bazel build -- //:grpc ""${EXCLUDED_TARGETS[@]}"" || FAILED_TESTS=""${FAILED_TESTS}Build ""  for TEST_DIRECTORY in ""${TEST_DIRECTORIES[@]}""; do   pushd ""test/distrib/bazel/$TEST_DIRECTORY/"" -  bazel test --test_output=all //:all || FAILED_TESTS=""${FAILED_TESTS}${TEST_DIRECTORY} Distribtest""+  tools/bazel test --cache_test_results=no --test_output=all //:all || FAILED_TESTS=""${FAILED_TESTS}${TEST_DIRECTORY} Distribtest""",There is a copied `tools/bazel` at https://github.com/grpc/grpc/blob/7942f30f6301f5baf93ea242d13433349640fc57/test/distrib/bazel/cpp/tools/bazel,X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29579,867287505,2022-05-07T01:59:31Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -54,22 +63,27 @@ #include ""src/core/ext/filters/client_channel/subchannel_interface.h"" #include ""src/core/ext/filters/client_channel/subchannel_interface_internal.h"" #include ""src/core/ext/filters/deadline/deadline_filter.h""-#include ""src/core/lib/backoff/backoff.h"" #include ""src/core/lib/channel/channel_args.h"" #include ""src/core/lib/channel/channel_stack.h""-#include ""src/core/lib/channel/connected_channel.h""-#include ""src/core/lib/channel/status_util.h""-#include ""src/core/lib/gpr/string.h""+#include ""src/core/lib/channel/channel_trace.h""+#include ""src/core/lib/config/core_configuration.h""+#include ""src/core/lib/debug/trace.h""+#include ""src/core/lib/gpr/useful.h""+#include ""src/core/lib/gprpp/chunked_vector.h""","I'm pretty sure it's a bug, but a harmless one.Instantiating line 2498, `batch_->Encode(&encoder);` ends up expanding `ChunkedVector<pair<Slice, Slice>>::ConstForwardIterator`, which gets misattributed to this file via the IWYU heuristics.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29593,867699995,2022-05-09T07:12:11Z,test/distrib/bazel/test_single_bazel_version.sh,"@@ -51,12 +51,31 @@ FAILED_TESTS="""" export OVERRIDE_BAZEL_VERSION=""$VERSION"" # when running under bazel docker image, the workspace is read only. export OVERRIDE_BAZEL_WRAPPER_DOWNLOAD_DIR=/tmp-bazel build -- //... ""${EXCLUDED_TARGETS[@]}"" || FAILED_TESTS=""${FAILED_TESTS}Build ""++# pick the Bazel wrapper+BAZEL=""$(pwd)/tools/bazel""++# validate the Bazel version+if ! ($BAZEL version | grep -q ""${VERSION}""); then+  echo ""Incorrect Bazel version! Want=${VERSION} Seen=$($BAZEL version)""+  exit 1+fi++# clean the caches and downloaded archives to make sure we build from scratch+$BAZEL clean --expunge","Requiring ""bazel clean"" seems speculative. Do we really have a clear evidence that this is necessary?I'm pretty sure bazel is not stupid and it recognizes that if you change the bazel version, a full rebuild is needed.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/29623,868332367,2022-05-09T18:59:06Z,doc/xds-test-descriptions.md,"@@ -745,3 +745,36 @@ the forwarding rule port `80` and the URL map host rule `myservice`. 1. No traffic goes to backends when configuring the target URI `xds:///myservice`, the forwarding rule port `80` and the host rule  `myservice::80`.++### outlier_detection+This test verifies that the client applies the outlier detection configuration+and temporarily drops traffic to a server that fails requests.++Client parameters:++1.  --num_channels=1+2.  --qps=100","We've been doing 25 qps in the new framework (no particular reason, just an observation).```suggestion2.  --qps=25```https://github.com/grpc/grpc/blob/ecde2a9f26bcab956f8431b406683131966d6605/tools/run_tests/xds_k8s_test_driver/framework/test_app/client_app.py#L291",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/29623,868365037,2022-05-09T19:31:14Z,doc/xds-test-descriptions.md,"@@ -745,3 +745,36 @@ the forwarding rule port `80` and the URL map host rule `myservice`. 1. No traffic goes to backends when configuring the target URI `xds:///myservice`, the forwarding rule port `80` and the host rule  `myservice::80`.++### outlier_detection+This test verifies that the client applies the outlier detection configuration+and temporarily drops traffic to a server that fails requests.++Client parameters:++1.  --num_channels=1+2.  --qps=100++Load balancer configuration:++1.  One MIG with five backends, with a `backendService` configuration with the","For most tests, this is not true anymore. If this interop test is implemented in the new framework, it will only be NEG. Practically both NEG and MIG serve the same purpose: providing the list of endpoints and their statuses. I wish there was a common term for NEG and MIG. ",
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/29593,868385938,2022-05-09T19:51:23Z,test/distrib/bazel/test_single_bazel_version.sh,"@@ -51,12 +51,31 @@ FAILED_TESTS="""" export OVERRIDE_BAZEL_VERSION=""$VERSION"" # when running under bazel docker image, the workspace is read only. export OVERRIDE_BAZEL_WRAPPER_DOWNLOAD_DIR=/tmp-bazel build -- //... ""${EXCLUDED_TARGETS[@]}"" || FAILED_TESTS=""${FAILED_TESTS}Build ""++# pick the Bazel wrapper+BAZEL=""$(pwd)/tools/bazel""++# validate the Bazel version+if ! ($BAZEL version | grep -q ""${VERSION}""); then+  echo ""Incorrect Bazel version! Want=${VERSION} Seen=$($BAZEL version)""+  exit 1+fi++# clean the caches and downloaded archives to make sure we build from scratch+$BAZEL clean --expunge++# test building all targets+$BAZEL build -- //... ""${EXCLUDED_TARGETS[@]}"" || FAILED_TESTS=""${FAILED_TESTS}Build ""  for TEST_DIRECTORY in ""${TEST_DIRECTORIES[@]}""; do   pushd ""test/distrib/bazel/$TEST_DIRECTORY/"" -  bazel test --test_output=all //:all || FAILED_TESTS=""${FAILED_TESTS}${TEST_DIRECTORY} Distribtest""+  # validate the Bazel version again, since we have a different WORKSPACE file+  if ! ($BAZEL version | grep -q ""${VERSION}""); then",This is duplicated. You probably want to factor out a function for this.,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29367,869355321,2022-05-10T15:05:56Z,include/grpc/event_engine/slice.h,"@@ -0,0 +1,300 @@+// Copyright 2021 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_EVENT_ENGINE_SLICE_H+#define GRPC_EVENT_ENGINE_SLICE_H++#include <grpc/support/port_platform.h>+","Let's leave a breadcrumb probably here and in src/core/lib/slice.h that this file and that file are very similar, and so likely changes in one warrant changes in the other.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29581,869414294,2022-05-10T15:49:13Z,src/core/ext/filters/client_channel/resolver/dns/c_ares/dns_resolver_ares.cc,"@@ -358,74 +362,83 @@ class AresClientChannelDNSResolverFactory : public ResolverFactory {  class AresDNSResolver : public DNSResolver {  public:-  class AresRequest : public DNSResolver::Request {+  class AresRequest {    public:     AresRequest(         absl::string_view name, absl::string_view default_port,         grpc_pollset_set* interested_parties,         std::function<void(absl::StatusOr<std::vector<grpc_resolved_address>>)>-            on_resolve_address_done)+            on_resolve_address_done,+        AresDNSResolver* resolver, intptr_t aba_token)         : name_(std::string(name)),           default_port_(std::string(default_port)),           interested_parties_(interested_parties),-          on_resolve_address_done_(std::move(on_resolve_address_done)) {+          on_resolve_address_done_(std::move(on_resolve_address_done)),+          completed_(false),+          resolver_(resolver),+          aba_token_(aba_token) {       GRPC_CARES_TRACE_LOG(""AresRequest:%p ctor"", this);       GRPC_CLOSURE_INIT(&on_dns_lookup_done_, OnDnsLookupDone, this,                         grpc_schedule_on_exec_ctx);+      MutexLock lock(&mu_);+      ares_request_ = std::unique_ptr<grpc_ares_request>(grpc_dns_lookup_ares(+          /*dns_server=*/"""", name_.c_str(), default_port_.c_str(),+          interested_parties_, &on_dns_lookup_done_, &addresses_,+          /*balancer_addresses=*/nullptr, /*service_config_json=*/nullptr,+          GRPC_DNS_ARES_DEFAULT_QUERY_TIMEOUT_MS));+      GRPC_CARES_TRACE_LOG(""AresRequest:%p Start ares_request_:%p"", this,+                           ares_request_.get());     } -    ~AresRequest() override {+    ~AresRequest() {       GRPC_CARES_TRACE_LOG(""AresRequest:%p dtor ares_request_:%p"", this,                            ares_request_.get());+      resolver_->UnregisterRequest(task_handle());     } -    void Start() override {+    bool Cancel() {","The Request base class is gone, no override required.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29367,869650053,2022-05-10T20:23:52Z,src/core/lib/event_engine/slice_buffer.cc,"@@ -0,0 +1,52 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include <grpc/event_engine/slice_buffer.h>+#include <grpc/slice.h>+#include <grpc/slice_buffer.h>++#include ""src/core/lib/slice/slice.h""+#include ""src/core/lib/slice/slice_internal.h""+#include ""src/core/lib/slice/slice_refcount.h""++namespace grpc_event_engine {+namespace experimental {++void SliceBuffer::Append(Slice slice) {+  grpc_slice_buffer_add(&slice_buffer_, slice.TakeCSlice());+}++size_t SliceBuffer::AppendIndexed(Slice slice) {+  return grpc_slice_buffer_add_indexed(&slice_buffer_, slice.TakeCSlice());+}++Slice SliceBuffer::TakeFirst() {+  return Slice(grpc_slice_buffer_take_first(&slice_buffer_));+}++void SliceBuffer::Prepend(Slice slice) {+  grpc_slice_buffer_undo_take_first(&slice_buffer_, slice.TakeCSlice());+}++Slice SliceBuffer::RefSlice(size_t index) {+  if (index >= Count()) {",I'd prefer this to just crash hard... working around it is likely to hide some bug,X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29304,870358081,2022-05-11T14:14:27Z,tools/internal_ci/macos/grpc_distribtests_ruby.sh,"@@ -15,42 +15,4 @@  set -ex -# avoid slow finalization after the script has exited.-source $(dirname $0)/../../../tools/internal_ci/helper_scripts/move_src_tree_and_respawn_itself_rc--# change to grpc repo root-cd $(dirname $0)/../../..--export PREPARE_BUILD_INSTALL_DEPS_RUBY=true-source tools/internal_ci/helper_scripts/prepare_build_macos_rc--# TODO(jtattermusch): can some of these steps be removed?-# needed to build ruby artifacts-gem install rubygems-update-update_rubygems-time bash tools/distrib/build_ruby_environment_macos.sh--# Build all ruby macos artifacts (this step actually builds all the native and source gems)-tools/run_tests/task_runner.py -f artifact macos ruby ${TASK_RUNNER_EXTRA_FILTERS} -j 4 -x build_artifacts/sponge_log.xml || FAILED=""true""--# Ruby ""build_package"" step is basically just a passthough for the ""grpc"" gems, so it's enough to just-# copy the native gems directly to the ""distribtests"" step and skip the ""build_package"" phase entirely.-# Note that by skipping the ""build_package"" step, we are also skipping the build of ""grpc-tools"" gem-# but that's fine since the distribtests only test the ""grpc"" native gems.--# The next step expects to find the artifacts from the previous step in the ""input_artifacts"" folder.-# in addition to that, preserve the contents of ""artifacts"" directory since we want kokoro-# to upload its contents as job output artifacts.-rm -rf input_artifacts-mkdir -p input_artifacts-cp -r artifacts/ruby_native_gem_*/* input_artifacts/ || true--# TODO(jtattermusch): Here we would normally run ruby macos distribtests, but currently no such tests are defined-# in distribtest_targets.py--tools/internal_ci/helper_scripts/store_artifacts_from_moved_src_tree.sh--if [ ""$FAILED"" != """" ]-then-  exit 1-fi+echo ""TODO(apolcyn): remove ruby macos distrib jobs, they don't test anything""",+1 for keeping the job for a while as a no-op. Might come handy if we need to revert and it avoids making this PR too complex.,
28165200,tdbhacks,https://api.github.com/repos/grpc/grpc/pulls/29436,870639856,2022-05-11T18:43:59Z,src/core/lib/security/security_connector/load_system_roots_linux.cc,"@@ -51,20 +51,27 @@ GPR_GLOBAL_CONFIG_DEFINE_STRING(grpc_system_ssl_roots_dir, """", namespace grpc_core { namespace { -const char* kLinuxCertFiles[] = {+#if defined(GPR_LINUX) || defined(GPR_ANDROID)+const char* kCertFiles[] = {     ""/etc/ssl/certs/ca-certificates.crt"", ""/etc/pki/tls/certs/ca-bundle.crt"",     ""/etc/ssl/ca-bundle.pem"", ""/etc/pki/tls/cacert.pem"",     ""/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem""};-const char* kLinuxCertDirectories[] = {+const char* kCertDirectories[] = {     ""/etc/ssl/certs"", ""/system/etc/security/cacerts"", ""/usr/local/share/certs"",     ""/etc/pki/tls/certs"", ""/etc/openssl/certs""};+#endif // GPR_LINUX || GPR_ANDROID+#ifdef GPR_FREEBSD","Thanks for taking a look! So `ifdef VAL` and `if defined(VAL)` are equivalent, the reason why `if defined(VAL)` is used above is that it allows multiple conditions to be combined into a single if statement (i.e. `ifdef GPR_LINUX || GPR_FREEBSD` would not work). See https://stackoverflow.com/questions/39290019/is-if-defined-macro-equivalent-to-ifdef-macroLet me know if you'd still like me to change it, happy to do it if you think it's cleaner that way.re: Mac PR: I haven't sent it yet because I'm wondering whether we should just rename this file, remove the ""_linux"" suffix, and add the Mac-specific path in here too, for the same reason why @drfloob suggested merging the FreeBSD and Linux implementations. That way we would only have this implementation for all platforms where we support loading system roots, and the fallback implementation for all other platforms. WDYT? If that sounds good to you I'll send in a follow-up PR once this is merged (unless you think it's fine to add both here).",
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/29620,870731042,2022-05-11T20:32:27Z,src/objective-c/tests/BUILD,"@@ -101,6 +101,7 @@ grpc_objc_testing_library(     hdrs = [""InteropTests/InteropTests.h""],     deps = [         "":InteropTestsBlockCallbacks-lib"",+        "":TestBase-lib"",",one NIT: we may also add the dep to https://github.com/grpc/grpc/blob/48749f739e7dd91cb69c988d57a89ffe93d8b501/src/objective-c/grpc_objc_internal_library.bzl#L112  (similar to :RemoteTest target) as this seems to be a common base dep for all test tagets. But this can be in a follow up ;  ),X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29539,870754077,2022-05-11T21:04:57Z,src/core/lib/resolver/server_address.h,"@@ -32,44 +31,74 @@  namespace grpc_core { -//-// ServerAddress-//--// A server address is a grpc_resolved_address with an associated set of-// channel args.  Any args present here will be merged into the channel-// args when a subchannel is created for this address.-class ServerAddress {+class ResolverAttributeMap {  public:   // Base class for resolver-supplied attributes.-  // Unlike channel args, these attributes don't affect subchannel-  // uniqueness or behavior.  They are for use by LB policies only.-  //-  // Attributes are keyed by a C string that is unique by address, not-  // by value.  All attributes added with the same key must be of the-  // same type.   class AttributeInterface {    public:     virtual ~AttributeInterface() = default; +    // The type name for this attribute.+    // There can be only one attribute of a given type in a given AttributeMap.+    // Attributes are keyed by the address of the string, not the value.","The main use-case for attributes is to provide a side channel to allow custom data to be passed from a resolver to an LB policy or between LB policies.  The intention is that when we make these APIs public, third parties can write their own resolvers and LB policies, and they can define their own attributes to pass data between them.This means that we need some way to ensure that there aren't going to be namespace conflicts between attributes defined by different third parties.  But we don't want to have some sort of registry here, since that's way too heavyweight.  Instead, we basically require each implementation to define its own string *instance* and use that as a key, so that only a pair of cooperating resolvers or LB policies that have access to the same attribute definition can actually access each others' attributes.  This ensures that there will be no conflicts even if two different third party authors happen to pick the same string for their attributes.FWIW, note that this really isn't a new thing in this PR.  Prior to this PR, we already used this attribute interface for the LB-policy attributes; all that's happening here is that we're now providing the same interface for the subchannel attributes, so that we can stop using channel args for those.Also, note that this is essentially the same way that this mechanism works in Java (and probably Go, not sure).",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29599,871005440,2022-05-12T06:46:53Z,tools/internal_ci/macos/grpc_objc_bazel_test.sh,"@@ -0,0 +1,88 @@+#!/usr/bin/env bash+# Copyright 2022 The gRPC Authors+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++set -ex++# avoid slow finalization after the script has exited.+source $(dirname $0)/../../../tools/internal_ci/helper_scripts/move_src_tree_and_respawn_itself_rc","I looked at the scripts and with the current setup, they should be almost instantaneous.I think the slow steps you're referring to are protected with PREPARE_BUILD_INSTALL_DEPS_OBJC=true,but that's not being set in this job (looks like those steps are unnecessary for bazel) and those steps are being skipped.https://github.com/grpc/grpc/blob/d19fa6235fba82fd124c1ec2ba72f1e51cab8a81/tools/internal_ci/helper_scripts/prepare_build_macos_rc#L97",
9566254,apolcyn,https://api.github.com/repos/grpc/grpc/pulls/29581,871610793,2022-05-12T16:53:51Z,src/core/lib/iomgr/resolve_address.h,"@@ -39,37 +41,40 @@ constexpr int kDefaultSecurePortInt = 443; // A singleton class used for async and blocking DNS resolution class DNSResolver {  public:-  // Tracks a single asynchronous DNS resolution attempt. The DNS-  // resolution should be arranged to be cancelled as soon as possible-  // when Orphan is called.-  class Request : public InternallyRefCounted<Request> {-   public:-    // Begins async DNS resolution-    virtual void Start() = 0;-  };+  using TaskHandle = ::grpc_event_engine::experimental::EventEngine::+      DNSResolver::LookupTaskHandle;+  static constexpr TaskHandle kNullHandle{0, 0};    virtual ~DNSResolver() {} +  static std::string HandleToString(TaskHandle handle);+   // Asynchronously resolve name. Use \a default_port if a port isn't designated   // in \a name, otherwise use the port in \a name. On completion, \a on_done is   // invoked with the result.   //   // Note for implementations: calls may acquire locks in \a on_done which-  // were previously held while calling Request::Start(). Therefore,-  // implementations must not invoke \a on_done inline from the call to-  // Request::Start(). The DNSCallbackExecCtxScheduler utility may help address-  // this.-  virtual OrphanablePtr<Request> ResolveName(+  // were previously held while starting the request. Therefore,+  // implementations must not invoke \a on_done inline from the call site that+  // starts the request. The DNSCallbackExecCtxScheduler utility may help+  // address this.+  virtual TaskHandle ResolveName(","I realized there's a problem with this API (which <i>might</i> be related to the macos bugs that caused revert):The ownership semantics of the `interested_parties` parameter seem broken. The problem is that, before, the caller needed to wait until `on_done` was invoked before it could delete `interested_parties`. But now, if `Cancel` returns true, the caller might delete `interested_parties` ~immediately, which can cause problems because subsequent cancellation inside c-ares can still [reference it](https://github.com/grpc/grpc/blob/adfd009d3a255b825ea91959620c11805418b22b/src/core/ext/filters/client_channel/resolver/dns/c_ares/grpc_ares_ev_driver_posix.cc#L56).",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29664,871737481,2022-05-12T19:27:32Z,tools/run_tests/run_tests.py,"@@ -1067,7 +1067,7 @@ def test_specs(self):                 environ=_FORCE_ENVIRON_FOR_WRAPPERS))         # TODO: replace with run_one_test_bazel.sh when Bazel-Xcode is stable         out.append(-            self.config.job_spec(['src/objective-c/tests/run_one_test.sh'],+            self.config.job_spec(['src/objective-c/tests/run_one_test_bazel.sh'],","please see https://github.com/grpc/grpc/pull/29665/files#diff-26e0bc5bba55be18a1e28b707c1768be5c3de415944115ec3b4eb006b1d39693R1081While I think they way of passing HOST_PORT_LOCALSSL and HOST_PORT_LOCAL env variables is useful (since for bazel we could only use static ports until now), I think we should not run bazel test via the run_tests.py script and via the run_one_test_with_bazel.shInstead we should use the grpc_objc_bazel_test I just introduced. That test already runs the unitTests target with bazel and does that together with other settings we want (reports in UI, results uploaded in bigquery) and in a way that more aligned with our other bazel test jobs. also it makes it easier to use remote caching in the future.I think we should focus on improving the grpc_objc_bazel_test job (there's plenty of TODOs and possible improvements) and after new tests are added there, we should then remove them from the run_tests.py suite (which is slower, more complicated and has other problems).",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29664,871739818,2022-05-12T19:30:33Z,src/objective-c/tests/BUILD,"@@ -24,13 +24,22 @@ load(""@build_bazel_rules_apple//apple:resources.bzl"", ""apple_resource_bundle"") load(""@build_bazel_rules_apple//apple:ios.bzl"", ""ios_application"", ""ios_unit_test"") load(""@build_bazel_rules_apple//apple:macos.bzl"", ""macos_unit_test"") load(""@build_bazel_rules_apple//apple:tvos.bzl"", ""tvos_application"", ""tvos_unit_test"")+load(""@build_bazel_rules_apple//apple/testing/default_runner:ios_test_runner.bzl"", ""ios_test_runner"")  licenses([""notice""])  package(default_visibility = [""//visibility:public""])  exports_files([""LICENSE""]) +ios_test_runner(","note that in bazel we already have this mechanism:https://github.com/grpc/grpc/blob/adfd009d3a255b825ea91959620c11805418b22b/src/objective-c/tests/BUILD#L60which is what we are currently using (and the bazel test are passing). I think your approach is better and more general, but we probably shouldn't have two different mechanisms for achieving the same thing.",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29665,872131676,2022-05-13T08:28:07Z,tools/run_tests/run_tests.py,"@@ -1038,70 +1042,88 @@ def test_specs(self):         #             'EXAMPLE_PATH': 'src/objective-c/examples/watchOS-sample',         #             'FRAMEWORKS': 'NO'         #         }))++        # TODO(jtattermusch): Create bazel target for the test and remove the test from here         out.append(             self.config.job_spec(['src/objective-c/tests/run_plugin_tests.sh'],                                  timeout_seconds=60 * 60,                                  shortname='ios-test-plugintest',                                  cpu_cost=1e6,                                  environ=_FORCE_ENVIRON_FOR_WRAPPERS))+        # Note that this test basically tests whether the codegen plugin works correctly by running protoc and checking the contents of the generated *.pbrpc.* files.+        # it doesn't really build any ObjC code.+        # TODO(jtattermusch): turn this test into a bazel test or come up with a better place where to put this test.         out.append(             self.config.job_spec(                 ['src/objective-c/tests/run_plugin_option_tests.sh'],                 timeout_seconds=60 * 60,                 shortname='ios-test-plugin-option-test',                 cpu_cost=1e6,                 environ=_FORCE_ENVIRON_FOR_WRAPPERS))+        # TODO(jtattermusch): move the test out of the test/core/iomgr/CFStreamTests directory?","do we know why the test lives under test/core/iomgr/ios/CFStreamTests?Btw, this test runs under various sanitizers (asan, tsan, msan).https://github.com/grpc/grpc/blob/03e9ac6f1f3132b81a2133ed63864fc72cd93e57/test/core/iomgr/ios/CFStreamTests/build_and_run_tests.sh",
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29665,872134440,2022-05-13T08:31:29Z,tools/run_tests/run_tests.py,"@@ -1038,70 +1042,88 @@ def test_specs(self):         #             'EXAMPLE_PATH': 'src/objective-c/examples/watchOS-sample',         #             'FRAMEWORKS': 'NO'         #         }))++        # TODO(jtattermusch): Create bazel target for the test and remove the test from here         out.append(             self.config.job_spec(['src/objective-c/tests/run_plugin_tests.sh'],                                  timeout_seconds=60 * 60,                                  shortname='ios-test-plugintest',                                  cpu_cost=1e6,                                  environ=_FORCE_ENVIRON_FOR_WRAPPERS))+        # Note that this test basically tests whether the codegen plugin works correctly by running protoc and checking the contents of the generated *.pbrpc.* files.+        # it doesn't really build any ObjC code.+        # TODO(jtattermusch): turn this test into a bazel test or come up with a better place where to put this test.         out.append(             self.config.job_spec(                 ['src/objective-c/tests/run_plugin_option_tests.sh'],                 timeout_seconds=60 * 60,                 shortname='ios-test-plugin-option-test',                 cpu_cost=1e6,                 environ=_FORCE_ENVIRON_FOR_WRAPPERS))+        # TODO(jtattermusch): move the test out of the test/core/iomgr/CFStreamTests directory?+        # How does one add the cfstream dependency in bazel?         out.append(             self.config.job_spec(                 ['test/core/iomgr/ios/CFStreamTests/build_and_run_tests.sh'],                 timeout_seconds=60 * 60,                 shortname='ios-test-cfstream-tests',                 cpu_cost=1e6,                 environ=_FORCE_ENVIRON_FOR_WRAPPERS))+        # TODO(jtattermusch): Create bazel target for the test and remove the test from here+        # TODO(jtattermusch): Clarify what do these tests do?         out.append(             self.config.job_spec(                 ['src/objective-c/tests/CoreTests/build_and_run_tests.sh'],                 timeout_seconds=60 * 60,                 shortname='ios-test-core-tests',                 cpu_cost=1e6,                 environ=_FORCE_ENVIRON_FOR_WRAPPERS))-        # TODO: replace with run_one_test_bazel.sh when Bazel-Xcode is stable+        # TODO(jtattermusch): Remove this task since the tests are already being run as part of the grpc_objc_bazel_test job.         out.append(             self.config.job_spec(['src/objective-c/tests/run_one_test.sh'],                                  timeout_seconds=60 * 60,                                  shortname='ios-test-unittests',                                  cpu_cost=1e6,                                  environ={'SCHEME': 'UnitTests'}))+        # TODO(jtattermusch): Make sure the //src/objective-c/tests:InteropTests bazel test passes reliably and remove the test from there.         out.append(             self.config.job_spec(['src/objective-c/tests/run_one_test.sh'],                                  timeout_seconds=60 * 60,                                  shortname='ios-test-interoptests',                                  cpu_cost=1e6,                                  environ={'SCHEME': 'InteropTests'}))+        # TODO(jtattermusch): Create bazel target for the test and remove the test from here+        # (how does one add the cronet dependency in bazel?)         out.append(             self.config.job_spec(['src/objective-c/tests/run_one_test.sh'],                                  timeout_seconds=60 * 60,                                  shortname='ios-test-cronettests',                                  cpu_cost=1e6,                                  environ={'SCHEME': 'CronetTests'}))+        # TODO(jtattermusch): Create bazel target for the test and remove the test from here.         out.append(             self.config.job_spec(['src/objective-c/tests/run_one_test.sh'],                                  timeout_seconds=30 * 60,                                  shortname='ios-perf-test',                                  cpu_cost=1e6,                                  environ={'SCHEME': 'PerfTests'}))+        # TODO(jtattermusch): Clarify what's the difference between PerfTests and PerfTestsPosix+        # TODO(jtattermusch): Create bazel target for the test and remove the test from here.         out.append(             self.config.job_spec(['src/objective-c/tests/run_one_test.sh'],                                  timeout_seconds=30 * 60,                                  shortname='ios-perf-test-posix',                                  cpu_cost=1e6,                                  environ={'SCHEME': 'PerfTestsPosix'}))+        # TODO(jtattermusch): Create bazel target for the test (how does one add the cronet dependency in bazel?)+        # TODO(jtattermusch): move the test out of the test/cpp/ios directory?",Do we know why the test lives under `test/cpp/ios`?https://github.com/grpc/grpc/blob/03e9ac6f1f3132b81a2133ed63864fc72cd93e57/test/cpp/ios/build_and_run_tests.sh,X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29539,872555549,2022-05-13T15:56:56Z,src/core/lib/resolver/server_address.h,"@@ -32,44 +31,74 @@  namespace grpc_core { -//-// ServerAddress-//--// A server address is a grpc_resolved_address with an associated set of-// channel args.  Any args present here will be merged into the channel-// args when a subchannel is created for this address.-class ServerAddress {+class ResolverAttributeMap {  public:   // Base class for resolver-supplied attributes.-  // Unlike channel args, these attributes don't affect subchannel-  // uniqueness or behavior.  They are for use by LB policies only.-  //-  // Attributes are keyed by a C string that is unique by address, not-  // by value.  All attributes added with the same key must be of the-  // same type.   class AttributeInterface {    public:     virtual ~AttributeInterface() = default; +    // The type name for this attribute.+    // There can be only one attribute of a given type in a given AttributeMap.+    // Attributes are keyed by the address of the string, not the value.","Providing a string instance does not solve this problem, and the design doesn't work.C++ implementations are allowed to (but not required to) merge duplicate string values.You've recently seen cases where the same string token was given two pointers in one build system.It would be a valid compile for two instances of `""foo""` in the same binary, coming from two different libraries, to be given the same pointer also.C++ does not provide the guarantees this code is asking for.Suggestions to improve the design:1. mandate a namespace prefix (`grpc.lb_attr1`, `grpc.lb_attr2`, `customer.lb_attr1`) as we do for channel args (indeed the reason we do this for channel args)2. implement an attribute class that actually provides the guarantees you're looking for (probably internally calling new on some non-zero length data structure to ensure a unique pointer), use that throughout instead of `const char *`.",
27655,johnnyshields,https://api.github.com/repos/grpc/grpc/pulls/29684,872579364,2022-05-13T16:25:16Z,Rakefile,"@@ -87,9 +87,10 @@ task 'dlls', [:plat] do |t, args|   plat_list = args[:plat]    build_configs = []+  w64_ucrt = { cross: 'x86_64-w64-mingw32', out: 'grpc_c.64-ucrt.ruby', platform: 'x64-mingw-ucrt' }","AFAIK the rake-compiler-dock [docker image for ucrt](https://github.com/rake-compiler/rake-compiler-dock/blob/master/mingw64-ucrt/Dockerfile) uses the same gcc/g++ packages as the x64-mingw image, and [patches them to use ucrt runtime](https://github.com/rake-compiler/rake-compiler-dock/blob/master/mingw64-ucrt/mingw-w64-enable-ucrt.patch).",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29670,873983015,2022-05-16T17:26:45Z,src/core/ext/filters/client_channel/resolver/dns/c_ares/dns_resolver_ares.cc,"@@ -381,9 +382,11 @@ class AresDNSResolver : public DNSResolver {       GRPC_CLOSURE_INIT(&on_dns_lookup_done_, OnDnsLookupDone, this,                         grpc_schedule_on_exec_ctx);       MutexLock lock(&mu_);+      pollset_set_ = grpc_pollset_set_create();",Please initialize this in the initializer list instead of in the body of the ctor.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29670,873985469,2022-05-16T17:29:33Z,src/core/ext/filters/client_channel/resolver/dns/c_ares/dns_resolver_ares.cc,"@@ -434,6 +436,8 @@ class AresDNSResolver : public DNSResolver {           }         }       }+      grpc_pollset_set_del_pollset_set(request->interested_parties_,","The two parameters are in the opposite order here than they are in `Cancel()` (line 411).I can never remember which order is correct, but I strongly suspect they need to be the same in both places.",
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/29665,875347635,2022-05-17T23:23:53Z,tools/run_tests/run_tests.py,"@@ -1038,70 +1042,88 @@ def test_specs(self):         #             'EXAMPLE_PATH': 'src/objective-c/examples/watchOS-sample',         #             'FRAMEWORKS': 'NO'         #         }))++        # TODO(jtattermusch): Create bazel target for the test and remove the test from here         out.append(             self.config.job_spec(['src/objective-c/tests/run_plugin_tests.sh'],                                  timeout_seconds=60 * 60,                                  shortname='ios-test-plugintest',                                  cpu_cost=1e6,                                  environ=_FORCE_ENVIRON_FOR_WRAPPERS))+        # Note that this test basically tests whether the codegen plugin works correctly by running protoc and checking the contents of the generated *.pbrpc.* files.+        # it doesn't really build any ObjC code.+        # TODO(jtattermusch): turn this test into a bazel test or come up with a better place where to put this test.         out.append(             self.config.job_spec(                 ['src/objective-c/tests/run_plugin_option_tests.sh'],                 timeout_seconds=60 * 60,                 shortname='ios-test-plugin-option-test',                 cpu_cost=1e6,                 environ=_FORCE_ENVIRON_FOR_WRAPPERS))+        # TODO(jtattermusch): move the test out of the test/core/iomgr/CFStreamTests directory?+        # How does one add the cfstream dependency in bazel?         out.append(             self.config.job_spec(                 ['test/core/iomgr/ios/CFStreamTests/build_and_run_tests.sh'],                 timeout_seconds=60 * 60,                 shortname='ios-test-cfstream-tests',                 cpu_cost=1e6,                 environ=_FORCE_ENVIRON_FOR_WRAPPERS))+        # TODO(jtattermusch): Create bazel target for the test and remove the test from here+        # TODO(jtattermusch): Clarify what do these tests do?         out.append(             self.config.job_spec(                 ['src/objective-c/tests/CoreTests/build_and_run_tests.sh'],                 timeout_seconds=60 * 60,                 shortname='ios-test-core-tests',                 cpu_cost=1e6,                 environ=_FORCE_ENVIRON_FOR_WRAPPERS))-        # TODO: replace with run_one_test_bazel.sh when Bazel-Xcode is stable+        # TODO(jtattermusch): Remove this task since the tests are already being run as part of the grpc_objc_bazel_test job.         out.append(             self.config.job_spec(['src/objective-c/tests/run_one_test.sh'],                                  timeout_seconds=60 * 60,                                  shortname='ios-test-unittests',                                  cpu_cost=1e6,                                  environ={'SCHEME': 'UnitTests'}))+        # TODO(jtattermusch): Make sure the //src/objective-c/tests:InteropTests bazel test passes reliably and remove the test from there.         out.append(             self.config.job_spec(['src/objective-c/tests/run_one_test.sh'],                                  timeout_seconds=60 * 60,                                  shortname='ios-test-interoptests',                                  cpu_cost=1e6,                                  environ={'SCHEME': 'InteropTests'}))+        # TODO(jtattermusch): Create bazel target for the test and remove the test from here+        # (how does one add the cronet dependency in bazel?)         out.append(             self.config.job_spec(['src/objective-c/tests/run_one_test.sh'],                                  timeout_seconds=60 * 60,                                  shortname='ios-test-cronettests',                                  cpu_cost=1e6,                                  environ={'SCHEME': 'CronetTests'}))+        # TODO(jtattermusch): Create bazel target for the test and remove the test from here.         out.append(             self.config.job_spec(['src/objective-c/tests/run_one_test.sh'],                                  timeout_seconds=30 * 60,                                  shortname='ios-perf-test',                                  cpu_cost=1e6,                                  environ={'SCHEME': 'PerfTests'}))+        # TODO(jtattermusch): Clarify what's the difference between PerfTests and PerfTestsPosix+        # TODO(jtattermusch): Create bazel target for the test and remove the test from here.         out.append(             self.config.job_spec(['src/objective-c/tests/run_one_test.sh'],                                  timeout_seconds=30 * 60,                                  shortname='ios-perf-test-posix',                                  cpu_cost=1e6,                                  environ={'SCHEME': 'PerfTestsPosix'}))+        # TODO(jtattermusch): Create bazel target for the test (how does one add the cronet dependency in bazel?)+        # TODO(jtattermusch): move the test out of the test/cpp/ios directory?","similar to CFStream test suite, these are for Cronet transport test run under iOS test environment. Will add a task to follow up and move these out of test/cpp/ios ",
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/29664,876403920,2022-05-18T21:38:19Z,src/objective-c/tests/BUILD,"@@ -24,13 +24,22 @@ load(""@build_bazel_rules_apple//apple:resources.bzl"", ""apple_resource_bundle"") load(""@build_bazel_rules_apple//apple:ios.bzl"", ""ios_application"", ""ios_unit_test"") load(""@build_bazel_rules_apple//apple:macos.bzl"", ""macos_unit_test"") load(""@build_bazel_rules_apple//apple:tvos.bzl"", ""tvos_application"", ""tvos_unit_test"")+load(""@build_bazel_rules_apple//apple/testing/default_runner:ios_test_runner.bzl"", ""ios_test_runner"")  licenses([""notice""])  package(default_visibility = [""//visibility:public""])  exports_files([""LICENSE""]) +ios_test_runner(+    name = ""ios_test_runner_with_env"",+    test_environment = {",can we check if using env paramters for ios_unit_test (https://github.com/bazelbuild/rules_apple/blob/master/doc/rules-ios.md#ios_unit_test-env) would work ? that way we don't have to define extra runner for each test.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29668,876431812,2022-05-18T22:29:18Z,src/core/ext/xds/xds_client.cc,"@@ -245,23 +256,23 @@ class XdsClient::ChannelState::AdsCallState         ABSL_EXCLUSIVE_LOCKS_REQUIRED(&XdsClient::mu_) {       if (error == GRPC_ERROR_NONE && timer_pending_) {         timer_pending_ = false;-        absl::Status watcher_error = absl::UnavailableError(absl::StrFormat(-            ""timeout obtaining resource {type=%s name=%s} from xds server"",-            type_->type_url(),-            XdsClient::ConstructFullXdsResourceName(-                name_.authority, type_->type_url(), name_.key)));         if (GRPC_TRACE_FLAG_ENABLED(grpc_xds_client_trace)) {-          gpr_log(GPR_INFO, ""[xds_client %p] xds server %s: %s"",+          gpr_log(GPR_INFO,+                  ""[xds_client %p] xds server %s: timeout obtaining resource ""+                  ""{type=%s name=%s} from xds server"",                   ads_calld_->xds_client(),                   ads_calld_->chand()->server_.server_uri.c_str(),-                  watcher_error.ToString().c_str());+                  std::string(type_->type_url()).c_str(),+                  XdsClient::ConstructFullXdsResourceName(+                      name_.authority, type_->type_url(), name_.key)+                      .c_str());         }         auto& authority_state =             ads_calld_->xds_client()->authority_state_map_[name_.authority];         ResourceState& state = authority_state.resource_map[type_][name_.key];         state.meta.client_status = XdsApi::ResourceMetadata::DOES_NOT_EXIST;-        ads_calld_->xds_client()->NotifyWatchersOnErrorLocked(state.watchers,-                                                              watcher_error);+        ads_calld_->xds_client()->NotifyWatchersOnResourceDoesNotExist(","It was a bug that the code was using `OnError()` instead of `OnResourceDoesNotExist()`.  The whole point of the timer is that it's the only way to know that a resource does not exist when it is first requested from the server.In practice, this probably doesn't make any difference, because the timer is used only when initially subscribing to the resource, which means that there is no previous version cached, and in that situation, the `OnError()` behavior in all of our watchers is basically equivalent to `OnResourceDoesNotExist()`.  But semantically, `OnResourceDoesNotExist()` is correct here.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/29597,877321855,2022-05-19T17:16:58Z,tools/distrib/fix_build_deps.py,"@@ -0,0 +1,410 @@+#!/usr/bin/env python3++# Copyright 2022 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.++from doctest import SKIP+import os+import re+import sys+import collections+import subprocess++# find our home+ROOT = os.path.abspath(os.path.join(os.path.dirname(sys.argv[0]), '../..'))+os.chdir(ROOT)++Vendor = collections.namedtuple('Vendor', ['name', 'score'])+vendors = {}+consumes = {}+no_update = set()+buildozer_commands = []+needs_codegen_base_src = set()++EXTERNAL_DEPS = {+    'absl/base/attributes.h':+        'absl/base:core_headers',+    'absl/base/call_once.h':+        'absl/base',+    # TODO(ctiller) remove this+    'absl/base/internal/endian.h':+        'absl-base',+    'absl/base/thread_annotations.h':+        'absl/base:core_headers',+    'absl/container/flat_hash_map.h':+        'absl/container:flat_hash_map',+    'absl/container/flat_hash_set.h':+        'absl/container:flat_hash_set',+    'absl/container/inlined_vector.h':+        'absl/container:inlined_vector',+    'absl/functional/bind_front.h':+        'absl/functional:bind_front',+    'absl/functional/function_ref.h':+        'absl/functional:function_ref',+    'absl/hash/hash.h':+        'absl/hash',+    'absl/memory/memory.h':+        'absl/memory',+    'absl/meta/type_traits.h':+        'absl/meta:type_traits',+    'absl/random/random.h':+        'absl/random',+    'absl/status/status.h':+        'absl/status',+    'absl/status/statusor.h':+        'absl/status:statusor',+    'absl/strings/ascii.h':+        'absl/strings',+    'absl/strings/cord.h':+        'absl/strings:cord',+    'absl/strings/escaping.h':+        'absl/strings',+    'absl/strings/match.h':+        'absl/strings',+    'absl/strings/numbers.h':+        'absl/strings',+    'absl/strings/str_cat.h':+        'absl/strings',+    'absl/strings/str_format.h':+        'absl/strings:str_format',+    'absl/strings/str_join.h':+        'absl/strings',+    'absl/strings/str_replace.h':+        'absl/strings',+    'absl/strings/str_split.h':+        'absl/strings',+    'absl/strings/string_view.h':+        'absl/strings',+    'absl/strings/strip.h':+        'absl/strings',+    'absl/strings/substitute.h':+        'absl/strings',+    'absl/synchronization/mutex.h':+        'absl/synchronization',+    'absl/synchronization/notification.h':+        'absl/synchronization',+    'absl/time/clock.h':+        'absl/time',+    'absl/time/time.h':+        'absl/time',+    'absl/types/optional.h':+        'absl/types:optional',+    'absl/types/span.h':+        'absl/types:span',+    'absl/types/variant.h':+        'absl/types:variant',+    'absl/utility/utility.h':+        'absl/utility',+    'address_sorting/address_sorting.h':+        'address_sorting',+    'ares.h':+        'cares',+    'gmock/gmock.h':+        'gtest',+    'gtest/gtest.h':+        'gtest',+    'opencensus/trace/context_util.h':+        'opencensus-trace-context_util',+    'opencensus/trace/propagation/grpc_trace_bin.h':+        'opencensus-trace-propagation',+    'opencensus/tags/context_util.h':+        'opencensus-tags-context_util',+    'openssl/bio.h':+        'libssl',+    'openssl/bn.h':+        'libcrypto',+    'openssl/buffer.h':+        'libcrypto',+    'openssl/crypto.h':+        'libcrypto',+    'openssl/engine.h':+        'libcrypto',+    'openssl/err.h':+        'libcrypto',+    'openssl/evp.h':+        'libcrypto',+    'openssl/hmac.h':+        'libcrypto',+    'openssl/pem.h':+        'libcrypto',+    'openssl/rsa.h':+        'libcrypto',+    'openssl/sha.h':+        'libcrypto',+    'openssl/ssl.h':+        'libssl',+    'openssl/tls1.h':+        'libssl',+    'openssl/x509.h':+        'libcrypto',+    'openssl/x509v3.h':+        'libcrypto',+    're2/re2.h':+        're2',+    'upb/def.h':+        'upb_lib',+    'upb/json_encode.h':+        'upb_json_lib',+    'upb/text_encode.h':+        'upb_textformat_lib',+    'upb/def.hpp':+        'upb_reflection',+    'upb/upb.h':+        'upb_lib',+    'upb/upb.hpp':+        'upb_lib',+    'xxhash.h':+        'xxhash',+    'zlib.h':+        'madler_zlib',+}++INTERNAL_DEPS = {+    'google/rpc/status.upb.h':+        'google_rpc_status_upb',+    'google/protobuf/any.upb.h':+        'protobuf_any_upb',+    'google/protobuf/duration.upb.h':+        'protobuf_duration_upb',+    'google/protobuf/struct.upb.h':+        'protobuf_struct_upb',+    'google/protobuf/timestamp.upb.h':+        'protobuf_timestamp_upb',+    'google/protobuf/wrappers.upb.h':+        'protobuf_wrappers_upb',+    'src/proto/grpc/channelz/channelz.grpc.pb.h':+        '//src/proto/grpc/channelz:channelz_proto',+    'src/proto/grpc/core/stats.pb.h':+        '//src/proto/grpc/core:stats_proto',+    'src/proto/grpc/health/v1/health.upb.h':+        'grpc_health_upb',+    'src/proto/grpc/lb/v1/load_reporter.grpc.pb.h':+        '//src/proto/grpc/lb/v1:load_reporter_proto',+    'src/proto/grpc/lb/v1/load_balancer.upb.h':+        'grpc_lb_upb',+    'src/proto/grpc/reflection/v1alpha/reflection.grpc.pb.h':+        '//src/proto/grpc/reflection/v1alpha:reflection_proto',+    'src/proto/grpc/gcp/transport_security_common.upb.h':+        'alts_upb',+    'src/proto/grpc/gcp/altscontext.upb.h':+        'alts_upb',+    'src/proto/grpc/lookup/v1/rls.upb.h':+        'rls_upb',+    'src/proto/grpc/lookup/v1/rls_config.upb.h':+        'rls_config_upb',+    'src/proto/grpc/lookup/v1/rls_config.upbdefs.h':+        'rls_config_upbdefs',+    'src/proto/grpc/testing/xds/v3/csds.grpc.pb.h':+        '//src/proto/grpc/testing/xds/v3:csds_proto',+    'xds/data/orca/v3/orca_load_report.upb.h':+        'xds_orca_upb',+    'xds/service/orca/v3/orca.upb.h':+        'xds_orca_service_upb',+    'xds/type/v3/typed_struct.upb.h':+        'xds_type_upb',+}++SKIP_DEPS = {'google/api/expr/v1alpha1/syntax.upb.h'}+++class FakeSelects:++    def config_setting_group(self, **kwargs):+        pass+++def grpc_cc_library(name,+                    hdrs=[],+                    public_hdrs=[],+                    srcs=[],+                    select_deps=None,+                    tags=[],+                    **kwargs):+    if select_deps or 'nofixdeps' in tags or 'grpc-autodeps' not in tags:+        no_update.add(name)+    score = len(public_hdrs + hdrs)+    if 'avoid_dep' in tags:+        score += 1000000+    if 'nofixdeps' in tags:+        score += 1000+    for hdr in hdrs + public_hdrs:+        vendors.setdefault(hdr, []).append(Vendor(name, score))",Nit: You could use [`collections.defaultdict(list)`](https://docs.python.org/3/library/tempfile.html#tempfile.TemporaryFile) instead.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29539,877336596,2022-05-19T17:28:12Z,src/core/lib/resolver/server_address.h,"@@ -32,44 +31,74 @@  namespace grpc_core { -//-// ServerAddress-//--// A server address is a grpc_resolved_address with an associated set of-// channel args.  Any args present here will be merged into the channel-// args when a subchannel is created for this address.-class ServerAddress {+class ResolverAttributeMap {  public:   // Base class for resolver-supplied attributes.-  // Unlike channel args, these attributes don't affect subchannel-  // uniqueness or behavior.  They are for use by LB policies only.-  //-  // Attributes are keyed by a C string that is unique by address, not-  // by value.  All attributes added with the same key must be of the-  // same type.   class AttributeInterface {    public:     virtual ~AttributeInterface() = default; +    // The type name for this attribute.+    // There can be only one attribute of a given type in a given AttributeMap.+    // Attributes are keyed by the address of the string, not the value.",Thanks for pointing this out!I've sent #29709 to introduce a `UniqueTypeName` API along the lines of your second suggestion and to change the other places in our code where we're using this pattern to use `UniqueTypeName` instead.I've now merged that PR into this one and changed the resolver attribute API to also use `UniqueTypeName`.,
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29714,877523563,2022-05-19T20:41:05Z,test/core/event_engine/test_suite/BUILD,"@@ -80,10 +83,44 @@ grpc_cc_test(  # -- Internal targets -- +grpc_cc_library(+    name = ""posix_oracle_event_engine"",+    testonly = True,+    srcs = [""posix_oracle_event_engine.cc""],+    hdrs = [""posix_oracle_event_engine.h""],+    deps = [+        "":conformance_test_base_lib"",+        ""//:grpc"",+        ""//test/core/util:grpc_test_util"",+    ],+)++grpc_cc_test(+    name = ""posix_oracle_event_engine_test"",","suggestion: consider naming these `oracle_event_engine_posix` and `oracle_event_engine_posix_test`, if only so that oracles show up together in sorted lists of test results.",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29714,877541492,2022-05-19T21:06:37Z,test/core/event_engine/test_suite/event_engine_test.cc,"@@ -20,9 +20,23 @@ std::function<std::unique_ptr<grpc_event_engine::experimental::EventEngine>()>*     g_ee_factory = nullptr; +std::function<std::unique_ptr<grpc_event_engine::experimental::EventEngine>()>*+    g_oracle_ee_factory = nullptr;+ void SetEventEngineFactory(     std::function<         std::unique_ptr<grpc_event_engine::experimental::EventEngine>()>         factory) {   testing::AddGlobalTestEnvironment(new EventEngineTestEnvironment(factory)); }++void SetEventEngineFactory(","I'd suggest removing the 1-argument SetEventEngineFactory, replace with `SetEventEngineFactories` or some such. It clarifies that both are required for the test suite, and reduces some duplication downstream. If the caller does not provide an oracle (e.g., `SetEventEngineFactory(ee_factory, nullptr)`, and a test requires one, the test crash should be pretty straightforward.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29714,877552097,2022-05-19T21:23:23Z,test/core/event_engine/test_suite/client_test.cc,"@@ -12,9 +12,188 @@ // See the License for the specific language governing permissions and // limitations under the License. +#include <random>+#include <string>+#include <thread>+#include <vector>++#include ""absl/status/status.h""+#include ""absl/strings/str_cat.h""+#include ""absl/time/time.h""++#include <grpc/event_engine/event_engine.h>+#include <grpc/event_engine/memory_allocator.h>+#include <grpc/support/log.h>++#include ""src/core/lib/address_utils/parse_address.h""+#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/uri/uri_parser.h"" #include ""test/core/event_engine/test_suite/event_engine_test.h""+#include ""test/core/event_engine/test_suite/event_engine_test_utils.h""  class EventEngineClientTest : public EventEngineTest {}; +using grpc_event_engine::experimental::ConnectionManager;+using ResolvedAddress =+    grpc_event_engine::experimental::EventEngine::ResolvedAddress;++static constexpr int kMinMessageSize = 1024;+static constexpr int kMaxMessageSize = 4096;+static constexpr int kNumExchangedMessages = 100;++namespace {++grpc_core::Mutex g_mu;++// Returns a random message with bounded length.+std::string GetNextSendMessage() {+  static const char alphanum[] =+      ""0123456789""+      ""ABCDEFGHIJKLMNOPQRSTUVWXYZ""+      ""abcdefghijklmnopqrstuvwxyz"";+  static std::random_device rd;+  static std::seed_seq seed{rd()};+  static std::mt19937 gen(seed);+  static std::uniform_real_distribution<> dis(kMinMessageSize, kMaxMessageSize);+  std::string tmp_s;+  int len;+  {+    grpc_core::MutexLock lock(&g_mu);+    len = dis(gen);+  }+  tmp_s.reserve(len);+  for (int i = 0; i < len; ++i) {+    tmp_s += alphanum[rand() % (sizeof(alphanum) - 1)];+  }+  return tmp_s;+}++}  // namespace+ // TODO(hork): establish meaningful tests-TEST_F(EventEngineClientTest, TODO) {}++// Create a connection using the test event engine to a non-existent listener+// and verify that the connection fails.+TEST_F(EventEngineClientTest, ConnectToNonExistentListenerTest) {+  ConnectionManager mgr(this->NewEventEngine(), this->NewOracleEventEngine());+  // Create a test event engine client endpoint and connect to a non existent+  // oracle listener.+  auto status = mgr.CreateConnection(+      ""ipv6:[::1]:7000"", absl::InfiniteFuture(),+      [](absl::Status status) {+        // The On-Connect callback will be called with the status reported by+        // the test event engine.+        GPR_ASSERT(!status.ok());+      },+      false);+  GPR_ASSERT(status.status() ==+             absl::CancelledError(""Failed to create connection.""));+}++// Create a connection using the test event engine to a listener created+// by the oracle event engine and exchange bi-di data over the connection.+// For each data transfer, verify that data written at one end of the stream+// equals data read at the other end of the stream.+TEST_F(EventEngineClientTest, ConnectExchangeBidiDataTransferTest) {+  ConnectionManager mgr(this->NewEventEngine(), this->NewOracleEventEngine());+  std::string target_addr = ""ipv6:[::1]:7000"";+  // Start an oracle localhost ipv6 listener+  GPR_ASSERT(+      mgr.StartListener(target_addr, /*listener_type_oracle=*/true).ok());",This test would be more readable if you had:```EventEngine* oracle = this->NewOracle();...auto listener = oracle.CreateListener(...);GPR_ASSERT(listener.ok());GPR_ASSERT(listener->Bind(target_addr);```,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29709,878569142,2022-05-20T22:03:33Z,src/core/lib/gprpp/unique_type_name.h,"@@ -0,0 +1,103 @@+//+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.+//++#ifndef GRPC_CORE_LIB_GPRPP_UNIQUE_TYPE_NAME_H+#define GRPC_CORE_LIB_GPRPP_UNIQUE_TYPE_NAME_H++#include <grpc/support/port_platform.h>++#include <string>++#include ""absl/strings/string_view.h""++#include ""src/core/lib/gpr/useful.h""++// Provides a type name that is unique by instance rather than by+// string content.  This is useful in cases where there are different+// implementations of a given interface that need to be differentiated from+// each other for down-casting purposes, where it is undesirable to provide+// a registry to avoid name collisions.+//+// Expected usage:+/*+// Interface has a virtual method that returns a UniqueTypeName.+class Interface {+ public:+  virtual ~Interface() = default;+  virtual UniqueTypeName type() const = 0;+};++// Implementation uses a static factory instance to return the same+// UniqueTypeName for every instance.+class FooImplementation : public Interface {+ public:+  UniqueTypeName type() const override {+    static UniqueTypeName::Factory* kFactory =+        new UniqueTypeName::Factory(""Foo"");+    return kFactory->Create();+  }+};+*/++namespace grpc_core {++class UniqueTypeName {+ public:+  // Factory class.  There should be a single static instance of this+  // for each unique type name.  However, this static instance must be+  // dynamically allocated, since it is not trivially destructible.+  class Factory {","What if we wrote:```class Factory { public:  Factory(absl::string_view name) : name_(new std::string(name)) {}  ~Factory() = default;  Factory(const Factory&) = delete;  Factory& operator=(const Factory&) = delete;    UniqueTypeName Create() { return UniqueTypeName(*name_); }   private:  std::string* name_;};```why?now we can create factories statically (cleans up the syntax for usage a little)*and* I think ASAN will let that fly, whilst accidentally creating a factory non-statically will cause an asan errorwdyt?(if we don't do this, please add the deleted copy/assignment operators here)",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29593,878569623,2022-05-20T22:04:55Z,test/distrib/bazel/test_single_bazel_version.sh,"@@ -51,12 +51,31 @@ FAILED_TESTS="""" export OVERRIDE_BAZEL_VERSION=""$VERSION"" # when running under bazel docker image, the workspace is read only. export OVERRIDE_BAZEL_WRAPPER_DOWNLOAD_DIR=/tmp-bazel build -- //... ""${EXCLUDED_TARGETS[@]}"" || FAILED_TESTS=""${FAILED_TESTS}Build ""++# pick the Bazel wrapper","Updated, and added more info about why we need Bazel wrapper, also what's the advantage of pinning. This pattern seems to be used in several other scripts, e.g., `src/objective-c/tests/run_plugin_tests.sh`.",
5564114,Vignesh2208,https://api.github.com/repos/grpc/grpc/pulls/29714,878612600,2022-05-21T01:00:39Z,test/core/event_engine/test_suite/client_test.cc,"@@ -12,9 +12,188 @@ // See the License for the specific language governing permissions and // limitations under the License. +#include <random>+#include <string>+#include <thread>+#include <vector>++#include ""absl/status/status.h""+#include ""absl/strings/str_cat.h""+#include ""absl/time/time.h""++#include <grpc/event_engine/event_engine.h>+#include <grpc/event_engine/memory_allocator.h>+#include <grpc/support/log.h>++#include ""src/core/lib/address_utils/parse_address.h""+#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/uri/uri_parser.h"" #include ""test/core/event_engine/test_suite/event_engine_test.h""+#include ""test/core/event_engine/test_suite/event_engine_test_utils.h""  class EventEngineClientTest : public EventEngineTest {}; +using grpc_event_engine::experimental::ConnectionManager;+using ResolvedAddress =+    grpc_event_engine::experimental::EventEngine::ResolvedAddress;++static constexpr int kMinMessageSize = 1024;+static constexpr int kMaxMessageSize = 4096;+static constexpr int kNumExchangedMessages = 100;++namespace {++grpc_core::Mutex g_mu;++// Returns a random message with bounded length.+std::string GetNextSendMessage() {+  static const char alphanum[] =+      ""0123456789""+      ""ABCDEFGHIJKLMNOPQRSTUVWXYZ""+      ""abcdefghijklmnopqrstuvwxyz"";+  static std::random_device rd;+  static std::seed_seq seed{rd()};+  static std::mt19937 gen(seed);+  static std::uniform_real_distribution<> dis(kMinMessageSize, kMaxMessageSize);+  std::string tmp_s;+  int len;+  {+    grpc_core::MutexLock lock(&g_mu);+    len = dis(gen);+  }+  tmp_s.reserve(len);+  for (int i = 0; i < len; ++i) {+    tmp_s += alphanum[rand() % (sizeof(alphanum) - 1)];+  }+  return tmp_s;+}++}  // namespace+ // TODO(hork): establish meaningful tests-TEST_F(EventEngineClientTest, TODO) {}++// Create a connection using the test event engine to a non-existent listener+// and verify that the connection fails.+TEST_F(EventEngineClientTest, ConnectToNonExistentListenerTest) {+  ConnectionManager mgr(this->NewEventEngine(), this->NewOracleEventEngine());+  // Create a test event engine client endpoint and connect to a non existent+  // oracle listener.+  auto status = mgr.CreateConnection(+      ""ipv6:[::1]:7000"", absl::InfiniteFuture(),+      [](absl::Status status) {+        // The On-Connect callback will be called with the status reported by+        // the test event engine.+        GPR_ASSERT(!status.ok());+      },+      false);+  GPR_ASSERT(status.status() ==+             absl::CancelledError(""Failed to create connection.""));+}++// Create a connection using the test event engine to a listener created+// by the oracle event engine and exchange bi-di data over the connection.+// For each data transfer, verify that data written at one end of the stream+// equals data read at the other end of the stream.+TEST_F(EventEngineClientTest, ConnectExchangeBidiDataTransferTest) {+  ConnectionManager mgr(this->NewEventEngine(), this->NewOracleEventEngine());+  std::string target_addr = ""ipv6:[::1]:7000"";+  // Start an oracle localhost ipv6 listener+  GPR_ASSERT(+      mgr.StartListener(target_addr, /*listener_type_oracle=*/true).ok());+  // Create a test event engine client endpoint and connect to oracle listener.+  auto status = mgr.CreateConnection(+      target_addr, absl::InfiniteFuture(),+      [](absl::Status status) { GPR_ASSERT(status.ok()); }, false);+  GPR_ASSERT(status.ok());+  int connection_id = *status;+  // Alternate message exchanges between client -- server and server -- client.+  for (int i = 0; i < kNumExchangedMessages; i++) {+    // Send from client to server and verify data read at the server.+    GPR_ASSERT(mgr.TransferFromClient(/*connection_id=*/connection_id,+                                      /*write_data=*/GetNextSendMessage())+                   .ok());++    // Send from server to client and verify data read at the client.+    GPR_ASSERT(mgr.TransferFromServer(/*connection_id=*/connection_id,+                                      /*write_data=*/GetNextSendMessage())+                   .ok());+  }+  mgr.CloseConnection(connection_id);+}++// Create a N listeners and M connections where M > N and exchange and verify+// data over each connection.+TEST_F(EventEngineClientTest, MatrixOfConnectionsToOracleListenersTest) {",Modified the test. Now it creates N connections to one listener and exchanges and verifies data on each connection.,
5564114,Vignesh2208,https://api.github.com/repos/grpc/grpc/pulls/29714,878613276,2022-05-21T01:06:05Z,test/core/event_engine/test_suite/client_test.cc,"@@ -12,9 +12,188 @@ // See the License for the specific language governing permissions and // limitations under the License. +#include <random>+#include <string>+#include <thread>+#include <vector>++#include ""absl/status/status.h""+#include ""absl/strings/str_cat.h""+#include ""absl/time/time.h""++#include <grpc/event_engine/event_engine.h>+#include <grpc/event_engine/memory_allocator.h>+#include <grpc/support/log.h>++#include ""src/core/lib/address_utils/parse_address.h""+#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/uri/uri_parser.h"" #include ""test/core/event_engine/test_suite/event_engine_test.h""+#include ""test/core/event_engine/test_suite/event_engine_test_utils.h""  class EventEngineClientTest : public EventEngineTest {}; +using grpc_event_engine::experimental::ConnectionManager;+using ResolvedAddress =+    grpc_event_engine::experimental::EventEngine::ResolvedAddress;++static constexpr int kMinMessageSize = 1024;+static constexpr int kMaxMessageSize = 4096;+static constexpr int kNumExchangedMessages = 100;++namespace {++grpc_core::Mutex g_mu;++// Returns a random message with bounded length.+std::string GetNextSendMessage() {+  static const char alphanum[] =+      ""0123456789""+      ""ABCDEFGHIJKLMNOPQRSTUVWXYZ""+      ""abcdefghijklmnopqrstuvwxyz"";+  static std::random_device rd;+  static std::seed_seq seed{rd()};+  static std::mt19937 gen(seed);+  static std::uniform_real_distribution<> dis(kMinMessageSize, kMaxMessageSize);+  std::string tmp_s;+  int len;+  {+    grpc_core::MutexLock lock(&g_mu);+    len = dis(gen);+  }+  tmp_s.reserve(len);+  for (int i = 0; i < len; ++i) {+    tmp_s += alphanum[rand() % (sizeof(alphanum) - 1)];+  }+  return tmp_s;+}++}  // namespace+ // TODO(hork): establish meaningful tests-TEST_F(EventEngineClientTest, TODO) {}++// Create a connection using the test event engine to a non-existent listener+// and verify that the connection fails.+TEST_F(EventEngineClientTest, ConnectToNonExistentListenerTest) {+  ConnectionManager mgr(this->NewEventEngine(), this->NewOracleEventEngine());+  // Create a test event engine client endpoint and connect to a non existent+  // oracle listener.+  auto status = mgr.CreateConnection(+      ""ipv6:[::1]:7000"", absl::InfiniteFuture(),+      [](absl::Status status) {+        // The On-Connect callback will be called with the status reported by+        // the test event engine.+        GPR_ASSERT(!status.ok());+      },+      false);+  GPR_ASSERT(status.status() ==+             absl::CancelledError(""Failed to create connection.""));+}++// Create a connection using the test event engine to a listener created+// by the oracle event engine and exchange bi-di data over the connection.+// For each data transfer, verify that data written at one end of the stream+// equals data read at the other end of the stream.+TEST_F(EventEngineClientTest, ConnectExchangeBidiDataTransferTest) {+  ConnectionManager mgr(this->NewEventEngine(), this->NewOracleEventEngine());+  std::string target_addr = ""ipv6:[::1]:7000"";+  // Start an oracle localhost ipv6 listener+  GPR_ASSERT(+      mgr.StartListener(target_addr, /*listener_type_oracle=*/true).ok());+  // Create a test event engine client endpoint and connect to oracle listener.+  auto status = mgr.CreateConnection(+      target_addr, absl::InfiniteFuture(),+      [](absl::Status status) { GPR_ASSERT(status.ok()); }, false);+  GPR_ASSERT(status.ok());+  int connection_id = *status;+  // Alternate message exchanges between client -- server and server -- client.+  for (int i = 0; i < kNumExchangedMessages; i++) {+    // Send from client to server and verify data read at the server.+    GPR_ASSERT(mgr.TransferFromClient(/*connection_id=*/connection_id,+                                      /*write_data=*/GetNextSendMessage())+                   .ok());++    // Send from server to client and verify data read at the client.+    GPR_ASSERT(mgr.TransferFromServer(/*connection_id=*/connection_id,+                                      /*write_data=*/GetNextSendMessage())+                   .ok());+  }+  mgr.CloseConnection(connection_id);+}++// Create a N listeners and M connections where M > N and exchange and verify+// data over each connection.+TEST_F(EventEngineClientTest, MatrixOfConnectionsToOracleListenersTest) {",I minimized the footprint of ConnectionMgr significantly now. It only supports binding and starting listeners and creating connections now. The connection manager API now returns unique_ptrs to client and server endpoints when the connection is successful.I still feel a connection manager is required to remove some boiler plate code - particularly to retrieve the server endpoint. The server endpoint can be accessed only inside the listener's accept callback. So we need some way to block until the accept callback runs and then transfer the server endpoint to the test code.  At that point we also need to identify which connection the server endpoint maps to. Connection manager simplifies this stuff (by internally maintaining this mapping) and returns both the client and server endpoint to the test to do as it pleases. Otherwise the test has to maintain/coordinate this for every connection it wants to create.,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29541,879950712,2022-05-24T00:05:15Z,tools/distrib/python/grpcio_tools/setup.py,"@@ -293,8 +293,14 @@ def extension_modules():                      'protobuf>=3.12.0, < 4.0dev',                      'grpcio>={version}'.format(version=grpc_version.VERSION),                      'setuptools',+                     'tomli>=1.2,<3;python_version<""3.11""'","In general, gRPC as a library wants to minimize our dependency. Adding a hard dependency means potential bloat, conflict, duplication. So unless there is a strong reason to do so, we won't add extra dependencies.That being said, can we consider following two directions?1. Include a minimum implementation of TOML in grpcio-tools;2. Consider a wrapper package over grpcio-tools to ship this feature (e.g., grpcio-setuptools).cc @gnossen ",X
18636401,plannigan,https://api.github.com/repos/grpc/grpc/pulls/29541,880447097,2022-05-24T12:34:33Z,tools/distrib/python/grpcio_tools/setup.py,"@@ -293,8 +293,14 @@ def extension_modules():                      'protobuf>=3.12.0, < 4.0dev',                      'grpcio>={version}'.format(version=grpc_version.VERSION),                      'setuptools',+                     'tomli>=1.2,<3;python_version<""3.11""'","I understand the concerns about adding a new dependency. There are a few reasons why I think this dependency is acceptable.* [tomli](https://github.com/hukkin/tomli) is a very small library that keeps bloat in mind. It only supports reading toml files. Writing support is part of a [different library](https://github.com/hukkin/tomli-w).* The requirement for the dependency goes away in Python 3.11 when the standard library adds `tomllib`. It will take years before it could be removed, but as time goes on, fewer people will be using the external dependency.* `tomli` is the implementation that will be added to the standard library in Python 3.11. So there won't be a need to a compatibility layer on top or worrying about different implementation details.Vendor-ing is an option. `tomli` is MIT licensed and the implementation is only 4 files and about 800 lines (including comments & blank lines).I'd lean away from a wrapper package because `grpcio-tools` already contains `setuptools` specific functionality. Having `setuptools` functionality split between two packages could be confusing to users. So unless the intent would be to move that functionality out of `grpcio-tools` into this new package and introducing more work, I would consider this the least good option.At the end of the day, I'm most interested in getting this feature accepted. So I'll defer the decision to people that work more closely with the project.",X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/29541,880836332,2022-05-24T18:37:05Z,tools/distrib/python/grpcio_tools/setup.py,"@@ -293,8 +293,14 @@ def extension_modules():                      'protobuf>=3.12.0, < 4.0dev',                      'grpcio>={version}'.format(version=grpc_version.VERSION),                      'setuptools',+                     'tomli>=1.2,<3;python_version<""3.11""'","Would it be possible to add a minimum version check of 3.11 for this feature and then just depend on the standard library toml implementation. This addresses our concerns about a [leftpad](https://www.davidhaney.io/npm-left-pad-have-we-forgotten-how-to-program/)-style disaster and also provides a well-known path for people who want to use the feature -- they just need to upgrade to 3.11.If that's not acceptable, then how about making this an optional dependency for interpreters lower than 3.11? Then the burden of installing the toml parser would fall on the user who wants to use this new feature and has no possibility of affecting others.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29082,880896785,2022-05-24T19:54:54Z,examples/python/helloworld/greeter_server.py,"@@ -24,6 +24,7 @@ class Greeter(helloworld_pb2_grpc.GreeterServicer):      def SayHello(self, request, context):+        print(""Received:"", request.name)","Can we update the print message to make sure client and server print messages are similar? For examle ""Greeter server received: ...."".",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29744,880917755,2022-05-24T20:21:08Z,src/core/ext/filters/client_channel/subchannel.cc,"@@ -803,8 +806,10 @@ void Subchannel::RequestConnection() { void Subchannel::ResetBackoff() {   MutexLock lock(&mu_);   backoff_.Reset();-  if (state_ == GRPC_CHANNEL_TRANSIENT_FAILURE) {-    grpc_timer_cancel(&retry_timer_);+  if (state_ == GRPC_CHANNEL_TRANSIENT_FAILURE &&+      GetDefaultEventEngine()->Cancel(retry_timer_handle_)) {+    OnRetryTimerLocked();+    WeakUnref(DEBUG_LOCATION, ""ResetBackoff"");","IMHO, `ReleasableMutexLock` is anti-pattern that we really shouldn't support, because it makes it harder to see where the lock is actually held.  Instead, please do something like this:```bool timer_cancelled = false;{  MutexLock lock(&mu_);  backoff_.Reset();  timer_cancelled =    state_ == GRPC_CHANNEL_TRANSIENT_FAILURE &&    GetDefaultEventEngine()->Cancel(retry_timer_handle_);  if (timer_cancelled) OnRetryTimerLocked();}if (timer_cancelled) WeakUnref(DEBUG_LOCATION, ""ResetBackoff"");```",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29714,881081284,2022-05-25T00:34:43Z,test/core/event_engine/test_suite/client_test.cc,"@@ -12,9 +12,188 @@ // See the License for the specific language governing permissions and // limitations under the License. +#include <random>+#include <string>+#include <thread>+#include <vector>++#include ""absl/status/status.h""+#include ""absl/strings/str_cat.h""+#include ""absl/time/time.h""++#include <grpc/event_engine/event_engine.h>+#include <grpc/event_engine/memory_allocator.h>+#include <grpc/support/log.h>++#include ""src/core/lib/address_utils/parse_address.h""+#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/uri/uri_parser.h"" #include ""test/core/event_engine/test_suite/event_engine_test.h""+#include ""test/core/event_engine/test_suite/event_engine_test_utils.h""  class EventEngineClientTest : public EventEngineTest {}; +using grpc_event_engine::experimental::ConnectionManager;+using ResolvedAddress =+    grpc_event_engine::experimental::EventEngine::ResolvedAddress;++static constexpr int kMinMessageSize = 1024;+static constexpr int kMaxMessageSize = 4096;+static constexpr int kNumExchangedMessages = 100;++namespace {++grpc_core::Mutex g_mu;++// Returns a random message with bounded length.+std::string GetNextSendMessage() {+  static const char alphanum[] =+      ""0123456789""+      ""ABCDEFGHIJKLMNOPQRSTUVWXYZ""+      ""abcdefghijklmnopqrstuvwxyz"";+  static std::random_device rd;+  static std::seed_seq seed{rd()};+  static std::mt19937 gen(seed);+  static std::uniform_real_distribution<> dis(kMinMessageSize, kMaxMessageSize);+  std::string tmp_s;+  int len;+  {+    grpc_core::MutexLock lock(&g_mu);+    len = dis(gen);+  }+  tmp_s.reserve(len);+  for (int i = 0; i < len; ++i) {+    tmp_s += alphanum[rand() % (sizeof(alphanum) - 1)];+  }+  return tmp_s;+}++}  // namespace+ // TODO(hork): establish meaningful tests-TEST_F(EventEngineClientTest, TODO) {}++// Create a connection using the test event engine to a non-existent listener+// and verify that the connection fails.+TEST_F(EventEngineClientTest, ConnectToNonExistentListenerTest) {+  ConnectionManager mgr(this->NewEventEngine(), this->NewOracleEventEngine());+  // Create a test event engine client endpoint and connect to a non existent+  // oracle listener.+  auto status = mgr.CreateConnection(+      ""ipv6:[::1]:7000"", absl::InfiniteFuture(),+      [](absl::Status status) {+        // The On-Connect callback will be called with the status reported by+        // the test event engine.+        GPR_ASSERT(!status.ok());+      },+      false);+  GPR_ASSERT(status.status() ==+             absl::CancelledError(""Failed to create connection.""));+}++// Create a connection using the test event engine to a listener created+// by the oracle event engine and exchange bi-di data over the connection.+// For each data transfer, verify that data written at one end of the stream+// equals data read at the other end of the stream.+TEST_F(EventEngineClientTest, ConnectExchangeBidiDataTransferTest) {+  ConnectionManager mgr(this->NewEventEngine(), this->NewOracleEventEngine());+  std::string target_addr = ""ipv6:[::1]:7000"";+  // Start an oracle localhost ipv6 listener+  GPR_ASSERT(+      mgr.StartListener(target_addr, /*listener_type_oracle=*/true).ok());+  // Create a test event engine client endpoint and connect to oracle listener.+  auto status = mgr.CreateConnection(+      target_addr, absl::InfiniteFuture(),+      [](absl::Status status) { GPR_ASSERT(status.ok()); }, false);+  GPR_ASSERT(status.ok());+  int connection_id = *status;+  // Alternate message exchanges between client -- server and server -- client.+  for (int i = 0; i < kNumExchangedMessages; i++) {+    // Send from client to server and verify data read at the server.+    GPR_ASSERT(mgr.TransferFromClient(/*connection_id=*/connection_id,+                                      /*write_data=*/GetNextSendMessage())+                   .ok());++    // Send from server to client and verify data read at the client.+    GPR_ASSERT(mgr.TransferFromServer(/*connection_id=*/connection_id,+                                      /*write_data=*/GetNextSendMessage())+                   .ok());+  }+  mgr.CloseConnection(connection_id);+}++// Create a N listeners and M connections where M > N and exchange and verify+// data over each connection.+TEST_F(EventEngineClientTest, MatrixOfConnectionsToOracleListenersTest) {","I agree with you that the endpoint pair mapping logic might be duplicated a handful of places, but that's mostly in line with the recommendation to write DAMP tests. We chatted about this offline, but since I'm going to be on leave for a while, I'll catalog it briefly here for whomever gets involved next:* prefer DAMP over DRY tests, even at the cost of some duplication, if it makes the test more readable. (See ToTT)* a free function like `std::pair<Endpoint, Endpoint> CreateConnection(EventEngine* client_engine, EventEngine* listener_engine, std::vector<std::string> addrs)` could be used instead for the stated purpose of reducing boilerplate required to get a pair of connected endpoints, but I don't think that function would be very big (~20 lines maybe?), and if so, I think tests would likely be more readable if that code were inlined in each test. I provided a few test rewrites that elide the ConnectionManager, I think those are good examples of this.* A method that has an argument of `bool create_type_a_or_b` is generally a design smell, which can likely be resolved with appropriate application of polymorphism.",X
18636401,plannigan,https://api.github.com/repos/grpc/grpc/pulls/29541,881655038,2022-05-25T13:29:22Z,tools/distrib/python/grpcio_tools/setup.py,"@@ -293,8 +293,14 @@ def extension_modules():                      'protobuf>=3.12.0, < 4.0dev',                      'grpcio>={version}'.format(version=grpc_version.VERSION),                      'setuptools',+                     'tomli>=1.2,<3;python_version<""3.11""'","I'd be open to making it an extra dependency. That way people that would like to use the functionality can either use 3.11 or install `grpcio-tools[toml]`. It would be straight forward to document. I also have an idea detecting cases when the user intended to use the functionality, but installed `grpcio-tools`.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29780,881777158,2022-05-25T15:10:38Z,test/core/util/test_config.cc,"@@ -66,9 +66,9 @@ int64_t grpc_test_sanitizer_slowdown_factor() {   } else if (BuiltUnderAsan()) {     sanitizer_multiplier = 3;   } else if (BuiltUnderMsan()) {-    sanitizer_multiplier = 4;+    sanitizer_multiplier = 15;","Increasing this multiplier seems like a good idea!  However, if we make it too high, we might make test runs take a lot longer, so how about choosing a smaller value and then increasing it again if it's not big enough?  I'm thinking something like 8 for msan and 10 for ubsan -- doubling the current value in both cases.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29779,881791227,2022-05-25T15:21:54Z,test/core/end2end/tests/grpc_authz.cc,"@@ -51,6 +51,13 @@ static gpr_timespec five_seconds_from_now(void) {   return n_seconds_from_now(5); } +static void wait_for_policy_reload(void) {+  // Wait for the provider's refresh thread to read the updated files.+  // TODO: Refactor the tests to use a more reliable mechanism of detecting","This is an end-to-end test, not a unittest.  It's covering the APIs that would be used by a real application, and those APIs don't provide a way to know when a reload happens, because that's not necessary in those APIs.  I don't think we should expose such a thing here.I do think we should have separate unit tests that cover more detailed internal behvior, and we could use some sort of dependency-injection mechanism in that kind of test to determine when the reload happens.  But I don't think we should introduce that kind of thing in an end-to-end test.With regard to making this test more debuggable, I don't think the fact that this took so long to resolve is related to it being hard to debug; I think the real problem was that it got caught between teams (this code is maintained by the security team, not by grpc-team, and they didn't make it enough of a priority to debug this flake).  And to the extent that understanding the test may have been a problem, I think the change you've already made here of introducing a function called ""wait_for_policy_reload()"" is sufficient to provide future engineers with a hint about what the test is waiting for.",X
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29781,881918880,2022-05-25T17:17:25Z,test/core/util/test_config.cc,"@@ -47,6 +47,12 @@ int64_t g_fixture_slowdown_factor = 1; int64_t g_poller_slowdown_factor = 1; +#if GPR_APPLE+static const int64_t kPlatformSlowdownFactor = 3;",- would be good to check how this influences the total macos test suite runtime.- also longer waits can potentially cause some tests to timeout- how was `3` chosen? I think the proper way of doing this is to run e.g. 20 full runs of macos debug C/C++ tests with this param and then look at the results (and e.g. try slowdown factor 2 vs slowdown factor 3).,X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29541,882065123,2022-05-25T19:59:45Z,tools/distrib/python/grpcio_tools/setup.py,"@@ -293,8 +293,14 @@ def extension_modules():                      'protobuf>=3.12.0, < 4.0dev',                      'grpcio>={version}'.format(version=grpc_version.VERSION),                      'setuptools',+                     'tomli>=1.2,<3;python_version<""3.11""'",Adding an extra dependency in the form of `grpcio-tools[toml]` sounds good to me.,
9939684,jtattermusch,https://api.github.com/repos/grpc/grpc/pulls/29800,882943863,2022-05-26T18:12:35Z,src/objective-c/tests/BUILD,"@@ -207,6 +207,10 @@ ios_unit_test(         "":InteropTestsLocalSSL-lib"",         "":InteropTestsRemote-lib"",     ],+    env = {",Note that we currently hardcode the ports here: https://github.com/grpc/grpc/blob/9c23d7999ce827614195fe2da6653af30cb4298c/tools/internal_ci/macos/grpc_objc_bazel_test.sh#L75and here: https://github.com/grpc/grpc/blob/9c23d7999ce827614195fe2da6653af30cb4298c/src/objective-c/tests/BUILD#L72(And my gut feel is that you're change doesn't count on that).Providing a short description for the PR would help make your intent clearer.,X
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/29541,882956104,2022-05-26T18:28:03Z,tools/distrib/python/grpcio_tools/setup.py,"@@ -293,8 +293,14 @@ def extension_modules():                      'protobuf>=3.12.0, < 4.0dev',                      'grpcio>={version}'.format(version=grpc_version.VERSION),                      'setuptools',+                     'tomli>=1.2,<3;python_version<""3.11""'",> I'd be open to making it an extra dependency. That way people that would like to use the functionality can either use 3.11 or install grpcio-tools[toml]Sounds good to me. Thanks for bearing with us. We're pretty risk averse.,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/29477,882962616,2022-05-26T18:36:50Z,tools/internal_ci/linux/grpc_psm_performance_gke_experiment.sh,"@@ -131,14 +131,12 @@ time ../test-infra/bin/prepare_prebuilt_workers \  # Run tests. time ../test-infra/bin/runner \-    -i ""psm_${PSM_PROXIED_TEST}_loadtest_with_prebuilt_workers_${WORKER_POOL_8CORE}.yaml"" \-    -i ""psm_${PSM_PROXYLESS_TEST}_loadtest_with_prebuilt_workers_${WORKER_POOL_8CORE}.yaml"" \-    -i ""psm_${PSM_PROXIED_TEST}_loadtest_with_prebuilt_workers_${WORKER_POOL_32CORE}.yaml"" \-    -i ""psm_${PSM_PROXYLESS_TEST}_loadtest_with_prebuilt_workers_${WORKER_POOL_32CORE}.yaml"" \+    -i ""psm_proxied_loadtest_with_prebuilt_workers_${WORKER_POOL_8CORE}.yaml"" \+    -i ""psm_proxyless_loadtest_with_prebuilt_workers_${WORKER_POOL_8CORE}.yaml"" \",8 core should be fine as long as we're not saturating the server.,
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/29799,882974883,2022-05-26T18:52:22Z,src/objective-c/tests/run_one_test_bazel.sh,"@@ -48,4 +48,7 @@ $INTEROP --port=$TLS_PORT --max_send_message_size=8388608 --use_tls &  trap 'kill -9 `jobs -p` ; echo ""EXIT TIME:  $(date)""' EXIT -../../../tools/bazel run $SCHEME+time \",+1https://github.com/grpc/grpc/blob/9c23d7999ce827614195fe2da6653af30cb4298c/tools/internal_ci/macos/grpc_objc_bazel_test.sh#L87  which currently run MacTest/UnitTest and codegen plugin test,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29637,883073917,2022-05-26T20:52:32Z,src/core/lib/surface/call.cc,"@@ -392,13 +388,13 @@ class FilterStackCall final : public Call {   /* Contexts for various subsystems (security, tracing, ...). */   grpc_call_context_element context_[GRPC_CONTEXT_COUNT] = {}; -  ManualConstructor<SliceBufferByteStream> sending_stream_;+  ManualConstructor<SliceBuffer> sending_stream_;","Does this still need to use `ManualConstructor<>` now that this is a C++ class?  Can we use `absl::optional<>` instead?Actually... at this point, maybe we don't need either one.  We could just directly create the `SliceBuffer` here.  We can populate it when we start the `send_message` op, and we can clear it when that op is complete.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29637,883079358,2022-05-26T21:00:09Z,src/core/lib/surface/call.cc,"@@ -392,13 +388,13 @@ class FilterStackCall final : public Call {   /* Contexts for various subsystems (security, tracing, ...). */   grpc_call_context_element context_[GRPC_CONTEXT_COUNT] = {}; -  ManualConstructor<SliceBufferByteStream> sending_stream_;+  ManualConstructor<SliceBuffer> sending_stream_;","Suggest calling these two fields `send_message_` and `recv_message_`, since they're no longer streams.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29637,883089105,2022-05-26T21:15:19Z,src/core/ext/filters/client_channel/subchannel_stream_client.cc,"@@ -374,39 +368,28 @@ void SubchannelStreamClient::CallState::RecvInitialMetadataReady(   self->call_->Unref(DEBUG_LOCATION, ""recv_initial_metadata_ready""); } -void SubchannelStreamClient::CallState::DoneReadingRecvMessage(-    grpc_error_handle error) {-  recv_message_.reset();-  if (error != GRPC_ERROR_NONE) {-    GRPC_ERROR_UNREF(error);-    Cancel();-    grpc_slice_buffer_destroy_internal(&recv_message_buffer_);-    call_->Unref(DEBUG_LOCATION, ""recv_message_ready"");-    return;-  }+void SubchannelStreamClient::CallState::RecvMessageReady() {   // Concatenate the slices to form a single string.-  std::unique_ptr<uint8_t> recv_message_deleter;-  uint8_t* recv_message;-  if (recv_message_buffer_.count == 1) {-    recv_message = GRPC_SLICE_START_PTR(recv_message_buffer_.slices[0]);+  std::vector<uint8_t> recv_message_buffer;+  const uint8_t* recv_message;+  if (recv_message_->Count() == 1) {+    recv_message = GRPC_SLICE_START_PTR(recv_message_->c_slice_at(0));   } else {-    recv_message =-        static_cast<uint8_t*>(gpr_malloc(recv_message_buffer_.length));-    recv_message_deleter.reset(recv_message);-    size_t offset = 0;-    for (size_t i = 0; i < recv_message_buffer_.count; ++i) {-      memcpy(recv_message + offset,-             GRPC_SLICE_START_PTR(recv_message_buffer_.slices[i]),-             GRPC_SLICE_LENGTH(recv_message_buffer_.slices[i]));-      offset += GRPC_SLICE_LENGTH(recv_message_buffer_.slices[i]);+    recv_message_buffer.reserve(recv_message_->Length());+    for (size_t i = 0; i < recv_message_->Count(); ++i) {+      const grpc_slice& slice = recv_message_->c_slice_at(0);",I think you meant to use `i` instead of `0` as the parameter to `c_slice_at()`.,X
933495,dennycd,https://api.github.com/repos/grpc/grpc/pulls/29799,883124889,2022-05-26T22:21:17Z,src/objective-c/tests/BUILD,"@@ -196,6 +196,10 @@ ios_unit_test(         "":NSErrorUnitTests-lib"",         "":RxLibraryUnitTests-lib"",     ],+    env = {","discussed offline, we can pass in default configs for host addrss here, similar to what we did for preprocessor macros.  Test script will have the option to override this with --test_env for custom port ",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29637,883136506,2022-05-26T22:46:26Z,src/core/ext/filters/http/message_compress/message_decompress_filter.cc,"@@ -187,101 +165,45 @@ void CallData::OnRecvMessageReady(void* arg, grpc_error_handle error) {     if (calld->algorithm_ != GRPC_COMPRESS_NONE) {       // recv_message can be NULL if trailing metadata is received instead of       // message, or it's possible that the message was not compressed.-      if (*calld->recv_message_ == nullptr ||-          (*calld->recv_message_)->length() == 0 ||-          ((*calld->recv_message_)->flags() & GRPC_WRITE_INTERNAL_COMPRESS) ==-              0) {+      if (!calld->recv_message_->has_value() ||+          (*calld->recv_message_)->Length() == 0 ||+          ((*calld->recv_message_flags_ & GRPC_WRITE_INTERNAL_COMPRESS) == 0)) {         return calld->ContinueRecvMessageReadyCallback(GRPC_ERROR_NONE);       }       if (calld->max_recv_message_length_ >= 0 &&-          (*calld->recv_message_)->length() >+          (*calld->recv_message_)->Length() >               static_cast<uint32_t>(calld->max_recv_message_length_)) {         GPR_DEBUG_ASSERT(calld->error_ == GRPC_ERROR_NONE);         calld->error_ = grpc_error_set_int(             GRPC_ERROR_CREATE_FROM_CPP_STRING(                 absl::StrFormat(""Received message larger than max (%u vs. %d)"",-                                (*calld->recv_message_)->length(),+                                (*calld->recv_message_)->Length(),                                 calld->max_recv_message_length_)),             GRPC_ERROR_INT_GRPC_STATUS, GRPC_STATUS_RESOURCE_EXHAUSTED);         return calld->ContinueRecvMessageReadyCallback(             GRPC_ERROR_REF(calld->error_));       }-      grpc_slice_buffer_destroy_internal(&calld->recv_slices_);-      grpc_slice_buffer_init(&calld->recv_slices_);-      return calld->ContinueReadingRecvMessage();+      SliceBuffer decompressed_slices;+      if (grpc_msg_decompress(calld->algorithm_,+                              (*calld->recv_message_)->c_slice_buffer(),+                              decompressed_slices.c_slice_buffer()) == 0) {+        GPR_DEBUG_ASSERT(calld->error_ == GRPC_ERROR_NONE);+        calld->error_ = GRPC_ERROR_CREATE_FROM_CPP_STRING(absl::StrCat(+            ""Unexpected error decompressing data for algorithm with ""+            ""enum value "",+            calld->algorithm_));+      } else {+        *calld->recv_message_flags_ =+            (*calld->recv_message_flags_ & (~GRPC_WRITE_INTERNAL_COMPRESS)) |+            GRPC_WRITE_INTERNAL_TEST_ONLY_WAS_COMPRESSED;+        (*calld->recv_message_)->Swap(&decompressed_slices);+      }+      calld->ContinueRecvMessageReadyCallback(GRPC_ERROR_REF(calld->error_));",made it a return to match the pattern in the rest of the function,X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/29728,883144957,2022-05-26T23:07:36Z,test/core/tsi/ssl_transport_security_test.cc,"@@ -1053,6 +1053,272 @@ void ssl_tsi_test_do_handshake_with_custom_bio_pair() {   tsi_test_fixture_destroy(fixture); } +absl::Status void do_handshake_helper(SSL** out_client, SSL** out_server) {+  if (out_client == nullptr || out_server == nullptr) {+    return absl::InvalidArgumentError(+        ""Client and server SSL object must not be null."");+  }+  std::string cert_pem =+      ""-----BEGIN CERTIFICATE-----\n""+      ""MIICZzCCAdCgAwIBAgIIN18/ctj3wpAwDQYJKoZIhvcNAQELBQAwKjEXMBUGA1UE\n""+      ""ChMOR29vZ2xlIFRFU1RJTkcxDzANBgNVBAMTBnRlc3RDQTAeFw0xNTAxMDEwMDAw\n""+      ""MDBaFw0yNTAxMDEwMDAwMDBaMC8xFzAVBgNVBAoTDkdvb2dsZSBURVNUSU5HMRQw\n""+      ""EgYDVQQDDAt0ZXN0X2NlcnRfMTCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEA\n""+      ""20oOyI+fNCCeHJ3DNjGooPPP43Q6emhVvuWD8ppta582Rgxq/4j1bl9cPHdoCdyy\n""+      ""HsWFVUZzscj2qhClmlBAMEA595OU2NX2d81nSih5dwZWLMRQkEIzyxUR7Vee3eyo\n""+      ""nQD4HSamaevMSv79WTUBCozEGITqWnjYA152KUbA/IsCAwEAAaOBkDCBjTAOBgNV\n""+      ""HQ8BAf8EBAMCBaAwHQYDVR0lBBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMAwGA1Ud\n""+      ""EwEB/wQCMAAwGQYDVR0OBBIEECnFWP/UkDrV+SoXra58k64wGwYDVR0jBBQwEoAQ\n""+      ""p7JSbajiTZaIRUDSV1C81jAWBgNVHREEDzANggt0ZXN0X2NlcnRfMTANBgkqhkiG\n""+      ""9w0BAQsFAAOBgQCpJJssfN62T3G5z+5SBB+9KCzXnGxcTHtaTJkb04KLe+19EwhV\n""+      ""yRY4lZadKHjcNS6GCBogd069wNFUVYOU9VI7uUiEPdcTO+VRV5MYW0wjSi1zlkBZ\n""+      ""e8OAfYVeGUMfvThFpJ41f8vZ6GHgg95Lwv+Zh89SL8g1J3RWll9YVG8HWw==\n""+      ""-----END CERTIFICATE-----\n"";+  std::string key_pem =+      ""-----BEGIN RSA PRIVATE KEY-----\n""+      ""MIICXQIBAAKBgQDbSg7Ij580IJ4cncM2Maig88/jdDp6aFW+5YPymm1rnzZGDGr/\n""+      ""iPVuX1w8d2gJ3LIexYVVRnOxyPaqEKWaUEAwQDn3k5TY1fZ3zWdKKHl3BlYsxFCQ\n""+      ""QjPLFRHtV57d7KidAPgdJqZp68xK/v1ZNQEKjMQYhOpaeNgDXnYpRsD8iwIDAQAB\n""+      ""AoGAbq4kZApJeo/z/dGK0/GggQxOIylo0puSm7VQMcTL8YP8asKdxrgj2D99WG+U\n""+      ""LVYc+PcM4wuaHWOnTBL24roaitCNhrpIsJfWDkexzHXMj622SYlUcCuwsfjYOEyw\n""+      ""ntoNAnh0o4S+beYAfzT5VHCh4is9G9u+mwKYiGpJXROrYUECQQD4eq4nuGq3mfYJ\n""+      ""B0+md30paDVVCyBsuZTAtnu3MbRjMXy5LLE+vhno5nocvVSTOv3QC7Wk6yAa8/bG\n""+      ""iPT/MWixAkEA4e0zqPGo8tSimVv/1ei8Chyb+YqdSx+Oj5eZpa6X/KB/C1uS1tm6\n""+      ""DTgHW2GUhV4ypqdGH+t8quprJUtFuzqH+wJBAMRiicSg789eouMt4RjrdYPFdela\n""+      ""Gu1zm4rYb10xrqV7Vl0wYoH5U5cMmdSfGvomdLX6mzzWDJDg4ti1JBWRonECQQCD\n""+      ""Umtq0j1QGQUCe5Vz8zoJ7qNDI61WU1t8X7Rxt9CkiW4PXgU2WYxpzp2IImpAM4bh\n""+      ""k+2Q9EKc3nG1VdGMiPMtAkARkQF+pL8SBrUoh8G8glCam0brh3tW/cdW8L4UGTNF\n""+      ""2ZKC/LFH6DQBjYs3UXjvMGJxz4k9LysyY6o2Nf1JG6/L\n""+      ""-----END RSA PRIVATE KEY-----\n"";++  // Create the context objects.+  SSL_CTX* client_ctx(SSL_CTX_new(TLS_method()));+  SSL_CTX* server_ctx(SSL_CTX_new(TLS_method()));++  BIO* client_cert_bio(BIO_new_mem_buf(cert_pem.c_str()));+  X509* client_cert = PEM_read_bio_X509(client_cert_bio, /*x=*/nullptr,+                                        /*cb=*/nullptr, /*u=*/nullptr);+  BIO* client_key_bio(BIO_new_mem_buf(key_pem.c_str()));+  EVPPKEY* client_key = PEM_read_bio_PrivateKey(client_key_bio, /*x=*/nullptr,+                                                /*cb=*/nullptr, /*u=*/nullptr);++  BIO* server_cert_bio(BIO_new_mem_buf(cert_pem.c_str()));+  X509* server_cert = PEM_read_bio_X509(server_cert_bio, /*x=*/nullptr,+                                        /*cb=*/nullptr, /*u=*/nullptr);+  BIO* server_key_bio(BIO_new_mem_buf(key_pem.c_str()));+  EVPPKEY* server_key = PEM_read_bio_PrivateKey(server_key_bio, /*x=*/nullptr,+                                                /*cb=*/nullptr, /*u=*/nullptr);++  // Set both client and server certificate and private key.+  SSL_CTX_use_certificate(client_ctx, client_cert);+  SSL_CTX_use_PrivateKey(client_ctx, client_key);+  SSL_CTX_use_certificate(server_ctx, server_cert);+  SSL_CTX_use_PrivateKey(server_ctx, server_key);++  EVP_PKEY_free(client_key);+  BIO_free(client_key_bio);+  X509_free(client_cert);+  BIO_free(client_cert_bio);++  EVP_PKEY_free(server_key);+  BIO_free(server_key_bio);+  X509_free(server_cert);+  BIO_free(server_cert_bio);++  // Configure both client and server to request (and accept any)+  // certificate but fail if none is sent.+  SSL_CTX_set_verify(client_ctx,+                     SSL_VERIFY_PEER | SSL_VERIFY_FAIL_IF_NO_PEER_CERT,+                     /*callback=*/nullptr);+  SSL_CTX_set_cert_verify_callback(client_ctx, VerifySucceed,+                                   /*arg=*/nullptr);+  SSL_CTX_set_verify(server_ctx,+                     SSL_VERIFY_PEER | SSL_VERIFY_FAIL_IF_NO_PEER_CERT,+                     /*callback=*/nullptr);+  SSL_CTX_set_cert_verify_callback(server_ctx, VerifySucceed,+                                   /*arg=*/nullptr);++  // Turns off the session caching.+  SSL_CTX_set_session_cache_mode(client_ctx, SSL_SESS_CACHE_OFF);+  SSL_CTX_set_session_cache_mode(server_ctx, SSL_SESS_CACHE_OFF);++  // Set both the min and max TLS version to 1.3+  SSL_CTX_set_min_proto_version(client_ctx, TLS1_3_VERSION);+  SSL_CTX_set_min_proto_version(server_ctx, TLS1_3_VERSION);+  SSL_CTX_set_max_proto_version(client_ctx, TLS1_3_VERSION);+  SSL_CTX_set_max_proto_version(server_ctx, TLS1_3_VERSION);++  // Create client and server connection objects and configure their BIOs.+  SSL* client(SSL_new(client_ctx));+  SSL* server(SSL_new(server_ctx));++  SSL_CTX_free(client_ctx);+  SSL_CTX_free(server_ctx);++  // Turns off issuance of session tickets by servers.+  SSL_set_options(client, SSL_OP_NO_TICKET);+  SSL_set_options(server, SSL_OP_NO_TICKET);++  SSL_set_connect_state(client);+  SSL_set_accept_state(server);+  BIO* bio1;+  BIO* bio2;+  BIO_new_bio_pair(&bio1, /*writebuf1=*/0, &bio2, /*writebuf2=*/0);+  SSL_set_bio(client, bio1, bio1);+  SSL_set_bio(server, bio2, bio2);++  // Drive both the client and server handshake operations to completion.+  while (true) {+    int client_ret = SSL_do_handshake(client);+    int client_err = SSL_get_error(client, client_ret);+    if (client_err != SSL_ERROR_NONE && client_err != SSL_ERROR_WANT_READ &&+        client_err != SSL_ERROR_WANT_WRITE &&+        client_err != SSL_ERROR_PENDING_TICKET) {+      return absl::InternalError(+          absl::StrCat(""Client error:"", SSL_error_description(client_err)));+    }++    int server_ret = SSL_do_handshake(server);+    int server_err = SSL_get_error(server, server_ret);+    if (server_err != SSL_ERROR_NONE && server_err != SSL_ERROR_WANT_READ &&+        server_err != SSL_ERROR_WANT_WRITE &&+        server_err != SSL_ERROR_PENDING_TICKET) {+      return absl::InternalError(+          absl::StrCat(""Server error:"", SSL_error_description(server_err)));+    }+    if (client_ret == 1 && server_ret == 1) {+      break;+    }+  }++  *out_client = client;+  *out_server = server;++  return absl::OkStatus();+}++size_t CalculateRecordSizeFromHeader(uint8_t fourth_header_byte,+                                     uint8_t fifth_header_byte) {+  return (static_cast<int>(fourth_header_byte & 0xff) << 8) ++         static_cast<int>(fifth_header_byte & 0xff);+}++void ssl_frame_protector_util_function_test_helper(","Historically the code under TSI is written in C, but now you can write C++ tests. I would suggest putting all the new tests in a new file, and write C++ tests. That way you can use techniques like class, etc, and I believe it will make your code much simpler.An example would be https://github.com/grpc/grpc/blob/master/test/core/security/grpc_tls_certificate_verifier_test.cc",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/29728,883145842,2022-05-26T23:09:37Z,src/core/tsi/ssl_transport_security.cc,"@@ -1049,25 +1049,22 @@ void tsi_ssl_session_cache_unref(tsi_ssl_session_cache* cache) {   reinterpret_cast<tsi::SslSessionLRUCache*>(cache)->Unref(); } -/* --- tsi_frame_protector methods implementation. ---*/--static tsi_result ssl_protector_protect(tsi_frame_protector* self,-                                        const unsigned char* unprotected_bytes,-                                        size_t* unprotected_bytes_size,-                                        unsigned char* protected_output_frames,-                                        size_t* protected_output_frames_size) {-  tsi_ssl_frame_protector* impl =-      reinterpret_cast<tsi_ssl_frame_protector*>(self);+/* --- tsi_frame_protector util methods implementation. ---*/+tsi_result ssl_protector_protect_util(",See https://github.com/grpc/grpc/blob/master/include/grpcpp/security/tls_certificate_verifier.h#L89-L102 for an example to for better API contract documentation,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29566,886007283,2022-05-31T18:31:51Z,src/core/ext/xds/xds_listener.cc,"@@ -976,10 +976,8 @@ grpc_error_handle LdsResourceParse(       envoy_config_listener_v3_Listener_api_listener(listener);   const envoy_config_core_v3_Address* address =       envoy_config_listener_v3_Listener_address(listener);-  if (api_listener != nullptr && address != nullptr) {-    return GRPC_ERROR_CREATE_FROM_STATIC_STRING(-        ""Listener has both address and ApiListener"");-  }+  // Current envoy proto has address as required, validators will reject if","Please change this to be a TODO(roth) indicating that as soon as https://github.com/istio/istio/issues/38914 is resolved, we should re-add the check that the listener does not have both the api_listener and address fields populated.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29816,886950617,2022-06-01T15:33:29Z,src/core/lib/event_engine/iomgr_engine/thread_pool.h,"@@ -0,0 +1,69 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_LIB_EVENT_ENGINE_IOMGR_ENGINE_THREAD_POOL_H+#define GRPC_CORE_LIB_EVENT_ENGINE_IOMGR_ENGINE_THREAD_POOL_H++#include <grpc/support/port_platform.h>++#include <functional>+#include <queue>+#include <vector>++#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/gprpp/thd.h""++namespace grpc_event_engine {+namespace iomgr_engine {++class ThreadPool final {+ public:+  explicit ThreadPool(int reserve_threads);+  ~ThreadPool();++  void Add(const std::function<void()>& callback);++ private:+  class Thread {+   public:+    explicit Thread(ThreadPool* pool);+    ~Thread();++   private:+    ThreadPool* pool_;+    grpc_core::Thread thd_;+    void ThreadFunc();+  };+  grpc_core::Mutex mu_;+  grpc_core::CondVar cv_;+  grpc_core::CondVar shutdown_cv_;+  bool shutdown_;+  std::queue<std::function<void()>> callbacks_;+  int reserve_threads_;+  int nthreads_;+  int threads_waiting_;+  std::vector<Thread*> dead_threads_;++  void ThreadFunc();",Please declare methods before data members (line 52).,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29816,886959797,2022-06-01T15:42:33Z,src/core/lib/event_engine/iomgr_engine/thread_pool.cc,"@@ -0,0 +1,123 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include ""src/core/lib/event_engine/iomgr_engine/thread_pool.h""++#include ""src/core/lib/gprpp/thd.h""++namespace grpc_event_engine {+namespace iomgr_engine {++ThreadPool::Thread::Thread(ThreadPool* pool)+    : pool_(pool),+      thd_(+          ""grpcpp_dynamic_pool"",+          [](void* th) { static_cast<ThreadPool::Thread*>(th)->ThreadFunc(); },+          this) {+  thd_.Start();+}+ThreadPool::Thread::~Thread() { thd_.Join(); }++void ThreadPool::Thread::ThreadFunc() {+  pool_->ThreadFunc();+  // Now that we have killed ourselves, we should reduce the thread count+  grpc_core::MutexLock lock(&pool_->mu_);+  pool_->nthreads_--;+  // Move ourselves to dead list+  pool_->dead_threads_.push_back(this);++  if ((pool_->shutdown_) && (pool_->nthreads_ == 0)) {+    pool_->shutdown_cv_.Signal();+  }+}++void ThreadPool::ThreadFunc() {+  for (;;) {+    // Wait until work is available or we are shutting down.+    grpc_core::ReleasableMutexLock lock(&mu_);+    if (!shutdown_ && callbacks_.empty()) {+      // If there are too many threads waiting, then quit this thread+      if (threads_waiting_ >= reserve_threads_) {+        break;+      }+      threads_waiting_++;+      cv_.Wait(&mu_);+      threads_waiting_--;+    }+    // Drain callbacks before considering shutdown to ensure all work+    // gets completed.+    if (!callbacks_.empty()) {+      auto cb = callbacks_.front();+      callbacks_.pop();+      lock.Release();+      cb();+    } else if (shutdown_) {+      break;+    }+  }+}++ThreadPool::ThreadPool(int reserve_threads)+    : shutdown_(false),","Nit: `shutdown_`, `nthreads_`, and `threads_waiting_` can be initialized where the data members are declared instead of being done here.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29816,886990019,2022-06-01T16:12:10Z,src/core/lib/event_engine/iomgr_engine/timer_manager.cc,"@@ -0,0 +1,228 @@+/*+ *+ * Copyright 2017 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include ""src/core/lib/event_engine/iomgr_engine/timer_manager.h""++#include <memory>++#include ""absl/memory/memory.h""+#include ""absl/time/time.h""+#include ""absl/types/optional.h""++#include <grpc/impl/codegen/gpr_types.h>+#include <grpc/support/time.h>++#include ""src/core/lib/gprpp/thd.h""++namespace grpc_event_engine {+namespace iomgr_engine {++TimerManager::ThreadCollector::~ThreadCollector() {+  for (auto& t : threads_) t.Join();+}++void TimerManager::StartThread() {+  ++waiter_count_;+  ++thread_count_;+  auto* thread = new RunThreadArgs();+  thread->self = this;+  thread->thread =+      grpc_core::Thread(""timer_manager"", &TimerManager::RunThread, thread);+  thread->thread.Start();+}++void TimerManager::RunSomeTimers(+    std::vector<experimental::EventEngine::Closure*> timers) {+  // if there's something to execute...+  ThreadCollector collector;+  {+    grpc_core::MutexLock lock(&mu_);+    // remove a waiter from the pool, and start another thread if necessary+    --waiter_count_;+    if (waiter_count_ == 0) {+      // The number of timer threads is always increasing until all the threads+      // are stopped. In rare cases, if a large number of timers fire+      // simultaneously, we may end up using a large number of threads.+      StartThread();+    } else {+      // if there's no thread waiting with a timeout, kick an existing untimed+      // waiter so that the next deadline is not missed+      if (!has_timed_waiter_) {+        cv_.Signal();+      }+    }+  }+  for (auto* timer : timers) {+    timer->Run();+  }+  {+    grpc_core::MutexLock lock(&mu_);+    collector.Collect(std::move(completed_threads_));+    // get ready to wait again+    ++waiter_count_;+  }+}++// wait until 'next' (or forever if there is already a timed waiter in the pool)+// returns true if the thread should continue executing (false if it should+// shutdown)+bool TimerManager::WaitUntil(grpc_core::Timestamp next) {+  grpc_core::MutexLock lock(&mu_);++  if (shutdown_) {+    return false;+  }++  // If g_kicked is true at this point, it means there was a kick from the timer",There are a bunch of comments in this file referring to `g_*` variables.  Can you please update them to reflect the new data member names?,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29816,887010961,2022-06-01T16:32:35Z,src/core/lib/event_engine/iomgr_engine/timer_manager.h,"@@ -0,0 +1,113 @@+/*+ *+ * Copyright 2017 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_LIB_EVENT_ENGINE_IOMGR_ENGINE_TIMER_MANAGER_H+#define GRPC_CORE_LIB_EVENT_ENGINE_IOMGR_ENGINE_TIMER_MANAGER_H++#include <grpc/support/port_platform.h>++#include <stddef.h>+#include <stdint.h>++#include <algorithm>+#include <memory>+#include <utility>+#include <vector>++#include ""absl/base/thread_annotations.h""++#include <grpc/event_engine/event_engine.h>+#include <grpc/support/log.h>++#include ""src/core/lib/event_engine/iomgr_engine/timer.h""+#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/gprpp/thd.h""+#include ""src/core/lib/gprpp/time.h""++namespace grpc_event_engine {+namespace iomgr_engine {++/* Timer Manager tries to keep only one thread waiting for the next timeout at+   all times, and thus effectively preventing the thundering herd problem. */+class TimerManager final : public TimerListHost {+ public:+  TimerManager();+  ~TimerManager();++  void Kick() override;+  grpc_core::Timestamp Now() override;++  void TimerInit(Timer* timer, grpc_core::Timestamp deadline,+                 experimental::EventEngine::Closure* closure);+  bool TimerCancel(Timer* timer);++ private:+  class ThreadCollector {","This can probably be moved to an anonymous namespace in the .cc file, since it's not used anywhere else and doesn't depend on any private data of `TimerManager`.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29816,887012227,2022-06-01T16:34:02Z,src/core/lib/event_engine/iomgr_engine/timer_manager.cc,"@@ -0,0 +1,228 @@+/*+ *+ * Copyright 2017 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#include <grpc/support/port_platform.h>++#include ""src/core/lib/event_engine/iomgr_engine/timer_manager.h""++#include <memory>++#include ""absl/memory/memory.h""+#include ""absl/time/time.h""+#include ""absl/types/optional.h""++#include <grpc/impl/codegen/gpr_types.h>+#include <grpc/support/time.h>++#include ""src/core/lib/gprpp/thd.h""++namespace grpc_event_engine {+namespace iomgr_engine {++TimerManager::ThreadCollector::~ThreadCollector() {+  for (auto& t : threads_) t.Join();+}++void TimerManager::StartThread() {+  ++waiter_count_;+  ++thread_count_;+  auto* thread = new RunThreadArgs();+  thread->self = this;+  thread->thread =+      grpc_core::Thread(""timer_manager"", &TimerManager::RunThread, thread);+  thread->thread.Start();+}++void TimerManager::RunSomeTimers(+    std::vector<experimental::EventEngine::Closure*> timers) {+  // if there's something to execute...+  ThreadCollector collector;+  {+    grpc_core::MutexLock lock(&mu_);+    // remove a waiter from the pool, and start another thread if necessary+    --waiter_count_;+    if (waiter_count_ == 0) {+      // The number of timer threads is always increasing until all the threads",This would be a good place for a TODO about decreasing the number of threads instead of just having unused threads wait forever.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29816,887022896,2022-06-01T16:45:59Z,src/core/lib/event_engine/iomgr_engine/timer.h,"@@ -0,0 +1,191 @@+/*+ *+ * Copyright 2015 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_LIB_EVENT_ENGINE_IOMGR_ENGINE_TIMER_H+#define GRPC_CORE_LIB_EVENT_ENGINE_IOMGR_ENGINE_TIMER_H++#include <grpc/support/port_platform.h>++#include <stddef.h>++#include <atomic>+#include <cstdint>+#include <memory>+#include <vector>++#include ""absl/base/thread_annotations.h""+#include ""absl/types/optional.h""++#include <grpc/event_engine/event_engine.h>++#include ""src/core/lib/event_engine/iomgr_engine/time_averaged_stats.h""+#include ""src/core/lib/event_engine/iomgr_engine/timer_heap.h""+#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/gprpp/time.h""++namespace grpc_event_engine {+namespace iomgr_engine {++struct Timer {+  int64_t deadline;+  // kInvalidHeapIndex if not in heap.+  size_t heap_index;+  bool pending;+  struct Timer* next;+  struct Timer* prev;+  experimental::EventEngine::Closure* closure;+#ifndef NDEBUG+  struct Timer* hash_table_next;+#endif++  grpc_event_engine::experimental::EventEngine::TaskHandle task_handle;+};++class TimerListHost {",Please add a comment here explaining what this interface is for.  It took me a while to figure out that it's basically for dependency injection.,X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29816,887047697,2022-06-01T17:12:39Z,src/core/lib/event_engine/iomgr_engine/timer_manager.h,"@@ -0,0 +1,113 @@+/*+ *+ * Copyright 2017 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_LIB_EVENT_ENGINE_IOMGR_ENGINE_TIMER_MANAGER_H+#define GRPC_CORE_LIB_EVENT_ENGINE_IOMGR_ENGINE_TIMER_MANAGER_H++#include <grpc/support/port_platform.h>++#include <stddef.h>+#include <stdint.h>++#include <algorithm>+#include <memory>+#include <utility>+#include <vector>++#include ""absl/base/thread_annotations.h""++#include <grpc/event_engine/event_engine.h>+#include <grpc/support/log.h>++#include ""src/core/lib/event_engine/iomgr_engine/timer.h""+#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/gprpp/thd.h""+#include ""src/core/lib/gprpp/time.h""++namespace grpc_event_engine {+namespace iomgr_engine {++/* Timer Manager tries to keep only one thread waiting for the next timeout at+   all times, and thus effectively preventing the thundering herd problem. */+class TimerManager final : public TimerListHost {","Can we use composition instead of inheritence here?  In this particular case, the overhead of an additional pointer doesn't seem terrible, since it's just one instance for the entire process, and it would avoid exposing this internal implementation detail as part of the public API of this class.",
2793282,veblush,https://api.github.com/repos/grpc/grpc/pulls/29861,888112550,2022-06-02T15:49:41Z,doc/cpp/static-grpc-object.md,"@@ -0,0 +1,38 @@+# gRPC Objects with static storage duration++In general, it's discouraged to define gRPC objects with static storage+duration because it can have a indeterministic crash due to certain+initialization order or termination order.+This is a well known issue as describied in+[Static Initialization Order Fiasco](https://en.cppreference.com/w/cpp/language/siof)+and guided in+[Static and Global Variables](https://google.github.io/styleguide/cppguide.html#Static_and_Global_Variables)++## Example++The following code may crash because `cq`, which is a `grpc::CompleitionQueue`+type is defined as a global static variable which is going to be initialized+before `main()`.+To initialize `grpc::CompletionQueue` type object, it needs fully initialized+gRPC system but it may or may not happen at that point.+In addition to that, when `cq` is destroyed, it also needs alive gRPC system+which may be terminiated already.++```+static grpc::CompletionQueue cq;",This code is for a bad example (cq created before main()) but cq in your code will be created in main(). Do you show it as a solution?,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29861,888160427,2022-06-02T16:33:07Z,doc/cpp/static-grpc-object.md,"@@ -0,0 +1,38 @@+# gRPC Objects with static storage duration++In general, it's discouraged to define gRPC objects with static storage+duration because it can have a indeterministic crash due to certain+initialization order or termination order.+This is a well known issue as describied in+[Static Initialization Order Fiasco](https://en.cppreference.com/w/cpp/language/siof)+and guided in+[Static and Global Variables](https://google.github.io/styleguide/cppguide.html#Static_and_Global_Variables)++## Example++The following code may crash because `cq`, which is a `grpc::CompleitionQueue`+type is defined as a global static variable which is going to be initialized+before `main()`.+To initialize `grpc::CompletionQueue` type object, it needs fully initialized+gRPC system but it may or may not happen at that point.+In addition to that, when `cq` is destroyed, it also needs alive gRPC system+which may be terminiated already.++```+static grpc::CompletionQueue cq;","But it will be destroyed after main... I posit this mainly to figure out the bounds of what we'd support.For example, I've certainly encouraged folks to have a singleton channel client side to do most of their requests - as it tends to avoid the anti-pattern of creating a new channel per request (which crops up frequently!).",X
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/29728,888224002,2022-06-02T17:47:28Z,src/core/tsi/ssl_transport_security.cc,"@@ -1049,25 +1049,22 @@ void tsi_ssl_session_cache_unref(tsi_ssl_session_cache* cache) {   reinterpret_cast<tsi::SslSessionLRUCache*>(cache)->Unref(); } -/* --- tsi_frame_protector methods implementation. ---*/--static tsi_result ssl_protector_protect(tsi_frame_protector* self,-                                        const unsigned char* unprotected_bytes,-                                        size_t* unprotected_bytes_size,-                                        unsigned char* protected_output_frames,-                                        size_t* protected_output_frames_size) {-  tsi_ssl_frame_protector* impl =-      reinterpret_cast<tsi_ssl_frame_protector*>(self);+/* --- tsi_frame_protector util methods implementation. ---*/+tsi_result ssl_protector_protect_util(","It seems `do_ssl_write` is also shared within two functions, so I think it makes sense to move them under the utility folder as well.Ideally, I'd like to restructure everything so that we could use class-based techniques to arrange everything, but that's beyond the scope of this pull request. Would you mind moving them into a utility library first as an intermediate solution?",
23140989,ZhenLian,https://api.github.com/repos/grpc/grpc/pulls/29793,888352885,2022-06-02T19:41:10Z,src/core/tsi/transport_security_grpc.h,"@@ -69,6 +80,9 @@ struct tsi_zero_copy_grpc_protector_vtable {   tsi_result (*unprotect)(tsi_zero_copy_grpc_protector* self,                           grpc_slice_buffer* protected_slices,                           grpc_slice_buffer* unprotected_slices);+  tsi_result (*unprotect_and_get_min_progress_size)(+      tsi_zero_copy_grpc_protector* self, grpc_slice_buffer* protected_slices,+      grpc_slice_buffer* unprotected_slices, int* min_progress_size);","This looks good to me overall, but have we considered the possibility to add `int* min_progress_size` directly to the `unprotect` function signature? Because right now the two functions are almost the same, except for the last parameter.How difficult it is if we change all the internal call-sites of TSI to fill in `min_progress_size`? I think doing that might require some extra work, but is generally beneficial to the code cleanliness in the long run.",
5564114,Vignesh2208,https://api.github.com/repos/grpc/grpc/pulls/29793,888357008,2022-06-02T19:47:01Z,src/core/tsi/transport_security_grpc.h,"@@ -69,6 +80,9 @@ struct tsi_zero_copy_grpc_protector_vtable {   tsi_result (*unprotect)(tsi_zero_copy_grpc_protector* self,                           grpc_slice_buffer* protected_slices,                           grpc_slice_buffer* unprotected_slices);+  tsi_result (*unprotect_and_get_min_progress_size)(+      tsi_zero_copy_grpc_protector* self, grpc_slice_buffer* protected_slices,+      grpc_slice_buffer* unprotected_slices, int* min_progress_size);","Yeah Craig and I talked about this. The cherrypick will unfortunately be large if we change  the unprotect method directly and we have to get LGTMs from all the teams/users for landing or rolling back this change. This way, we can incrementally switch each user one by one to use the new method and then after everyone is migrated we can delete the old method and rename this in one go.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29882,888500888,2022-06-02T23:49:51Z,src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi,"@@ -382,7 +382,7 @@ cdef object _watch_connectivity_state(           state.c_connectivity_completion_queue, <cpython.PyObject *>tag)       state.connectivity_due.add(tag)     else:-      raise ValueError('Cannot invoke RPC: %s' % state.closed_reason)+      raise ValueError('Cannot monitor channel state: %s' % state.closed_reason)",Should we update the type of the exception to be `RuntimeError`?,
1644595,gnossen,https://api.github.com/repos/grpc/grpc/pulls/29882,888502650,2022-06-02T23:55:00Z,src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi,"@@ -382,7 +382,7 @@ cdef object _watch_connectivity_state(           state.c_connectivity_completion_queue, <cpython.PyObject *>tag)       state.connectivity_due.add(tag)     else:-      raise ValueError('Cannot invoke RPC: %s' % state.closed_reason)+      raise ValueError('Cannot monitor channel state: %s' % state.closed_reason)","While I would agree if this were new code, someone could be depending on the particular type of this exception, so this could be considered a behavior change. Leaving this as it is for now.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/29804,888519428,2022-06-03T00:45:10Z,tools/run_tests/xds_k8s_test_driver/README.md,"@@ -269,7 +227,44 @@ envsubst < config/local-dev.cfg.example > config/local-dev.cfg  Learn more about flagfiles in [abseil documentation](https://abseil.io/docs/python/guides/flags#a-note-about---flagfile). -### Helper scripts+## Test suites++See the full list of available test suites in the [`tests/`](https://github.com/grpc/grpc/tree/master/tools/run_tests/xds_k8s_test_driver/tests) folder. ++### xDS Baseline Tests++Test suite meant to confirm that basic xDS features work as expected. Executing+it before other test suites will help to identify whether test failure related+to specific features under test, or caused by unrelated infrastructure+disturbances.++```shell+# Help+python -m tests.baseline_test --help+python -m tests.baseline_test --helpfull++# Run the baseline test with local-dev.cfg settings+python -m tests.baseline_test --flagfile=""config/local-dev.cfg""+  +# Same as above, but using the helper script+./run.sh tests/baseline_test.py","People onboarding are likely to use `./run.sh` - that's from my experience of a few folks recently. Regarding command arguments, there are a lot of them. That's why I'm suggesting `--help` above. But I think we could list all *required* arguments. Would it be ok if this is tackled outside of this PR?",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29793,888976313,2022-06-03T14:03:24Z,src/core/tsi/transport_security_grpc.h,"@@ -69,6 +80,9 @@ struct tsi_zero_copy_grpc_protector_vtable {   tsi_result (*unprotect)(tsi_zero_copy_grpc_protector* self,                           grpc_slice_buffer* protected_slices,                           grpc_slice_buffer* unprotected_slices);+  tsi_result (*unprotect_and_get_min_progress_size)(+      tsi_zero_copy_grpc_protector* self, grpc_slice_buffer* protected_slices,+      grpc_slice_buffer* unprotected_slices, int* min_progress_size);","I agree with Zhen on this.  I would really prefer to see this parameter added to the existing method rather than cluttering the API with new method variants this way.It's not clear to me how it helps the import to add a new method anyway.  Either way, you will need to change all existing implementations, whether that's to add a new method or to add a parameter to the existing method.",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29793,889293445,2022-06-03T19:49:58Z,src/core/tsi/transport_security_grpc.h,"@@ -69,6 +80,9 @@ struct tsi_zero_copy_grpc_protector_vtable {   tsi_result (*unprotect)(tsi_zero_copy_grpc_protector* self,                           grpc_slice_buffer* protected_slices,                           grpc_slice_buffer* unprotected_slices);+  tsi_result (*unprotect_and_get_min_progress_size)(+      tsi_zero_copy_grpc_protector* self, grpc_slice_buffer* protected_slices,+      grpc_slice_buffer* unprotected_slices, int* min_progress_size);","Hey Mark, we're not talking a few callsites here... it's a fairly large change.The plan here is to introduce the new function, migrate incrementally, and then delete the old function - effectively eliminating the old method (this was mentioned up-thread).Not doing so for this one would be too risky - it's going to be an LSC internally to get it done as it is.",
5564114,Vignesh2208,https://api.github.com/repos/grpc/grpc/pulls/29793,889341564,2022-06-03T20:53:44Z,src/core/tsi/transport_security_grpc.h,"@@ -69,6 +80,9 @@ struct tsi_zero_copy_grpc_protector_vtable {   tsi_result (*unprotect)(tsi_zero_copy_grpc_protector* self,                           grpc_slice_buffer* protected_slices,                           grpc_slice_buffer* unprotected_slices);+  tsi_result (*unprotect_and_get_min_progress_size)(+      tsi_zero_copy_grpc_protector* self, grpc_slice_buffer* protected_slices,+      grpc_slice_buffer* unprotected_slices, int* min_progress_size);",I'll remove this method and do the cherrypick for this one. A similar change has to be made for the non_zero_copy based protector as well that cherrypick would be much larger.,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29793,889343008,2022-06-03T20:55:20Z,src/core/tsi/transport_security_grpc.h,"@@ -69,6 +80,9 @@ struct tsi_zero_copy_grpc_protector_vtable {   tsi_result (*unprotect)(tsi_zero_copy_grpc_protector* self,                           grpc_slice_buffer* protected_slices,                           grpc_slice_buffer* unprotected_slices);+  tsi_result (*unprotect_and_get_min_progress_size)(+      tsi_zero_copy_grpc_protector* self, grpc_slice_buffer* protected_slices,+      grpc_slice_buffer* unprotected_slices, int* min_progress_size);","Sounds good, thanks!Another option to consider is adding the parameter but making it an optional parameter, so that there's no need to modify any caller.",X
5564114,Vignesh2208,https://api.github.com/repos/grpc/grpc/pulls/29793,889352570,2022-06-03T21:09:13Z,src/core/tsi/transport_security_grpc.h,"@@ -69,6 +80,9 @@ struct tsi_zero_copy_grpc_protector_vtable {   tsi_result (*unprotect)(tsi_zero_copy_grpc_protector* self,                           grpc_slice_buffer* protected_slices,                           grpc_slice_buffer* unprotected_slices);+  tsi_result (*unprotect_and_get_min_progress_size)(+      tsi_zero_copy_grpc_protector* self, grpc_slice_buffer* protected_slices,+      grpc_slice_buffer* unprotected_slices, int* min_progress_size);",Craig doesn't like optional parameters I think :) Its probably better to force every caller to use it. I'll make the change. I got confused before with regards to which cherrypick would be large (zero copy vs non-zero-copy). There don't appear to be a lot of callers for zero copy protect/unprotect,
16571023,ananda1066,https://api.github.com/repos/grpc/grpc/pulls/29853,889401126,2022-06-03T22:48:07Z,src/core/lib/resource_quota/periodic_update.h,"@@ -0,0 +1,52 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_RESOURCE_QUOTA_PERIODIC_UPDATE_H+#define GRPC_CORE_LIB_RESOURCE_QUOTA_PERIODIC_UPDATE_H++#include <grpc/support/port_platform.h>++#include <inttypes.h>++#include <atomic>++#include ""src/core/lib/gprpp/time.h""+#include ""src/core/lib/iomgr/exec_ctx.h""++namespace grpc_core {++class PeriodicUpdate {",Please comment on what this class is intended for.,
16571023,ananda1066,https://api.github.com/repos/grpc/grpc/pulls/29853,889409121,2022-06-03T23:19:12Z,src/core/lib/resource_quota/periodic_update.cc,"@@ -0,0 +1,60 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include ""src/core/lib/resource_quota/periodic_update.h""++namespace grpc_core {++bool PeriodicUpdate::MaybeEndPeriod() {+  auto now = ExecCtx::Get()->Now();+  Duration time_so_far = now - period_start_;+  if (time_so_far < period_) {+    // Double the number of updates remaining until the next period.+    int64_t better_guess;+    if (time_so_far.millis() == 0) {+      better_guess = expected_updates_per_period_ * 2;+    } else {+      const double scale = period_.seconds() / time_so_far.seconds();+      if (scale > 2) {+        better_guess = expected_updates_per_period_ * 2;+      } else {+        better_guess = expected_updates_per_period_ * scale;+        if (better_guess <= expected_updates_per_period_) {+          better_guess = expected_updates_per_period_ + 1;+        }+      }+    }+    const int64_t add = better_guess - expected_updates_per_period_;+    expected_updates_per_period_ = better_guess;+    const int64_t past_remaining =+        updates_remaining_.fetch_add(add, std::memory_order_release);+    // If adding more things still didn't get us back above 0 things remaining+    // then we should continue adding more things.+    if (add + past_remaining <= 0) return MaybeEndPeriod();",I'm not sure I understand when we could hit this case - add always has to be at least 1 based on line 36 and past_remaining has to be at least 1 based on the condition by which we call this function? I'm probably missing something here..,X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29853,889797473,2022-06-06T03:27:41Z,src/core/lib/resource_quota/periodic_update.h,"@@ -0,0 +1,52 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#ifndef GRPC_CORE_LIB_RESOURCE_QUOTA_PERIODIC_UPDATE_H+#define GRPC_CORE_LIB_RESOURCE_QUOTA_PERIODIC_UPDATE_H++#include <grpc/support/port_platform.h>++#include <inttypes.h>++#include <atomic>++#include ""src/core/lib/gprpp/time.h""+#include ""src/core/lib/iomgr/exec_ctx.h""++namespace grpc_core {++class PeriodicUpdate {+ public:+  explicit PeriodicUpdate(Duration period) : period_(period) {}++  // Tick the update, return true if we think the period expired.+  GRPC_MUST_USE_RESULT bool Tick() {+    if (updates_remaining_.fetch_sub(1, std::memory_order_acquire) == 1) {+      return MaybeEndPeriod();+    }+    return false;+  }++ private:+  GRPC_MUST_USE_RESULT bool MaybeEndPeriod();++  const Duration period_;+  Timestamp period_start_ = ExecCtx::Get()->Now();+  int64_t expected_updates_per_period_ = 1;",Added a description of the thread safety rules for this class because yeah they were really lacking. LMK if it makes sense.,
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29853,889797980,2022-06-06T03:29:43Z,src/core/lib/resource_quota/periodic_update.cc,"@@ -0,0 +1,60 @@+// Copyright 2022 gRPC authors.+//+// Licensed under the Apache License, Version 2.0 (the ""License"");+// you may not use this file except in compliance with the License.+// You may obtain a copy of the License at+//+//     http://www.apache.org/licenses/LICENSE-2.0+//+// Unless required by applicable law or agreed to in writing, software+// distributed under the License is distributed on an ""AS IS"" BASIS,+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+// See the License for the specific language governing permissions and+// limitations under the License.++#include <grpc/support/port_platform.h>++#include ""src/core/lib/resource_quota/periodic_update.h""++namespace grpc_core {++bool PeriodicUpdate::MaybeEndPeriod() {+  auto now = ExecCtx::Get()->Now();+  Duration time_so_far = now - period_start_;+  if (time_so_far < period_) {+    // Double the number of updates remaining until the next period.+    int64_t better_guess;+    if (time_so_far.millis() == 0) {+      better_guess = expected_updates_per_period_ * 2;+    } else {+      const double scale = period_.seconds() / time_so_far.seconds();+      if (scale > 2) {+        better_guess = expected_updates_per_period_ * 2;+      } else {+        better_guess = expected_updates_per_period_ * scale;+        if (better_guess <= expected_updates_per_period_) {+          better_guess = expected_updates_per_period_ + 1;+        }+      }+    }+    const int64_t add = better_guess - expected_updates_per_period_;+    expected_updates_per_period_ = better_guess;+    const int64_t past_remaining =+        updates_remaining_.fetch_add(add, std::memory_order_release);+    // If adding more things still didn't get us back above 0 things remaining+    // then we should continue adding more things.+    if (add + past_remaining <= 0) return MaybeEndPeriod();","Added some more commentary - there can be other threads still decrementing concurrently, and for the threading semantics to work we *must* get above zero, so we need to keep on incrementing.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29925,890650136,2022-06-06T23:46:33Z,tools/run_tests/xds_k8s_test_driver/framework/helpers/skips.py,"@@ -23,34 +25,66 @@ from framework import xds_k8s_flags  -def _get_lang(image_name: str) -> str:-    return re.search(r'/(\w+)-(client|server):', image_name).group(1)-+class Lang(enum.Flag):+    UNKNOWN = enum.auto()+    CPP = enum.auto()+    GO = enum.auto()+    JAVA = enum.auto()+    PYTHON = enum.auto()+    NODE = enum.auto()+    # Languages supported by the framework, that follow the same gRPC version+    # release scheme.+    VERSION_TYPE_1 = CPP | GO | JAVA | PYTHON","We could probably give them a more informative name? For example, something like the comment, GKE_INSTRUMENTED_LANGS, and we can leave Node out until the Node support is finished.Also, it's probably a good idea to hide the version scheme from folks who implement tests. Ideally, this module handles those differences, and give a clean `TestConfig` for downstream to choose and pick.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29925,890652909,2022-06-06T23:53:38Z,tools/run_tests/xds_k8s_test_driver/tests/authz_test.py,"@@ -47,12 +48,14 @@ class AuthzTest(xds_k8s_testcase.SecurityXdsKubernetesTestCase):     }      @staticmethod-    def isSupported(config: skips.TestConfig) -> bool:-        if config.client_lang in ['cpp', 'python']:-            return config.version_ge('v1.44.x')-        elif config.client_lang in ['java', 'go']:-            return config.version_ge('v1.42.x')-        return False+    def is_supported(config: skips.TestConfig) -> bool:+        # Per ""Authorization (RBAC)"" in+        # https://github.com/grpc/grpc/blob/master/doc/grpc_xds_features.md+        if config.client_lang in _Lang.CPP | _Lang.PYTHON:+            return not config.version_lt('v1.44.x')+        elif config.client_lang in _Lang.GO | _Lang.JAVA:+            return not config.version_lt('v1.42.x')+        return True","I would recommend to use allowlist pattern. When a new test come online, people will need to enable it one language at a time. When a new language (Node) is added, people can slowly enable tests that are passing consistently for the new language in each release.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/29925,890662998,2022-06-07T00:20:45Z,tools/run_tests/xds_k8s_test_driver/tests/authz_test.py,"@@ -47,12 +48,14 @@ class AuthzTest(xds_k8s_testcase.SecurityXdsKubernetesTestCase):     }      @staticmethod-    def isSupported(config: skips.TestConfig) -> bool:-        if config.client_lang in ['cpp', 'python']:-            return config.version_ge('v1.44.x')-        elif config.client_lang in ['java', 'go']:-            return config.version_ge('v1.42.x')-        return False+    def is_supported(config: skips.TestConfig) -> bool:+        # Per ""Authorization (RBAC)"" in+        # https://github.com/grpc/grpc/blob/master/doc/grpc_xds_features.md+        if config.client_lang in _Lang.CPP | _Lang.PYTHON:+            return not config.version_lt('v1.44.x')+        elif config.client_lang in _Lang.GO | _Lang.JAVA:+            return not config.version_lt('v1.42.x')+        return True","I'm a bit worried we're not going to run something on accident (again). Maybe we could add do something specific for node? ```pyif config.client_lang is _Lang.NODE:    return False ```Or remove node from the list of languages for now, and do```pyif config.client_lang is _Lang.UNKNOWN:    return False ```cc @murgatroid99 ",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/29921,890706244,2022-06-07T02:16:48Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -424,11 +365,86 @@ def assertAllBackendsReceivedRpcs(self, lb_stats):                 msg=f'Backend {backend} did not receive a single RPC')  -class TdPropagationRetryableError(Exception):-    pass+class XdsKubernetesIsolatedTestCase(XdsKubernetesBaseTestCase,+                                    metaclass=abc.ABCMeta):+    """"""Isolated test case.++    Base class for tests cases where infra resources are created before+    each test, and destroyed after.","Most of it  is delegated to the classes to implement, see `initTrafficDirectorManager`, `initKubernetesServerRunner`, `initKubernetesClientRunner`. I think for the rest the code is self-documenting enough.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/29921,890707616,2022-06-07T02:20:31Z,tools/run_tests/xds_k8s_test_driver/framework/xds_k8s_testcase.py,"@@ -424,11 +365,86 @@ def assertAllBackendsReceivedRpcs(self, lb_stats):                 msg=f'Backend {backend} did not receive a single RPC')  -class TdPropagationRetryableError(Exception):-    pass+class XdsKubernetesIsolatedTestCase(XdsKubernetesBaseTestCase,+                                    metaclass=abc.ABCMeta):+    """"""Isolated test case.++    Base class for tests cases where infra resources are created before+    each test, and destroyed after.","On another hand, the test itself controls what's created. This class mostly does the cleanup, and provides some nonce for the resources.",
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29925,891532096,2022-06-07T17:35:28Z,tools/run_tests/xds_k8s_test_driver/tests/authz_test.py,"@@ -47,12 +48,14 @@ class AuthzTest(xds_k8s_testcase.SecurityXdsKubernetesTestCase):     }      @staticmethod-    def isSupported(config: skips.TestConfig) -> bool:-        if config.client_lang in ['cpp', 'python']:-            return config.version_ge('v1.44.x')-        elif config.client_lang in ['java', 'go']:-            return config.version_ge('v1.42.x')-        return False+    def is_supported(config: skips.TestConfig) -> bool:+        # Per ""Authorization (RBAC)"" in+        # https://github.com/grpc/grpc/blob/master/doc/grpc_xds_features.md+        if config.client_lang in _Lang.CPP | _Lang.PYTHON:+            return not config.version_lt('v1.44.x')+        elif config.client_lang in _Lang.GO | _Lang.JAVA:+            return not config.version_lt('v1.42.x')+        return True","Discussed offline. The advantage of ""exclude"" pattern is that it's more maintainable in the long term, because all tests will be run unless explicitly disabled for a certain branch, which will help us catch silently-skipped-tests issue. The advantage of ""include"" pattern is that it's easier to onboard new tests and new language. In close comparison, the former is more favorable.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29867,891586407,2022-06-07T18:36:59Z,src/core/ext/filters/client_channel/client_channel.cc,"@@ -656,7 +657,15 @@ class ClientChannel::SubchannelWrapper : public SubchannelInterface {       // Ignore update if the parent WatcherWrapper has been replaced       // since this callback was scheduled.       if (watcher_ != nullptr) {-        watcher_->OnConnectivityStateChange(state_change.state);+        // Propagate status only in state TF.+        // We specifically want to avoid propagating the status for","I don't know that it would necessarily break anything, but it would definitely not be the intended semantic that we want to expose to the LB policy API.  The intended semantic is that a non-OK status is returned only in state TRANSIENT_FAILURE.The only reason that this is an issue is the special-case hack that we added in #29428 to have the real subchannel return a non-OK status in IDLE state so that it has a way of returning the keepalive info.  We don't want to expose that hack to the subchannel interface that is used by the LB policies.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29867,891592583,2022-06-07T18:43:07Z,src/core/ext/filters/client_channel/lb_policy/outlier_detection/outlier_detection.cc,"@@ -176,27 +176,34 @@ class OutlierDetectionLb : public LoadBalancingPolicy {        void Eject() {         ejected_ = true;-        if (last_seen_state_.has_value() &&-            *last_seen_state_ != GRPC_CHANNEL_TRANSIENT_FAILURE) {-          watcher_->OnConnectivityStateChange(GRPC_CHANNEL_TRANSIENT_FAILURE);+        if (last_seen_state_.has_value()) {","Yes.  The outlier detection policy indicates an ejected subchannel by reporting TRANSIENT_FAILURE for that subchannel instead of the real connectivity state.  This means that it's possible to have ""duplicate"" TRANSIENT_FAILURE notifications if we eject a subchannel whose real state was already TRANSIENT_FAILURE or if we uneject a subchannel whose real state is TRANSIENT_FAILURE.Previously, we were avoiding sending these duplicate notifications by checking whether the real state was already TRANSIENT_FAILURE.  However, now that we're sending the status along with the connectivity state, the ""duplicate"" notifications aren't actually duplicates anymore, because even though the connectivity state isn't changing, the status is.  And we don't want to avoid updating the status, because that will lead to incorrect RPC failure status messages -- e.g., we might report ""subchannel ejected by outlier detection"" when the real problem is actually a connectivity issue, or vice versa.  To avoid that, I've changed the code here to send the notifications on ejection and unejection even if the connectivity state is not changing.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29436,891643671,2022-06-07T19:42:31Z,src/core/lib/security/security_connector/load_system_roots_linux.cc,"@@ -51,20 +51,27 @@ GPR_GLOBAL_CONFIG_DEFINE_STRING(grpc_system_ssl_roots_dir, """", namespace grpc_core { namespace { -const char* kLinuxCertFiles[] = {+#if defined(GPR_LINUX) || defined(GPR_ANDROID)+const char* kCertFiles[] = {     ""/etc/ssl/certs/ca-certificates.crt"", ""/etc/pki/tls/certs/ca-bundle.crt"",     ""/etc/ssl/ca-bundle.pem"", ""/etc/pki/tls/cacert.pem"",     ""/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem""};-const char* kLinuxCertDirectories[] = {+const char* kCertDirectories[] = {     ""/etc/ssl/certs"", ""/system/etc/security/cacerts"", ""/usr/local/share/certs"",     ""/etc/pki/tls/certs"", ""/etc/openssl/certs""};+#endif  // GPR_LINUX || GPR_ANDROID","Instead of `#endif` and then another independent `#if`, please use `#elif`.  That way, we know for sure that only one of these two definitions will be used.",X
961599,murgatroid99,https://api.github.com/repos/grpc/grpc/pulls/29960,892950400,2022-06-08T23:40:38Z,tools/run_tests/xds_k8s_test_driver/tests/url_map/timeout_test.py,"@@ -81,6 +81,8 @@ class TestTimeoutInRouteRule(_BaseXdsTimeOutTestCase):      @staticmethod     def is_supported(config: skips.TestConfig) -> bool:+        if config.client_lang == skips.Lang.NODE:+            return not config.version_lt('v1.4.x')","That comment is on a `server_lang` check, because only the Java server handles the specified metadata properly. This is a `client_lang` check, so it's a separate issue.",X
7394928,lidizheng,https://api.github.com/repos/grpc/grpc/pulls/29960,892954039,2022-06-08T23:47:38Z,tools/run_tests/xds_k8s_test_driver/tests/url_map/csds_test.py,"@@ -37,6 +39,12 @@  class TestBasicCsds(xds_url_map_testcase.XdsUrlMapTestCase): +    @staticmethod+    def is_supported(config: skips.TestConfig) -> bool:+        if config.client_lang == _Lang.NODE:+            return not config.version_lt('v1.5.x')","If we can simplify the condition, I would recommend to do it. But it's not a hard blocker in any way.",X
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/29960,894850802,2022-06-10T19:58:19Z,tools/run_tests/xds_k8s_test_driver/tests/url_map/timeout_test.py,"@@ -81,6 +81,8 @@ class TestTimeoutInRouteRule(_BaseXdsTimeOutTestCase):      @staticmethod     def is_supported(config: skips.TestConfig) -> bool:+        if config.client_lang == skips.Lang.NODE:+            return not config.version_lt('v1.4.x')","Ah, this is probably just a safety check then. I've noticed we always use java server for urlmap tests: https://github.com/grpc/grpc/blob/f8eedac1fc59823d4c478663f12b53b71c2d682a/tools/run_tests/xds_k8s_test_driver/config/url-map.cfg#L4-L8Let's do the same check for node too:```py    @staticmethod    def is_supported(config: skips.TestConfig) -> bool:        # TODO(lidiz) either add support for rpc-behavior to other languages, or we        # should always use Java server as backend.        if config.server_lang != 'java':            return False        if config.client_lang == skips.Lang.NODE:            return not config.version_lt('v1.4.x')        return True```",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29994,896130867,2022-06-13T20:59:08Z,test/core/promise/activity_test.cc,"@@ -14,8 +14,13 @@  #include ""src/core/lib/promise/activity.h"" -#include <gmock/gmock.h>-#include <gtest/gtest.h>+#include <stdlib.h>++#include <functional>+#include <tuple>++#include ""gmock/gmock.h""","I did with the presubmit thing, and it has a shiny tick.Our tools are gonna enforce that consistency, let's see how long it takes to roll this through (suspecting not long now)",X
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29816,896238999,2022-06-13T23:03:16Z,src/core/lib/event_engine/iomgr_engine/timer_manager.h,"@@ -0,0 +1,113 @@+/*+ *+ * Copyright 2017 gRPC authors.+ *+ * Licensed under the Apache License, Version 2.0 (the ""License"");+ * you may not use this file except in compliance with the License.+ * You may obtain a copy of the License at+ *+ *     http://www.apache.org/licenses/LICENSE-2.0+ *+ * Unless required by applicable law or agreed to in writing, software+ * distributed under the License is distributed on an ""AS IS"" BASIS,+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+ * See the License for the specific language governing permissions and+ * limitations under the License.+ *+ */++#ifndef GRPC_CORE_LIB_EVENT_ENGINE_IOMGR_ENGINE_TIMER_MANAGER_H+#define GRPC_CORE_LIB_EVENT_ENGINE_IOMGR_ENGINE_TIMER_MANAGER_H++#include <grpc/support/port_platform.h>++#include <stddef.h>+#include <stdint.h>++#include <algorithm>+#include <memory>+#include <utility>+#include <vector>++#include ""absl/base/thread_annotations.h""++#include <grpc/event_engine/event_engine.h>+#include <grpc/support/log.h>++#include ""src/core/lib/event_engine/iomgr_engine/timer.h""+#include ""src/core/lib/gprpp/sync.h""+#include ""src/core/lib/gprpp/thd.h""+#include ""src/core/lib/gprpp/time.h""++namespace grpc_event_engine {+namespace iomgr_engine {++/* Timer Manager tries to keep only one thread waiting for the next timeout at+   all times, and thus effectively preventing the thundering herd problem. */+class TimerManager final : public TimerListHost {",done - although I'll note at a degregation of code clarity - I think the additional indirections negatively affect the readability of this code and add complexity for little value.,
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/29816,896274746,2022-06-14T00:34:18Z,src/core/lib/event_engine/iomgr_engine/iomgr_engine.cc,"@@ -67,10 +54,29 @@ std::string HandleToString(EventEngine::TaskHandle handle) {  }  // namespace +struct IomgrEventEngine::ClosureData final : public EventEngine::Closure {","""ClosureData is a Closure"" is an odd turn of phrase. I'd recommend against inheritance here.",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/29816,896287028,2022-06-14T01:09:27Z,src/core/lib/event_engine/iomgr_engine/iomgr_engine.cc,"@@ -67,10 +54,29 @@ std::string HandleToString(EventEngine::TaskHandle handle) {  }  // namespace +struct IomgrEventEngine::ClosureData final : public EventEngine::Closure {","Not using inheritance here would lead to some relatively nasty casting that I'd like to avoid.I'd be more ok with making TimerManager take a different type (`EventEngine::Closure` was chosen for convenience, but I'm not sure that we're getting that)",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/29745,897121667,2022-06-14T17:45:10Z,test/cpp/ext/filters/census/stats_plugin_end2end_test.cc,"@@ -565,7 +630,58 @@ TEST_F(StatsPluginEnd2EndTest, TestApplicationCensusContextFlows) {   EXPECT_TRUE(status.ok()); } +TEST_F(StatsPluginEnd2EndTest, TestAllSpansAreExported) {+  {+    // Client spans are ended when the ClientContext's destructor is invoked.+    auto channel = CreateChannel(server_address_, InsecureChannelCredentials());+    ResetStub(channel);+    EchoRequest request;+    request.set_message(""foo"");+    EchoResponse response;++    grpc::ClientContext context;+    ::opencensus::trace::AlwaysSampler always_sampler;+    ::opencensus::trace::StartSpanOptions options;+    options.sampler = &always_sampler;+    auto sampling_span =+        ::opencensus::trace::Span::StartSpan(""sampling"", nullptr, options);+    grpc::CensusContext app_census_context(""root"", &sampling_span,+                                           ::opencensus::tags::TagMap{});+    context.set_census_context(+        reinterpret_cast<census_context*>(&app_census_context));+    context.AddMetadata(kExpectedTraceIdKey,+                        app_census_context.Span().context().trace_id().ToHex());+    traces_recorder_->StartRecording();+    grpc::Status status = stub_->Echo(&context, request, &response);+    EXPECT_TRUE(status.ok());+  }+  ::opencensus::trace::exporter::SpanExporterTestPeer::ExportForTesting();+  traces_recorder_->StopRecording();+  auto recorded_spans_ = traces_recorder_->GetSpansAndClearState();","This is a local variable, not a data member, so it should not have an `_` suffix.https://google.github.io/styleguide/cppguide.html#Variable_Names",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/30013,898389026,2022-06-15T20:28:26Z,src/core/ext/filters/client_channel/lb_policy/weighted_target/weighted_target.cc,"@@ -444,44 +445,32 @@ void WeightedTargetLb::UpdateStateLocked() { WeightedTargetLb::WeightedChild::DelayedRemovalTimer::DelayedRemovalTimer(     RefCountedPtr<WeightedTargetLb::WeightedChild> weighted_child)     : weighted_child_(std::move(weighted_child)) {-  GRPC_CLOSURE_INIT(&on_timer_, OnTimer, this, nullptr);-  Ref().release();-  grpc_timer_init(&timer_, ExecCtx::Get()->Now() + kChildRetentionInterval,-                  &on_timer_);+  timer_handle_ = GetDefaultEventEngine()->RunAt(+      absl::Now() + kChildRetentionInterval, [self = Ref()] {","Done. As discussed, the fact that [this behavior](https://godbolt.org/z/5sn1dzhcE) sometimes surprises folks is why I prefer to not use mutable, but I'm fine to use here it and move on given that it's not a problem in this case.",
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/30023,898441674,2022-06-15T21:20:32Z,include/grpc/event_engine/event_engine.h,"@@ -389,7 +393,7 @@ class EventEngine {   /// The \a closure will execute when time \a when arrives unless it has been   /// cancelled via the \a Cancel method. If cancelled, the closure will not be   /// run, nor will it be deleted. Ownership remains with the caller.-  virtual TaskHandle RunAt(absl::Time when, Closure* closure) = 0;+  virtual TaskHandle RunAt(Duration when, Closure* closure) = 0;","Suggest renaming this family of methods `RunAfter(Duration wait_time, ...)`",X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/30023,898448054,2022-06-15T21:30:12Z,src/core/lib/event_engine/iomgr_engine.cc,"@@ -50,15 +50,12 @@ struct ClosureData {   EventEngine::TaskHandle handle; }; -// Timer limits due to quirks in the iomgr implementation.-// If deadline <= Now, the callback will be run inline, which can result in lock-// issues. And absl::InfiniteFuture yields UB.-absl::Time Clamp(absl::Time when) {-  absl::Time max = absl::Now() + absl::Hours(8766);-  absl::Time min = absl::Now() + absl::Milliseconds(2);-  if (when > max) return max;-  if (when < min) return min;-  return when;+grpc_core::Timestamp ToTimestamp(EventEngine::Duration when) {","Might be worth putting this in an EventEngine time_util file, along with the EventEngine::Duration -> absl::Timestamp conversion we'll need elsewhere.",
672669,sergiitk,https://api.github.com/repos/grpc/grpc/pulls/29954,898491088,2022-06-15T22:29:44Z,tools/run_tests/xds_k8s_test_driver/tests/bootstrap_generator_test.py,"@@ -0,0 +1,343 @@+# Copyright 2022 gRPC authors.+#+# Licensed under the Apache License, Version 2.0 (the ""License"");+# you may not use this file except in compliance with the License.+# You may obtain a copy of the License at+#+#     http://www.apache.org/licenses/LICENSE-2.0+#+# Unless required by applicable law or agreed to in writing, software+# distributed under the License is distributed on an ""AS IS"" BASIS,+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.+# See the License for the specific language governing permissions and+# limitations under the License.+import datetime+import logging++from absl import flags+from absl.testing import absltest+from absl.testing import parameterized++from framework import xds_k8s_testcase+from framework.helpers import rand as helpers_rand+from framework.helpers import retryers+from framework.helpers import skips",```suggestion```This is unused,
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/30007,899215582,2022-06-16T15:31:24Z,src/core/lib/compression/compression_internal.cc,"@@ -255,11 +255,9 @@ int DefaultGzipCompressionLevelFromChannelArgs(const grpc_channel_args* args) {   for (size_t i = 0; i < args->num_args; i++) {     if (strcmp(args->args[i].key, GRPC_GZIP_COMPRESSION_LEVEL) ==         0) {-      if (args->args[i].type == GRPC_ARG_INTEGER) {-        return args->args[i].value.integer;-      } else if (args->args[i].type == GRPC_ARG_STRING) {-        return Z_DEFAULT_COMPRESSION;-      }+      return grpc_channel_arg_get_integer(","Sorry, I meant to say that you should use `grpc_channel_args_find_integer()`, not `grpc_channel_arg_get_integer()`.  If you use the former, there is no need for the enclosing loop or any of the other checks; this entire function can be just the call to `grpc_channel_args_find_integer()`.https://github.com/grpc/grpc/blob/2d0d1775a9208fa8f137d2e3546a3f3701d895ea/src/core/lib/channel/channel_args.h#L320",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/30007,899229321,2022-06-16T15:43:16Z,src/core/lib/compression/message_compress.h,"@@ -24,13 +24,18 @@ #include <grpc/impl/codegen/compression_types.h> #include <grpc/slice.h> +struct gzip_compression_options {+   int gzip_compression_level;+   int compression_lower_bound;","I just noticed that this parameter is not actually gzip-specific.  It looks like you meant this to apply to all compression algorithms.Given that, I don't think it should be plumbed into this code to begin with.  Instead, I suggest doing this in the message compression filter.  You can change the filter's `ChannelData` to read the channel arg and save the value in a data member, similar to what's being done for the compression algorithm:https://github.com/grpc/grpc/blob/2d0d1775a9208fa8f137d2e3546a3f3701d895ea/src/core/ext/filters/http/message_compress/message_compress_filter.cc#L61Then, in the filter's `CallData`, you can add a check for this in the `SkipMessageCompression()` method:https://github.com/grpc/grpc/blob/2d0d1775a9208fa8f137d2e3546a3f3701d895ea/src/core/ext/filters/http/message_compress/message_compress_filter.cc#L164",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/30007,899260770,2022-06-16T16:05:32Z,src/core/lib/compression/message_compress.h,"@@ -24,13 +24,18 @@ #include <grpc/impl/codegen/compression_types.h> #include <grpc/slice.h> +struct gzip_compression_options {","This doesn't really address my prior comment.  The problem here is that the `grpc_msg_compress()` API is intended to be algorithm-agnostic, so it does not make sense to pass into it anything that is specific to any one algorithm.  Moving the gzip-specific option into a gzip-specific struct doesn't really solve that problem, because in both cases, we're passing something specific to gzip into a function that is not specific to gzip.I think we need an abstraction here to allow us to pass in the options in an opaque way, so that the parameters are not gzip-specific.  I suggest something like the following:- In message_compress.h:  ```  namespace grpc_core {  class CompressionOptions {   public:    virtual ~CompressionOptions() = default;  };  std::unique_ptr<CompressionOptions> MakeCompressionOptions(const grpc_channel_args* args);  }  // namespace grpc_core    int grpc_msg_compress(grpc_compression_algorithm algorithm,                        grpc_slice_buffer* input, grpc_slice_buffer* output,                        const grpc_core::CompressionOptions* options);  ```- In message_compress.cc:  ```  namespace grpc_core {  class CompressionOptionsImpl : public CompressionOptions {   public:    explicit CompressionOptionsImpl(const grpc_channel_args* args)        : gzip_compression_level_(              DefaultGzipCompressionLevelFromChannelArgs(args)) {}    int gzip_compression_level() const { return gzip_compression_level_; }       private:    int gzip_compression_level_;  };    std::unique_ptr<CompressionOptions> MakeCompressionOptions(const grpc_channel_args* args) {    return absl::make_unique<CompressionOptionsImpl>(args);  }  }  // namespace grpc_core    int grpc_msg_compress(grpc_compression_algorithm algorithm,                        grpc_slice_buffer* input, grpc_slice_buffer* output,                        const grpc_core::CompressionOptions* options) {    if (!compress_inner(algorithm, input, output,                        static_cast<grpc_core::CompressionOptionsImpl*>(options))) {      copy(input, output);      return 0;    }    return 1;  }    ```- In the message compress filter, in `ChannelData`, call `grpc_core::MakeCompressionOptions()` and save the result in a data member.  Then, in `CallData`, use that data member from `ChannelData` to pass the option to `grpc_msg_compress()`.",X
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/30007,899370379,2022-06-16T17:47:30Z,src/core/lib/compression/message_compress.h,"@@ -24,13 +24,18 @@ #include <grpc/impl/codegen/compression_types.h> #include <grpc/slice.h> +struct gzip_compression_options {","Yes, a struct is simpler, but getting the opaque API that we want here with a struct would make the casting less type-safe.  I think using a class interface is preferable here.",
18664614,markdroth,https://api.github.com/repos/grpc/grpc/pulls/30034,899479940,2022-06-16T20:05:18Z,doc/interop-test-descriptions.md,"@@ -1143,10 +1147,15 @@ Procedures:       }     }     ```-The call carries a reference to receive the load report, e.g. using CallOptions.-The reference will be passed to the custom LB policy as part of the `OrcaOobReportListener` API.-* Client asserts that, after 1.5 second, the latest OOB load report received is equal to the test load report.-* Client sends another unary call to the server. The call request sets `orca_oob_report` to a +The test then blocks until the first response is received. This means the server lock is acquired.+It is important that if the stream is completed or terminated with error at this time, +the test aborts and fails.+* Client samples the latest OOB load report by doing empty unary calls. The call ","Instead of having to make additional unary calls to do this, how about simply having the LB policy invoke an injected callback that the client provides at the time that it registers the LB policy?  This is essentially what we do for C++ in our end-to-end tests:https://github.com/grpc/grpc/blob/d9f64437b09d97bd9aebc6bfa46d5a61e7477c41/test/cpp/end2end/client_lb_end2end_test.cc#L2579This way, there's no polling needed; instead, we get a callback when each OOB backend metric report is received.",X
4111145,YifeiZhuang,https://api.github.com/repos/grpc/grpc/pulls/30034,899622805,2022-06-16T22:45:18Z,doc/interop-test-descriptions.md,"@@ -1143,10 +1147,15 @@ Procedures:       }     }     ```-The call carries a reference to receive the load report, e.g. using CallOptions.-The reference will be passed to the custom LB policy as part of the `OrcaOobReportListener` API.-* Client asserts that, after 1.5 second, the latest OOB load report received is equal to the test load report.-* Client sends another unary call to the server. The call request sets `orca_oob_report` to a +The test then blocks until the first response is received. This means the server lock is acquired.+It is important that if the stream is completed or terminated with error at this time, +the test aborts and fails.+* Client samples the latest OOB load report by doing empty unary calls. The call ","Yea no polling has its advantage. It avoids sleeps between each polling, and processes immediately after callback is invoked. Part of the reason java does polling is because it saves some complexity of extra injection, both implementation and description here. In java can re-use what we already have (using callOptions)for per-query test so it would be easier to reason and consistent.In my opinion either way would work, what about I make the doc less specific and allow each language to implement their verification.  The timeout should be the same tho.",
4181124,yashykt,https://api.github.com/repos/grpc/grpc/pulls/29987,900296826,2022-06-17T16:14:52Z,test/core/transport/chttp2/flow_control_end2end_test.cc,"@@ -1,379 +0,0 @@-/*",are we sure this is no longer required?,X
295906,drfloob,https://api.github.com/repos/grpc/grpc/pulls/30022,900297135,2022-06-17T16:15:18Z,test/core/end2end/fuzzers/api_fuzzer.cc,"@@ -41,10 +44,28 @@ #include ""src/libfuzzer/libfuzzer_macro.h"" #include ""test/core/end2end/data/ssl_test_data.h"" #include ""test/core/end2end/fuzzers/api_fuzzer.pb.h""+#include ""test/core/event_engine/fuzzing_event_engine/fuzzing_event_engine.h"" #include ""test/core/util/passthru_endpoint.h"" -static constexpr uint64_t kMaxAdvanceTimeMicros =-    31536000000000;  // 1 year (24 * 365 * 3600 * 1000000)+static grpc_event_engine::experimental::FuzzingEventEngine* g_event_engine =+    new grpc_event_engine::experimental::FuzzingEventEngine(+        grpc_event_engine::experimental::FuzzingEventEngine::Options());+static int g_unused_initialize_event_engine = []() {+  grpc_event_engine::experimental::SetDefaultEventEngineFactory(+      new std::function<std::unique_ptr<+          grpc_event_engine::experimental::EventEngine>()>([]() {+        // HACK HACK HACK+        // We know that this event engine will never be deleted by the+        // caller, instead release() will be called and the value stashed in","If the engine can be put into a bad state, that's going to hurt subsequent test runs. Why not create a new engine each time?",
10120821,ctiller,https://api.github.com/repos/grpc/grpc/pulls/30022,900355881,2022-06-17T17:07:22Z,test/core/end2end/fuzzers/api_fuzzer.cc,"@@ -41,10 +44,28 @@ #include ""src/libfuzzer/libfuzzer_macro.h"" #include ""test/core/end2end/data/ssl_test_data.h"" #include ""test/core/end2end/fuzzers/api_fuzzer.pb.h""+#include ""test/core/event_engine/fuzzing_event_engine/fuzzing_event_engine.h"" #include ""test/core/util/passthru_endpoint.h"" -static constexpr uint64_t kMaxAdvanceTimeMicros =-    31536000000000;  // 1 year (24 * 365 * 3600 * 1000000)+static grpc_event_engine::experimental::FuzzingEventEngine* g_event_engine =+    new grpc_event_engine::experimental::FuzzingEventEngine(+        grpc_event_engine::experimental::FuzzingEventEngine::Options());+static int g_unused_initialize_event_engine = []() {+  grpc_event_engine::experimental::SetDefaultEventEngineFactory(+      new std::function<std::unique_ptr<+          grpc_event_engine::experimental::EventEngine>()>([]() {+        // HACK HACK HACK+        // We know that this event engine will never be deleted by the+        // caller, instead release() will be called and the value stashed in","I don't think the engine can be put into a bad state?I'd like to create a new engine each fuzzer run, but it'll require a change to how we track the default event engine (that's not resettable), and I think we can deal with that as a separate work item.",X
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/30034,900400159,2022-06-17T18:03:44Z,doc/interop-test-descriptions.md,"@@ -1391,22 +1404,38 @@ fully communicate metadata. ### Backend metrics report [Backend Metrics Report]: #backend-metrics-report -Server reports backend metrics data in both per-query and out-of-band cases, with metrics data-indicated from the unary call request.+#### Report per-query metrics++Server reports backend metrics data in per-query case, echoing metrics data from the unary call request.  Using ORCA APIs we install the per-query metrics reporting server interceptor, so that it can attach  metrics per RPC.-Also using ORCA APIs we register the `OpenRCAService` implementation to the server, so that it can-report metrics periodically. The minimum report interval in the ORCA service is set to 1 sec. -During test, the server will receive unary requests from the client that each may contain up to two-test load report, indicating whether it needs to update metrics for the current call or at the OOB server.+During `orca_per_query` test, the server will receive unary requests from the client that may contain a ","Don't specify which test. The tests get to choose which features they use. Just say ""If `SimpleRequest.orca_per_rpc_report` is set, the server will add the metric data from `orca_per_rpc_report` to the RPC using the language's CallMetricRecorder."" You can still say something about ""Both utilization and request cost metrics should be copied.""",X
2811396,ejona86,https://api.github.com/repos/grpc/grpc/pulls/30034,900416010,2022-06-17T18:28:20Z,doc/interop-test-descriptions.md,"@@ -1143,10 +1147,14 @@ Procedures:       }     }     ```-The call carries a reference to receive the load report, e.g. using CallOptions.-The reference will be passed to the custom LB policy as part of the `OrcaOobReportListener` API.-* Client asserts that, after 1.5 second, the latest OOB load report received is equal to the test load report.-* Client sends another unary call to the server. The call request sets `orca_oob_report` to a +The test then blocks until the first response is received. This means the server lock is acquired.","Take a look at ping_pong for how simply we try to express the test process. Terseness is helpful because it makes it much easier to verify a test implementation is correct.This can just be a step (2) ""Client waits for a reply."" Stuff like ""this means"" should be removed. The ""it is important"" was important to the Java implementation, and all implementations should consider stuff like that, but isn't a good fit for discussing here. If anything, just say ""Client asserts it receives a reply.""Yes, we were careful on the implementation side of things, but that was really about me hating bad error codes. How that is done in each language and what turns out to be important in each language will vary.",X